---
layout: post
title:  "kvm stats"
author: fuqiang
date:   2024-11-29 21:17:00 +0800
categories: [live_migration,kvm stats]
tags: [kvm-stats]
---

## 背景

我们如果对一个只启动bios的虚拟机做热迁移，发现其实际迁移的数据量不大，
如下:

qemu 参数:
```sh
# src
qemu-system-x86_64 -m 16G --nographic --enable-kvm --serial tcp:localhost:6666,server,nowait --monitor stdio
# dst: 
qemu-system-x86_64 --incoming tcp:xxxx:9000 -m 16G --nographic --enable-kvm
```

执行热迁移命令:
```
(qemu) migrate -d tcp:xxxx:9000
(qemu) info migrate
globals:
store-global-state: on
only-migratable: off
send-configuration: on
send-section-footer: on
decompress-error-check: on
clear-bitmap-shift: 18
Migration status: completed
total time: 3064 ms
downtime: 10 ms
setup: 6 ms
transferred ram: 38349 kbytes
throughput: 102.73 mbps
remaining ram: 0 kbytes
total ram: 16794440 kbytes
duplicate: 4198330 pages
skipped: 0 pages
normal: 283 pages
normal bytes: 1132 kbytes
dirty sync count: 3
page size: 4 kbytes
multifd bytes: 0 kbytes
pages-per-second: 1387941
precopy ram: 38022 kbytes
downtime ram: 12 kbytes
```
可以发现
* `transferrd ram` : 37M
* `normal` : 283 pages
* `duplicate` : 4198330 pages


其中`normal` 表示正常发送的页, `duplicate` 表示一些特殊处理的页，可能
多个page只发送一份，或者只发送特征（如zero page，就无需发送实际数据).

> NOTE
>
> TODO: XXX
> 我们在XXXX 中会介绍，在qemu read Page 时，会不会触发 mmu notify to 
> kvm mmu page

而VM在启动初期，guest大部分内存都不会访问，所以大部分内存在qemu看起来
都是zero page。

我们来看下这部分内存qemu是如何处理的。

## details

### QEMU migration save page

热迁移save page 流程大致如下:
```sh
qemu_savevm_state_iterate
=> foreach_savevm_state(se)
   => se->ops->save_live_iterate()
      => ram_find_and_save_block
         => get_queued_page()   # for post copy
         # 从ramlist.blocks的bitmap中获取到dirty RAMBlocks
         => found = find_dirty_block()  
         => if found
            # 将RAMBlocks->dirty_bitmap 
            => ram_save_host_page
ram_save_host_page
  => foreach dirtybitmap bit 
     => migration_bitmap_clear_dirty  ## deley clean log
        => migration_clear_memory_region_dirty_bitmap
           => memory_region_clear_dirty_bitmap
              => foeach memorylistener
                 ## nodify kvm set WP or clear D
                 => listener->log_clear()
        => test_and_clear_bit(page, rb->bmap); # rb = RAMBlock
        => rs->migration_dirty_pages--
     => ram_save_target_page ## save page
        => control_save_page() ## control pages
        ## compress ....
        => if (save_compress_page()) return 1;
        ## ------(1)-----
        => save_zero_page()
           => len += save_zero_page_to_file()
              => if (is_zero_range(p, TARGET_PAGE_SIZE)) 
                 ## only transport FREW Bytes
                 => len += HEADER
              => return 0;
           ## !!!!
           => if (len)
              => ram_counters.duplicate++;
              => ram_counters.transferred += len;
              => return 1
           => else
              ## means not zero page
              => return -1;
        => XBZERLE handler... return
        => multifd handler ... return
        => ram_save_page()
           => save_normal_page()
              => save_page_header()
              => if async  ## XBZLER related
                 => qemu_put_buffer_async
              => else
                 ## ------(2)------
                 ## !!!!!! transport HOLE page !!!!
                 => qemu_put_buffer(TARGET_PAGE_SIZE)
           => ram_counters.transferred += header_bytes
           => ram_counters.transferred += TARGET_PAGE_SIZE;
           ## !!!!
           => ram_counters.normal++;
```

判断zero page的的方法很简单，就是判断该页中的每一个byte是不是0，
当然有很多加速的指令，我们下面会介绍。

如果是zero page，则会发送很少量的数据（一些header数据），并且
自增`ram_counters.duplicate`, 如果不是zero page(还有一些其他的，暂时
不介绍), 则会发送`header + 整个的page`.

那么，我们在热迁移之前想看下这些zero page有多少, 如果zero page很多，
并且脏页速率很低，我们可以认为该机器比较适合热迁移，在各个节点资源调度时，
可以优先处理这部分机器。

我们来看下相关细节


### How to determine if a page is a zero page

上面提到过，判断page是否是zero page的函数为`is_zero_range`, 大致
流程为:
```sh
is_zero_range()
=> buffer_is_zero()
   => __builtin_prefetch()
   => select_accel_fn()
      => if len >= length_to_accel
         => return buffer_accel()
      ## else
      => return buffer_zero_int
```
QEMU 会利用SIMD 对该操作做一些优化. 看下相关初始化代码:

```sh
init_cpuid_cache
=> cpuid(1,...)
=> init `cache` var base on cpuid
   => sse2 --> cache |= CACHE_SSE2
   => avx  --> cache |= CACHE_SSE4
   => avx2 --> cache |= CACHE_AVX2
   => avx512f --> cache |= CACHE_AVX512F
=> cpuid_cache = cache
=> init_accel
   => if cache & CACHE_SSE2  buffer_accel = buffer_zero_sse2; length_to_accel = 64;
   => if cache & CACHE_SSE4  buffer_accel = buffer_zero_sse4; length_to_accel = 64;
   => if cache & CACHE_AVX2  buffer_accel = buffer_zero_avx2; length_to_accel = 128;
   => if cache & CACHE_AVX512F  buffer_accel = buffer_zero_avx512; length_to_accel = 256;
```

做简单性能测试:

程序代码:

<details markdown=1 open>
<summary>测试代码</summary>

```cpp
#include <stdio.h>
#include <stdlib.h>

#include <immintrin.h>
#include <stdio.h>
#include <stdlib.h>
#include <sys/mman.h>
#include <fcntl.h>
#include <unistd.h>
#include <time.h>
#include <stdint.h>
#include <stdbool.h>

unsigned long length;
#define DATA     0
char * my_mmap(void) {
    // 定义要分配的内存大小
    // 使用 mmap 分配内存
    length = 1024*1024*1024;
    length = length * 1;
    void* addr = mmap(NULL, length, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
    if (addr == MAP_FAILED) {
        perror("mmap failed");
        return NULL;
    }

    // 初始化随机数生成器
    srand(time(NULL));

    // 将内存初始化为随机数据
    unsigned char* data = (unsigned char*)addr;
    for (size_t i = 0; i < length; ++i) {
            //data[i] = rand() % 256; // 生成 0 到 255 的随机字节
        data[i] = DATA;
    }

    // 打印前 16 个字节的内容作为示例
    printf("Random data:\n");
    for (size_t i = 0; i < 16; ++i) {
        printf("%02x ", data[i]);
    }
    printf("\n");

    return addr;
}


static bool buffer_zero_avx2(const void *buf, size_t len)
{
    /* Begin with an unaligned head of 32 bytes.  */
    __m256i t = _mm256_loadu_si256(buf);
    __m256i *p = (__m256i *)(((uintptr_t)buf + 5 * 32) & -32);
    __m256i *e = (__m256i *)(((uintptr_t)buf + len) & -32);

    /* Loop over 32-byte aligned blocks of 128.  */
    while (p <= e) {
        __builtin_prefetch(p);
        if (!_mm256_testz_si256(t, t)) {
            return false;
        }
        t = p[-4] | p[-3] | p[-2] | p[-1];
        p += 4;
    } ;

    /* Finish the last block of 128 unaligned.  */
    t |= _mm256_loadu_si256(buf + len - 4 * 32);
    t |= _mm256_loadu_si256(buf + len - 3 * 32);
    t |= _mm256_loadu_si256(buf + len - 2 * 32);
    t |= _mm256_loadu_si256(buf + len - 1 * 32);

    return _mm256_testz_si256(t, t);
}

char *print_timestamp() {
    // 获取当前时间
    time_t raw_time;
    struct tm *time_info;
    static char buffer[80];

    // 获取当前时间
    time(&raw_time);

    // 将时间转换为本地时间
    time_info = localtime(&raw_time);

    // 格式化时间为字符串
    strftime(buffer, sizeof(buffer), "%Y-%m-%d %H:%M:%S", time_info);

    // 打印时间戳
    return buffer;
}
#define MY_DEB(...) do {printf("%s: ", print_timestamp()); printf(__VA_ARGS__);}while(0)

static inline uint64_t ldq_he_p(const void *ptr)
{
    uint64_t r;
    __builtin_memcpy(&r, ptr, sizeof(r));
    return r;
}
static bool
buffer_zero_int(const void *buf, size_t len)
{
    if (len < 8) {
        /* For a very small buffer, simply accumulate all the bytes.  */
        const unsigned char *p = buf;
        const unsigned char *e = buf + len;
        unsigned char t = 0;

        do {
            t |= *p++;
        } while (p < e);

        return t == 0;
    } else {
        /* Otherwise, use the unaligned memory access functions to
           handle the beginning and end of the buffer, with a couple
           of loops handling the middle aligned section.  */
        uint64_t t = ldq_he_p(buf);
        const uint64_t *p = (uint64_t *)(((uintptr_t)buf + 8) & -8);
        const uint64_t *e = (uint64_t *)(((uintptr_t)buf + len) & -8);

        for (; p + 8 <= e; p += 8) {
            __builtin_prefetch(p + 8);
            if (t) {
                return false;
            }
            t = p[0] | p[1] | p[2] | p[3] | p[4] | p[5] | p[6] | p[7];
        }
        while (p < e) {
            t |= *p++;
        }
        t |= ldq_he_p(buf + len - 8);

        return t == 0;
    }
}

int main()
{
        char *buffer;
        size_t len = 0;
        int i = 0;
        int j = 0;

        MY_DEB("mmap begin \n");
        buffer = my_mmap();
        MY_DEB("mmap  end \n");

        MY_DEB("random end\n");

        for (i = 0; i < 30; i++) {
                if (i % 10 == 0) {
                        MY_DEB("i = %d\n", i);
                }
                for (j = 0; j < length; j=j+4096)  {
                        buffer_zero_avx2(buffer + j, 4096);
                }
        }
        MY_DEB("buffer zero avx end\n");

        for (i = 0; i < 30; i++) {
                if (i % 10 == 0) {
                        MY_DEB("i = %d\n", i);
                }
                for (j = 0; j < length; j=j+4096)  {
                        buffer_zero_int(buffer + j, 4096);
                }
        }
        MY_DEB("buffer zero int\n");

        while(1) sleep(10);
        return 0;
}
```

</details>

该部分代码主要copy qemu的 `buffer_zero_xxx`, 并在`Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz`
cpu上进行测试. DATA设置为0，性能最差. 

我们主要以其为基准得到测试结果

```
Random data:
00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
2024-12-02 15:34:01: mmap  end
2024-12-02 15:34:01: random end
2024-12-02 15:34:01: i = 0
...
2024-12-02 15:34:09: i = 90
2024-12-02 15:34:10: buffer zero avx end
2024-12-02 15:34:10: i = 0
...
2024-12-02 15:34:20: i = 90
2024-12-02 15:34:22: buffer zero int
```
令人意外的是，两者性能差距并不是很大，大概都是1s处理10G数据。不过无论是哪种方法，
性能都不符合当前需求。

能不能通过一种粗略的方式，来获取KVM 建立 ept 映射 内存的数量, 来粗略估计zero page
数量。

内核中提供了kvm stats 相关api，可以完成这个事情。我们先来看下kvm stats 相关历史.

## kvm stat

### ORG KVM PATCH
KVM stat 其实在 kvm的第一个patch中就引入了，当时的kvm stats 比较简单，使用全局变量
来进行全局的 kvm 事件统计, 并通过`debugfs`来和用户态交互。

* 全局变量

  ```cpp
  struct kvm_stat {
      u32 pf_fixed;
      u32 pf_guest;
      u32 tlb_flush;
      u32 invlpg;

      u32 exits;
      u32 io_exits;
      u32 mmio_exits;
      u32 signal_exits;
      u32 irq_exits;
  };

  struct kvm_stat kvm_stat;
  ```
* debugfs 相关
  ```cpp
  static struct kvm_stats_debugfs_item {
      const char *name;
      u32 *data;
      struct dentry *dentry;
  } debugfs_entries[] = {
      { "pf_fixed", &kvm_stat.pf_fixed },
      { "pf_guest", &kvm_stat.pf_guest },
      { "tlb_flush", &kvm_stat.tlb_flush },
      { "invlpg", &kvm_stat.invlpg },
      { "exits", &kvm_stat.exits },
      { "io_exits", &kvm_stat.io_exits },
      { "mmio_exits", &kvm_stat.mmio_exits },
      { "signal_exits", &kvm_stat.signal_exits },
      { "irq_exits", &kvm_stat.irq_exits },
      { 0, 0 }
  };
  static __init void kvm_init_debug(void)
  {
      struct kvm_stats_debugfs_item *p;

      debugfs_dir = debugfs_create_dir("kvm", 0);
      for (p = debugfs_entries; p->name; ++p)
          p->dentry = debugfs_create_u32(p->name, 0444, debugfs_dir,
                             p->data);
  }
  ```
  以`pf_fixed`为例，其会在fixed page fault exception，相关处理流程中自增
  ```sh
  FNAME(page_fault)
  => if write_fault
     # main use for dirty tracking 
     => fixed = FNAME(fix_write_pf)
  => else 
     # for emulate CR4.WP 
     => fixed = fix_read_pf()
  => if fixed 
     => ++kvm_stat.pf_fixed;
  ```

###

## 参考链接
* [v2: Improve KVM per VM monitoring](https://lore.kernel.org/kvm/1463570785-6766-1-git-send-email-frankja@linux.vnet.ibm.com/#t)
* [kvm: x86: export TSC information to user-space](https://lore.kernel.org/all/ed02f9c9-ae4a-8694-316d-88fc82677b4d@redhat.com/T/#mb548aae542f17352f728f10ac3339d9324060177)
* [KVM stat basedfd mail](https://patchwork.kernel.org/project/linux-kselftest/cover/20210611124624.1404010-1-jingzhangos@google.com/)
* [QEMU kvm stat based-fd mail](https://patchwork.kernel.org/project/qemu-devel/cover/20220530150714.756954-1-pbonzini@redhat.com/)
