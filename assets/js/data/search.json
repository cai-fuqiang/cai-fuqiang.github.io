[ { "title": "sequence counters and sequential locks", "url": "/posts/seqcounter-and-seqlock/", "categories": "os, synchronization", "tags": "os, synchronization, seqlock", "date": "2026-01-07 16:35:00 +0800", "content": "introduce Definination of sequence: In mathematics, a sequence is an infinite list $x_1, x_2, x_3$, … (Sometimes finite lists are also called sequence) 2 大概的意思是序列是一个无限列表。而counter的含义是一个计数器。计数器的特点是 计数前后的差值为1。那么 sequence counter 的特点是, $0,1,2,3 …$ 这样的一个 列表。 sequence counters/locks 是一种 reader-writer consistency mechanism, 特点是 lockless readers(read-only retry loops), 不会有写饥饿。 1 原文是 Sequence counters are a reader-writer consistency mechanism with lockless readers (read-only retry loops), and no writer starvation. 关于reader-writer consistency，我自己的理解: 大家可以想象下，完整性是对谁而言的? writer？ NONONO, 对于写者而言，本身没有什么一致性可言，其只负责将数据写入, 不负责 观测该object完整性. 而对于读者而言肯定需要确保观测到完整的数据。 所以, 这个机制有点类似于RCU。但和RCU 达成的效果却截然相反，rcu为 lockess writers, no read starvation 该方法适合读多写少的场景, 读者愿意为读取到一致性的信息在信息发生变化时重试。 实现方法 sequence counters 实现起来很简单: 读者端临界区开始处读取到偶数的序列数，并且在临界区结束处读取到相同的序列数则 可以认为数据是一致的。否则，需要发起重试。 写者端, 在临界区开始处将序列号变更为奇数，并在临界区结束时将序列号变更为偶数。 内核中的同步机制，在一侧出现类似于自旋阻塞时，要很小心处理这部分。防止死锁。 这种情况一般发生在其互斥部分被强行中断，切换上下文执行到该阻塞部分。发生上下文 切换的上下文有: bottom half interrupt NMI preempt-schedule 而 sequence counters场景阻塞部分为reader，和reader互斥部分为writer。所以， reader 绝不能抢占/中断 writer的执行! 否则如前面所说，会造成死锁 writer reader sequence_counter++ sequence_counter is odd writing... broken by interrupt spin wait sequence_counter become even... can't return back.. write done sequence_count++ 另外, 如果受保护的数据是指针，则不能使用该机制，因为writer可能因为reader正在跟踪 指针而失败.. 为什么非指针可以，但是指针不行. 我们想象一个场景 非指针 reader writer enter read crtial section LOOP: A = s.a; B = s.b; sequence_counter++ s.a=xxx;s.b=xxx;s.c=xxx; sequence_counter++ C = s.c; sequence_counter change goto LOOP; DO NOTHING FOR sequence counter 指针: reader writer enter read crtial section LOOP: A = s-&gt;a; B = s-&gt;b; sequence_counter++ tmp_s-&gt;a=xxx, tmp_s-&gt;b=xxx, tmp_s-&gt;c=xxx; old_s=s CAN do s=tmp_s, release s.. NO! the reader is in crtial section..just faulted sequence_counter++ C = s-&gt;c; sequence_counter change goto LOOP; 可以看到只要用到了指针。就需要等待读者完整，倒反天罡了这是… 另外这种情况下, 一般采用RCU 算法。 sequence counters 有很多的变体: seqcount_t(本体) seqcount_LOCKNAME_t seqcount_latch_t seqlock_t 我们分别介绍下: sequence counter (seqcount_t) 这仅是一个原始的计数机制，无法满足多个写者同时写入. 如果有多个写者，需要调用者 自己通过外部锁串行写操作。 另外, 如果 写入序列化接口未隐式关抢占，则 必须在进入写临界区之前显示的关抢占, 另外，如果read section 可能会在中断上下文或者软中断上下文中调用。在进入write section 之前, 也必须禁用 中断/bottom half。 如果需要自动处理多写者和非抢占性要求，请使用seqlock_t(这个在内部使用自旋锁保证) 使用示例: initialization : /* dynamic */ seqcount_t foo_seqcount; seqcount_init(&amp;foo_seqcount); /* static */ static seqcount_t foo_seqcount = SEQCNT_ZERO(foo_seqcount); /* C99 struct init */ struct { .seq = SEQCNT_ZERO(foo.seq), } foo; 初始化流程主要是将 seqcount_t-&gt;sequence初始化为0 write path : /* Serialized context with disabled preemption */ write_seqcount_begin(&amp;foo_seqcount); /* ... [[write-side critical section]] ... */ write_seqcount_end(&amp;foo_seqcount); write_seqcount_begin() 将 sequence counter 变为 odd, 表示 写临界区正在执行，读到的数据可能是不一致的。 write_seqcount_end() 将 sequence counter 恢复为 even read path : do { seq = read_seqcount_begin(&amp;foo_seqcount); /* ... [[read-side critical section]] ... */ } while (read_seqcount_retry(&amp;foo_seqcount, seq)); read_seqcount_begin() 会获取 sequence counter的值，如果是奇数, 则再次重试， 直到尝试获取到偶数。如果获取到偶数，则将 获取到的counter值返回。在这里，记录到 seq 变量中 而在读临界区之外，调用read_seqcount_retry() 之外再次获取sequence counter的 值 并和seq 变量进行对比 eq: return 0 ne: return 1 当如果不相等时，说明在读临界区中，有writer 进入了写临界区。读临界区中发生的读 取动作很可能获取到不一致的数据。需要重试。所以，这里while() 会获取 read_seqcount_retry()返回值，如果是true 则重启循环。 前面提到多个写者之间要通过其他的互斥机制并行, 能不能封装一个新的接口，让串行多写者 的方式封装到接口中呢? 可以! seqlock 实现方法是在seqlock_t中封装一个自旋锁: typedef struct { unsigned sequence; spinlock_t lock; } seqlock_t; 相关的接口也需要改变: initialization: /* dynamic */ seqlock_t foo_seqlock; seqlock_init(&amp;foo_seqlock); /* static */ static DEFINE_SEQLOCK(foo_seqlock); /* C99 struct init */ struct { .seql = __SEQLOCK_UNLOCKED(foo.seql) } foo; Write path: write_seqlock(&amp;foo_seqlock); /* ... [[write-side critical section]] ... */ write_sequnlock(&amp;foo_seqlock); write sequence 相关接口, 增加了写操作 static inline void write_seqlock(seqlock_t *sl) { spin_lock(&amp;sl-&gt;lock); ++sl-&gt;sequence; smp_wmb(); } static inline void write_sequnlock(seqlock_t *sl) { smp_wmb(); sl-&gt;sequence++; spin_unlock(&amp;sl-&gt;lock); } read path: 读取路径分为三种: 普通: 不阻塞writer: do { seq = read_seqbegin(&amp;foo_seqlock); /* ... [[read-side critical section]] ... */ } while (read_seqretry(&amp;foo_seqlock, seq)); 摇身一变，变身spinlock read_seqlock_excl(&amp;foo_seqlock); /* ... [[read-side critical section]] ... */ read_sequnlock_excl(&amp;foo_seqlock); 直接给读临界区加自旋锁: static inline void read_seqlock_excl(seqlock_t *sl) { spin_lock(&amp;sl-&gt;lock); } 如果发现有写者正在运行, 使用sequence counter, 如果发现有写者运行. 直接用自 旋锁, 该方式的好处在writer 运行不频繁的情况下，可以达到读写锁的效果 – 允许 多个写者同时运行. /* marker; even initialization */ int seq = 0; do { read_seqbegin_or_lock(&amp;foo_seqlock, &amp;seq); /* ... [[read-side critical section]] ... */ } while (need_seqretry(&amp;foo_seqlock, seq)); done_seqretry(&amp;foo_seqlock, seq); 相关接口代码 static inline void read_seqbegin_or_lock(seqlock_t *lock, int *seq) { //如果是偶数，说明没有写者运行。那用 sequence_counter if (!(*seq &amp; 1)) /* Even */ *seq = read_seqbegin(lock); //有写者运行，不再用sequence_counter 的方式重试等待，而直接用 //spinlock else /* Odd */ read_seqlock_excl(lock); } static inline void done_seqretry(seqlock_t *lock, int seq) { //如果seq是奇数，按照`read_seqbegin_or_lock()`的逻辑会上自旋锁。 //那么在这里解锁 if (seq &amp; 1) read_sequnlock_excl(lock); } 无论是sequence counter(raw) 还是seqlock。 整个算法是比较简单的，但是sequence counter 在内核中，有一个很大的问题 – 容易死锁。因为内核有很多异步的上下文， 这些异步的上下文很可能会打断当前正在运行的写临界区，如果该临界区中会运行读临界区。 那就有死锁的风险。 而死锁问题比较难定位的是，造成死锁的两个互斥区不一定都能在死锁现场中暴露出来。例 如AA死锁。第一次加锁的现场很可能已经被覆盖掉。所以内核在很多锁中使用了lockdep 那能不能也使用lockdep来帮助定位 sequence counter/lock 的死锁现场呢? 可以! sequence counter with lockdep 前面提到, 如果在writer 临界区中进行上下文切换，而目标上下文中运行reader则可能造成 死锁。那么, 我们需要在reader和writer相关的api中lockdep以检测死锁. //writer static inline void do_write_seqcount_begin_nested(seqcount_t *s, int subclass) { seqcount_acquire(&amp;s-&gt;dep_map, subclass, 0, _RET_IP_); do_raw_write_seqcount_begin(s); } //reader #define read_seqcount_begin(s) \\ ({ \\ seqcount_lockdep_reader_access(seqprop_const_ptr(s)); \\ raw_read_seqcount_begin(s); \\ }) static inline void seqcount_lockdep_reader_access(const seqcount_t *s) { seqcount_t *l = (seqcount_t *)s; unsigned long flags; local_irq_save(flags); seqcount_acquire_read(&amp;l-&gt;dep_map, 0, 0, _RET_IP_); seqcount_release(&amp;l-&gt;dep_map, _RET_IP_); local_irq_restore(flags); } TODO 如果写者执行seqcount_acquire() 切换到读者执行seqcount_acquire_read()则触发 死锁检测. 但是多个读者侧执行并不会造成死锁。因为其执行的是acquire_read, 并在 acquire_read() 后 释放锁。 这块还没有详细了解lockdep的接口和原理，纯瞎猜 虽然可以检测读者写者死锁，但是能不能检测进入写临界区的写锁一定是加锁状态呢? 每次手动关抢占太麻烦了, 能不能自动关抢占呢 ? 都可以! sequence counters with associated locks(seqcount_LOCKNAME_t) seqcount_LOCKNAME_t 可以实现几个目标: 检测未在调用writer相关接口前加锁的代码 某些类型的锁可能会隐式关抢占, 某些锁不会, 该功能将在接口中根据锁类型自动按需 关闭/开启抢占。(例如某些接口可能会隐式关抢占, 例如spinlock，当使用 seqcount_spinlock_t时就不会自动关抢占). 我们看其是如何实现的: 在编译CONFIG_LOCKDEP || CONFIG_PREEMPT_RT 情况下才使用该功能 #if defined(CONFIG_LOCKDEP) || defined(CONFIG_PREEMPT_RT) #define __SEQ_LOCK(expr) expr #else #define __SEQ_LOCK(expr) #endif #define SEQCOUNT_LOCKNAME(lockname, locktype, preemptible, lockbase) \\ typedef struct seqcount_##lockname { \\ seqcount_t seqcount; \\ __SEQ_LOCK(locktype *lock); \\ } seqcount_##lockname##_t; SEQCOUNT_LOCKNAME(raw_spinlock, raw_spinlock_t, false, raw_spin) SEQCOUNT_LOCKNAME(spinlock, spinlock_t, __SEQ_RT, spin) SEQCOUNT_LOCKNAME(rwlock, rwlock_t, __SEQ_RT, read) SEQCOUNT_LOCKNAME(mutex, struct mutex, true, mutex) #undef SEQCOUNT_LOCKNAME 前者可以理解， 无论是检测是否加write锁，还是检测是否开抢占，这都是lockdep相关 功能。后者我们需要根据后面的代码看下. 定义seqcount_LOCKNAME_t的各个helper, 这些helper 有哪些呢 : #define __seqprop_case(s, lockname, prop) \\ seqcount_##lockname##_t: __seqprop_##lockname##_##prop #define __seqprop(s, prop) _Generic(*(s), \\ seqcount_t: __seqprop_##prop, \\ __seqprop_case((s), raw_spinlock, prop), \\ __seqprop_case((s), spinlock, prop), \\ __seqprop_case((s), rwlock, prop), \\ __seqprop_case((s), mutex, prop)) #define seqprop_ptr(s) __seqprop(s, ptr)(s) #define seqprop_const_ptr(s) __seqprop(s, const_ptr)(s) #define seqprop_sequence(s) __seqprop(s, sequence)(s) #define seqprop_preemptible(s) __seqprop(s, preemptible)(s) #define seqprop_assert(s) __seqprop(s, assert)(s) seqprop_xxx: seqprop_(const_)ptr: 返回seqcount_t的地址 seqprop_sequence: 返回seqcount_t-&gt;sequence的值 seqprop_preemptible: 表示该锁的可抢占性(会不会隐式关抢占), 如果可抢占的 话, 则在写接口中自动将抢占关闭。 可抢占 – 不会隐式关抢占: return true 不可抢占 – 会隐式关抢占: return false 具体情况: 不编译CONFIG_PREEMPT_RT的情况下，根据各个锁类型来看: raw_spinlock: false spinlock_t: true rwlock_t: true mutex: true 编译CONFIG_PREEMPT_RT情况下，所有锁都返回false (因为CONFIG_PREEMPT_RT 不允许关抢占) seqprop_assert: 判断是否持有了锁 helper 展开 作者用_Generic 语法对各个锁类型的进行了抽象 #define __seqprop_case(s, lockname, prop) \\ seqcount_##lockname##_t: __seqprop_##lockname##_##prop #define __seqprop(s, prop) _Generic(*(s), \\ seqcount_t: __seqprop_##prop, \\ __seqprop_case((s), raw_spinlock, prop), \\ __seqprop_case((s), spinlock, prop), \\ __seqprop_case((s), rwlock, prop), \\ __seqprop_case((s), mutex, prop)) _Generic 的用法 : _Generic 展开 _Generic( 表达式, 类型1: 结果1, 类型2: 结果2, ... 类型n: 结果n, default: 默认结果 ) 举例: #include &lt;stdio.h&gt; #define type_check(x) _Generic((x), \\ int: \"int\", \\ float: \"float\", \\ double: \"double\", \\ default: \"other\") int main() { int i = 0; double d = 3.0; printf(\"%s\\n\", type_check(i)); // 输出 int printf(\"%s\\n\", type_check(d)); // 输出 double printf(\"%s\\n\", type_check(\"hello\")); // 输出 other return 0; } 综上来看, _Generic 根据*(s)的类型，走不同的分支: __seqprop_##lockname##_##prop 再看__seqprop_##lockname##_##prop()具体展开实现之前我先来看在原有接口上的改动 write: #define write_seqcount_begin(s) \\ do { \\ //判断是否持有写锁 \\ seqprop_assert(s); \\ \\ //如果目前是可抢占的(锁没有关抢占并且可以关抢占) \\ if (seqprop_preemptible(s)) \\ preempt_disable(); \\ \\ do_write_seqcount_begin(seqprop_ptr(s)); \\ } while (0) 所以writer这边做了两个优化: 判断了是否持有锁（没有持有锁通过lockdep报告) 如果锁没有关抢占，在这里自动关抢占。无需在外部手动关抢占 reader 这边主要是对CONFIG_PREEMPT_RT的优化。首先说下为什么要做这个优化: CONFIG_PREEMPT_RT 实时性要求会更改一些锁的行为, 例如将spinlock自旋抢锁， 改为了睡眠锁, 这样在 高优先级任务就可以强 spinlock, 而sequence counter 要求写者不能关抢占的这个要求不能在CONFIG_PREEMPT_RT中执行，所以 seqprop_preemptible() 会在该配置下，始终返回false. 而例如spinlock也 不会在spin_lock() 接口中关抢占。 而假设在writer临界区中关闭抢占, 走到reader临界区，按照sequence counter 的逻辑其会类似自旋，我们应该要避免掉自旋。让其调度走，回到writer。于是: static __always_inline unsigned \\ __seqprop_##lockname##_sequence(const seqcount_##lockname##_t *s) \\ { \\ unsigned seq = smp_load_acquire(&amp;s-&gt;seqcount.sequence); \\ \\ //==(1)== if (!IS_ENABLED(CONFIG_PREEMPT_RT)) \\ return seq; \\ \\ //==(2)== if (preemptible &amp;&amp; unlikely(seq &amp; 1)) { \\ //==(3)== __SEQ_LOCK(lockbase##_lock(s-&gt;lock)); \\ __SEQ_LOCK(lockbase##_unlock(s-&gt;lock)); \\ \\ /* \\ * Re-read the sequence counter since the (possibly \\ * preempted) writer made progress. \\ */ \\ seq = smp_load_acquire(&amp;s-&gt;seqcount.sequence); \\ } \\ \\ return seq; \\ } 如果没有CONFIG_PREEMPT_RT, 还是按照原来的逻辑走，得到值直接返回。 如果锁本身支持抢占, 并且此时正在写临界区。按照之前的逻辑得自旋等待 写临界区退出（seq 变偶), 但是此时锁是可抢占的。不如让其抢占了。牺牲延迟 带来更好的实时性。（最终要的避免死锁) 如果不能抢占呢? 例如 raw_spinlock (只有这一个), 那不好意思，死锁把你 直接调用睡眠锁（可抢占锁). 让其随眠。唤醒后（大概率是writer临界区结束，唤醒) 再重新获取 sequence 的值 以spinlock为例, 点击展开 SEQCOUNT_LOCKNAME(spinlock, spinlock_t, __SEQ_RT, spin) 主要的是第三个参数: __SEQ_RT: 在配置了CONFIG_PREEMPT_RT 的情况下是 true, 不配置是false: 配置CONFIG_PREEMPT_RT, 说明其是可以被抢占的。那么在seqprop_sequence()中 会使用睡眠锁，而不是类似自旋的raw sequence counter的方式等待写者退出临界区。 不配置CONFIG_PREEMPT_RT, 说明其不可以被抢占。那么在seqprop_preemptible() 的调用结果为false, 在write_seqcount_begin() 时不再关抢占。（因为在进入 write_seqcount_begin() 之前，spin_lock()接口做了关闭抢占。 看下seqcount_spinlock_t的实际用法(以 blk-iocost 模块中的 ioc)为例子: 数据结构: struct ioc { ... spinlock_t lock; ... seqcount_spinlock_t period_seqcount; ... }; 初始化: static int blk_iocost_init(struct gendisk *disk) { seqcount_spinlock_init(&amp;ioc-&gt;period_seqcount, &amp;ioc-&gt;lock); } 写: ioc_timer_fn ## 先加自旋锁 =&gt; spin_lock_irq(&amp;ioc-&gt;lock); =&gt; ioc_start_period(ioc, &amp;now); ## 在调用`write_seqcount_begin` =&gt; write_seqcount_begin(&amp;ioc-&gt;period_seqcount); =&gt; ioc-&gt;period_at = now-&gt;now; =&gt; ioc-&gt;period_at_vtime = now-&gt;vnow; =&gt; write_seqcount_end(&amp;ioc-&gt;period_seqcount); =&gt; spin_unlock_irq(&amp;ioc-&gt;lock); 读: static void ioc_now(struct ioc *ioc, struct ioc_now *now) { ... do { seq = read_seqcount_begin(&amp;ioc-&gt;period_seqcount); now-&gt;vnow = ioc-&gt;period_at_vtime + (now-&gt;now - ioc-&gt;period_at) * vrate; } while (read_seqcount_retry(&amp;ioc-&gt;period_seqcount, seq)); } 读和之前没有变化 能不能在NMI中执行reader(NMI无法屏蔽)? 可以!!! Latch sequence counters (seqcount_latch_t) 在上一节我们讲述了CONFIG_PREEMPT_RT 不允许关抢占，导致可能在写临界区被别的进程 抢占走，从而走到读上下文. 但是其可以调用睡眠锁再次切换到writer上下文。 但是想NMI/interrupt/bh 机制不同。其本身就是在 当前进程的上下文切换, 如果写 临界区因为上述机制打断，进入读临界区，其读操作就不好处理. 所以之前的策略是如果 在interrupt/bh上下文中可能执行reader, 则将interrupt/bh关闭. 但是NMI 不同! 其无法通过上述手段屏蔽（虽然NMI 可以block, 但是block机制并不是 为了让软件在常规流程中disable NMI). 所以应该通过另一种机制. latch : 一词的含义有: 门闩, 插销; 锁存器, 锁住的含义。在该功能中也有这个意思。 就是reader要访问的值，是被锁住的, writer “暂时” 无法修改。 其做法非常简单，就是使用双副本机制。writer在写时，依次对两个副本进行写入。那么 在任意时刻, 总有一个副本是完整的。如下图所示: 我们来看下具体代码: 数据结构: typedef struct { seqcount_t seqcount; } seqcount_latch_t; //需要在外部嵌套: struct latch_struct { seqcount_latch_t seq; struct data_struct data[2]; }; write : //写的时候，需要按照下面的方式调用 void latch_modify(struct latch_struct *latch, ...) { write_seqcount_latch_begin(&amp;latch-&gt;seq); modify(latch-&gt;data[0], ...); write_seqcount_latch(&amp;latch-&gt;seq); modify(latch-&gt;data[1], ...); write_seqcount_latch_end(&amp;latch-&gt;seq); } static __always_inline void write_seqcount_latch_begin(seqcount_latch_t *s) { kcsan_nestable_atomic_begin(); raw_write_seqcount_latch(s); } static __always_inline void write_seqcount_latch(seqcount_latch_t *s) { raw_write_seqcount_latch(s); } static __always_inline void write_seqcount_latch_end(seqcount_latch_t *s) { kcsan_nestable_atomic_end(); } 对比raw的方式，就是在原来write_seqcount_end() 将sequence_counter 变更为偶数 后，对latch-&gt;data[1] 再做更改。 reader: struct entry *latch_query(struct latch_struct *latch, ...) { struct entry *entry; unsigned seq, idx; do { //读seq seq = read_seqcount_latch(&amp;latch-&gt;seq); //如果seq是奇数，说明写者正在修改data[0]。那么 //读data[1] // //如果是偶数, 说明写者还没有修改data[0]. 那么 //读data[0] idx = seq &amp; 0x01; entry = data_query(latch-&gt;data[idx], ...); // This includes needed smp_rmb() //当然这里还需要判断seq是被写者更改，如果被写者更改再次重试 } while (read_seqcount_latch_retry(&amp;latch-&gt;seq, seq)); return entry; } 正如前面所说，如果写者被NMI打断，进入读临界区，可以根据seq的值访问写者没有在操作 的区域。从而保证了读者读取数据完整性。 TMP note 随笔记录, 暂时保存在这 seqcount_##lockname /* * For PREEMPT_RT, seqcount_LOCKNAME_t write side critical sections cannot * disable preemption. It can lead to higher latencies, and the write side * sections will not be able to acquire locks which become sleeping locks * (e.g. spinlock_t). * * To remain preemptible while avoiding a possible livelock caused by the * reader preempting the writer, use a different technique: let the reader * detect if a seqcount_LOCKNAME_t writer is in progress. If that is the * case, acquire then release the associated LOCKNAME writer serialization * lock. This will allow any possibly-preempted writer to make progress * until the end of its writer serialization lock critical section. * * This lock-unlock technique must be implemented for all of PREEMPT_RT * sleeping locks. See Documentation/locking/locktypes.rst */ #if defined(CONFIG_LOCKDEP) || defined(CONFIG_PREEMPT_RT) #define __SEQ_LOCK(expr) expr #else #define __SEQ_LOCK(expr) #endif #define SEQCOUNT_LOCKNAME(lockname, locktype, preemptible, lockbase) \\ typedef struct seqcount_##lockname { \\ seqcount_t seqcount; \\ __SEQ_LOCK(locktype *lock); \\ } seqcount_##lockname##_t; SEQCOUNT_LOCKNAME(raw_spinlock, raw_spinlock_t, false, raw_spin) SEQCOUNT_LOCKNAME(spinlock, spinlock_t, __SEQ_RT, spin) SEQCOUNT_LOCKNAME(rwlock, rwlock_t, __SEQ_RT, read) SEQCOUNT_LOCKNAME(mutex, struct mutex, true, mutex) #undef SEQCOUNT_LOCKNAME __seqprop #define __seqprop(s, prop) _Generic(*(s), \\ seqcount_t: __seqprop_##prop, \\ __seqprop_case((s), raw_spinlock, prop), \\ __seqprop_case((s), spinlock, prop), \\ __seqprop_case((s), rwlock, prop), \\ __seqprop_case((s), mutex, prop)) #define __seqprop_case(s, lockname, prop) \\ seqcount_##lockname##_t: __seqprop_##lockname##_##prop 根据s不同的类型，调用不同的helper, 例如s如果为spinlock , prop 为 assert 则调用__seqprop_spinlock_assert() __seqprop__##lockname##_##assert static __always_inline void \\ __seqprop_##lockname##_assert(const seqcount_##lockname##_t *s) \\ { \\ __SEQ_LOCK(lockdep_assert_held(s-&gt;lock)); \\ } //raw sequence counter static inline void __seqprop_assert(const seqcount_t *s) { //断言这里已经关闭抢占 lockdep_assert_preemption_disabled(); } write_seqcount_begin /** * write_seqcount_begin() - start a seqcount_t write side critical section * @s: Pointer to seqcount_t or any of the seqcount_LOCKNAME_t variants * * Context: sequence counter write side sections must be serialized and * non-preemptible. Preemption will be automatically disabled if and * only if the seqcount write serialization lock is associated, and * preemptible. If readers can be invoked from hardirq or softirq * context, interrupts or bottom halves must be respectively disabled. * * sequence counter write side 必须被序列化(防止多个write)并且不能被抢占 * 如果 write serialization lock 被关联，并且是可抢占的情况下，会自动关闭 * 抢占。如果读操作可能在`hardirq` 或者 `softirq` 上下文中被调用，必须分别 * 禁用中断或者 bottom half */ #define write_seqcount_begin(s) \\ do { \\ seqprop_assert(s); \\ \\ if (seqprop_preemptible(s)) \\ preempt_disable(); \\ \\ do_write_seqcount_begin(seqprop_ptr(s)); \\ } while (0) #define seqprop_assert(s) __seqprop(s, assert)(s) 参考链接 Sequence counters and sequential locks Sequence wiki 相关 commit seqlock for xtime bb59cfa4c9113214f91fa0ce744fd92fe2745039 Stephen Hemminger shemminger@osdl.org Tue Feb 4 23:25:27 2003 -0800 seqcount: Add lockdep functionality to seqcount/seqlock structures 1ca7d67cf5d5a2aef26a8d9afd789006fa098347 John Stultz john.stultz@linaro.org Mon Oct 7 15:51:59 2013 -0700 seqlock: Extend seqcount API with associated locks 55f3560df975f557c48aa6afc636808f31ecb87a Ahmed S. Darwish a.darwish@linutronix.de Mon Jul 20 17:55:15 2020 +0200 seqlock: Extend seqcount API with associated locks seqlock: seqcount_LOCKNAME_t: Introduce PREEMPT_RT support 8117ab508f9c476e0a10b9db7f4818f784cf3176 Author: Ahmed S. Darwish a.darwish@linutronix.de Date: Fri Sep 4 17:32:30 2020 +0200 " }, { "title": "rcu - classic", "url": "/posts/rcu-classic/", "categories": "os, synchronization", "tags": "os, synchronization, rcu", "date": "2026-01-05 10:00:00 +0800", "content": "本文主要讲解 经典rcu (classical rcu) 历史. 在介绍具体实现之前, 我们先明确几个概 念: quiescent state: 该CPU 上运行的所有 RCU 读取端临界区都已完成1 grace period: rcu 删除分为三部分, emoval ,Grace Period, and Reclamation. 宽限期 结束以所有cpu rcu 读临界区完成, 即所有cpu 都经历一次 quiescent state rcu callback: 某些rcu writer将释放动作封装为一个rcu_head, 通过调用 call_rcu()注册回调，允许异步执行释放动作。 rcu处理流程的关键点是: 如何发现新的rcu callback, 发起一个新的宽限期 如何判定该宽限期结束, 调用相关rcu callback 如下图所示: 这里有几个问题需要思考下: 谁持有rcu read lock(和问题三相关联) 什么时候需要发起一个新的宽限期 怎么确定该cpu 进入静默状态 怎么确定所有cpu 都经历了一次静默状态（当前宽限期结束) 我们带着这些问题看接下来的内容: first version struct 相关数据结构: global: rcu_ctrblk: 全局数据结构，和全局的宽限期”version”, 以及cpu静默状态位图 curbatch: 当前宽限期的”version” maxbatch: rcu callback “预定的” 最大宽限期 “version” rcu_cpu_mask: 当前宽限期处于静默状态位图 per cpu: rcu_tasklet: 用于定义rcu_tasklet, 用户在softirq中处理rcu。 rcu_data: 用于记录每个cpu的静默期，以及待处理的rcu callback 链表, 以及 batch “version” qsctr: 当前静默期”version” last_qsctr: 上一次记录的静默期 “version” batch: 当前 curlist 处于宽限期的”version” curlist: 处于宽限期的rcu callback列表 nxtlist: 表示待处理的rcu callback 列表(还未发起宽限期) 处理流程 add rcu_callback to head void call_rcu(struct rcu_head *head, void (*func)(void *arg), void *arg) { int cpu; unsigned long flags; //===(1)=== head-&gt;func = func; head-&gt;arg = arg; //===(2)=== local_irq_save(flags); cpu = smp_processor_id(); //===(3)=== list_add_tail(&amp;head-&gt;list, &amp;RCU_nxtlist(cpu)); local_irq_restore(flags); } 构造rcu_head 数据机构 关中断。因为下面要操作RCU_nxtlist(), 防止该流程被中断打断（中断也可能执行这部分流程) 将新构造的 head 串到 RCU_nxtlist() cpu experience a quiescent state 上面讲述了，如何将rcu callback 注册到相应的数据结构中。那什么时候处理(执行)rcu callback呢? – 等一个完整的宽限期结束. 这里为什么要提完整的宽限期呢? 那就得讨论下是否要支持全局的宽限期。可以设想下，每个cpu 都可以发起宽限期。每个 cpu 负责记录自己的静默状态，并标记这些并在记录后，再处理每个cpu的宽限期状态。 这样实现起来太繁琐了。所以Linux 将宽限期定义为一个全局的状态。 举个例子, 如果rcu callback在每个时刻都会产生的话，整个的时间线将分割为不同的宽 限期. ======================================================================&gt; timeline |-- grace period 1 --|-- grace period 2 --|-- grace period 3--|--next 那假设在period 1 阶段调用call_rcu(), 那call_rcu()产生的callback能不能在grace period 1 结束后执行么? 不可以，因为此时已经有一些cpu 进入下一个宽限期。可能正 处于读临界区中。所以需要等到grace period 2 结束。 那么如何判定完整的宽限期结束呢? 在发起宽限期后，所有cpu 都经历一个静默状态. 那什 么时候可以判断静默状态结束呢? 明显的答案是rcu_read_unlock()结束。因为这意味着读临界区结束。但是这样可能会和 Linux 本身要求rcu达到的效果相违背: 安全高效。 rcu_read_unlock() 的问题: 全局状态更新太频繁: 在某个流程中频繁的调用rcu_read_lock(), rcu_read_unlock(). 频繁的更新全局状态会让写端(其实是处理grace period 流程)开销陡增（cache conherence cost) 并未找到官方说明, 所以这里我只是猜测。不知道是否有其他更深层次的原因。 于是开发者们, 在两个点定义静默状态: timer interrupt from USERSPACE, idle schedule() 因为rcu读临界区都发生在内核代码中，所以从userspace 触发的中断可以断定一定没有处 于rcu读临界区。另外, idle比较特殊，其虽然位于内核空间，但是其是空闲的（什么都不 做), 所以也可以认为其属于静默状态. 判断条件在rcu_check_callback() 代码中: rcu_check_callbacks =&gt; if (user || (idle_cpu(cpu) &amp;&amp; !in_softirq() &amp;&amp; hardirq_count() &lt;= 1)) =&gt; RCU_qsctr(cpu)++; RCU_qsctr(cpu)++ 表示当前cpu已经经历了一次静默期。关于idle分支的判断 要稍微复杂一些: idle_cpu(cpu): 表示当前cpu 正在执行的任务是idle任务 !in_softirq(): 不处于softirq 上下文 hardirq_count &lt;= 1: 不处于中断上下文（该时钟中断的前一个上下文，而时钟中断 本身位于中断上下文, 所以这里要 &lt;=1) when to initiate a new grace period and handle 当我们通过call_rcu() 注册一个异步callback后，这些callback需要经历一个完整的 宽限期。我们如何将这些callback和具体的宽限期联系起来。并在宽限期结束后处理 他们呢? 如上图所示, 在时钟中断处理流程中，scheduler_tick() 会判断是否有rcu相关 的事情要处理, 如果有则调用 rcu_check_callbacks() 处理。该函数不仅会判断 是否经历了一次静默状态，同时也会调用tasklet_schedule() 调用rcu_tasklet 做进一步的下半部处理。 关于rcu_pending()代码: static inline int rcu_pending(int cpu) { if ((!list_empty(&amp;RCU_curlist(cpu)) &amp;&amp; rcu_batch_before(RCU_batch(cpu), rcu_ctrlblk.curbatch)) || (list_empty(&amp;RCU_curlist(cpu)) &amp;&amp; !list_empty(&amp;RCU_nxtlist(cpu))) || test_bit(cpu, &amp;rcu_ctrlblk.rcu_cpu_mask)) return 1; else return 0; } 有两种情况需要在下半部进一步处理 当前有未处理的 rcu_callback curlist 不为空，但是curlist 所在的batch 已经expired.(说明curlist所在的 宽限期已经结束), 或者 curlist 是空，nxtlist不为空。说明, 需要未nxtlist 发起一个新的宽限期 其他情况: 例如curlist 不为空，但是curlist 所在的batch 还没有 expired. 这说明curlist 所在的宽限期还没有结束。还不能为nxtlist分配下一个宽 限期 判断rcu_ctrlblk-&gt;rcu_cpu_mask 是否有该cpu bit, rcu_cpu_mask用来标记 哪些cpu还没有在本次宽限期中经历静默状态; 如果为1 说明有宽限期正在等待 该cpu 到达静默状态。所以，需要该cpu 根据自己静默状态修改rcu_cpu_mask 这部分工作也放在了下半部处理。 work in rcu_tasklet 而位于rcu_tasklet中的流程，是rcu 处理的主流程, 其主要有几部分工作: 为新的rcu callback分配宽限期 判断该cpu是否处于静默状态, 并修改rcu_cpu_mask 判断该cpu curlist 所在的宽限期是否结束，如果结束执行相应的callback，并根据 nxtlist链表情况, 要不要发起下一个宽限期. 代码并不复杂，我们直接看代码: static void rcu_process_callbacks(unsigned long unused) { int cpu = smp_processor_id(); LIST_HEAD(list); //==(1)== if (!list_empty(&amp;RCU_curlist(cpu)) &amp;&amp; rcu_batch_after(rcu_ctrlblk.curbatch, RCU_batch(cpu))) { list_splice(&amp;RCU_curlist(cpu), &amp;list); INIT_LIST_HEAD(&amp;RCU_curlist(cpu)); } //==(2)== local_irq_disable(); //==(3)== if (!list_empty(&amp;RCU_nxtlist(cpu)) &amp;&amp; list_empty(&amp;RCU_curlist(cpu))) { list_splice(&amp;RCU_nxtlist(cpu), &amp;RCU_curlist(cpu)); INIT_LIST_HEAD(&amp;RCU_nxtlist(cpu)); local_irq_enable(); /* * start the next batch of callbacks */ spin_lock(&amp;rcu_ctrlblk.mutex); RCU_batch(cpu) = rcu_ctrlblk.curbatch + 1; rcu_start_batch(RCU_batch(cpu)); spin_unlock(&amp;rcu_ctrlblk.mutex); } else { local_irq_enable(); } //==(4)== rcu_check_quiescent_state(); //==(5)== if (!list_empty(&amp;list)) rcu_do_batch(&amp;list); } curlist中没有成员，并且 rcu_ctrblk.curbatch 比 RCU_batch(cpu) 要高，说明 当前全局的宽限期，已经比cpu curlist所在的宽限期要高，所以cpu curlist宽限期 已经结束。为此可以执行该cpu curlist中的rcu callback 这里比较有意思，访问 RCU_curlist() 没有关中断，但是访问RCU_nxtlist() 却关中断, 原因是因为nxtlist 可能会在中断上下文中更新。 如果nxtlist不为空，但是curlist为空, 则需要为nxtlist 分配一个新的宽限期. 首先将nxtlist 链表转移至 curlist, 接着分配 RCU_batch(cpu) 宽限期”version” 为global current batch + 1(rcu_ctrlblk-&gt;curbatch + 1). 然后调用 rcu_staret_batch()（下面讲) 该函数会判断当前函数是否经历一次完整的静默期. 根据 1 可知，list中的rcu callback肯定经历了一次完整的静默期，可以执行release rcu_callback 流程 rcu_start_batch(): static void rcu_start_batch(long newbatch) { //maxbatch 永远记录当前\"申请的\" 最大的宽限期版本 if (rcu_batch_before(rcu_ctrlblk.maxbatch, newbatch)) { rcu_ctrlblk.maxbatch = newbatch; } //如果maxbatch 比curbatch 早，说明 curbatch 已经涨上来了。 //（curbatch - 1 已经结束了) // //反之，并且`rcu_ctrblk.rcu_cpu_mask == 0`, 说明旧的宽限期已经结束，并且有 //新的宽限期需要发起. if (rcu_batch_before(rcu_ctrlblk.maxbatch, rcu_ctrlblk.curbatch) || (rcu_ctrlblk.rcu_cpu_mask != 0)) { return; } //发起一个新的宽限期 rcu_ctrlblk.rcu_cpu_mask = cpu_online_map; } 怎么才算发起一个新的宽限期呢? 还记得rcu_pending()的条件么? 只要该cpu 的 rcu_cpu_mask 置位，说明该cpu 需要关注自己的静默状态, 并在达到静默状态后， 清除rcu_cpu_mask相应状态，所以，将rcu_cpu_mask全部置位，所有的cpu 都要重新关注自己的静默状态。这样算是发起了新的宽限期。 发起宽限期的条件之一是 rcu_ctrlblk.maxbatch &gt;= rcu_ctrlblk.curbatch, 所以无论是curbatch改变，还是maxbatch改变都有可能发起新的宽限期. 而该调用路径: rcu_process_callback() =&gt; rcu_start_batch() =&gt; rcu_ctrlblk.rcu_cpu_mask = cpu_online_map 其实是描述的maxbatch改变，在cpu检测到宽限期结束，自增全局curbatch时， 也会让这个天平倾斜 rcu_check_quiescent_state() static void rcu_check_quiescent_state(void) { int cpu = smp_processor_id(); //未置位的原因是该cpu 在该宽限期已经是静默状态. if (!test_bit(cpu, &amp;rcu_ctrlblk.rcu_cpu_mask)) { return; } /* * Races with local timer interrupt - in the worst case * we may miss one quiescent state of that CPU. That is * tolerable. So no need to disable interrupts. */ //这个流程可能会和local timer interrupt 冲突?? //冲突意味着 RCU_qsctr() 会更改, 但是结合`rcu_check_callbacks()` //代码来看其不会更改 RCU_qsctr() // //那还有一种可能 -- 调度, 但是softirq 不能被抢占。但是ksoftirq 可以 //被抢占, 这里的意思难道是ksoftirq可以被抢占? 导致抢占后 RCU_qsctr //更改? //==(1)== if (RCU_last_qsctr(cpu) == RCU_QSCTR_INVALID) { RCU_last_qsctr(cpu) = RCU_qsctr(cpu); return; } //==(1.2)== //说明当前记录的宽限期(last_qsctr) 还未结束 if (RCU_qsctr(cpu) == RCU_last_qsctr(cpu)) { return; } spin_lock(&amp;rcu_ctrlblk.mutex); //这个地方也很奇怪, 前面也检查过该cpu的mask，确定 //present后，才会向下执行，但是这里为什么要加自旋锁 //再检查下 //==(2)== if (!test_bit(cpu, &amp;rcu_ctrlblk.rcu_cpu_mask)) { spin_unlock(&amp;rcu_ctrlblk.mutex); return; } //运行到这里说明宽限期已经结束 clear_bit(cpu, &amp;rcu_ctrlblk.rcu_cpu_mask); //last_qsctr 置为 RCU_QSCTR_INVALID(0) RCU_last_qsctr(cpu) = RCU_QSCTR_INVALID; if (rcu_ctrlblk.rcu_cpu_mask != 0) { spin_unlock(&amp;rcu_ctrlblk.mutex); return; } //处理下一个宽限期 rcu_ctrlblk.curbatch++; //发起下一个宽限期, //==(3)== rcu_start_batch(rcu_ctrlblk.maxbatch); spin_unlock(&amp;rcu_ctrlblk.mutex); } 为什么qsctr 要经历RCU_QSCTR_INVAILD-&gt; RCU_qsctr(cpu) -&gt; RCU_qsctr()++ 这样的变化。而不能直接在宽限期结束后，不重制RCU_last_qsctr(). 当宽限期结束后，有新的宽限期发起，这时如果走到1，会将 last_qsctr 赋值为 qsctr, 但是如果做这个流程，必须得是新的宽限期发起后才做，而宽限期发起后， 如果执行了一次这个流程就无需在做，只需要等待1.2条件满足即可。 所以这里将qsctr赋值为RCU_QSCTR_INVAILD, 为了给下次进入该函数识别新宽限期 发起后，首次执行该函数作准备 这个地方着实没看懂 这里使用maxbatch作为参数调用rcu_start_batch(), maxbatch 前面提到过, 可 以认为是 目前”预定的”最大版本的宽限期. 相当于pending的最大版本的宽限期， 如果这个宽限期都处理完了，说明所有的cpu-&gt;curlist 都处理完了。 处理流程图示 流程图示展开 初始状态 cpu0 rcu writer 调用 call_rcu() 异步释放object CPU0 在时钟中断中发现有rcu事情需要处理，唤起 rcu tasklet, 将nxtlist 移动至 cutlist 发起一个新的宽限期新的宽限期为2 (maxbatch(2) 表示当前申请的最大的宽限期), rcu_cpu_mask 赋值为 cpu_online_mask(1,1,1,1), 表示所有的cpu豆未经历静默状态。 CPU0, CPU1, CPU2, CPU3 在检测自己是否进入静默状态是，现将 last_qsctr重置为qsctr，不过后者也是0。 CPU0, CPU1, CPU2 进入静默状态，清除自己cpu的 rcu_cpu_mask CPU3 进入静默状态，并清除其cpu的rcu_cpu_mask, 作为最后一个清除rcu_cpu_mask 的cpu, 最终会将rcu_cpu_mask 更改为0。更新至0 意味着所有的cpu 都进入静默状态。 也就是该宽限期(1)结束。 宽限期(1) 结束，但是CPU0 curlist申请的不是宽限期1而是宽限期2(maxbatch), 所以 该宽限期结束不会处理任何callback，但是会发起进入下一个宽限期. 等待所有cpu又经历一个宽限期后, cpu0的rcu callback可以得到处理。 处理过后，maxbatch仍然是2，而curbatch 更新至3，curbatch &gt; maxbatch, 说明pending 的宽限期已经处理完成，没有必要再处理curbatch。等待maxbatch 更新上来。 可以看到这里有些流程是不太好的。例如当我们重新发起宽限期时(move nxtlist-&gt;curlist), 总是将RCU_batch(cpu) = rcu_ctrlblk.curbatch + 1, 并没有看当前的宽限期活不活跃。 这样就会多经历一个额外的宽限期 NOHZ support s390首先引入了nohz(commit 2), nohz意味着空闲的核心将可能在一段事件之内不会 有时钟中断。而发起一个新的宽限期后, 会等待所有的cpu进入静默状态。而 每个 cpu调整自己的静默状态是依赖时钟中断的执行rcu_pending(), 然后再唤醒rcu tasklet 所以, 当关闭某个cpu的时钟中断后，原有的rcu的处理逻辑就要变动。 首先，处于nohz的cpu肯定是idle的, 并且不会处于中断上下文和软中断上下文。 所以, 在发起新的宽限期时，可以不选择等待处于nohz 的cpu static void rcu_start_batch(long newbatch) { + cpumask_t active; + if (rcu_batch_before(rcu_ctrlblk.maxbatch, newbatch)) { rcu_ctrlblk.maxbatch = newbatch; } @@ -111,7 +113,9 @@ static void rcu_start_batch(long newbatch) return; } /* Can't change, since spin lock held. */ - rcu_ctrlblk.rcu_cpu_mask = cpu_online_map; + active = idle_cpu_mask; + cpus_complement(active); + cpus_and(rcu_ctrlblk.rcu_cpu_mask, cpu_online_map, active); } 另外，如果在宽限期中，如果一个cpu并未进入静默状态，说明有宽限期在等待 该cpu进入静默状态，然后结束宽限期。此时该cpu不能进入nohz。(相当于不能 在读临界区中长期阻塞) stop_hz_timer =&gt; if (rcu_pending(smp_processor_id()) || local_softirq_pending()) ## return 1 表示 CPU 没有停掉timer =&gt; return 1 我这里有一点疑问: 这里访问idle_cpu_mask 并未使用同步源语. 那另一个cpu对 idle_cpu_mask 的更新，可能得等一段事件才能被rcu_start_batch()发现，那么这会有影 响么? 例如: cpu0 cpu1 ============================================== start_hz_timer update idle_cpu_mask enter rcu crtical section get data1 delete data1 in list start a new grace period copy idle_cpu_mask(but copy old data) 我个人认为可能会有这种情况. CPU HotPlug 当发起一个新的宽限期时，会将rcu_cpu_mask 赋值为cpu_online_map. 其只关注 online的cpu。从当cpu拓扑处于静态状态(没有热插拔), 来看没有什么问题. 但是当支持热插拔后, 事情有一些复杂。我们分为两个部分: 热插 当热插后，cpu online 流程会更新cpu_online_map, 然后再进入读临界区。(中间 可能会有内存屏障。所以对于发起宽限期流程来说不会有影响。 关于cpu online 处理流程 纯个人猜测，没有找到代码(置位 cpu_online_map) 热拔 热拔流程会受一些影响主要为: 在热拔时，该cpu可能在当前的宽限期中还未进入静默状态（也就意味着有人在等) 在热拔时, 可能有一些rcu callback还未执行 那我们展开下这部分改动 rcu 关于 cpu热拔新增改动 首先在增加CPU_DEAD notify: @@ -214,7 +269,11 @@ static int __devinit rcu_cpu_notify(struct notifier_block *self, case CPU_UP_PREPARE: rcu_online_cpu(cpu); break; - /* Space reserved for CPU_OFFLINE :) */ +#ifdef CONFIG_HOTPLUG_CPU + case CPU_DEAD: + rcu_offline_cpu(cpu); + break; +#endif default: break; } 查看相关回调: 代码以及解释展开 /* warning! helper for rcu_offline_cpu. do not use elsewhere without reviewing * locking requirements, the list it's pulling from has to belong to a cpu * which is dead and hence not processing interrupts. */ /* * 这种处理往往是危险的。 * * 首先中断上下文可能会调用`call_rcu()`, 所以一般情况下，操作`RCU_nxtlist()`都会 * 关中断。 * * 另外list所在的cpu 未offline那更危险，因为操作 per cpu的 rcu_data不会加锁。 * 所以可能有两个cpu同时操作一个rcu_data... * * 所以`rcu_move_batch` 的约束为 offline cpu 另外关中断(在rcu_move_batch() 中已 * 经将中断关了 */ static void rcu_move_batch(struct list_head *list) { struct list_head *entry; //获取当前cpu int cpu = smp_processor_id(); local_irq_disable(); while (!list_empty(list)) { entry = list-&gt;next; //从之前的list转移到当前cpu 的 RCU_nxtlist()上. list_del(entry); list_add_tail(entry, &amp;RCU_nxtlist(cpu)); } local_irq_enable(); } static void rcu_offline_cpu(int cpu) { /* if the cpu going offline owns the grace period * we can block indefinitely waiting for it, so flush * it here */ //cpu offline之前先标记他已经进入静默状态, //和 rcu_check_quiescent_state() 类似, 当有cpu标记其进入静默状态， //该cpu还需要负责检查宽限期是否结束，如果结束，根据是否有pending //的宽限期发起新的宽限期 spin_lock_irq(&amp;rcu_ctrlblk.mutex); if (!rcu_ctrlblk.rcu_cpu_mask) goto unlock; cpu_clear(cpu, rcu_ctrlblk.rcu_cpu_mask); if (cpus_empty(rcu_ctrlblk.rcu_cpu_mask)) { //标记该宽限期已经结束 rcu_ctrlblk.curbatch++; /* We may avoid calling start batch if * we are starting the batch only * because of the DEAD CPU (the current * CPU will start a new batch anyway for * the callbacks we will move to current CPU). * However, we will avoid this optimisation * for now. */ /* * 但是有必要在这里发起宽限期么? * * 当前cpu(非hotplug cpu) 会因为宽限期已经结束，rcu_pending * 会返回true。从而进入`rcu_process_callbacks()`, 如果此时iu * `nxtlist`有值则会发起一个新的宽限期 * * 如果在这里取消调用`rcu_start_batch()`，就比较依赖`nxtlist` * 有值，但是一定会有值么? * * 首先如果当前cpu本身nxtlist 有值 那没有问题。 * * 那如果当前cpu没有值，但是hotplug的 curlist, nxtlist 有值，那也没有问 * 题，请看`rcu_move_batch()` * * 但是如果两个都没有值，但是maxbatch &gt;= curbatch (这里大概率是 等于), * 说明什么呢? 说明曾经有cpu预定过下一个宽限期。那后续就没有办法及时 * 发起曾经pending的宽限期。 * * 所以这个地方如果想优化，还得做一些其他改动。 */ rcu_start_batch(rcu_ctrlblk.maxbatch); } unlock: spin_unlock_irq(&amp;rcu_ctrlblk.mutex); //将offline cpu的 curlist移动到 当前cpu的 nxtlist rcu_move_batch(&amp;RCU_curlist(cpu)); //将offline cpu的 nxtlist移动到 当前cpu的 nxtlist rcu_move_batch(&amp;RCU_nxtlist(cpu)); tasklet_kill_immediate(&amp;RCU_tasklet(cpu), cpu); } rcu_cpu_mask is too busy rcu_cpu_mask 表示哪些cpu在本次宽限期中有没有进入静默状态: 0: 进入静默状态 1: 尚未进入静默状态 一版来说, 内核的宽限期都不长，所以该字段可能面临频繁更新。（尤其是cpu很多的情况 下), 而rcu_cpu_mask的访问频次又很高。出现在: check look for quiescent states rcu_pending() rcu_check_quiescent_state() 这两个位置都会获取当前cpu 是否在宽限期中已经处于静默状态。这种情况下，面临严重的 cacheline trash. (可以回忆下 directory cache conherence read miss 的场景)。 但是我们在宽限期中进入静默状态后，在新的宽限期到来之前，该cpu的静默状态不再会更 改。（换句话说一个cpu的静默状态在一次宽限期中只会变更一次）。其他cpu静默状态变化 并不影响该cpu的静默状态。但是因为代码实现的原因，而导致其他cpu静默状态变化，影响 该cpu获取静默状态的性能，显然是不合理的。 因此cpu 是否处于静默状态更适合使用percpu vars保存，并且该变量的行为更像是 read-only (write-less). 尽量避免对写频繁的 rcu_cpu_mask 访问. 于是, Manfred Spraul在上一版rcu实现中，做了如下改动: Separating Write-Hot and Write-Cold Variables in rcu_ctrblk cacheline trash 问题往往发生在对全局变量的更新中。所以Manfred 将write-hot部分 和write-cold分为两个cacheline: struct rcu_ctrlblk { - spinlock_t mutex; /* Guard this struct */ - long curbatch; /* Current batch number. */ - long maxbatch; /* Max requested batch number. */ - cpumask_t rcu_cpu_mask; /* CPUs that need to switch in order */ - /* for current batch to proceed. */ + /* \"const\" members: only changed when starting/ending a grace period */ + struct { + long cur; /* Current batch number. */ + long completed; /* Number of the last completed batch */ + } batch ____cacheline_maxaligned_in_smp; + /* remaining members: bookkeeping of the progress of the grace period */ + struct { + spinlock_t mutex; /* Guard this struct */ + int next_pending; /* Is the next batch already waiting? */ + cpumask_t rcu_cpu_mask; /* CPUs that need to switch */ + /* in order for current batch to proceed. */ + } state ____cacheline_maxaligned_in_smp; }; 这里有一些新面孔: cur: 同curbatch, 在宽限期结束后更新 completed: 上一个完成的 batch, 在宽限期结束后更新 next_pending: 有下一个宽限期pending, 在发起新的宽限期时更新 该改动主要是优化了先前的maxbatch的逻辑。 之前如何判断是否有一个新的宽限期需要发起呢? maxbatch &gt; curbatch 但是maxbatch的值，往往最大=curbatch + 1, 表示下一个预定的宽限期比当前宽限期大， 也就是有pending的宽限期，由于只大一，所以其也只能表示是否有pending. 所以这里干 脆就将这个值删除，直接搞一个next_pending表示是否有新的宽限期正在阻塞，当前 宽限期结束后，需要立即发起该宽限期。 另外, 关于宽限期是否结束的判断逻辑也更改了，之前是判断: rcu_batch_before(RCU_batch(cpu), rcu_ctrlblk.curbatch) curbatch 如果完成，就自增为curbatch+1 现在使用completed替代. 表已经完成的宽限期的最大版本, 所以判断逻辑更改为: !rcu_batch_before(rcu_ctrlblk.batch.completed,RCU_batch(cpu)) 发起新宽限期的代码也有变动: -static void rcu_start_batch(long newbatch) +static void rcu_start_batch(int next_pending) { cpumask_t active; - if (rcu_batch_before(rcu_ctrlblk.maxbatch, newbatch)) { - rcu_ctrlblk.maxbatch = newbatch; + if (next_pending) + rcu_ctrlblk.state.next_pending = 1; + + if (rcu_ctrlblk.state.next_pending &amp;&amp; + rcu_ctrlblk.batch.completed == rcu_ctrlblk.batch.cur) { + rcu_ctrlblk.state.next_pending = 0; + /* Can't change, since spin lock held. */ + active = nohz_cpu_mask; + cpus_complement(active); + cpus_and(rcu_ctrlblk.state.rcu_cpu_mask, cpu_online_map, active); + rcu_ctrlblk.batch.cur++; } - if (rcu_batch_before(rcu_ctrlblk.maxbatch, rcu_ctrlblk.curbatch) || - !cpus_empty(rcu_ctrlblk.rcu_cpu_mask)) { - return; +} 不再维护newbatch，而是传入next_pending表示，调用该函数的原因是由于 发起了新的宽限期? – next_pending = 1 当前宽限期结束，可能有pending的宽限期需要处理 – next_pending == 1 然后再根据rcu_ctrlblk.statet.next_pending决定是否要发起新的宽限期. 而调用流程和之前类似: 发起了新的宽限期的调用者: 将nxtlist-&gt;curlist, 发起新的宽限期 @@ -236,10 +268,10 @@ static void rcu_process_callbacks(unsigned long unused) /* * start the next batch of callbacks */ - spin_lock(&amp;rcu_ctrlblk.mutex); - RCU_batch(cpu) = rcu_ctrlblk.curbatch + 1; - rcu_start_batch(RCU_batch(cpu)); - spin_unlock(&amp;rcu_ctrlblk.mutex); + spin_lock(&amp;rcu_ctrlblk.state.mutex); + RCU_batch(cpu) = rcu_ctrlblk.batch.cur + 1; + rcu_start_batch(1); + spin_unlock(&amp;rcu_ctrlblk.state.mutex); cpu_quiet() 表示该宽限期已经结束，可能有pending的宽限期需要处理（发起pending 的宽限期) cpu_quiet =&gt; cpus_empty(rcu_ctrlblk.state.rcu_cpu_mask) ## 重新赋值 completed-&gt;cur, 表示该宽限期已经结束 =&gt; rcu_ctrlblk.batch.completed = rcu_ctrlblk.batch.cur; ## 可能有pending的宽限期，尝试发起 =&gt; rcu_start_batch(0); record per cpu batch 前面提到过，判断该cpu是否要处理rcu宽限期需要判断全局的rcu_cpu_mask, 性能不好 要切换到per-cpu vars来记录, 那么就需要记录，在当前宽限期内，是否进入过静默 状态。在per cpu 的rcu_data中增加 struct rcu_data { + /* 1) quiescent state handling : */ + long quiescbatch; /* Batch # for grace period */ long qsctr; /* User-mode/idle loop etc. */ long last_qsctr; /* value of qsctr at beginning */ /* of rcu grace period */ + int qs_pending; /* core waits for quiesc state */ + + /* 2) batch handling */ long batch; /* Batch # for current RCU batch */ struct list_head nxtlist; struct list_head curlist; quiescbatch: 当前cpu所处的宽限期 qs_pending: 在quiescbatch所表示的宽限期中，该cpu是否处于静默状态 我们首先来看rcu_pending()处的改动: static inline int rcu_pending(int cpu) { - if ((!list_empty(&amp;RCU_curlist(cpu)) &amp;&amp; - rcu_batch_before(RCU_batch(cpu), rcu_ctrlblk.curbatch)) || - (list_empty(&amp;RCU_curlist(cpu)) &amp;&amp; - !list_empty(&amp;RCU_nxtlist(cpu))) || - cpu_isset(cpu, rcu_ctrlblk.rcu_cpu_mask)) + /* This cpu has pending rcu entries and the grace period + * for them has completed. + */ + if (!list_empty(&amp;RCU_curlist(cpu)) &amp;&amp; + !rcu_batch_before(rcu_ctrlblk.batch.completed,RCU_batch(cpu))) + return 1; + /* This cpu has no pending entries, but there are new entries */ + if (list_empty(&amp;RCU_curlist(cpu)) &amp;&amp; + !list_empty(&amp;RCU_nxtlist(cpu))) + return 1; + /* The rcu core waits for a quiescent state from the cpu */ //有新的宽限期到达, 需要重新关注该cpu在该宽限期内的静默状态 //或者 //在当前的宽限期内，该cpu还未进入静默状态 + if (RCU_quiescbatch(cpu) != rcu_ctrlblk.batch.cur || RCU_qs_pending(cpu)) return 1; - else - return 0; + /* nothing to do */ + return 0; } 可以看到在判断是否需要处理静默状态时，不再访问rcu_cpu_mask 我们再来看下rcu_check_quiescent_state() 是怎么处理静默状态的: @@ -127,7 +161,19 @@ static void rcu_check_quiescent_state(void) { int cpu = smp_processor_id(); - if (!cpu_isset(cpu, rcu_ctrlblk.rcu_cpu_mask)) //==(1)== + if (RCU_quiescbatch(cpu) != rcu_ctrlblk.batch.cur) { + /* new grace period: record qsctr value. */ + RCU_qs_pending(cpu) = 1; + RCU_last_qsctr(cpu) = RCU_qsctr(cpu); + RCU_quiescbatch(cpu) = rcu_ctrlblk.batch.cur; + return; + } + //==(2)== + /* Grace period already completed for this cpu? + * qs_pending is checked instead of the actual bitmap to avoid + * cacheline trashing. + */ + if (!RCU_qs_pending(cpu)) return; //==(3)== 当判断有新的宽限期到达时，将RCU_quiescbatch更新为新的宽限期版本,并置位 RCU_qs_pending 当cpu在该宽限期内不再处于静默状态时, 则不需要再处理. 直接返回 说明该cpu在该宽限期在之前未处于静默状态，需要继续判断现在是否已经进入静默状态 @@ -135,27 +181,19 @@ static void rcu_check_quiescent_state(void) * we may miss one quiescent state of that CPU. That is * tolerable. So no need to disable interrupts. */ - if (RCU_last_qsctr(cpu) == RCU_QSCTR_INVALID) { - RCU_last_qsctr(cpu) = RCU_qsctr(cpu); - return; - } if (RCU_qsctr(cpu) == RCU_last_qsctr(cpu)) return; //==(1)== + RCU_qs_pending(cpu) = 0; - spin_lock(&amp;rcu_ctrlblk.mutex); - if (!cpu_isset(cpu, rcu_ctrlblk.rcu_cpu_mask)) - goto out_unlock; - - cpu_clear(cpu, rcu_ctrlblk.rcu_cpu_mask); - RCU_last_qsctr(cpu) = RCU_QSCTR_INVALID; - if (!cpus_empty(rcu_ctrlblk.rcu_cpu_mask)) - goto out_unlock; - - rcu_ctrlblk.curbatch++; - rcu_start_batch(rcu_ctrlblk.maxbatch); + spin_lock(&amp;rcu_ctrlblk.state.mutex); + /* + * RCU_quiescbatch/batch.cur and the cpu bitmap can come out of sync + * during cpu startup. Ignore the quiescent state. + */ //==(2)== + if (likely(RCU_quiescbatch(cpu) == rcu_ctrlblk.batch.cur)) + cpu_quiet(cpu); -out_unlock: - spin_unlock(&amp;rcu_ctrlblk.mutex); + spin_unlock(&amp;rcu_ctrlblk.state.mutex); } cpu 在该宽限期已经处于静默状态，置位RCU_qs_pending() 走到这里，说明cpu已经处于静默状态，但是时第一次进入该函数，需要将cpu 在 rcu_cpu_mask中移除: +/* + * cpu went through a quiescent state since the beginning of the grace period. + * Clear it from the cpu mask and complete the grace period if it was the last + * cpu. Start another grace period if someone has further entries pending + */ +static void cpu_quiet(int cpu) + { + cpu_clear(cpu, rcu_ctrlblk.state.rcu_cpu_mask); + if (cpus_empty(rcu_ctrlblk.state.rcu_cpu_mask)) { + /* batch completed ! */ //表示该宽限期结束 + rcu_ctrlblk.batch.completed = rcu_ctrlblk.batch.cur; + rcu_start_batch(0); } - /* Can't change, since spin lock held. */ - active = nohz_cpu_mask; - cpus_complement(active); - cpus_and(rcu_ctrlblk.rcu_cpu_mask, cpu_online_map, active); } 所以经过该改动后，在每个宽限期内, 每个cpu 只会读写各一次rcu_cpu_mask. 大大 减少了cacheline trash 还有高手? 即便是做了上面的优化, rcu_cpu_mask仍然会有可扩展性的问题. 主要原因在于其变量在 多个cpu之间共享, 虽然上面的patch已经大大减少了访问次数, 但是, clear(rcu_cpu_mask) 在自旋锁下处理. 当临界区较小时，会造成严重的争用。 TODO, 关于rcu 和CPU 节能, 之后分析 参考链接 LWN: Hierarchical RCU 相关 commit Read-Copy Update infrastructure 1477a825d7e6486a077608c7baf6abbb6f27ed95 Dipankar Sarma dipankar@in.ibm.com Tue Oct 15 05:40:46 2002 -0700 percpu: convert RCU c12e16e28b4cf576840cff509caf0c06ff4dc299 Dipankar Sarma dipankar@in.ibm.com Tue Oct 29 23:31:27 2002 -0800 DESC: This patch convers RCU per_cpu data to use per_cpu data area and makes it safe for cpu_possible allocation by using CPU notifiers. Hotplug CPUs: Read Copy Update Changes 211b2fcef6366298877f1a8c0ba95d43db86ef85 Rusty Russell rusty@rustcorp.com.au Thu Mar 18 16:03:35 2004 -0800 s390: no timer interrupts in idle. 1bd4c02c645161959a69be858ee1efc4d0273507 Martin Schwidefsky schwidefsky@de.ibm.com Mon Apr 26 09:00:52 2004 -0700 rcu lock update: Add per-cpu batch counter 5c60169a01af712b0b1aa1f5db3fcb8776b22d9f Manfred Spraul manfred@colorfullife.com Wed Jun 23 18:49:33 2004 -0700 rcu, debug: detect stalled grace periods 67182ae1c42206e516f7efb292b745e826497b24 Paul E. McKenney paulmck@linux.vnet.ibm.com Sun Aug 10 18:35:38 2008 -0700 rcu: RCU-based detection of stalled CPUs for Classic RCU 2133b5d7ff531bc15a923db4a6a50bf96c561be9 Paul E. McKenney paulmck@linux.vnet.ibm.com Thu Oct 2 16:06:39 2008 -0700 " }, { "title": "intel tdx (spec)", "url": "/posts/tdx-spec/", "categories": "tee, arch_intel", "tags": "tee, tdx", "date": "2025-12-30 14:00:00 +0800", "content": "overflow add new operation mode Secure Arbitration Mode (SEAM) 是对于VMX 架构的扩展。在 vmx root/non-root operation(我们只有称为 legacy vmx XXX operation)下新增了两组模式: SEAM VMX root operation SEAM VMX non-root operation SEAM VMX root operation 中托管了成为 Intel Trust Domain Extensions(Intel TDX) 模块, 用于管理 TD虚拟机(可以理解为机密虚拟机). Intel TDX 模块实现了对TD虚拟机的: operation build tear down start execution 而VMM 提供TD 所需的内存资源，并通过intel TDX 模块提供的API 来调用TD。 和legacy VMX operation 类似, 在 SEAM VMX root operation 中通过 launched/resumed 操作， 切换到 SEAM VMX non-root operation. 而运行在SEAM mode下的虚拟机, 则受 SEAM 扩展保护，防止host VMM和non-SEAM 下的软件访问或修改 (confidentiality and integrity) TD memory/CPU state. 另外 SEAM VMX root operation 除了运行 Intel TDX module之外，也会运行Intel Persistent SEAMLDR(Intel P-SEAMLDR) 来load &amp; update Intel TDX module seam vmx root operation run in SPEICAL memory range 上面提到的运行在SEAM VMX root operation 的两个module, 均运行在 由 SEAM range register (SEAMRR) 所定义的地址范围中。这段预留的内存需要platform owner 通过BIOS 设置。 SEAMRR range被划分为两个 sub-ranges: MODULE_RANGE P_SEAMLDR_RANGE MODULE_RANGE 用于P-SEAMLDR安装(其实不仅是安装，还包括安装前的measure, verify) Intel TDX module, 该过程发生在系统启动，或者RUNTIME。 而P_SEAMLDR_RANGE用于 NP-SEAMLDR安装 P-SEAMLDR. how to switch between SEAM mode &amp; LEGACY mode 当 VMM 想要切到 SEAM mode时，需要执行SEAMCALL指令。而SEAM mode要切到VMM 需要执行SEAMRET指令。 另外, 上面提到SEAM vmx-root operation运行两个module, VMM如何选择将进入 哪个module呢? 通过调用SEAMCALL时，指定EAX: bit63(0): MODULE_RANGE bit63(1): P_SEAMLDR_RANGE 对于VMM管理TD而言, 其控制权的转移增加了额外的转换: VMM-&gt;TD legacy VMX root -- SEAMCALL =&gt; SEAM VMX root -- vmlanched/vmresume =&gt; SEAM VMX non-root TD-&gt;VMM SEAM VMX non-root -- vm exit =&gt; SEAM VMX root -- SEAMRET =&gt; legacy VMX root 如下图所示: INTEL TDX module and intel P-SEAMLDR module Intel TDX module和Intel P-SEAMLDR 均运行在SEAMRR接口定义的内存范围。并且 对该范围内的访问仅限于SEAM VMX root operation。另外，在SEAM VMX root operation下只能运行SEAMRR范围内的代码，如果运行其他地址的代码，会导致 不可中断关机… IA32_MTRRCAP中的SEAMRRbit 15位表示是否支持SEAMRR接口, 用于划出一个受限制 的内存区域，这个内存区域被 SEAM vmx-root operation下的TDX module使用. base 地址以32 位对齐。另外，MASK寄存器有lock bit(10) 该bit有BIOS配置完 寄存器的其他字段再设置。设置lock bit后，再对相关寄存器进行写操作则会触发 #GP。 重头戏来了, 在base地址偏移 4K处，存放一个VMCS的数组，用作legacy vmx root operation 到 SEAM vmx root operation 切换，用于save/restore 各自模 式的上下文. 既然VMCS位于SEAMRR，那么SEAM vmx root就类似于VMM, 而legacy vmx root 类似于vm. SEAM -&gt; Legacy: 相当于触发了 vmentry. Legacy-&gt; SEAM: 相当于触发了 vm-exit. 每个cpu对应的 tranfer vmcs地址为: IA32_SEAMRR_PHYS_BASE + 4096 + CPUID.B.0.EDX [31:0] * 4096 NOTE 但是我们需要思考下, VMCS 除了有 state area用于保存恢复上下文的区域 , 还有VM-exit control, VM-entry control等等和虚拟机强项关的区域。 这些区域会在这个过程中起作用么。 个人认为不会。这里只是借用了VMCS 的格式。有待进一步考究。 不过，从VMCS的归属来看, SEAM VMX root operation 对legacy VMX root operation有着绝对的控制权，前者不高兴了，可以随意破坏后者的上下文。 而后者没有这个权利。 SEAMRR是划分MODULE_RANGE区间，用于Intel TDX module, 还有一个 module – P-SEAMLDR, 其使用 P_SEAMLDR_RANGE, 切换到该模式（前面讲过 如何切换) 应该如何保存上下文呢? 答案也是通过VMCS，但和Intel TDX module不同，P-SEAMLDR 只能有一个cpu运行。 那VMCS结构需要一个就够了. P-SEAMLDR 模块用于在 SEAMRR 的MODULE_RANGE中加载 TDX module, 并初始化 和TDX module相关的 tranfer VMCS。 而NP-SEAMLDR 则会帮助初始化SEAM range,并初始化 P-SEAMLDR transfer VMCS, load P-SEAMLDR 到P_SEAMLDR_RANGE中 The OS can launch the NP-SEAMLDR ACM using the GETSEC[ENTERACCS] instruction if the SEAMRR range enable bit (bit 11) of IA32_SEAMRR_PHYS_MASK MSR is 1. EMMM… 为什么os会调用这个… 难道是更新 P-SEAMLDR? SEAM VMX ROOT OPERATION TDX module 帮助给TD VM 进行资源分配, 并且会切换到 SEAM VMX non-root operation 来运行TD VM。TDX module使用 MK-TME 技术，用于保护TD VM 私有数据的机密性和完 整性。 TDX private KeyID TDX 为每个TD VM 分配不同的MK-TME keyID 用来加密内存，另外, KeyID 分配给 TD 的情 况下，不可信软件不应被允许读取或写入内存! 完整性是怎么保证的呢? Intel Trust Domain Extensions 允许预留一定数量的KeyID 给 TD VM 用。在支持 SEAM 的soc上, MK-TME也会保证该内存的完整性。 Intel TDX 在这里搞得相当复杂下面是自己的理解 (参考2 第16章) 以Non-ACT Platforms: Cryptographic Integrity (Ci)为例, 每个cacheline 对应message authentication code (MAC)(用于校验该cacheline完整性) 的元数据, 当在 SEAM non-root operation 下使用 MK-TME private key修改写内 存时，cacheline中的数据其实是解密的，但是 MK-TME 会去private key计算 cacheline data的MAC 并保存到元数据, 并标记该cacheline的 TD-owner元数据为1. 这些元数据也会随着cacheline flush，最终会保存在内存元数据中，受ecc保护… 此时假设 legacy VMX-root operation 去修改这块内存, 此时其不会检查TD-owner, 直接修改内存, 此时会使用shared key重新计算MAC, 并保存到cache元数据中，接下来, SEAM non-root operation 下如果在访问这段内存， MK-TME会重新计算MAC 并和元 数据中的MAC做比对。如果不相同，则触发Integrity Check failed, 标记cacheline被 标记为poisoned, 并返回值为0. 防止SEAM non-root operation继续使用该cache. MKTME 支持最多KeyIDs 数量由IA32_TME_CAPABILITY MSR 枚举, 但是激活的数量以及分赔给 TDX预留keyid 的数量则有下面寄存器配置: IA32_TME_ACTIVATE MK_TME_KEYID_BITS(35:32): 分配给MKTME用于KeyID的物理地址位数 TDX_RESERVED_KEYID_BITS(39:36): 这些位被保留用于 Intel TDX , 用来编码TDX 私有KeyID 而配置工作由BIOS去做，BIOS分配之后的各个密钥的数量（shared，tdx own) 则可以通过 读下面MSR获取: IA32_MKTME_KEYID_PARTITIONING NUM_MKTME_KIDS(31:0) : 激活 MKTME 密钥ID总数 NUM_TDX_PRIV_KIDS(63:32): 激活TDX私有密钥数量 MKTME KeyID 的密钥使用 PCONFIG 指令进行编程。PCONFIG 指令设计为仅允许在 SEAM 中 对 TDX 私有 KeyID 进行密钥编程。 地址划分如下: MAXPHYADDR 表示当前枚举的物理地址最高位: [MAXPHYADDR - 1, MAXPHYADDR - n]: 用于 MKTME keyid. 而在这个地址空间中对shared keyid和分配给TDX 的private KEYID做了切分 [MAXPHYADDR-1, MAXPHYADDR-L] all zero: shared if not: TDX private KEYID MKTME 用于 TD 内存保护时，某些维护操作可能需要 Intel TDX 模块回写缓存并使缓存失 效。对于某些维护操作，Intel TDX 模块可能需要回写缓存，但不需要使缓存内容失效。 Intel TDX 模块可以使用 WBINVD/WBNOINVD 指令来辅助执行这些操作。 emmm, 这个没有想出来有什么场景 具体方法是, 软件通过EDX: EAX 指定一块内存区域, 如果 WRMSR MSR_WBINVDP : 回写并invalid MSR_WBNOINVDP : 只回写 这两个寄存器只能在SEAM mode下操作 软件可以使用 RDMSR 调用 MSR_WBINVDP 或 MSR_WBNOINVDP 读取最大缓存子块数 (NUM_CACHE_SUB_BLOCKS)。 指令解码如下: IF RDMSR IF inSEAM==0 THEN #GP(0); IF invoked from VMX load/store list THEN #GP(0); EDX:EAX = MAX_CACHE_SUB_BLOCKS; ENDIF IF WRMSR IF inSEAM==0 THEN #GP(0); IF invoked from VMX load/store list THEN #GP(0); IF EDX:EAX &gt;= MAX_CACHE_SUB_BLOCKS THEN #GP(0); Flush cache sub-block indexed by EDX:EAX IF ECX == MSR_WBINVDP THEN Invalidate cache sub-block indexed by EDX:EAX; ENDIF MEMORY TYPE seam vmx root operation: CR0.CD == 1 : effective memory type 为 UC CR0.CD == 0 : 则有效内存类型取决于所访问的物理地址和用于访问的 KeyID 如果要访问 SEAMRR，则 MTRR 不会对内存类型产生影响，访问的有效内存类型仅由 PAT 决定。 如果访问的是 SEAMRR 之外的物理内存，并且该访问使用了 TDX 私钥 ID， 则 MTRR 不会影响内存类型。内存类型仅由 PAT 决定。 如果要访问 SEAMRR 之外的物理内存，并且访问未使用 TDX 私钥 ID，则根据与物理 地址和 PAT 匹配的 MTRR 来确定访问的内存类型。 CR0.CD cache disable 明确强制指定内存类型的特殊操作（例如，快速字符串、MOVDIR64 等）将继续按照这些指 令定义的特殊内存类型运行。 Caching Translation Information(TLB) 在原有框架上增加了 SEAM 状态, 在 SEAM VMX-root operation, in-SEAM 为1. 在 SEAM VMX root operation 期间，逻辑处理器可能会缓存并使用针对线性地址的缓存映 射，这些线性地址由当前 CR3 寄存器的值（直接或间接引用的分页结构）派生，并将这些 映射关联到: Current VPID Current PCID (non-global translations) or any PCID (global translations) NOTE 这里想表达的意思是, SEAM VMX root operation, 有自己独立的tlb entry (这些entry 中in-SEAM =1) 为什么要这么做.. EVENT HANDLING SEAM VMX root operation 时, 处理器可以 inhibit NMI/SMI. 在 inhibit 状态下，他们 将被设置为挂起状态，并在inhibit状态解除后处理。在SEAM VMX root operation中 可以 解除对NMI, external interrupts inhibit. 在 SEAM VMX root operation 中，可以读取MSR_INTR_PENDING 来确认是否有挂起的事 件. 另外，和 legacy VMX root operation类似, 在SEAM VMX non-root operation中， 可以通过配置 TD VMCS 来改变对 NMI, SMI的 inhibit状态。 在 SEAM VMX non-root operation 期间发生的 SMI，或者在切换到 SEAM VMX non-root operation 之前已经处于挂起状态的 SMI，可能会导致VM-exit 并进入 SEAM VMX root operation，此时退出原因会被设置为“IO SMI” 或 “Other SMI”。如果该 SMI 是由machine check 触发的 SMI（MSMI），则退出限定的第 0 位应该被设置为 1。 在VM-exit 后，该 SMI 仍将保持挂起状态。 SEAMRET 切换到 legacy VMX root operation时, NMI SMI 的inhibit状态可以恢复到 SEAMCALL 时的inhibit状态, 如果未inhibit的话，任何待处理的NMI/SMI 都将被 deliver. MSR_INTR_PENDING: The reporting of these pending events in MSR_INTR_PENDING is designed not to be affected by: 大概的意思时，MSR_INTR_PENDING报告事件和事件inhibit是独立的, 虽然这些事件会被inhibit, 但是不会影响 MSR_INTR_PENDING 报告 inhibit 事件。 EFLAGS.IF. NMI blocking. SEAM blocking of SMI, or NMI. SMI inhibited by SENTER. MOV-SS/POP-SS blocking. STI blocking SEAM VMX NON-ROOT OPERATION TD VM 运行在 SEAM VMX non-root operation, 本节主要讨论leagcy VMX non-root opearation 和 SEAM VMX non-root operation有何不同 SEAM VMX Non-Root Execution Controls 前面提到过, 每一个TD 都有一个相关联的 VMCS, 同legacy VMX non-root operation 功能相同。但是字段上有一些扩展: Share EPT Pointer: 用于指定共享EPT页表的指针。在SEAM VMX non-root operation中, 有两套EPT: VMCS EPTP 指向的private EPT VMCS shared-EPTP 指向的 shared EPT 该字段: 11:0 bit 被预留 (MAXPHYADDR - 1):12 bit 包含(MAXPHYADDR - 1): 12 位 4k-byte对齐的 EPT PML4/PML5 table (64, MAXPHYADDR) bit 被预留 TD-KeyID 32bit字段用于指定分配给TD的MKTME的密钥ID. 处理器使用这个KeyID来 访问有 EPTP 指向的EPT 系列页表. 也就是说, EPTP 以及 EPTP指向的一系列EPT页表中的物理地址字段都带有该KeyID Guest Physical Address Width(GPAW) : GPAW和EPT walk levels (4 or 5 level) 共同确定GPA宽度, 从而确定GPA中的 SHARED bit 位置. TD的GPA划分为两部分: shared GPA range SHARED bit 为1 使用 shared-EPTP 指向的Shared EPT translate GPA 处理器使用shared-EPT的 KeyID 来获取 shared GPA的访问权限 private GPA range. SHARED bit 为0 使用 EPTP 指向的 EPT translate GPA CR3, PDPTRs, and HLAT root pointe中的GPA 也使用 EPTP进行translate 处理器使用TD-KeyID 获取 private GPA的 访问权限. (因为由一个bit确定，所以各占一半) 怎么确定SHARED BIT位置: 使用4-level EPT, SHARED bit 永远是 bit 47 使用5-level EPT GPAW == 1 : bit 51 GPAW == 0 : bit 47 based execution control bit 17 设置为1 可以active, tertiary, processor-based execution control primary, processor based execution control. 而 IA32_VMX_PROCBASED_CTLS3 bit 5 则报告 GPAW execution control 是否能在 tertiary中使能。如果 GPAW execution control 可以被设置为1, 处理器才允许编程 VMCS中的Shared-EPTP和TD-KeyID字段。 Guest Physical Address Translation 有一些额外的条件会触发EPT misconfiguration 或者 EPT violation 或者page fault: 当GPA 使用 shared EPT, 并且entry is present 并且 页表中的 phyiscal address 的(MAXPHYADDR - 1, MAXPHYADDR - TDX_RESERVED_KEYID_BITS)被设置. 则触发EPT misconfiguration CPU’s maximum physical-address width (MAXPA) is 52, 而guest physical address width 为 48, 那么 GPA bit 51:48 中不全为 0 的访问可能会导致 EPT-violation， 并且即使“EPT-violation #VE” execution control 为 1，也不会被变异为#VE。 如果 CPU 的物理地址宽度（MAXPA）小于 48，并且SHARED bit 为第 47 位，GPA 位 47 将被保留，GPA 位 46:MAXPA 将被保留。在这样的 CPU 上，任何页面结构中的位 51:48 或位 46:MAXPA 设置会导致保留位页面错误。 Linear Address Translation 在 SEAM VMX non-root operation 中, 所有用于线性地址转换为GPA的 paging structures 都必须位于private GPA space. 如何CR3中的地址 或者引用另一个paging structure 的地 址的SHARED bit为1， 都会造成 page fault . MEMORY TYPEING 对于translated或者guest物理访问（未开分页）通常应该根据CR0.CD, PAT和 EPT memory type来决定。 某些操作强制使用某种memory type , (e.g., fast string, MOVDIR64,等), 旨在继续使用 其特定的memory type(强制使用的)) 使用TDX private KeyID访问VMCS相关的数据结构(MSR bitmap, VAPIC page等) CR0.CD == 1: UC otherwize: WB Caching Translations Information (略) 有点看不懂 OPERATION OUTSIDE SEAM 当不在 SEAM operation 中时, 物理地址中用于编码 TDX private KeyID 的位被保留为保留 位。 当在 SEAM operation 之外tranlate地址时，任何 paging structure 条目中的物理地址字段 使用了上述位时，处理器都会报一个reserved bit page fault exception 而 legacy EPT中如果使用上述位时，则产生EPT-violation 还有一些指令会直接使用物理地址，如果使用了上述位时，行为如下: 参考链接 Intel® Trust Domain CPU Architectural Extensions 343754-002US MAY 2021 Intel® Trust Domain Extensions (Intel® TDX) Module Base Architecture Specification Scalable Memory Protection in the Penglai Enclave " }, { "title": "rcu - overflow", "url": "/posts/rcu-overflow/", "categories": "os, synchronization", "tags": "os, synchronization, rcu", "date": "2025-12-29 20:26:00 +0800", "content": "background 本章节主要参考9 在介绍RCU之前，我们先来思考下，如何提升程序的性能? 一个最直接的方法是, 提升 并发量, 但是并发程序往往会造成多个线程(cpu) 访问同一个资源, 我们暂时先不考虑, 假设一个程序只有读者，每个读者都会去访问一个read-only list, 那么thread 数量和 吞吐关系图如下: 程序的性能会随着线程数量线性增长，这可真是简单粗暴的性能提升方法，但是生活并 不总是真么美好, read-only 场景很少，通常出现在科学计算中，更多的场景是 almost read-only。 于是我们希望, 在写入很少的场景下, 读者性能不受影响。 首先我们来看几种方案: lock-free lock-free(list with atomic shared pointers) 虽然是一种比较通用的方案, 其允许 多个写者, 但在read-only 的场景下也会有不小的开销，那在 write-rare的场景下就不太 适用，似乎读写锁更适用一些: 可以发现读写锁虽然比lock-free 性能要好，但在延展性上的表现还是比较差。 造成延展性问题的原因在哪呢？在于这些 同步机制的atomic 操作. 可以发现即便是在thread 为1时，使用图中的同步机制也会出现性能下降，个人猜测可能和 调度负载均衡相关，导致 ATOMIC 变量在各个cpu cacheline中均有副本。 这里 不清楚作者是不是想表达这个意思, 需要看视频确认 TODO 那我们能不能避免这些atomic操作呢? 于是出现了rcu。 RCU 基本概念 RCU (Read-Copy update) 是一种同步机制, 基本原理是, 将写动作分为下面几个步骤(正如 其完整的英文名所示): copy write update 即在写入时首先copy一个副本, 然后在该副本上完成write，在替换原来的就数据(update)。 该机制优秀的点在于其对读者访问特别友好, 因为在写者更新的过程中更新的是副本 (new_node), 读者读取的object(node) 不会被写者更改，所以在读者看来，自己读取 的时候，数据都是read-only的。 在读者端访问不需要加锁，不需要原子操作, 甚至不需要内存屏障(Alpha 除外) 但是对于写者就没有那么友好, 首先 copy 动作需要写者来做，另外，update后替换旧副本释放 旧副本资源，也增加了写者复杂度。如果是同步操作的话，甚至需要写者阻塞等待读临界区 完成. 我们接下来看下rcu具体的机制: 机制 RCU 的机制主要有以下三个部分1: Publish-Subscribe Mechanism (for insertion) Wait For Pre-Existing RCU Readers to Complete(for deletion) Maintain Multiple Versions of Recently Updated Objects (for readers) Publish-Subscribe Mechanism 在看rcu 的 insert 流程之前，我们先简单了解发布订阅机制（模式）概念: 发布订阅模式是一种消息传递模式, 用于将消息生成组件与消息消费组件解耦。其核心思想 是，发送者不与接受者直接通信，而是通过一个中间媒介（事件总线 / 主题中心）传递消息， 双方无需知道对方的存在，从而降低耦合度6, 7。 而对应于rcu而言，其中间媒介就是critical resource(可以理解为一个地址), 发布者和订阅 者通过相应的API去操作”中间媒介”. NOTE 上图来自2 而对于rcu而言对于 updater 和reader 之间也提出了一些要求: 读者可以看不到最新的更新，但是不能看到不完整的更新。 因为读者只想看到read-only的数据, 这个数据一旦发布后，就不能在更改。 例如下面的写者程序1 1 struct foo { 2 int a; 3 int b; 4 int c; 5 }; 6 struct foo *gp = NULL; 7 8 /* . . . */ 9 10 p = kmalloc(sizeof(*p), GFP_KERNEL); 11 p-&gt;a = 1; 12 p-&gt;b = 2; 13 p-&gt;c = 3; 14 gp = p; 写者需要负责让读者看到的new publish foo 为 (a: 1, b: 2, c: 3) 的组合，不能是一个 中间状态。聪明的小伙伴可以想到，这是一个典型的内存模型（memory order)的问题。也就是 让gp = p的这个动作一定发生在p数据结构初始化之后。所以gp = p这个动作需要内存屏障进行 封装, 内核中用rcu_assign_pointer(gp, p); 实现. (关于rcu_assign_pointer(, )的实现(内存屏障相关)，在 TODO TODO TODO TODO 中详细描述。 对于读者来说，也需要保证一定的内存顺序1, 例如下面的顺序: 1 p = gp; 2 if (p != NULL) { 3 do_something_with(p-&gt;a, p-&gt;b, p-&gt;c); 4 } 这个程序看起来有内存顺序的问题么? 我们期望的是p-&gt;a, p-&gt;b, p-&gt;c的组合是一致 性的，但是这三个值看起来是在获取到p的值之后才能获取, 而写者保证了p赋值时， p-&gt;x已经更新, 所以直观上来说，这个程序不会有问题。不需要任何内存屏障。 但是, 实际上并不是这样。文章1中提到了两个场景: alpha 架构推测执行优化 value-speculation compiler optimizations 总之，在这些场景下，会在获取p 值之前，先获取到p-&gt;a, p-&gt;b, p-&gt;c 的值. value-speculation compiler optimizations 优化比较直观，其会先推测p的值, 然后 获取p-&gt;a, p-&gt;b, p-&gt;c的值，然后在获取实际的 p的值，对比自己的猜测是否正确。 作者在10 处, 给出了详细的解答。 对于下面代码: 11 p-&gt;a = 1; 12 p-&gt;b = 2; 13 p-&gt;c = 3; 14 gp = p; 关于编译器: 我们将gp = p 这条代码提前至11行之前运行。但是gp的值可能存在寄存器中， 但是为了执行11-13行，可能将寄存器值溢出（寄存器不够用了). 但是为了能够安全的 提前执行14行(单线程)，需要编译器判断gp 的地址和 p-&gt;a, p-&gt;b, p-&gt;c这段区域不重 叠，然而编译器往往有这种能力，并且大部分情况下也不重叠。这种情况是有可能发生的。 规避的方法是，合理使用 barrier() 编译器屏障， 或者 volatile 关键字 (首先14 提到11行之前运行本身单线程下就没什么问题，但是需要关注地址重叠问题) 关于CPU: 可以设想A cpu 执行了 该段代码，此时STORE p-&gt;a, p-&gt;b, p-&gt;c, gp被顺序写入write buffer, 但此时, gp 的cacheline在 A cpu 中存在，但是 p cacheline 在其他cpu 中存在,在A cpu 中不存在, 此时STORE p-&gt;a,b,c 就会较快的执行完，过一段时间之后， p的cacheline 才从其他的cpu获取到…(回想下 directory cache conherence) 我们上面定义的读操作定义为Publish, 写操作为Subscribe. rcu可以应用于各个场景， 其一个典型场景是链表。Linux 为链表定义了一套 RCU的版本， 并将这些publish, Subscribe 嵌入到这些代码中。见附录1。 正如9 的标题 – read, copy, update... Then what? 。是的, 更新完之后 还需要做什么么。但其实接下来的工作才是重点: 当我们将新数据替换掉老数据, 老数据该 怎么处理? 释放! 但是什么时候释放呢? 虽然rcu 的全称为read copy update, 但这只是整个rcu机制实现的很小的一部分. In its most basic form, RCU is a way of waiting for things to finish.1 Wait For Pre-Existing RCU Readers to Complete rcu 的机制特色在于等待事件完成的方法, 等待事件完成的方法有很多，例如引用计数, 读 写锁，事件等等。但是RCU 最大的优点在于, 它可以等待很多事件, 无需显式关注每一事件。 也无需担心显式跟踪方案中固有的性能下降、可扩展性限制、复杂的死锁场景以及内存泄漏 风险1。 例如引用计数, 每个object 有一个引用计数, 如果要释放object, 需要关注到每个object 的引用计数是否为0。这可能会带来很多问题，例如如果有多个CPU都用atomic更改refcount, object 一多，可能会引起大量的cacheline抖动。而rcu 从另一个更高的角度 – 关注全局 性质的读临届区，而读临界区中可能包含多个object的访问，这些object 可能有多个 delete的事件。 所以rcu 并不是直接等待事件完成，而是简接通过判断读临界区完成，而间接的判断事件完成 的方法。 如上图所示, writer动作分为了三个步骤: removal: 对应于我们前面提到的publish流程 Grace Period: 宽限期, 宽限期开始会记录正在运行的reader，一直等待这些reader中的最后 一个离开临界区 reclaimation: 释放 object 需要注意的是，宽限期只会等待在宽限期执行之前已经存在的readers，而不会等待在宽限 期开始之后的readers。 使用下图举例: prepare new data: 初始状态为p指针指向data 1, 此时write copy 了一个副本 (data 2)，在副本中准备新数据 publish new data: 发布新数据，将p指针指向data2 wait for readers to leave: 在发布新数据后，准备将老数据删除。这时，还有一些reader 正在读取老数据(p1)(当然 不一定正在读取p1, 而是在读临界区中, 执行了rcu_read_lock(), 但是还未执行 rcu_read_unlock()), 需要等这些readers退出临界区。 safe to delete data1: 等待最后一个reader退出临界区后, 释放data 1 从上图中可以看出，我们需要关注reader 进入读临界区的 时间是否在 publish new data 之后(图中的 readers p1 表示在publish之间，而 p2 表示在 publish之后), 如果是在 publish 之后，对于data1 的释放就可以不用等待这些readers。 Maintain Multiple Versions of Recently Updated Objects (for readers) 上面提到, 在数据更新过程中，对于读者而言可能会暂时读到old version data。这时，整 个系统中，不同的reader可能读取到的数据不同（对于单写者rcu而言，就会存在两个版本 数据). 本节主要举两个例子来看下: Example 1: Maintaining Multiple Versions During Deletion 首先 看删除的例子: 1 p = search(head, key); 2 if (p != NULL) { 3 list_del_rcu(&amp;p-&gt;list); 4 synchronize_rcu(); 5 kfree(p); 6 } 图中的三元组表示p-&gt;a, p-&gt;b, p-&gt;c的值. 红色外框表示元素正在有人引用。 p 表示第 一行 search 后要删除的元素。 第三行执行结束后, 会在链表中删除该成员。但是p指向的object 还未释放, 因为此时 可能有读者在引用: 此时不同的读者可能会”看到”, 或者”看不到” p 指向的object。呈现出两个版本的链表 共存。 当所有的读者退出读临界区, 表示所有的读者将不再占用p指向的object。此时第四行 synchronize_rcu()返回, 此时链表恢复为单一版本: 紧接着, 代码执行到5, 释放p指向的内存。 Example 2: Maintaining Multiple Versions During Replacement 1 q = kmalloc(sizeof(*p), GFP_KERNEL); 2 *q = *p; 3 q-&gt;b = 2; 4 q-&gt;c = 3; 5 list_replace_rcu(&amp;p-&gt;list, &amp;q-&gt;list); 6 synchronize_rcu(); 7 kfree(p); 和delete 比较类似: 初始状态: kmalloc()申请一个新节点: copy p指向内存到新的object 更新object q-&gt;b=2, q-&gt;c=3, 此时执行synchronize_rcu()等待宽限期结束: 宽限期结束后，synchronize_rcu()返回，此时没有读者再对p指向的object 有引用: 调用kfree() 释放p指向内存: 总结 对于almost read-only场景, 往往期望读操作的性能非常高（接近于read-only), 而对写 操作的性能不怎么要求. 使用rcu算法可以达到非常好的效果。 RCU在更新数据时，采用了类似于 发布订阅机制，用来结耦reader和writer流程。 RCU虽然全称为 read copy and update, 但是其主要的工作并不在这，rcu 本身是一种 等待事件完成的算法, 其主要的工作量是在等待要删除的object在何时释放。 NEXT 接下来我们来看下具体的RCU实现，包括: kernel Userspace(QEMU imitating from librcu) 附录 1. Linux rcu list add static inline void list_add_rcu(struct list_head *new, struct list_head *head) { __list_add_rcu(new, head, head-&gt;next); } static inline void __list_add_rcu(struct list_head *new, struct list_head *prev, struct list_head *next) { if (!__list_add_valid(new, prev, next)) return; new-&gt;next = next; new-&gt;prev = prev; rcu_assign_pointer(list_next_rcu(prev), new); next-&gt;prev = new; } new是要添加的成员, 所以在publish之前，需要将其成员初始化完整. 那么问题来了, publish动作是对应是 对一个指针更新？那选择那个指针更新呢? prev-&gt;next Linux链表遍历是正向遍历，也就意味着，读者在遍历过程中只读取entry-&gt;next, 所以其保证，该object 初始化完成后(包括其object.list), 对 prev-&gt;next 执行 合理的publish动作即可. list_next_rcu 也有RCU的版本: #define list_next_rcu(list) (*((struct list_head __rcu **)(&amp;(list)-&gt;next))) # define __rcu __attribute__((noderef, address_space(__rcu))) 这里更像是做了一些编译器check TODO loop list_for_each_rcu()的代码: #define list_for_each_rcu(pos, head) \\ for (pos = rcu_dereference((head)-&gt;next); \\ !list_is_head(pos, (head)); \\ pos = rcu_dereference(pos-&gt;next)) 代码非常直接在获取entry-&gt;next时, 使用rcu_dereference()接口 del 关于del大家可以简单思考下, 其需要内存屏障来保序么? add 操作需要内存屏障是因为防 止object 更新在发布之后执行。而del操作不会在更改这个object。只是等着释放了。 static inline void __list_del(struct list_head * prev, struct list_head * next) { next-&gt;prev = prev; WRITE_ONCE(prev-&gt;next, next); } 这里的WRITE_ONCE只是voliate 原语, 防止编译器优化, 例如读写撕裂(Read/write \"tearing\") 8。 update update 怎么做呢? 也比较简单, 需要将上面几个步骤结合下: copy 旧数据到副本中 在副本中 write delete 旧数据 update 新数据 参考链接 What is RCU, Fundamentally? Linux 核心設計: RCU 同步機制 QEMU RCU implementation Using RCU (Read-Copy-Update) for synchronization Hierarchical RCU 发布订阅模式详解 Publish–subscribe pattern WRITE_ONCE in linux kernel lists Read, Copy, Update… Then what? Forcing the compiler and CPU to execute assignment statements in order ? " }, { "title": "intel memory encryption technologies", "url": "/posts/intel-memory-encryption/", "categories": "tee, arch_intel", "tags": "tee, tme, mktme", "date": "2025-12-29 16:10:00 +0800", "content": "introduction TME Total Memory Encryption (TME) – the capability to encrypt the entirety of physical memory of a system. This capability is typically enabled in the very early stages of the boot process with a small change to BIOS and once configured and locked, will encrypt all the data on external memory buses of an SoC using the NIST standard AES-XTS algorithm with 128-bit keys or 256-bit keys depending on the algorithm availability and selection. The encryption key used for TME uses a hardware random number generator implemented in the Intel SoC, and the keys are not accessible by software or using external interfaces to the Intel SoC. TME capability is intended to provide protections of AES-XTS to external memory buses and DIMMs. The architecture is flexible and will support additional memory protection schemes in the future. This capability, when enabled, is intended to support (unmodified) existing system and application software. Overall performance impact of this capability is likely to be relatively small and is highly dependent on workload intel TME 引入自 3rd generation Intel ® Xeon ® Scalable Processor Family 用来 提供内存加密支持。全内存加密 (TME) 功能可对系统的全部物理内存进行加密。此功能通 常在启动过程的早期阶段通过对 BIOS 进行少量更改即可启用。配置并锁定后，TME 将使用 NIST 标准的 AES-XTS 算法，根据算法的可用性和选择情况，使用 128 位或 256 位密钥， 对 SoC 外部内存总线上的所有数据进行加密。 TME 使用的加密密钥由 Intel SoC 中实现 的硬件随机数生成器生成，软件或通过 Intel SoC 的外部接口均无法访问这些密钥。TME 功能旨在为外部内存总线和 DIMM 提供 AES-XTS 加密保护。该架构具有灵活性，未来将支 持其他内存保护方案。启用此功能后，旨在支持（未经修改的）现有系统和应用程序软件。 此功能对整体性能的影响可能相对较小，并且高度依赖于工作负载。 MKTME Total Memory Encryption-Multi-Key (TME-MK) builds on TME and adds support for multiple encryption keys. The SoC implementation supports a fixed number of encryption keys, and software can configure the SoC to use a subset of available keys. Software manages the use of keys and can use each of the available keys for encrypting any page of the memory. Thus, TME-MK allows page granular encryption of memory. By default, TME-MK uses the TME encryption key unless explicitly specified by software. In addition to supporting a CPU generated ephemeral key (not accessible by software or using external interfaces to the SoC), TME-MK also supports software provided keys. Software provided keys are particularly useful when used with non-volatile memory or when combined with attestation mechanisms and/or used with key provisioning services. In a virtualization scenario, we anticipate the VMM or hypervisor managing the use of keys to transparently support legacy operating systems without any changes (thus, TME-MK can also be viewed as TME virtualization in such a deployment scenario). An OS may be enabled to take additional advantage of the TME-MK capability both in native and in a virtualized environment. When properly enabled, TME-MK is available to each guest OS in a virtualized environment, and the guest OS can take advantage of TME-MK in the same way as a native OS. TME-MK 则在TME基础上实现, 增加了对多个密钥的支持。SoC 实现支持固定数量的加密密钥， 软件可以配置 SoC 使用可用密钥的子集。软件管理密钥的使用，并可以使用每个可用密钥 加密内存的任何页面。因此，TME-MK 允许对内存进行页级加密。默认情况下，TME-MK 使用 TME 加密密钥，除非软件明确指定。除了支持 CPU 生成的临时密钥（软件无法访问，也无 法通过 SoC 的外部接口访问）之外，TME-MK 还支持软件提供的密钥。软件提供的密钥在与 非易失性内存一起使用，或与认证机制结合使用和/或与密钥配置服务一起使用时特别有用。 在虚拟化场景中，我们预期 VMM 或虚拟机管理程序将管理密钥的使用，从而透明地支持旧 版操作系统而无需任何更改（因此，在这种部署场景下，TME-MK 也可被视为 TME 虚拟化。 操作系统可以启用 TME-MK 功能，以便在原生环境和虚拟化环境中都能充分利用其优势。正 确启用后，虚拟化环境中的每个客户操作系统都可以使用 TME-MK，并且客户操作系统可以 像原生操作系统一样利用 TME-MK。 总结 TME 是intel 内存加密的早期版本，其支持一个key。可以允许对所有物理内存进行 AES-XTS 加密。该功能算是一个基础功能，为之后其他功能的引入奠定了基础。 而 TME-MK 对比 TME 的最大的改进是，引入了多个key的管理。而其一个很重要的应用 场景是虚拟化，可以为不同的虚拟机分配不同的密钥，进一步增加安全性。 TME 简介 加密引擎位于direct data path to external memory buses(总之卡在接口处, upstream 是解密downstream 是加密), 上半部分 主要是cache等都是解密的。而下面部分，主要是 DRAM 为加密。 另外系统如果使用了NVRAM, 可以让NVRAM视为 DRAM, 也可以使用密钥进行加披靡 MKTME简介 硬件架构基本与TME相同。不同点在于 MKTME 是使用多个key进行加密。如上图所示: 上图是一个虚拟化环境有两个VM, VM1 VM2，另外有0,1,2,3 四个keyid。其中 key1, key2 分别是 VM1, VM2的私有key，用来加密内部不共享的内存。而key0为 TME 的key，可 以对任何页面使用(个人认为一般用户和host 访问的share page), key3用于加密 vm1, vm2共享的内存。 另外，这种机制不仅用于普通页表，也用于 IA 页表 IOMMU 页表。(IA页表个人认为普通页表) Memory Encryption Emulation and Control Registers emulation TME 和 TME-MK 通过一些MSR将功能暴露给软件。BIOS通过MSR激活该功能，必须在启动初期选择 用于 TME-MK 的密钥数量。激活后，链接到CPU的所有内存都会使用AES-XTS加密。另外, TME 可以支持encryption bypass, 使用该功能的话，所有使用KeyID0 的访问均绕过加 解密。 TME CPUID.TME (CPUID.(EAX=07H, ECX=0H): ECX[13]) 枚举下面MSR: IA32_TME_CAPABILITY – Address 981H IA32_TME_ACTIVATE – Address 982H IA32_TME_EXCLUDE_MASK – Address 983H IA32_TME_EXCLUDE_BASE – Address 984H TME-MK IA32_TME_CAPABILITY 将进一步枚举TME, TME-MK 的功能, TME-MK 由 BIOS 使用 IA32_TME_ACTIVATE MSR 启用/配置。TME-MK 需要 TME，因此必须启用 TME 才能启用 TME-MK。 IA32_TME_CAPABILITY 31: 用于枚举前面提到的TME旁路（不过这里是枚举，是否启用得看activate 配置, 下面的字段同理) 35:32 : MK_TME_MAX_KEYID_BITS 可分配用作多密钥内存加密密钥标识符的位数 （如果不支持TME-MK, 该字段设置为0) 50:36 : MK_TME_MAX_KEYS : 表示可供使用的最大按键数量 Memory Encryption Configuration and Status Registers IA32_TME_ACTIVATE 0 : 锁定位，写过一次该msr就会置1 1: 是否启用TME 31 : Bypass enable bivt 35:32: 用于 TME-MK 的key 的数量。注意，不能大于IA32_TME_CAPABILITY MK_TME_CORE_ACTIVATE (略, 没看懂干啥的) Exclusion Range MSRs TME 和 TME-MK（ 仅限 KeyID=0 ）支持一个用于特殊情况的排除范围。此 MSR 中指定 的物理地址范围不应用本文档中描述的内存加密。 该功能主要用于系统访问bios 初始化的一些内存（bios在早起初始化时，还没有加密，所以当时 init的内存，是未加密的内存). MASK(MAXPHYSADDR-1:12) 和 BASE(MAXPHYSADDR-1:12) 两者构造了一段地址空间，用于 keyID0不加密区间 Runtime Behavior of TME-MK Changes to Specification of Physical Address 同前面所述, KeyID 存放在[Max_pa bit , 0] 的高位部分: IA Paging 不使用 EPT的情况下，IA 分页entry 中的字段从 MAX_PA 开始的高位将被重新用作 keyID 位。类似地，CR3 中物理地址的高位也将以相同的方式处理。 当 EPT 处于活动状态时，IA 分页不用 HPA ，而是用 GPA。GPA 不会被 TME-MK 修改，并 将继续像启用 TME-MK 之前一样索引到 EPT 页表遍历中。 EPT Paging 启用 EPT 时，EPT entry 也是这样使用 KeyID 。和 CR3 类似，EPTP 中pa 也是相同方式处理。 请注意，guest 也可以在 IA 中使用 KeyID，并且 EPT 会使用完整的 guest PA（包括 KeyID）。 其他地址 例如 VMCS, physically addressed bitmaps也是采用类似的方式寻址。 TME-MK Key Programming (略) Software Life Cycle: Managing Pages with KeyID (略) 参考资料 &lt;&lt;Intel ® Architecture Memory Encryption Technologies Specification&gt;&gt; " }, { "title": "non-scalable VS scalable spinlock", "url": "/posts/non-scalable-vs-scalable-spinlock/", "categories": "synchronization", "tags": "synchronization, spinlock", "date": "2025-12-26 11:00:00 +0800", "content": "自旋锁是一种会让尝试获取它的线程陷入循环 （“自旋”）并不断检查锁是否可用的锁 1。 和mutex 不同，mutex 可以睡眠，将cpu让渡给其他的程序，而自旋锁则是占据着cpu资源忙 等。忙等最主要的优点是，避免了调度所带来的上下文开销, 可以提升等锁进程获得锁的延 迟。另外，如果加锁的临界区很小，自旋锁忙等所带来的开销，可能会小于上下文切换的开 销，自旋锁的收益就会非常大。所以，自旋锁适用于临界区小的场景。 overflow 本文主要是来讲述, spinlock 的可伸缩性(scalable). 可扩展性是指系统处理不断增长的 工作量的能力。软件系统的可扩展性定义之一是，可以通过向系统添加资源来实现 2. 而对于spinlock而言, 如何增加其工作量呢？ 增加并行调用spinlock的cpu数量 在介绍之前我们先思考下，自旋锁和 “smp”, “up” 关系3。首先按照自旋锁的 逻辑，自旋锁的临界区是不能睡眠的，这个动作非常危险，容易造成死锁。所以 UP下(单 CPU) 自旋锁是没有意义的。 所以自旋锁是服务于smp的。而随着cpu的发展，cpu的核心越来越多, 并行调用spinlock的 数量也大大增加。而传统的spinlock则在 smp 场景中体现出了 non-scalable。 接下来的章节，我们首先介绍一些硬件背景，包括 cache snoop method cost of cache coherence 而之后，便介绍因cache coherence 给spinlock带来的性能影响。最后，展示下scalable spinlock 带来的性能提升。 文本主要参考老黄和狗哥的文章4,5 以及Non-scalable locks are dangerous 论文6. cache coherence 关于cache coherence 很值得单独写一篇文章去总结( TODO ). 这里我们简单看下 snoop-based coherence non-scalable 问题。以及其改进版本 directory-based coherence 给spinlock 所带来的non-scalable问题。 本节内容(包括大量的图片) 均参考 CMU 15-418/618 lecture7, 8 为了降低cpu访问内存的延迟, 在cpu 和 内存之间加了一层cache。但是在smp架构下，每个 cpu都有自己的cache，所以相当于main memory 所存储的数据在各个cpu上都有了副本。 这回带来一致性问题。而cpu硬件实现了一套无需软件参与的缓存一致性协议，用来维护各 个 cpu cache之间以及和主存之间的一致性。 在早期的实现中，cpu数量不多。cache一致性使用snoop-based的方式，该方式的特点是 广播，而随着cpu数量越来越多，尤其是NUMA架构的兴起，广播的代价越来越大， snoop-based方式则变得non-scalable. 于是大佬们搞出了非广播的点对点的 directory-based coherence一致性协议。 snoop based snoop-based coherence 是在缓存状态改变时，通过广播MESI 消息来维护各个cpu. 图中状态左侧部分表示，当前cpu 发起访存动作是要做的一些动作(包括发一些广播信号, 以 及置当前cacheline的状态)。而图中右侧则表示 cpu 被动的收到广播信号时, 所需要执行 的动作（包括置cacheline状态以及一些额外的动作, E.g. flush). 我们举一个例子，不展开各个状态。 如果当前cpu要write一个内存，该内存所对应的cache是 invalid(对应右侧I-&gt;M) 该cpu需要 发起BusRdx 消息广播. (BuxRdx 表示一次写广播请求, 表示当前有人要写该 cacheline, 这里有一些serialization的问题这里不展开) 将该内存地址对应的cacheline的状态由 I -&gt; M 执行PrWr(PrWr表示cpu发起写操作，可以理解为改cacheline的内容了) 而其他cpu收到BusRdx后需要 将cacheline状态修改X-&gt;I(X为M, E, S 任意状态) 如果是M状态, 可能还要执行flush动作.(这里有一些序列化的问题, 例如这个flush 的动作要不要发生发起写操作的cpu的PrWr 之前) snoop-based non-scalable 原因主要来自广播。每次cache miss(invalid)发生时，需要通知 其他所有的cache。而在numa架构下这个问题更为突出。 在numa架构下，软件尽可能的设计成numa亲和性方案，即让当前numa上cpu运行的程序，访 问该numa的内存，但是由于 snoop-based conherence 的存在, 当我们访问near memory时 还是需要向所有的cpu进行广播, 这导致软件层面的优化几乎不起作用。 于是大佬们在思考能不能不再广播，而是通过点对点的方式，向特定的cpu发送cache y conherence 消息。而这就需要一个数据库，记录cache在各个cpu中的状态，从而辅助 发起消息的cpu选择向哪些cpu发送。 directory-based 既然需要一个数据库，我们则维护一个名为directory 的database: 每个内存 在directory中维护了一个条目，每一个条目包括: Dirty bit: 表示在某个cpu-cache中。其缓存是dirty的。 $p$ presence bit: 这是一个数组，每个字节表示该内存在其代表的cpu cache中 是否存在 directory存在于L3 缓存组中(可能哈, 个人认为将l3当成了一个directory，该缓存条目中 除了包括常规的缓存信息外，还包括了和 directory 相关的信息。个人瞎猜). 我们来看下两个场景, read miss to dirty line 和write miss(这对应于竞争较激烈的 spinlock场景) read miss to dirty line 初始状态 cpu 0 要访问 near cpu 1 memory cpu 0 中没有cache, directory entry dirtybit 为1 cpu2 中有该地址的cache, 并且没有writeback, directory entry $p$ 中有 cpu 2 执行步骤: (步骤1)cpu 0 发起一个read miss msg的request. (步骤2)对应的memory 的 directory 查询其该条目中的 $p$ array, 查到cpu 2。并返 回消息给 cpu0 (步骤3) cpu0 向cpu2 请求缓存内容 (步骤4) cpu2 返回给cpu0 (步骤5) cpu2 返回给cpu1, dir的变更信息包括:cpu 0 请求了该cacheline，所以 $p$中 要置位 cpu0 bit 另外, 将dirty data 也传递到cpu1, cpu1 收到后，将dirty data flush 到memory, 并 clear dirty bit non-scalable spinlock 的问题主要就在这一过程中，我们在之后的后面的章节中展开 我们接下来看下write miss write-miss 初始状态 cpu0 要写 near cpu 1 memory cpu0 没有cache，directory entry dirtybit为0 cpu0 没有cache, cpu1, cpu2 上有clean cache, $p$ 中有 cpu1, cpu2 执行步骤: (步骤1) cpu0 向directory 发 write miss msg (步骤2) 由于cache 是clear的, directory 直接将clear data + ids 返回给cpu0 directory 修改 $p$ clean {0, 1, 1} –&gt; dirty {1, 0, 0} (步骤3) cpu0 向cpu1, 2发送 invalid msg 消息 (步骤4) cpu1, cpu2 发送ack 给cpu0 其实这个过程性能并不低，因为这是一个一对多，并且允许每个点对点的{request, response} 可以异步执行。但是这是一个触发器，其在spinlock过程中会将其他cpu的cache line invalid。 其他cpu 会如果再访问该内存会执行read miss流程… 文章4, 5介绍缓存一致性时，从write-update, write-invalid角度展开 两个缓存一致性协议，个人查找资料发现write-update似乎不在主流的缓存一致性实现中 9 大家可以设想下，write-update一个好处是，在write操作发起时，会将cache 更新到 其他的缓存中，而不是让其失效。这样其他cpu 后续访问该缓存时，可以直接从当前 cpu local cache 中获取到干净的（而不是从主存)。 但是只update 不invalid 会有明显的问题，因为被update 的cpu可能很长一段时间不会访问 该cache，而这段时间内如果有cpu write 这个地址，都会update 该cpu的cache。这会 造成很大的浪费。而基于 invalid 协议，会在写操作时invalid，在read时 触发cache-miss, 同时，也会从其他cpu的缓存中获取数据而不是直接从缓存获取, 总之，这种在 read-miss时按需获取再搭配 directory-based conherence 协议看起来比简单的 write-update 协议优秀很多。(当然可能cpu有一些更高级的预测功能，可以知道其他cpu可能在短时间 内需要访问这个cache(例如预取等等), 此时带宽又很空闲，会去做write-update? 但这远远 超出了我的知识范畴，也远远超出了本文要讲述的知识的范畴) 总之综上所述，本文接下来关于所有的cache conherence, 都不考虑write update, 均以write invalid 为例. ok, 关于cache conherence 这个基础而又宏大的话题就再此戛然而止吧。我们来简单总结 下: 总结 snoop-based conherence在核非常多(尤其在numa架构下)面临着non-scalable的问题， 其主要的问题在于 MESI的各个消息都需要采用广播的机制。为了解决这一问题, 大佬们搞出了directory-based conherence，其通过点对点的方式，按需和某些cpu 消息 通信。 但是, 在某些场景下，这仍然造成了non-scalable的问题。 non-scalable spinlock How to cause non-scalable 来设想下: non-scalable 一般是怎么出现的。往往是在资源扩张后(例如cpu数量), 执行 一些操作时，这些操作的复杂度往往会随着资源扩张线性增长。 而 spinlock 尤为突出，因为spinlock这种阻塞忙等性质的同步机制不仅会影响发起慢操 作的cpu运行，而且由于其阻塞性质，会影响全局的进度。 在介绍具体的触发机制之前，我们先来看下 non-scalable spinlock的实现方式 non-scalable spinlock implementation wild spinlock wild 的意思是粗暴, 未经训化的意思, 俗称原始人。我们也用比较原始的汇编 指令展现其简介性: lock 代码 lock: .long 0 # 锁变量，初始为0 spin_lock: movl $1, %eax # eax = 1，表示“想要锁” spin_loop: xchgl %eax, [lock] # 原子交换 eax 和 lock testl %eax, %eax # 判断之前的 lock 是否为0 jne spin_loop # 如果不为0，继续自旋 # 获取锁成功，继续执行临界区代码 unlock代码: spin_unlock: movl $0, lock # 直接写0，释放锁 但是 wild spinlock有个很大的问题。就是公平性。不同的cpu”看到”的[lock] 变为0的时间不同, 所以哪个核获取到锁充满了随机性。这种不公平性体现在 获取自旋锁的延迟极不稳定，而软件又往往期待平稳的延迟。 这个地方其实涉及同时发起atomic操作的串性执行的仲裁问题。这个仲裁是 有谁来做的，是如何排序pending的atomic操作。这些知识在本人的知识储备 外，代后续补充. 于是 ticket spinlock 闪亮登场。 ticket spinlock ticket spinlock 会让每个自旋锁的申请者记录自己的”号牌”, 然后等待叫号. 这个很 像显示中的 排号系统。大家从排号机顺序取号，叫号时也会按顺序。 简单的小程序如下5: 数据结构 struct spinlock_t { // 上锁者自己的排队号 int my_ticket; // 当前叫号 int curr_ticket; } 加锁: void spin_lock(spinlock_t *lock) { int my_ticket; // 顺位拿到自己的ticket号码； my_ticket = atomic_inc(lock-&gt;my_ticket) - 1; while (my_ticket != lock-&gt;curr_ticket) ; // 自旋等待！ } 解锁: void spin_unlock(spinlock_t *lock) { // 呼叫下一位！ lock-&gt;curr_ticket++; } non-scalable ticket spinlock 我们以ticket spinlock为例来看下其在directory-based conherence 中 的non-scala. 按照ticket spinlock的设计, 当持有锁的cpu释放锁后，将会有一个确定的锁owner等待着 获取锁。但是按照前文提到的 directory-based conherence协议并不能第一时间让 这个确定的cpu看到，这就是矛盾所在。 我们来画图说明: 初始状态 初始状态为 CPU0 持有自旋锁，CPU1, CPU2 等待自旋锁。CPU 0 已经持有了一段时间， 此时CPU1, CPU2 中的cache已经更新为最近更新的值。 CPU0 解锁 CPU0 准备释放自旋锁，首先请求 directory 查询哪些cpu中还有该地址的cache, 并要求directory做一些invalid操作。 directory clean掉这些cpu bit，并且标记该cache 为dirty. 另外directory 将信息返回 cpu0. CPU0 发起invalid cpu1, cpu2 该cache的请求。 CPU1, CPU2 返回该req处理完成。该cacheline已经被invalid CPU2 read-miss 此时cpu2 仍然在自旋 while (my_ticket != lock-&gt;curr_ticket) 由于此时CPU0 在解锁过程中，将cpu 2的lock-&gt;curr_ticket更新了，cpu2的cacheline 被invalid，其读取会遇到 read-miss。 cpu2 read-miss 需要向directory 发起请求 directory 告诉cpu2这个cacheline 是 dirty 的，最新的数据在cpu0 CPU2 向CPU0请求最新数据 CPU0 将最新的数据返回 CPU0 请求directory 将请求flush cache(writeback)并请求更新ids(将CPU2 bit置位) if cpu2 not own the update curr_ticket 如果CPU2 没有持有更新后的curr_ticket, 此时cpu2 仍然会自旋，而其他的cpu也会重复 cpu2 的流程，但是如果cpu特别多的话，directory 处理消息则会是瓶颈（假设directory 处理消息为串行). 见上图, CPU (0-&gt;9) 都自旋 curr_ticket, CPU5 是当前ticket的owner但是CPU5 “看到” 该cacheline的更新比较晚（接收到CPU 0 invalid REQ比较慢, 或者因为其他的一些原因) 导致其在directory 中排队比较靠后，所以在CPU5 即便是发出了 read-miss 相关的消息 后，也会延迟一段时间，等待directory 处理完前面排队的消息。参与wait spinlock的cpu 越多，该延迟越明显。 总结 本段用前面讲述的directory-based cache conherence协议 描述了ticket spinlock 在多核架构下的non-scalable 的底层原因。其原因主要是在消息通信过程中，会不可 避免的发生多对一的消息通信，导致”一”这一侧的处理成为了瓶颈。 接下来的章节我们将看下大佬们在 Linux kernel 侧的一些实践，将看到 non-scalable spinlock 是多么可怕么，并用数学方法解释相关现象。 non-scalable lock are dangerous NOTE 本文主要是参照6 在实践过程中，大佬们发现system throughput(系统吞吐)会因non-scalable lock突然崩溃。例如，在系统在25 个核心的cpu 上运行的良好，但是在30个核心 CPU上运行却完全崩溃. 并且令人惊讶的是，造成崩溃的原因居然可以 临界区非常小 的锁。 上图中的曲线有一些共同点，均是当Cpu和增长到一定的数量后，再增长几个核就会出现 极具的性能下降。 下图是各个负载的具体情况（其中EXIM, 是临界区特别小的场景)。 文中提出了几个问题: 为什么崩溃会如此早的开始 (例如FOPS 居然在3，4个核心就出发崩溃 )。 为什么性能会最终会大幅下降。 为什么性能会迅速下降。 为了解释上面这些问题，文中建立了马可夫链(Markov chain)模型。 一共有 $0, 1, 2, …. n$ 即$n + 1$个状态, $k$ 表示系统中共有 $k$ 个cpu在等待自旋 锁解锁。$A[k]$ 表示 状态从 $k$ 转换到 $k+1$ 的频率。(加锁的频率, 但是注意并不是 一个cpu申请锁的频率，而是 系统中共有$k$ 个等锁cpu到 $k+1$ 个等锁cpu的频率)。 而 $S[k]$ 我们定义 $a$ 为单个核心上连续两次获取锁之间的平均时间。在没有竞争的情况下，单个 核心尝试获取锁的速率为 $\\frac{1}{a}$, 因此，如果已有 k 个核心在等待锁，则新竞争 者的到达率为 $(n − k)/a$.( $n-k$ 增在从不等待自旋锁到等待自旋锁的状态，每个cpu的 频率为 $1/a$, 所以两者相乘). 因此图中的 $a[k]$为: \\[A[k] = \\frac{n-k}{a} \\tag{1}\\] 我们再关注下解锁阶段: $S[k]$ 。我们将 $S[k]$ 阶段所消耗的时间分为两部分，一部分 是临界区代码所消耗的时间, 记做$E$, 另一部分是$unlock$操作消耗的时间，记做 $R$。 根据我们上面讲的 directory-based cache conherence的简单模型，在解锁阶段有一些 流程需要是串行的。所以我们假设处理一个更新核心的cache所消耗时间为 $c$, 那么 $k$ 个核在等待相同自旋锁的情况下，将所有核的cache更新需要的时间为 $k \\times c$, 但是 先处理哪个cpu，这个是随机的，所以curr_ticket owner cache 更新时间取个这些cpu更新 时间的平均值 $k \\times c /2$, 那么可得: \\[S[k]=\\dfrac{1}{E+\\dfrac{k×c}{2}} \\tag{2}\\] 可以看到 $S[k]$ 和$k$ 是成反比的关系。核心越多，更新cacheline的cost越高。 有了上面基本的结论，搭配下面一个稳态 Markov chain 状态转换率守恒的基本原则，即可 导出模型本身： 假設 $P_0, P_1, P_2, … P_n$ 系统处在这 $n$ 个状态的机率，显然 \\(\\sum P_k=1 \\tag{3}\\) 当系统处在稳态时，下方式子成立： \\[P_k \\times A[k] = P_{k+1} \\times S[k]\\] 这时一个递推, 结合(1), (2), (3)可以求出 $P_k$ 关于 $k$ 的表达式: \\[P_k = \\dfrac{\\frac{1}{T_{arrive}^k(n-k)!}\\prod_{i=1}^k (E+ic)}{\\sum_{i=0}^{n}\\frac{1}{T_{arrive}^i(n-i)!}\\prod_{j=1}^i (E+jc)}\\] 有了上面的公式，我们可以求出任意时刻整个系统等待自旋锁状态的CPU总量: （有1个cpu 等锁概率 * 1 + 有两个cpu等锁概率 * 2 + ... + 有n个cpu等锁 * n) \\[C = \\sum_{i=0}^{n}iP_i\\] 我们定义一个加速比的该你那，即在 $x$ 个cpu争抢 spinlock 时，有多少cpu未处于自旋锁 等锁的状态. \\[S=x - C\\] 下图是作者使用上面公式得到的推测值以及实际测试过程中得到的数据的对比图: 可以发现两者较为重合。 NOTE 上面公式本人未推导。均参考论文6以及狗哥文章4,5 scalable spinlock NOTE 本文不讨论scalable spinlock的具体实现。关于这部分我们在另外一篇文章 介绍内核qspinlock的实现. 论文中提到了几种scalable spinlock并对这些锁做了性能测试. 其中MCS和 CLH lock性能较好。 另外，作者还对比了 MCS vs ticket spinlock在上面提到的四种场景下的性能对比 : 均取得了良好的效果。 NOTE 有小伙伴可能不理解为什么采用scalable spinlock后仍然会导致性能吞吐量下降的问 题。但这并不是锁的消耗导致的。文章有一段介绍FOPS性能降低原因: Figure 12(a) shows the performance of FOPS with MCS locks. Going from one to two cores, performance with both ticket locks and MCS locks increases. For more than two cores, performance with the ticket spin lock decreases continuously. Performance with MCS locks initially also decreases from two to four cores, then re- mains relatively stable. The reason for this decrease in performance is that the time spent executing the critical section increases from 450 cycles on two cores to 852 cycles on four cores. The critical section is executed multiple times per-operation and modifies shared data, which incurs costly cache misses. As more cores are added, it is less likely that a core will have been the last core to execute the critical section, and therefore it will incur more cache misses, which will increase the length of the critical section 大概的意思是, 临界区中的代码会遇到较严重的cache miss, 核数越多，这种现象就越明 显(因为核数越多，下次运行到该核上的概率就越低) 另外大家可以想一下，即便是没有cache-miss 核数一直增长性能就会一直增长么? 答案是肯定不是，因为这些程序并不全是完全并行的，要不就不会用到spinlock来保护 临界区。因为临界区的存在导致一部分流程必须串行。临界区越大的这种现象越明显。 而在结合临界区中因cache-miss而变长的情况，所以上面的测试得到的结果往往会 性能平稳的略微下降。 结论 non-scalable spinlock 会因 cache 一致性而造成非常严重的性能问题。通过替换scalable spinlock会大大缓解该问题。 参考链接 Wikipedia: Spinlock Scalability 深入理解Linux内核之自旋锁 sysprog: 从CPU cache一致性的角度看Linux spinlock的不可伸缩性(non-scalable) dog250: 从CPU cache一致性的角度看Linux spinlock的不可伸缩性(non-scalable) Non-scalable locks are dangerous Snooping-Based Cache Coherence Directory-Based Cache Coherence Cache Coherence - when do modern CPUs update invalidated cache lines " }, { "title": "non-blocking algorithm", "url": "/posts/non-blocking-algorithm/", "categories": "synchronization", "tags": "synchronization, non-blocking algorithm, rcu preliminaries", "date": "2025-12-25 10:00:00 +0800", "content": "在计算机科学中，如果任何线程的故障或挂起不会导致其他线程的故障或挂起，则称该算法 为非阻塞算法1。根据非阻塞算法的达到效果，可以分为两类: wait-free: if there is also guaranteed per-thread progress lock-free: if there is guaranteed system-wide progress Obstruction-free: 只要某个线程能独占执行（其他线程不干扰），该线程的操作必在有限步骤内完成。 pre-thread progress vs system-wide progress progress的意思是进展。system-wide表示整个程序或者整个系统，而pre-thread要求每 个thread. 拿CAS来说, 可能某个thread在非常多次交换中都失败，但是从整个程序在说，在每一 次交换，都有thread 交换成功。所以其在system-wide取得进展，而不是pre-thread Motivation 在并行编程中，保护共享数据是一个很重要的议题。通常的方法有两种: blocking synchronization non-blocking synchronization 阻塞同步实现起来简单: 我们将访问共享资源的代码区成为临界区。步骤如下: 当程序进入临界区之前，首先检测该临界区是否上锁，如果上锁则等待其解锁。 如果发现并未上锁，或者已经解锁，则对该临界区加锁，离开临界区后，解锁。 总之, 阻塞同步的方法就是让某个程序在临界区中独享该资源。阻塞其他程序进在该时刻进 入临界区。通常的方法有: mutex, sem 等。 但是，阻塞式同步有很多缺点1, 导致其可能不适合很多场景。一个比较明显的 原因是: 线程在被阻塞时, 无法做任何操作: 当被阻塞的线程正在执行高优先级任务，或者 实时任务时，这是非常不明智的。 还有一些其他的原因，例如deadlock, livelock, priority inversion.(这个是wiki1 中写的，难道上面的例子不是priority inversion 么) 而非阻塞式算法没有这些问题。因为非阻塞式算法最低的要求(obstruction-free), 如果某个 线程因某种原因不再执行, 而另一个线程可以在有限步骤之内完成. 并不会无限期阻塞。 这也就 意味着，其他线程可以安全的抢占该当前访问共享资源的线程。另外使用无锁结构可以提高并行性 能, 因为对共享数据结构的访问不需要串行化以保持一致性。 wait-free vs lock-free vs obstruction-free blocking 我们先来看下, blocking synchronization的一个例子: mov eax, 1 spin: xchg eax, [lock] test eax, eax jnz spin //临界区 do something mov 0, [lock] 这是一个简单的自旋锁, 当一个线程持有自旋锁被中断时，其他线程也无法继续执行 如下图所示: obstruction-free 我们来设想一个场景。某个程序是以事务机制写入数据(例如xfs文件系统), 大概流程如下: BEG record do what do this completed 而另外一个程序用来检测是否有病毒入侵，如果有，立即中断程序。并杀死病毒（有内鬼， 终止交易).如果在事务未完成时终止程序, 下次程序再次启动时，会认为该事务未完成， 回滚掉这个事务。 可以看出，整个的事务写入是不需要加任何锁的，如果没有病毒侵入，该事务写入是可以在 一定的步骤内完成. 但是在疯狂的病毒入侵下, 该程序会不断的写入事务，回滚事务. 可能 在很长一段时间内, no system-wide progress.(这个例子并不太好，但是想不出其他恰当 的例子) 在该图中, Slip3时刻，T1, T2, T3均未取得进展。 NOTE 图片来自于4 *** lock-free 举例: 班级有一个班费账户，班级的人向这个账户存钱。由于大家在不同的银行柜台同时存 钱，这里会涉及并行. 设计程序如下: spin: mov [total_¥], eax mov eax, ebx mov eax, ecx ## 存钱 + 1 inc ebx CMPXCHG [total_¥], ebx test ecx, eax ## 发现有别人在存钱 jnz spin 运行该程序的线程时，可能会在某个时间段都不会有进展，但是在system-wide, 总有人可以存钱进去，所以属于lock-free. 虽然在Slip3, T3并未取得进展，但是此时T1, T2取得进展. wait-free wait-free 则是在per-thread层次的，要求在一段时间内，某个thread一定会有进展。 我们如下设计上面程序，在上面spin循环中，增加如果尝试次数超过100次，则启动一个 额外的线程, 继续尝试，不再阻塞当前线程。 这也类似于rcu callbak的作用，rcu callbak不会因读者临界区，而阻塞写者更新流程。 而写者关于old data的delete流程放到其他流程异步执行。所以对于写者update而言，其 是wait-free的。(单写者, 多读者) T1, T2, T3在所有时刻均取得进展。 参考链接 Non-blocking algorithm 并行程式设计: Lock-Free Programming 深入理解RCU|核心原理 对wait-free和lock-free的理解 RCU初学参考资料 " }, { "title": "qemu coroutine", "url": "/posts/coroutine/", "categories": "qemu, coroutine", "tags": "qemu, qemu_coroutine, completed", "date": "2025-02-25 11:00:00 +0800", "content": " Introduction Linux User Context Switch qemu coroutine 协程状态机 CREATE and INIT enter switch yield Use Case for QEMU Introduction 多线程和协程都可以用于并行编程，但是他们实现方式和使用场景 有很大的区别，我们来对比下: 对比项 协程 多线程 实现方式 在用户态单线程中，完成上下文切换 内核态完成上下文切换 开销 开销较低 线程创建销毁，以及切换都需要进入内核态，开销较高 并发 只能在单个线程中来回切换完成并发 可以实现真正的并行处理（在多核cpu) 调度(切换) 协程类似于非抢占式调度，只能在主动切换 线程可以在任何时刻被中断和切换 程序复杂度 协程处理同步和资源共享较简单 多线程编程需要处理线程间的同步和资 源共享问题, 复杂度更高, 往往需要借助系统api(锁，信号量) 使用场景上: 协程 当应用程序主要是 I/O 密集型任务，如网络请求、文件操作等 当需要高并发但不需要并行计算 使用多线程的场景 当应用程序是 CPU 密集型任务，需要利用多核 CPU 的并行计算能力 当需要处理大量需要同时执行的计算任务时 协程比较适合那种需要wait的任务, 例如上面提到的I/O 密集型任务(qemu中的 aio, 可以在协程中下发多个aio，然后等待io complete event) 我们举个例子: 在该图中, 有两个cpu core, A进程有3个thread, 其中thread1和thread2在 cpu0上运行，thread 3 有四个协程，在cpu1上运行, task A 的计算负载可 以分别落在cpu 0 和cpu 1上, 这也是多线程的很大的优点: 可以最大化的利 用多核cpu的并行处理能力。 thread1和thread2其靠kernel的任务抢占机制，来共享cpu 0, 在任何时间都有 可能被对方抢占. 而thread3 中的各个协程则是 根据自己任务的完成情况， 或者当前任务是否需要等待而主动选择调度。 Linux User Context Switch 我们需要思考下，context switch 完成哪些任务: init new task context like pthread_create() need init IP, SP(a new stack), Params context switch save… load… destroy 如果用户态要完成context switch，需要处理好上面所列的三件事。而这些事情涉及的东西 太底层了，如设置ip，如传参等等，所以libc中提供了ucontext系列接口来完成这些事情: ucontext ucontext API API name 作用 getcontext(ucontext_t *ucp) 获取当前上下文, 保存到ucp中 setcontext(ucp) 切换到目标(ucp)上下文 makecontext(ucp, (*func)(), int argc, …) 用来modify ucp, 下面详述 swapcontext(oucp, ucp) saves current thread context in oucp and makes *ucp the currently active context. 在执行makecontext()之前，需要做一些准备工作: 调用 getcontext() 来init ucp, 需要为其分配stack, init ucontext_t.uc_stack 相关成员 ss_sp: 指向具体的堆栈地址 ss_size: 堆栈大小 ss_flags: 设置ucp-&gt;uc_link参数，根据是否设置ucp-&gt;uc_link 来确定func() 返回时， 所执行的动作: NULL: 进程退出 隐式调用 setcontext(ucp-&gt;uc_link) 我们编写一个例子来演示下，该接口的使用方法和效果 ucontext example 测试程序展开 #include &lt;stdio.h&gt; #include &lt;ucontext.h&gt; #include &lt;stdlib.h&gt; #define STACK_SIZE (4096 * 2) void print_current_stack() { unsigned long stack_pointer; __asm__(\"movq %%rsp, %0\" : \"=r\"(stack_pointer)); printf(\"stack pointer(%lx)\\n\", stack_pointer); } void func(int a, int b) { printf(\"the co exec, sum(%d)\\n\", a+b); printf(\"print co stack \\n\"); print_current_stack(); return; } int main() { int ret; char *stack = (char *)malloc(STACK_SIZE); ucontext_t uc, old_uc; int a, b = 0; printf(\"the new stack is %p\\n\", stack); printf(\"print main stack:\\n\"); print_current_stack(); getcontext(&amp;uc); uc.uc_stack.ss_sp = stack; uc.uc_stack.ss_size = STACK_SIZE; uc.uc_link = &amp;old_uc; while(1) { printf(\"main co a(%d) b(%d)\\n\", a, b); makecontext(&amp;uc, (void (*)(void))func, 2, a, b); printf(\"swap context\\n\"); swapcontext(&amp;old_uc, &amp;uc); printf(\"swap context end\\n\"); if (a++ == 3) break; b=b+2; } return 0; } 在main中jum构建一个循环，来在另一个上下文中调用func(), 并设置 返回的context为调用者(main())的context，这样func()返回后， 直接返回到main()的while的上下文, 继续执行循环。 输出示例 输出如下: the new stack is 0x9d22a0 print main stack: stack pointer(7fff0939ee50) main co a(0) b(0) swap context the co exec, sum(0) print co stack stack pointer(9d4250) swap context end main co a(1) b(2) swap context the co exec, sum(3) print co stack stack pointer(9d4250) swap context end main co a(2) b(4) swap context the co exec, sum(6) print co stack stack pointer(9d4250) swap context end main co a(3) b(6) swap context the co exec, sum(9) print co stack stack pointer(9d4250) swap context end 由上图可见，func()和main()运行在两个上下文，并且两个上下文切换示意图 如下: 另外，linux中还支持另外一组上下文切换的API – sigsetjmp, siglongjmp sigsetjmp, siglongjmp 该系列函数一般用于实现C语言中的异常处理，如在信号处理流程中，跳转到 其他的执行流程. 避免再次执行到异常代码. 我们先来看下其API sigsetjmp(sigjmp_buf env, int savemask) 功能: 保存当前的上下文和信号掩码，以便以后可以通过 siglongjmp 恢复 参数: env: 保存上下文信息 savemask: 如果非0， 当前的信号掩码也会被保存 返回值: 调用者返回0 如果通过siglongjmp恢复，而返回siglongjmp 传递的值 siglongjmp(sigjmp_buf env, int val) 功能: 恢复由 sigsetjmp 保存的上下文信息和信号掩码，并从 sigsetjmp 返回。 参数: env: 由sigsetjmp保存的环境信息 val: sigsetjmp 0: return 1 x(x != 0) : return x 没有返回值(因为已经跳走了) 看起来sigxxxjmp也可以实现上下文切换，但是该系列接口有个很大的问题，比较适合 recover, 但不适合new。其不像ucontext接口, 可以通过makecontext()接口先new一个context, sigsetjmp 只能保存当前的现场, 所以相当于只能先走到要切换的流程中埋好点，然后才能切换，很不方便. 但是sigxxxjmp()对比makecontext()也有好处. 其更加轻量化. 它不会涉及完整的上下文切换, 例如其可以设置不切换信号掩码，减少因系统调用而产生的切换损耗. 而qemu中的协程实现主要有三种 ucontext + sigjmp: util/coroutine-ucontext.c sigaltstack: util/coroutine-sigaltstack.c coroutine-win32 本文主要介绍第一种，由ucontext和sigjmp结合实现。其中，ucontext系列接口负责 new context, 为sigjmp接口埋点, 而sigjmp 系列接口负责协程切换. 接下来，我们来看下qemu实现: qemu coroutine 协程状态机 这是一个典型的由 leader 创建协程的状态机，进入协程上下文会做两种事: 埋sigxxxjmp跳转点, 为之后再次切换进协程做准备 work… 协程运行期间，可能因为wait io等事件选择先切出协程(COROUTINE_YIELD), 此时协程是suspend状态。 等待协程处理完完整的事物后，会切出协程上下文，并置为terminal 状态. 另外除了首次进入协程是使用ucontext接口, 剩余的协程/leader之间的切换， 均使用sigxxxjmp系列接口，这样可以尽量减少因切换上下文带来性能损耗。 整体流程 整个流程如下图: CREATE and INIT create流程主要是为协程准备好上下文环境, init 流程主要是在协程中 打好跳转点, 流程包括: 为协程分配堆栈空间 使用makecontext(), swapcontext() 执行到一个新的上下文 在协程上下文中，埋 sig jmp的点 跳转回leader 上下文 INIT流程只是为协程搭建了一个上下文，但是该上下文接下来要执行什么任务， 需要leader指明，所以在切回leader上下文后，leader还需要为协程准备协程要 执行的函数，以及函数参数(红底蓝字部分) enter 在create &amp;&amp; INIT 章节中，我们介绍到首次进入协程是通过swapcontext()接口, 而之后再次进入协程，就需要使用sigxxxjmp系列接口，本章节主要介绍第二种。 而enter这个动作既有可能发生在leader上下文，也有可能发生在协程上下文, 所以我们以下面的场景为例子，看下qemu是怎么处理的。 leader enter 协程A 协程A enter 协程B 假设协程A, 协程B 在处理过程中不会yield, 直接terminal. 整个流程如下图: 这样处理，会导致协程只能串行，不能嵌套执行。 我们来想下为什么要这样做, 首先我们来看下，两者上下文切换次数: 串行执行 leader-&gt;A-&gt;leader-&gt;B-&gt;leader 切换4次 嵌套执行 leader-&gt;A-&gt;B-&gt;A-&gt;leader 切换4次 两者切换次数相同。 所以这里的原因(猜测)很可能是，防止协程可能带来的 同步问题（避免A上下文中嵌入B的上下文从而带来死锁) switch 接下来，我们再来看下switch过程。switch过程比较简单。主要的函数是, qemu_coroutine_switch(), 函数原型: CoroutineAction qemu_coroutine_switch(Coroutine *from_, Coroutine *to_, CoroutineAction action); 参数有三个: from: 切出的协程 to: 切入的协程 action: 本次操作的类型 COROUTINE_YIELD: 暂停from协程 COROUTINE_TERMINATE: 终止from协程 COROUTINE_ENTER: 进入to协程 我们以一个没有执行过yield协程生命周期来看下switch的细节: 可以看到在执行qemu_coroutine_switch()时，action参数会作为 siglongjmp(, action)传入，这样在另一个上下文中，会通过 sigsetjmp()的返回值，获取到action, 而qemu_aio_coroutine_enter() 会根据协程返回状态，来选择一些action: COROUTINE_TERMINATE: 销毁协程 COROUTINE_YIELD: 忽略，继续执行leader流程 这里我们来总结下，不同的switch过程: leader-&gt;co ENTER: co-&gt;leader TERMINATE YIELD yield yield是一个比较特殊的存在，因为yield动作时，还需要保存协程的现场， 以便之后，再次切回协程。并且在协程yield切回leader后，leader会继续 运行执行其他流程。等待该协程的等待的事件到来后，需要再次执行enter 切换回该协程，如下图所示: Use Case for QEMU 附录 virtio-blk触发堆栈 virtio_blk_handle_vq ## 从avail ring中获取req =&gt; blk_io_plug() =&gt; while (virtio_blk_get_request()) =&gt; virtio_blk_submit_multireq() =&gt; foreach request: ## 可能会merge submit =&gt; submit_requests() =&gt; init qemu iovc =&gt; blk_aio_pwritev/blk_aio_preadv =&gt; blk_io_unplug() blk_aio_pwritev =&gt; blk_aio_prwv(,,,,co_entry::blk_aio_write_entry, flags, cb:: virtio_blk_rw_complete,opaque) =&gt; init acb::BlkAioEmAIOCB =&gt; qemu_coroutine_create(co_entry, acb) =&gt; bdrv_coroutine_enter(blk_bs(blk), co) 参考资料 huangyong – 深入理解qemu协程 _银叶先生 – 协程的原理与实现：qemu 之 Coroutine " }, { "title": "qspinlock", "url": "/posts/qspinlock/", "categories": "synchronization", "tags": "synchronization, spinlock", "date": "2023-09-11 10:00:00 +0800", "content": "简介 内核中的自旋锁是互斥锁。而内核中的自旋锁经过多个版本的演进， 最终是在 mcs 自旋锁 算法之上，根据kernel 本身的需求，作了改进。 我们这里不去回顾 Linux 自旋锁的历史，简单介绍下 mcs 自旋锁算法， 并详细讲解 kernel 中的 mcs自旋锁的变体。 NOTE 如果想要了解 kernel 自旋锁的演进，可以看下 深入理解Linux内核之自旋锁 1 该文章详细讲解了kernel自旋锁的演进，并通过举例子 的方式，讲解了各个算法（包括最新的算法），十分值 得看, 本文主要分析kernel 最新的 自旋锁算法。 MCS 自旋锁 MCS 自旋锁是在为了解决票号自旋锁带来的 cache line bouncing2, 实现了一个队列，使其各自自旋各自的地址, 而不是自旋一个地址，这样就 解决了这个问题。 NOTE 1 这个我们放到附录1中讲述 什么是cache line bouncing，为什么cache line bouncing在 spinlock场景会尤为突出。 所以 MCS自旋锁 即保持票号自旋锁的保证线程获取锁的顺序的优点， 同时解决票号自旋锁带来的cache 抖动问题。但是也有缺点，我们下面会介绍。 我们先来看下 MCS自旋锁的实现 而MCS自旋锁有什么缺点呢 ? 更改了spinlock的相关接口 锁占用的内存变大 kernel 对此做了一些优化，我们来看下 KERNEL MCS 变体 spinlock的相关数据结构(struct qspinlock)，仍然保持之前的样子 大小为sizeof(u32)。但是该空间分割成主要的三个成员 (locked, pending,tail)。我们来看下kernel代码定义: typedef struct qspinlock { union { atomic_t val; /* ¦* By using the whole 2nd least significant byte for the ¦* pending bit, we can allow better optimization of the lock ¦* acquisition for the pending bit holder. ¦*/ #ifdef __LITTLE_ENDIAN struct { u8 locked; u8 pending; }; struct { u16 locked_pending; u16 tail; }; #else struct { u16 tail; u16 locked_pending; }; struct { u8 reserved[2]; u8 pending; u8 locked; }; #endif }; } arch_spinlock_t; 可以看到访问该数据结构，应该使用原子操作(qspinlock.val), 根据的线程(cpu)的情况可能需要关注的东西不同, 例如: mcs.locked = 1的cpu需要关注 (qspinlock.locked_pending)成员。 而pending的cpu仅需要关注(qspinlock.locked), 我们下面会介绍具体的流程 简单来说，是占据pending 的cpu抢占locked, 而 占据 mcs head 的cpu 抢占 locked_pending, 而非mcs head的cpu, 先抢占 self.mcs.locked == 1, 等待该条件满足时，表示其为 mcs head, 然后再抢占 locked_pending 每个 cpu有四个 mcs0, 代表4中状态（代表4层执行流，线程、软中断、 硬中断、屏蔽中断), 举个例子，线程拿到了锁，这时候来了一个硬 中断，硬中断处理完后，进入软中断处理及流程，这时拿了一把锁， 此时又来了一个硬中断，该中断处理中又拿了一个自旋锁，还未解锁时， 来了一个NMI又拿了一个自旋锁。 这样每个CPU最多持有四把自旋锁。而 每个自旋锁，如果都需要使用 mcs结构 enqueue， 最多需要4个mcs结构。 tail的计算也是基于上面。每个cpu最多拿四个mcs, 所以tail[bit:1,bit:0] 用来表示当前用了几个自旋锁。cpu index记录在剩余的bits中。另外 tail == 0有特殊的含义 – 表示没有 mcs 在抢占自旋锁。所以不存在 cpu0.mcs0的这种情况，需要将 cpu_index++, 也就是下面的公式: tail = ((cpu_index + 1) &lt;&lt; 2) + mcs_index 代码分析 NOTE 下面 频繁使用三元组 (tail, pending, locked) , 例如(0, 0, 1) 表示 locked = 1, pending = 0, tail = 0 另外 tail = n , 表示tail位被占用 tail = z, 则表示tail 为任意值 locked, pending == x, y 表示任意值 我们直接看 queue_spin_lock()的相关代码: queued_spin_lock static __always_inline void queued_spin_lock(struct qspinlock *lock) { u32 val; //=============(1)================= val = atomic_cmpxchg_acquire(&amp;lock-&gt;val, 0, _Q_LOCKED_VAL); if (likely(val == 0)) return; //============(2)================== queued_spin_lock_slowpath(lock, val); } 查看lock-&gt;val 是否为0, 如果为0, 则说明没有人在使用该锁, 将 (0, 0, 0) 修改为 (0, 0, 1) 如果有人占用锁，则走slowpath流程 slow path lock pending – part 1 void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val) { struct mcs_spinlock *prev, *next, *node; u32 old, tail; int idx; BUILD_BUG_ON(CONFIG_NR_CPUS &gt;= (1U &lt;&lt; _Q_TAIL_CPU_BITS)); //===============(1)================== if (pv_enabled()) goto pv_queue; if (virt_spin_lock(lock)) return; /* * Wait for in-progress pending-&gt;locked hand-overs with a bounded * number of spins so that we guarantee forward progress. * * 0,1,0 -&gt; 0,0,1 */ //===============(2)================== if (val == _Q_PENDING_VAL) { int cnt = _Q_PENDING_LOOPS; val = atomic_cond_read_relaxed(&amp;lock-&gt;val, (VAL != _Q_PENDING_VAL) || !cnt--); } /* ¦* If we observe any contention; queue. ¦*/ //===============(3)================== if (val &amp; ~_Q_LOCKED_MASK) goto queue; /* * trylock || pending * * 0,0,* -&gt; 0,1,* -&gt; 0,0,1 pending, trylock */ //===============(4)================== val = queued_fetch_set_pending_acquire(lock); ... 我们这里先不关注半虚拟化 如果是(0, 1, 0)， 则等待其进入(0, 0, 1), 这样做的好处是，如果其进入了 (0,0,1) 则直接抢占pending位，状态为(0,1,1), 就不用走queue的流程 如果除 locked位，其他位不为0, 则说明有被别的线程抢占了，则走queue的流程 该代码为: static __always_inline u32 queued_fetch_set_pending_acquire(struct qspinlock *lock) { return atomic_fetch_or_acquire(_Q_PENDING_VAL, &amp;lock-&gt;val); } 这里进行按位或操作，并返回之前的值，我们来想下在原子操作前有那 几种可能的情况。 (z, 1, y) : 进行逻辑或操作无影响。但是本次抢占锁失败 (n, 0, y) : !! 这种情况就打乱了顺序，不允许, 并且需要将pending位还原 (0, 0, 1) : 抢占pending位，spin lock位 (0, 0, 0) : 抢占了pending位，然后这时，只有自己能抢占lock位，再抢占lock位 我们继续分析(4) 之后的代码: lock pending – part2 /* * If we observe contention, there is a concurrent locker. * * Undo and queue; our setting of PENDING might have made the * n,0,0 -&gt; 0,0,0 transition fail and it will now be waiting * on @next to become !NULL. */ //============(1)==================== if (unlikely(val &amp; ~_Q_LOCKED_MASK)) { //============(1.1)==================== /* Undo PENDING if we set it. */ if (!(val &amp; _Q_PENDING_MASK)) clear_pending(lock); goto queue; } /* * We're pending, wait for the owner to go away. * * 0,1,1 -&gt; 0,1,0 * * this wait loop must be a load-acquire such that we match the * store-release that clears the locked bit and create lock * sequentiality; this is because not all * clear_pending_set_locked() implementations imply full * barriers. */ //===============(2)================ if (val &amp; _Q_LOCKED_MASK) atomic_cond_read_acquire(&amp;lock-&gt;val, !(VAL &amp; _Q_LOCKED_MASK)); /* * take ownership and clear the pending bit. * * 0,1,0 -&gt; 0,0,1 */ clear_pending_set_locked(lock); lockevent_inc(lock_pending); return; 除了locked字段以外还有值，则抢锁失败 pending字段有值，则为(z, 1, y), 抢锁失败不需要做什么事情, pending 字段未有值，那tail字段肯定有值，则为(n, 0, y), 抢锁 失败，还需要将pending位还原为0 以上两种情况都需要入队 这种情况为 (0, 0, y) 如果为 (0, 0, 1), 则spin lock位，等待其变为0 如果位 (0, 0, 0), clear pending 并且 抢占 lock位 queue – part1 void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val) { ... queue: lockevent_inc(lock_slowpath); pv_queue: //=================(1)======================= node = this_cpu_ptr(&amp;qnodes[0].mcs); idx = node-&gt;count++; tail = encode_tail(smp_processor_id(), idx); /* * 4 nodes are allocated based on the assumption that there will * not be nested NMIs taking spinlocks. That may not be true in * some architectures even though the chance of needing more than * 4 nodes will still be extremely unlikely. When that happens, * we fall back to spinning on the lock directly without using * any MCS node. This is not the most elegant solution, but is * simple enough. */ //=================(2)======================= if (unlikely(idx &gt;= MAX_NODES)) { lockevent_inc(lock_no_node); while (!queued_spin_trylock(lock)) cpu_relax(); goto release; } //=================(3)======================= node = grab_mcs_node(node, idx); ... } 获取mcs结构，关于该node的count,在 qnodes[0].mcs中记录。 encode_tail() 会根据 当前cpu idx和原来的count值计算出一个 值作为tail的值。 idx &gt;= MAX_NODES其实是不正常的（说明已经获取了 MAX_NODES(4) 次锁), 注释中写了可能的原因，我们这里先不细看 (!!后续补充!!) 获取 qnodes[idx].mcs node queue – part2 void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val) { ... /* * 上节代码: * idx = node-&gt;count++; */ \t\t... /* * Keep counts of non-zero index values: */ lockevent_cond_inc(lock_use_node2 + idx - 1, idx); //=============(1)=============== /* ¦* Ensure that we increment the head node-&gt;count before initialising ¦* the actual node. If the compiler is kind enough to reorder these ¦* stores, then an IRQ could overwrite our assignments. ¦*/ barrier(); //=============(2)=============== node-&gt;locked = 0; node-&gt;next = NULL; pv_init_node(node); /* ¦* We touched a (possibly) cold cacheline in the per-cpu queue node; ¦* attempt the trylock once more in the hope someone let go while we ¦* weren't watching. ¦*/ //=============(3)=============== if (queued_spin_trylock(lock)) goto release; \t\t... } locking/qspinlock: Ensure node-&gt;count is updated before initialising node commit 11dc13224c975efcec96647a4768a6f1bb7a19a8 该 barrier是为了避免编译器将 init node 和 idx = node-&gt;count++顺序搞乱。 在单执行流跑的时候不会有问题。当遇到下面的情况 ``` (1) 未加barrier()之前, 编译器乱序 normal thread 触发中断 //乱序 init node[0] init node[0] idx = node-&gt;count++ (2) 加了barrier normal thread idx = node-&gt;count++ 触发中断 init node[1] init node[0] ``` 未加 barrier()之前，会遇到中断执行流和normal thread执行流，使用 一个node的情况。 初始化该node 这时候，我们只修改了node, 如果能抢到锁，恢复原来的node也很好恢复。 所以尝试抢锁，抢到就血赚。 queue – part3 void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val) { /* * 上节代码:(1) * node-&gt;locked = 0; * node-&gt;next = NULL; */ ... /* ¦* Ensure that the initialisation of @node is complete before we ¦* publish the updated tail via xchg_tail() and potentially link ¦* @node into the waitqueue via WRITE_ONCE(prev-&gt;next, node) below. ¦*/ //=============(1)=============== smp_wmb(); /* ¦* Publish the updated tail. ¦* We have already touched the queueing cacheline; don't bother with ¦* pending stuff. ¦* ¦* p,*,* -&gt; n,*,* ¦*/ //==============(2)=============== old = xchg_tail(lock, tail); next = NULL; /* ¦* if there was a previous node; link it and wait until reaching the ¦* head of the waitqueue. ¦*/ //==============(3)=============== if (old &amp; _Q_TAIL_MASK) { prev = decode_tail(old); /* Link @node into the waitqueue. */ //==============(4)=============== WRITE_ONCE(prev-&gt;next, node); pv_wait_node(node, prev); //==============(5)=============== arch_mcs_spin_lock_contended(&amp;node-&gt;locked); /* ¦* While waiting for the MCS lock, the next pointer may have ¦* been set by another lock waiter. We optimistically load ¦* the next pointer &amp; prefetch the cacheline for writing ¦* to reduce latency in the upcoming MCS unlock operation. ¦*/ //==============(6)=============== next = READ_ONCE(node-&gt;next); if (next) prefetchw(next); } ... } 关于此处的分析，请看 locking/qspinlock: Ensure node is initialised before updating prev-&gt;next locking/qspinlock: Elide back-to-back RELEASE operations with smp_wmb publish the updated tail (替换tail) 判断 old 是否有 tail值，如果有，则说明mcs链表中有成员, 需要获取 prev, 并且更新prev-&gt;next 更新 prev-&gt;next 这时需要自旋等待 node-&gt;locked == 1 #define arch_mcs_spin_lock_contended(l) \\ do { \\ smp_cond_load_acquire(l, VAL); \\ } while (0) 因为在(5)处等待了一段时间，很可能node-&gt;next就有值了，因为这时候抢到了 锁，如果有next需要再将next-&gt;locked = 1, 但是距离写操作还有一些指令， 这里执行prefetchw 操作，暗示cpu会有对next地址的写操作，使其在写操作 发生之前，将该地址的内容load到 cache中。 queue – part4 void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val) { ... /* ¦* we're at the head of the waitqueue, wait for the owner &amp; pending to ¦* go away. ¦* ¦* *,x,y -&gt; *,0,0 ¦* ¦* this wait loop must use a load-acquire such that we match the ¦* store-release that clears the locked bit and create lock ¦* sequentiality; this is because the set_locked() function below ¦* does not imply a full barrier. ¦* ¦* The PV pv_wait_head_or_lock function, if active, will acquire ¦* the lock and return a non-zero value. So we have to skip the ¦* atomic_cond_read_acquire() call. As the next PV queue head hasn't ¦* been designated yet, there is no way for the locked value to become ¦* _Q_SLOW_VAL. So both the set_locked() and the ¦* atomic_cmpxchg_relaxed() calls will be safe. ¦* ¦* If PV isn't active, 0 will be returned instead. ¦* ¦*/ //================(1)====================== if ((val = pv_wait_head_or_lock(lock, node))) goto locked; //================(2)====================== val = atomic_cond_read_acquire(&amp;lock-&gt;val, !(VAL &amp; _Q_LOCKED_PENDING_MASK)); ... } pv先不看 !!!!! 自旋 lock-&gt;lock_pending == 0 (n, 1, 0) 或者 (n,0,1) –&gt;(n, 0, 0), 这里只有该进程自旋这个地址，所以如果变为(n, 0, 0) , 只有这个进程可以 抢到。 locked void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val) { locked: /* ¦* claim the lock: ¦* ¦* n,0,0 -&gt; 0,0,1 : lock, uncontended ¦* *,*,0 -&gt; *,*,1 : lock, contended ¦* ¦* If the queue head is the only one in the queue (lock value == tail) ¦* and nobody is pending, clear the tail code and grab the lock. ¦* Otherwise, we only need to grab the lock. ¦*/ /* ¦* In the PV case we might already have _Q_LOCKED_VAL set, because ¦* of lock stealing; therefore we must also allow: ¦* ¦* n,0,1 -&gt; 0,0,1 ¦* ¦* Note: at this point: (val &amp; _Q_PENDING_MASK) == 0, because of the ¦* above wait condition, therefore any concurrent setting of ¦* PENDING will make the uncontended transition fail. ¦*/ //==============(1)==================== if ((val &amp; _Q_TAIL_MASK) == tail) { //==============(2)==================== if (atomic_try_cmpxchg_relaxed(&amp;lock-&gt;val, &amp;val, _Q_LOCKED_VAL)) goto release; /* No contention */ } /* ¦* contended path; wait for next if not observed yet, release. ¦*/ //==============(3)==================== if (!next) next = smp_cond_load_relaxed(&amp;node-&gt;next, (VAL)); /* ¦* Either somebody is queued behind us or _Q_PENDING_VAL got set ¦* which will then detect the remaining tail and queue behind us ¦* ensuring we'll see a @next. ¦*/ //==============(4)==================== set_locked(lock); //==============(5)==================== arch_mcs_spin_unlock_contended(&amp;next-&gt;locked); pv_kick_node(lock, next); release: /* ¦* release the node ¦*/ __this_cpu_dec(qnodes[0].mcs.count); } 如果tail 就是 当前的mcs, 说明，没有其他人抢了，这时将 状态变为 (0, 0, 1) atomic_try_cmpxchg_relaxed() 如果lock-&gt;val == val , val保持不变，返回值为true 如果Lock-&gt;val != val, val=lock-&gt;val, 返回值为false 所以, 如果返回为真，说明为(this_node, 0, 0)则 直接替换成(0, 0, 1) 即可, 并且释放该mcs 如果next没有值，并且这时lock-&gt;tail != this_node, 说明 有人抢锁，并执行完old = xchg_tail(lock, tail); 这行代码， 但是还没有执行WRITE_ONCE(prev-&gt;next, node);, 这行代码， 这时，需要等待 node-&gt;next 被赋值。 这时，可以抢占锁了。 commit c61da58d8a9ba9238250a548f00826eaf44af0f7 将next-&gt;locked 赋值为1 #define arch_mcs_spin_unlock_contended(l) \\ smp_store_release((l), 1) 附录 附录1 : cacheline bouncing 参考链接 深入理解Linux内核之自旋锁 What is cache line bouncing? How may a spinlock trigger this frequently? 從 CPU cache coherence 談 Linux spinlock 可擴展能力議題 " } ]
