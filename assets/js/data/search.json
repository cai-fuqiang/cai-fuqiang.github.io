[ { "title": "[论文翻译] Lottery and Stride Scheduling: Flexible Proportional-Share Resource Management", "url": "/posts/stride_sched_paper/", "categories": "schedule, paper", "tags": "sched", "date": "2025-09-05 22:18:00 +0800", "snippet": "Resource Management FrameworkThis chapter presents a general, flexible framework for specifying resourcemanagement policies in concurrent systems. Resource rights are encapsulated byabstract, first...", "content": "Resource Management FrameworkThis chapter presents a general, flexible framework for specifying resourcemanagement policies in concurrent systems. Resource rights are encapsulated byabstract, first-class objects called tickets. Ticket-based policies areexpressed using two basic techniques: ticket transfers and ticket inflation.Ticket transfers allow resource rights to be directly transferred andredistributed among clients. Ticket inflation allows resource rights to bechanged by manipulating the overall supply of tickets. A powerful currencyabstraction provides flexible, modular control over ticket inflation. Currenciesalso support the sharing, protecting, and naming of resource rights. Severalexample resource management policies are presented to demonstrate theversatility of this framework 本章提出了一个用于在并发系统中指定资源管理策略的通用且灵活的框架。资源权限通过被称为“票据”（tickets）的抽象一等对象进行封装。基于票据的策略通过两种基本技术来表达：票据转移和票据膨胀。票据转移允许资源权限在客户端之间直接转移和重新分配。票据膨胀则通过操作票据的整体供应量来改变资源权限。一个强大的货币抽象为票据膨胀提供了灵活、模块化的控制。货币还支持资源权限的共享、保护和命名。本章还通过几个示例资源管理策略，展示了该框架的多样性和灵活性。 first-class: 一类对象：是指一个实体，拥有编程语言中和其他变量相同的权利和能力 例如：函数, 可以像普通变量一样 赋值给变量 当作参数传递给其他函数 在函数中返回 (等等)这里的ticket 提到是first-class 表示为可以灵活的进行操作和传递，而不是受限的，只能在特定场景下使用(from GPT4.1) 2.1 TicketsResource rights are encapsulated by first-class objects called tickets. Ticketscan be issued in different amounts, so that a single physical ticket mayrepresent any number of logical tickets. In this respect, tickets are similar tomonetary notes which are also issued in different denominations. For example, asingle ticket object may represent one hundred tickets, just as a single $100bill represents one hundred separate $1 bills.Tickets are owned by clients that consume resources. A client is considered tobe active while it is competing to acquire more resources. An active client isentitled to consume resources at a rate proportional to the number of ticketsthat it has been allocated. Thus, a client with twice as many tickets as anotheris entitled to receive twice as much of a resource in a given time interval. Thenumber of tickets allocated to a client also determines its entitled responsetime. Client response times are defined to be inversely proportional to ticketallocations. Therefore, a client with twice as many tickets as another isentitled to wait only half as long before acquiring a resource. 资源权限通过被称为“票据”（tickets）的一等对象进行封装。票据可以以不同的数量发行，因此一张实际的票据可以代表任意数量的逻辑票据。在这方面，票据类似于以不同面额发行的货币纸币。例如，一个票据对象可以代表一百张票，就像一张100美元的钞票代表一百张1美元的钞票一样。 票据由消耗资源的客户端拥有。只要客户端正在争取获取更多资源，就被视为活跃客户端。活跃客户端有权按照其分配到的票据数量按比例消耗资源。因此，某个客户端的票据数量是另一个客户端的两倍时，它在一定时间间隔内获得的资源也应是后者的两倍。分配给客户端的票据数量还决定了它应有的响应时间。客户端的响应时间被定义为与其票据分配数量成反比。因此，某个客户端的票据数量是另一个的两倍时，它在获取资源前的等待时间也只有后者的一半。Tickets encapsulate resource rights that are abstract, relative, and uniform.Tickets are abstract because they quantify resource rights independently ofmachine details. Tickets are relative since the fraction of a resource that theyrepresent varies dynamically in proportion to the contention for that resource.Thus, a client will obtain more of a lightly contended resource than one that ishighly contended. In the worst case, a client will receive a share proportionalto its share of tickets in the system. This property facilitates adaptiveclients that can benefit from extra resources when other clients do not fullyutilize their allocations. Finally, tickets are uniform because rights forheterogeneous resources can be homogeneously represented as tickets. Thisproperty permits clients to use quantitative comparisons when making decisionsthat involve tradeoffs between different resources. 票据（tickets）封装了抽象的、相对的和统一的资源权限。票据是抽象的，因为它们在量化资源权限时不依赖于具体的机器细节。票据是相对的，因为它们所代表的资源份额会根据对该资源的竞争情况动态变化。因此，当资源竞争较少时，客户端可以获得更多资源；而当资源竞争激烈时，客户端获得的资源就会减少。在最坏的情况下，客户端获得的资源份额会与其在系统中拥有的票据份额成正比。这个特性有利于自适应客户端——当其他客户端没有充分利用分配给自己的资源时，它们可以获得额外的资源。最后，票据是统一的，因为不同类型的资源权限都可以用票据这种统一的形式来表示。这个特性使客户端在需要在不同资源之间进行权衡决策时，可以采用定量比较的方法。In general, tickets have properties that are similar to those of money incomputational economies [WHH 92]. The only significant difference is thattickets are not consumed when they are used to acquire resources. A client mayreuse a ticket any number of times, but a ticket may only be used to compete forone resource at a time. In economic terms, a ticket behaves much like a constantmonetary income stream. 一般来说，票据具有与计算经济学中的货币类似的属性 [WHH 92]。唯一显著的区别在于，票据在用于获取资源时并不会被消耗。一个客户端可以无限次地重复使用同一个票据，但一个票据在同一时刻只能用于竞争一种资源。从经济学的角度来看，票据的行为很像一条恒定的货币收入流。2.2 Ticket TransfersA ticket transfer is an explicit transfer of first-class ticket objects from oneclient to another. Ticket transfers can be used to implement resource managementpolicies by directly redistributing resource rights. Transfers are useful in anysituation where one client blocks waiting for another. For example, Figure 2-1illustrates the use of a ticket transfer during a synchronous remote procedurecall (RPC). A client performs a temporary ticket transfer to loan its resourcerights to the server computing on its behalf. 票据转移是将一等票据对象从一个客户端显式转移到另一个客户端的过程。票据转移可以通过直接重新分配资源权限来实现资源管理策略。在任何一个客户端因等待另一个客户端而阻塞的情况下，票据转移都非常有用。例如，图2-1展示了在同步远程过程调用（RPC）期间票据转移的用法。客户端通过临时票据转移，将其资源权限借给代表其进行计算的服务器。Ticket transfers also provide a convenient solution to the conventional priorityinversion problem in a manner that is similar to priority inheritance [SRL90].For example, clients waiting to acquire a lock can temporarily transfer ticketsto the current lock owner. This provides the lock owner with additional resourcerights, helping it to obtain a larger share of processor time so that it canmore quickly release the lock. Unlike priority inheritance, transfers frommultiple clients are additive. A client also has the flexibility to split tickettransfers across multiple clients on which it may be waiting. These featureswould not make sense in a priority-based system, since resource rights do notvary smoothly with priorities. 票据转移还为传统的优先级反转问题提供了一种方便的解决方案，其方式类似于优先级继承 [SRL90]。例如，等待获取锁的客户端可以临时将票据转移给当前的锁拥有者。这样会赋予锁拥有者额外的资源权限，帮助其获得更多的处理器时间，从而可以更快地释放锁。与优先级继承不同，来自多个客户端的票据转移是可以累加的。客户端还可以灵活地将票据转移拆分给多个它正在等待的其他客户端。这些特性在基于优先级的系统中是没有意义的，因为资源权限不会随着优先级的变化而平滑变化。Ticket transfers are capable of specifying any ticket-based resource managementpolicy, since transfers can be used to implement any arbitrary distribution oftickets to clients. However, ticket transfers are often too low-level toconveniently express policies. The exclusive use of ticket transfers imposes aconservation constraint: tickets may be redistributed, but they cannot becreated or destroyed. This constraint ensures that no client can deprive anotherof resources without its permission. However, it also complicates thespecification of many natural policies. 票据转移能够实现任何基于票据的资源管理策略，因为通过转移可以将票据按照任意方式分配给各个客户端。然而，票据转移往往过于底层，不便于方便地表达各种策略。仅使用票据转移会带来一个守恒约束：票据只能被重新分配，不能被创建或销毁。这个约束能够确保没有客户端在未经许可的情况下剥夺其他客户端的资源，但它也使得很多自然的策略变得难以实现。For example, consider a set of processes, each a client of a time-sharedprocessor resource. Suppose that a parent process spawns child subprocesses andwants to allocate resource rights equally to each child. To achieve this goal,the parent must explicitly coordinate ticket transfers among its childrenwhenever a child process is created or destroyed. Although ticket transfersalone are capable of supporting arbitrary resource management policies, theirspecification is often unnecessarily complex 例如，考虑一组进程，每个进程都是时间共享处理器资源的客户端。假设一个父进程产生了子进程，并希望将资源权限平均分配给每个子进程。为了实现这一目标，父进程必须在每次创建或销毁子进程时，显式地协调子进程之间的票据转移。尽管仅通过票据转移可以支持任意的资源管理策略，但其具体实现往往不必要地复杂。2.3 Ticket Inflation and DeflationTicket inflation and deflation are alternatives to explicit ticket transfers.Client resource rights can be escalated by creating more tickets, inflating thetotal number of tickets in the system. Similarly, client resource rights can bereduced by destroying tickets, deflating the overall number of tickets. Ticketinflation and deflation are useful among mutually trusting clients, since theypermit resource rights to be reallocated without explicitly reshuffling ticketsamong clients. This can greatly simplify the specification of many resourcemanagement policies. For example, a parent process can allocate resource rightsequally to child subprocesses simply by creating and assigning a fixed number oftickets to each child that is spawned, and destroying the tickets owned by eachchild when it terminates. 票据膨胀和收缩是显式票据转移的替代方案。通过创建更多的票据，可以提升客户端的资源权限，从而增加系统中票据的总数，即票据膨胀。类似地，通过销毁票据，可以减少客户端的资源权限，从而降低系统中票据的总数，即票据收缩。票据膨胀和收缩在相互信任的客户端之间非常有用，因为它们允许在不需要显式地在客户端之间重新分配票据的情况下，重新分配资源权限。这可以极大地简化许多资源管理策略的实现。例如，父进程可以在生成每个子进程时，通过创建并分配固定数量的票据给每个子进程，从而实现资源权限的平均分配；当子进程终止时，则销毁其持有的票据。However, uncontrolled ticket inflation is dangerous, since a client canmonopolize a resource by creating a large number of tickets. Viewed from aneconomic perspective, inflation is a form of theft, since it devalues thetickets owned by all clients. Because inflation can violate desirable modularityand insulation properties, it must be either prohibited or strictly controlled 然而，不受控制的票据膨胀是危险的，因为客户端可以通过创建大量票据来垄断资源。从经济学的角度来看，膨胀是一种盗窃行为，因为它会使所有客户端持有的票据贬值。由于膨胀可能破坏理想的模块化和隔离特性，因此必须禁止或严格控制票据膨胀A key observation is that the desirability of inflation and deflation hinges ontrust. Trust implies permission to appropriate resources without explicitauthorization. When trust is present, explicit ticket transfers are often morecumbersome and restrictive than simple, local ticket inflation. When trust isabsent, misbehaving clients can use inflation to plunder resources. Distilledinto a single principle, ticket inflation and deflation should be allowed onlywithin logical trust boundaries. The next section introduces a powerfulabstraction that can be used to define trust boundaries and safely exploitticket inflation. 一个关键的观点是，票据膨胀和收缩的可取性取决于信任。信任意味着可以在没有明确授权的情况下占用资源。当存在信任时，显式票据转移往往比简单的本地票据膨胀更加繁琐和受限。而在缺乏信任的情况下，行为不端的客户端可能会利用膨胀来掠夺资源。归结为一个原则，票据膨胀和收缩应仅在逻辑信任边界内被允许。下一节将介绍一种强大的抽象方法，可以用来定义信任边界，并安全地利用票据膨胀。Ticket CurrenciesA ticket currency is a resource management abstraction that contains the effectsof ticket inflation in a modular way. The basic concept of a ticket is extendedto include a currency in which the ticket is denominated. Since each ticket isdenominated in a currency, resource rights can be expressed in units that arelocal to each group of mutually trusting clients. A currency derives its valuefrom backing tickets that are denominated in more primitive currencies. Theticketsthat back a currency are said to fund that currency. The value of acurrency can be used to fund other currencies or clients by issuing ticketsdenominated in that currency. The effects of inflation are locally contained byeffectively maintaining an exchange rate between each local currency and acommon base currency that is conserved. The values of tickets denominated indifferent currencies are compared by first converting them into units of thebase currency 票据货币是一种资源管理抽象，可以以模块化的方式限制票据膨胀的影响。票据的基本概念被扩展为包含其所计价的货币。由于每张票据都以某种货币计价，资源权限可以用只在一组相互信任的客户端内部有效的单位来表达。一种货币的价值来源于以更原始货币计价的、为其提供支持的票据。为某种货币提供支持的票据被称为为该货币“提供资金”。货币的价值可以通过发行以该货币计价的票据，来为其他货币或客户端提供资金。通过有效地维护每种本地货币与一个守恒的通用基础货币之间的汇率，膨胀的影响被局部限制。不同货币计价的票据价值可以通过先将其兑换为基础货币单位，再进行比较。Figure 2-2 depicts key aspects of ticket and currency objects. A ticket objectconsists of an amount denominated in some currency; the notation amount.currencywill be used to refer to a ticket. A currency object consists of a unique name,a list of backing tickets that fund the currency, a list of tickets issued inthe currency, and an amount that contains the total number of active ticketsissued in the currency. In addition, each currency should maintain permissionsthat determine which clients have the right to create and destroy ticketsdenominated in that currency. A variety of well-known schemes can be used toimplement permissions [Tan92]. For example, an access control list can beassociated with each currency to specify those clients that have permission toinflate it by creating new tickets. 图2-2展示了票据和货币对象的关键方面。一个票据对象由以某种货币计价的金额组成，记作 amount.currency。一个货币对象由唯一名称、为该货币提供资金的支持票据列表、以该货币发行的票据列表，以及表示该货币已发行的有效票据总数的金额组成。此外，每种货币还应维护权限，用于确定哪些客户端有权创建和销毁以该货币计价的票据。可以采用多种知名方案来实现权限管理 [Tan92]。例如，可以为每种货币关联一个访问控制列表，以指定哪些客户端有权限通过创建新票据来进行膨胀。Currency relationships may form an arbitrary acyclic graph, enabling a widevariety of different resource management policies. One useful currencyconfiguration is a hierarchy of currencies. Each currency divides its value intosubcurrencies that recursively subdivide and distribute that value by issuingtickets. Figure 2-3 presents an example currency graph with a hierarchical treestructure. In addition to the common base currency at the root of the tree,distinct currencies are associated with each user and task. Two users, Alice andBob, are competing for computing resources. The alice currency is backed by 3000tickets denominated in the base currency (3000.base), and the bob currency isbacked by 2000 tickets denominated in the base currency (2000.base). Thus, Aliceis entitled to 50% more resources than Bob, since their currencies are funded ata 3 : 2 ratio. 货币关系可以形成任意的无环图，从而支持多种不同的资源管理策略。其中一种有用的货币配置是货币层级结构。每种货币通过发行票据，将其价值分割成子货币，并递归地细分和分配这些价值。图2-3展示了一个具有层级树结构的货币图示例。除了树根处的公共基础货币之外，每个用户和任务都关联着不同的货币。两个用户，Alice 和 Bob，正在竞争计算资源。alice 货币由以基础货币计价的 3000 张票据（3000.base）支持，bob 货币由以基础货币计价的 2000 张票据（2000.base）支持。因此，Alice 有权获得比 Bob 多50% 的资源，因为他们的货币支持比例为 3 : 2。Alice is executing two tasks, task1 and task2. She subdivides her allocationbetween these tasks in a 2 : 1 ratio using tickets denominated in her owncurrency – 200.alice and 100.alice. Since a total of 300 tickets are issued inthe alice currency, backed by a total of 3000 base tickets, the exchange ratebetween the alice and base currencies is 1 : 10. Bob is executing a single task,task3, and uses his entire allocation to fund it via a single 100.bob ticket.Since a total of 100 tickets are issued in the bob currency, backed by a totalof 2000 base tickets, the bob : base exchange rate is 1 : 20. If Bob were tocreate a second task with equal funding by issuing another 100.bob ticket, thisexchange rate would become 1 : 10. Alice 正在执行两个任务：task1 和 task2。她通过以自己货币计价的票据——200.alice和 100.alice——按照 2 : 1 的比例将分配的资源划分给这两个任务。由于 alice 货币一共发行了 300 张票据，而其背后由 3000 张基础货币票据支持，alice 与基础货币之间的汇率为 1 : 10。Bob 正在执行一个任务 task3，并通过一张 100.bob 的票据将他全部的分配资源用于该任务。由于 bob 货币一共发行了 100 张票据，由 2000 张基础货币票据支持，bob 与基础货币之间的汇率为 1 : 20。如果 Bob 再创建一个任务，并通过再发行一张 100.bob 的票据给予同等资源支持，那么 bob 货币与基础货币的汇率将变为 1 :10。The currency abstraction is useful for flexibly sharing, protecting, and namingresource rights. Sharing is supported by allowing clients with properpermissions to inflate or deflate a currency by creating or destroying tickets.For example, a group of mutually trusting clients can form a currency that poolsits collective resource rights in order to simplify resource management.Protection is guaranteed by maintaining exchange rates that automatically adjustfor intra-currency fluctuations that result from internal inflation ordeflation. Currencies also provide a convenient way to name resource rights atvarious levels of abstraction. For example, currencies can be used to name theresource rights allocated to arbitrary collections of threads, tasks,applications, or users. 货币抽象对于灵活地共享、保护和命名资源权限非常有用。共享通过允许拥有适当权限的客户端，通过创建或销毁票据来膨胀或收缩货币得以实现。例如，一组相互信任的客户端可以创建一个货币，将其集体资源权限集中起来，以简化资源管理。保护则通过维护汇率来保证，这些汇率会自动调整，以应对由于内部膨胀或收缩而导致的货币内部波动。货币还为在不同抽象层次上命名资源权限提供了方便的方法。例如，货币可以用来为任意线程、任务、应用或用户集合分配的资源权限命名。Since there is nothing comparable to a currency abstraction in conventionaloperating systems, it is instructive to examine similar abstractions that areprovided in the domain of programming languages. Various aspects of currenciescan be related to features of objectoriented systems, including data abstraction,class definitions, and multiple inheritance. 由于传统操作系统中并没有类似于货币抽象的机制，因此研究编程语言领域中提供的类似抽象是很有启发意义的。货币的各个方面可以与面向对象系统中的一些特性相关联，包括数据抽象、类定义以及多重继承等。For example, currency abstractions for resource rights resemble dataabstractions for data objects. Data abstractions hide and protectrepresentations by restricting access to an abstract data type. By default,access is provided only through abstract operations exported by the data type.The code that implements those abstract operations, however, is free to directlymanipulate the underlying representation of the abstract data type. Thus, anabstraction barrier is said to exist between the abstract data type and itsunderlying representation [LG86]. 例如，用于资源权限的货币抽象类似于用于数据对象的数据抽象。数据抽象通过限制对抽象数据类型的访问，隐藏并保护其内部表示。默认情况下，只有通过该数据类型导出的抽象操作才能进行访问。然而，实现这些抽象操作的代码可以直接操作抽象数据类型的底层表示。因此，在抽象数据类型与其底层表示之间，存在一个所谓的抽象屏障 [LG86]。A currency defines a resource management abstraction barrier that providessimilar properties for resource rights. By default, clients are not trusted, andare restricted from interfering with resource management policies thatdistribute resource rights within a currency. The clients that implement acurrency’s resource management policy, however, are free to directly manipulateand redistribute the resource rights associated with that currency. 一种货币定义了一个资源管理的抽象屏障，为资源权限提供了类似的属性。默认情况下，客户端是不被信任的，因此被限制不能干扰在该货币内部分配资源权限的资源管理策略。而实现某种货币资源管理策略的客户端，则可以自由地直接操作和重新分配与该货币相关的资源权限。The use of currencies to structure resource-right relationships also resemblesthe use of classes to structure object relationships in object-oriented systemsthat support multiple inheritance. A classinheritsits behavior from a setofsuperclasses, which are combined and modified to specify new behaviors forinstances of that class. A currency inherits its funding from a set of backingtickets, which are combined and then redistributed to specify allocations fortickets denominated in that currency. However, one difference between currenciesand classes is the relationship among the objects that they instantiate. When acurrency issues a new ticket, it effectively dilutes the value of all existingtickets denominated in that currency. In contrast, the objects instantiated by aclass need not affect one another. 用货币来构建资源权限关系的方式，也类似于在支持多重继承的面向对象系统中用类来构建对象关系。一个类从一组超类继承其行为，并通过组合和修改这些超类，来为该类的实例指定新的行为。同样，货币通过一组支持票据获得其资金，这些票据被组合并重新分配，用于指定以该货币计价票据的分配。然而，货币和类之间有一个重要的区别，那就是它们所实例化对象之间的关系。当一种货币发行新票据时，实际上会稀释该货币下所有现有票据的价值。相比之下，由一个类实例化的对象则不会相互影响。2.5 Example PoliciesA wide variety of resource management policies can be specified using thegeneral framework presented in this chapter. This section examines severaldifferent resource management scenarios, and demonstrates how appropriatepolicies can be specified. 可以使用本章提出的通用框架来制定各种资源管理策略。本节将分析几种不同的资源管理场景，并展示如何指定合适的策略。2.5.1 Basic PoliciesUnlike priorities which specify absolute precedence constraints,tickets arespecifically designed to specify relative service rates. Thus, the most basicexamples of ticket-based resource management policies are simple service ratespecifications. If the total number of tickets in a system is fixed, then aticket allocation directly specifies an absolute share of a resource. Forexample, a client with 125 tickets in a system with a total of 1000 tickets willreceive a 12.5% resource share. Ticket allocations can also be used to specifyrelative importance. For example, a client that is twice as important as anotheris simply given twice as many tickets. 与优先级（用于指定绝对优先约束）不同，票据（tickets）专门用于指定相对服务速率。因此，基于票据的资源管理策略最基本的例子就是简单的服务速率规定。如果系统中的票据总数是固定的，那么票据分配就直接指定了资源的绝对份额。例如，在一个总票据数为1000的系统中，某个客户端拥有125张票据，则它将获得12.5%的资源份额。票据分配也可以用来指定相对重要性。例如，一个客户端比另一个重要两倍时，只需分配给它两倍的票据即可。Ticket inflation and deflation provide a convenient way for concurrent clientsto implement resource management policies. For example, cooperative(AND-parallel) clients can independently adjust their ticket allocations basedupon application-specific estimates of remaining work. Similarly, competitive(OR-parallel) clients can independently adjust their ticket allocations based onapplication-specific metrics for progress. One concrete example is themanagement of concurrent computations that perform heuristic searches. Suchcomputations typically assign numerical values to summarize the progress madealong each search path. These values can be used directly as ticket assignments,focusing resources on those paths which are most promising, without starving theexploration of alternative paths. 票据的膨胀和收缩为并发客户端实现资源管理策略提供了一种便捷的方法。例如，协作式（AND-并行）客户端可以根据应用特定的剩余工作量估算，独立地调整其票据分配。同样，竞争式（OR-并行）客户端可以根据应用特定的进度指标，独立地调整其票据分配。一个具体的例子是管理执行启发式搜索的并发计算。这类计算通常会为每条搜索路径分配数值，以总结其进展情况。这些数值可以直接用作票据分配，将资源集中于最有前景的路径，同时不会让其他替代路径的探索陷入饥饿状态。Tickets can also be used to fund speculative computations that have thepotential to accelerate a program’s execution, but are not required forcorrectness. With relatively small ticket allocations, speculative computationswill be scheduled most frequently when there is little contention for resources.During periods of high resource contention, they will be scheduled veryinfrequently. Thus, very low service rate specifications can exploit unusedresources while limiting the impact of speculation on more importantcomputations. 票据还可以用于支持具有加速程序执行潜力但并非正确性所必需的投机性计算。通过分配较少的票据，投机性计算通常会在资源竞争较小的时候被频繁调度；而在资源竞争激烈的时期，它们则很少被调度。因此，极低的服务速率设定既能利用未被使用的资源，又能限制投机性计算对更重要计算任务的影响。If desired, tickets can also be used to approximate absolute priority levels.For example, a series of currencies $c_1$, $c_2$, … , $c_n$ can be definedsuch that currency $c_i$ has 100 times the funding of currency $c_i-1$. A clientwith emulated priority level is allocated a single ticket denominated currency$c_i$. Clients at priority level $i$ will be serviced 100 times more frequentlythan clients $i-1$, approximating a strict priority ordering. 如果需要，票据也可以用来近似实现绝对优先级。例如，可以定义一系列货币 c₁、c₂、…、$c_n$, 使得货币 cᵢ 的资金量是货币 cᵢ-1 的 100 倍。具有模拟优先级的客户端会被分配一个以货币 cᵢ 计价的票据。处于优先级 i 的客户端将比处于优先级 i-1 的客户端获得100 倍的服务频率，从而近似实现严格的优先级排序。2.5.2 Administrative PoliciesFor long-running computationssuch as those found in engineering and scientificenvironments, there is a need to regulate the consumption of computing resourcesthat are shared among users and applications of varying importance [Hel93].Currencies can be used to isolate the policies of projects, users, andapplicationsfrom one another, and relative funding levels can be used to specifyimportance.For example, a system administrator can allocate ticket levels to differentgroups based on criteria such as project importance, resource needs, or realmonetary funding. Groups can subdivide their allocations among users based uponneed or status within the group; an egalitarian approach would give each user anequal allocation. Users can directly allocate their own resource rights toapplications based upon factors such as relative importance or impendingdeadlines. Since currency relationships need not follow a strict hierarchy,users may belong to multiple groups. It is also possible for one group tosubsidize another. For example, if group A is waiting for results from group B,it can issue a ticket denominated in currency A, and use it to fund group B. 对于工程和科学领域中常见的长时间运行计算，有必要对在不同重要性用户和应用之间共享的计算资源进行调控 [Hel93]。可以通过货币机制将项目、用户和应用的策略相互隔离，并利用相对资金水平来指定其重要性。 例如，系统管理员可以根据项目重要性、资源需求或实际资金等标准，为不同的群组分配票据额度。各群组可以根据成员的需求或地位将分配的票据进一步划分给用户；采用平等主义方法时，则为每个用户分配相同额度。用户又可以根据应用的相对重要性或临近的截止时间，将自己的资源权直接分配给具体应用。由于货币关系不必遵循严格的层级结构，用户可以属于多个群组。同时，一个群组也可以补贴另一个群组。例如，如果群组A在等待群组B的结果，A可以发行以货币A计价的票据，并用其资助群组B2.5.3 Interactive Application PoliciesFor interactive computations such as databases and media-based applications,programmers and users need the ability to rapidly focus resources on those tasksthat are currently important. In fact, research in computer-human interactionhas demonstrated that responsiveness is often the most significant factor indetermining user productivity [DJ90].Many interactive systems, such as databases and the World Wide Web, arestructured using a client-server framework. Servers process requests from a widevariety of clients that may demand different levels of service. Some requestsmay be inherently more important or time-critical than others. Users may alsovary in importance or willingness to pay a monetary premium for better service.In such scenarios, ticket allocations can be used to specify importance, andticket transfers can be used to allow servers to compute using the resourcerights of requesting clients. 对于交互式计算（如数据库和基于媒体的应用），程序员和用户需要能够快速将资源集中到当前重要的任务上。事实上，计算机与人类交互的研究已经表明，响应速度往往是决定用户生产力的最重要因素之一 [DJ90]。 许多交互式系统（如数据库和万维网）都采用客户端-服务器框架。服务器要处理来自各种客户端的请求，这些请求可能对服务水平有不同的需求。有些请求本身就比其他请求更重要或更具时效性。用户之间也可能因重要性或愿意为更好服务支付额外费用而有所不同。在这种情况下，可以通过票据分配来指定重要性，通过票据转移让服务器使用请求客户端的资源权进行计算。Another scenario that is becoming increasingly common is the need to control thequality of service when two or more video viewers are displayed [CT94]. Adaptiveviewers are capable of dynamically altering image resolution and frame rates tomatch current resource availability. Coupled with dynamic ticket inflation,adaptive viewers permit users to selectively improve the quality of those videostreams to which they are currently paying the most attention. For example, agraphical control associated with each viewer could be manipulated to smoothlyimprove or degrade a viewer’s quality of service by inflating or deflating itsticket allocation. Alternatively, a preset number of tickets could be associatedwith the window that owns the current input focus. Dynamic ticket transfers makeit possible to shift resources as the focus changes, e.g., in response to mousemovements. With an input device capable of tracking eye movements, a similartechnique could even be used to automatically adjust the performance ofapplications based upon the user’s visual focal point. 另一种日益常见的场景是，当同时显示两个或更多视频播放器时，需要控制服务质量[CT94]。自适应播放器能够根据当前的资源可用性动态调整图像分辨率和帧率。结合动态票据膨胀机制，自适应播放器允许用户有选择地提升他们当前最关注的视频流的服务质量。例如，可以通过与每个播放器关联的图形控制组件，平滑地提升或降低某个播放器的服务质量，方法是膨胀或收缩其票据分配。或者，可以将预设数量的票据分配给当前拥有输入焦点的窗口。动态票据转移机制使得在焦点变化（如响应鼠标移动）时能够灵活地重新分配资源。如果输入设备能够跟踪眼动，甚至可以利用类似技术根据用户的视觉焦点自动调节应用程序的性能。In addition to user-directed control over resource management, programmaticapplication- level control can also be used to improve responsiveness despiteresource limitations [DJ90, TL93]. For example, a graphics-intensive programcould devote a large share of its processing resources to a rendering operationuntil it has displayed a crude but usable outline or wire- frame. The share ofresources devoted to rendering could then be reduced via ticket deflation,allowing a more polished image to be computed while most resources are devotedto improving the responsiveness of more critical operations. 除了用户对资源管理的直接控制之外，应用程序级的编程控制也可以在资源有限的情况下提升响应速度 [DJ90, TL93]。例如，一个对图形处理要求较高的程序可以在渲染操作期间，分配大量处理资源，直到显示出一个粗略但可用的轮廓或线框。随后，可以通过票据收缩减少分配给渲染的资源份额，使得在计算更精细图像的同时，大部分资源用于提升更关键操作的响应速度。Chapter 3 Proportional-Share MechanismsThis chapter presents mechanisms that can be used to efficiently implement theresource management framework described in Chapter 2. Several novel schedulingalgorithms are introduced, including both randomized and deterministictechniques that provide proportional- share control over time-shared resources.The algorithms are presented in the order that they were developed, followed bya discussion of their application to the general resource management framework. 本章介绍了可用于高效实现第2章所述资源管理框架的机制。提出了几种新颖的调度算法，包括能够对时间共享资源实现比例分配控制的随机和确定性技术。这些算法按照其开发顺序进行介绍，随后讨论了它们在通用资源管理框架中的应用。One common theme is the desire to achieve proportional sharing with a highdegree of accuracy. The throughput accuracy of a proportional-share schedulercan be characterized by measuring the difference between the specified andactual number of allocations that a client receives during a series ofallocations. If a client has $t$ tickets in a system with a total of $T$ tickets,then its specified allocation after consecutive $n_a$ allocations is $n_a t / T$. Due to quantization, it is typically impossible to achieve this ideal exactly.A client’s absolute error is defined as the absolute value of the differencebetween its specified and actual number of allocations. The pairwise relativeerror between clients $c_i$ and $c_j$ is defined as the absolute error for thesubsystem containing only $c_i$ and $c_j$, where $T = t_i + t_j$, and $n_a$ isthe total number of allocations received by both clients. 一个共同的主题是希望实现高度精确的比例共享。比例分配调度器的吞吐精度可以通过测量在一系列分配过程中，客户端实际获得的分配次数与其规定分配次数之间的差异来衡量。如果某个客户端拥有 $t$ 张票据，而系统中共有 $T$ 张票据，那么在连续 $n_a$ 次分配后，该客户端的规定分配次数为 $n_a \\cdot \\frac{t}{T}$。由于量化的存在，通常无法完全达到这一理想值。客户端的绝对误差定义为其规定分配次数与实际分配次数之差的绝对值。客户端 $c_i$ 和 $c_j$ 之间的成对相对误差定义为仅包含 $c_i$ 和 $c_j$ 的子系统中的绝对误差，其中 $T = t_i + t_j$，$n_a$ 是这两个客户端实际获得的分配次数之和。Another key issue is the challenge of providing efficient, systematic supportfor dynamic operations, such as modifications to ticket allocations, and changesin the number of clients competing for a resource. Support for fast dynamicoperations is also required for low-overhead implementations of higher-levelabstractions such as ticket transfers, ticket inflation, and ticket currencies.Many proportional-share mechanisms that are perfectly reasonable for staticenvi- ronments exhibit ad-hoc behavior or unacceptable performance in dynamicenvironments. 另一个关键问题是如何高效、系统地支持动态操作，比如票据分配的修改，以及竞争某项资源的客户端数量的变化。对于高层抽象（如票据转移、票据膨胀和票据货币）的低开销实现，也需要对动态操作提供快速支持。许多在静态环境下完全合理的比例分配机制，在动态环境下却表现出临时性的行为或不可接受的性能。After initial experimentation with a variety of different techniques, Idiscovered that ran- domization could be exploited to avoid most of thecomplexity associated with dynamic op- erations. This realization led to thedevelopment of lottery scheduling, a new randomized resource allocationmechanism [WW94]. Lottery scheduling performs an allocation by hold- ing alottery; the resource is granted to the client with the winning ticket. Due toits inherent use of randomization, a client’s expected relative error andexpected absolute error under lottery scheduling are both $O\\sqrt{n_a}$. Thus,lottery scheduling can exhibit substantial variability over small numbers ofallocations. Attempts to limit this variability resulted in an investigation ofmulti-winner lottery scheduling, a hybrid technique with both randomized anddeterministic components. 在对多种不同技术进行初步实验后，我发现可以利用随机化来避免与动态操作相关的大部分复杂性。这一认识促使我开发了彩票调度（lottery scheduling），这是一种新的随机化资源分配机制 [WW94]。彩票调度通过举办一次“彩票抽奖”来进行分配，资源会被分配给持有中奖票据的客户端。由于其本质上采用了随机化方法，客户端在彩票调度下的期望相对误差和期望绝对误差都是 $O\\sqrt{n_a}$。因此，彩票调度在分配次数较少时可能会表现出较大的波动。为了限制这种波动，我进一步研究了多赢家彩票调度（multi-winnerlottery scheduling），这是一种结合了随机和确定性成分的混合技术。A desire for even more predictable behavior over shorter time scales prompted arenewed effort to develop a deterministic algorithm with efficient support fordynamic operations. Optimization of an inefficient algorithm that I originallydeveloped before the conception of lottery scheduling resulted in stridescheduling [WW95]. Stride scheduling is a deterministic algorithm that computesa representation of the time interval, or stride, that each client must waitbetween successive allocations. Under stride scheduling, the relative error forany pair of clients is never greater than one, independent of . However, forskewed ticket distributions it is still possible for a client to have $O(n_c)$absolute error, where $n_c$ is the number of clients. 对在较短时间尺度内实现更加可预测行为的需求，促使我重新努力开发一种对动态操作具有高效支持的确定性算法。对我在提出彩票调度之前开发的一个低效算法进行优化，最终诞生了步进调度（stride scheduling）[WW95]。步进调度是一种确定性算法，它计算每个客户端在连续分配之间必须等待的时间间隔（即步进）。在步进调度下，任意两个客户端之间的相对误差永远不会大于 1，并且与 无关。然而，对于票据分布极度不均的情况，某个客户端的绝对误差仍然可能达到 $O(n_c)$ , 其中 $n_c$ 是客户端的数量。I later discovered that the core allocation algorithm used in stride schedulingis nearly iden- tical to elements of rate-based flow-control algorithms designedfor packet-switched networks [DKS90, Zha91, ZK91, PG93]. Thus, stride schedulingcan be viewed as a cross-application of these networking algorithms to scheduleother resources such as processor time. However, the original network-orientedalgorithms did not address the issue of dynamic operations, such as changes toticket allocations. Since these operations are extremely important in domainssuch as processor scheduling, I developed new techniques to efficiently supportthem. These techniques can also be used to support frequent changes in bandwidthallocations for networks. 后来我发现，步进调度中使用的核心分配算法与为分组交换网络设计的基于速率的流量控制算法中的某些元素几乎完全相同 [DKS90, Zha91, ZK91, PG93]。因此，步进调度可以被视为将这些网络算法跨领域应用于诸如处理器时间等其他资源的调度。然而，原始面向网络的算法并没有解决动态操作的问题，比如票据分配的变动。由于这些操作在处理器调度等领域极为重要，我开发了新的技术来高效支持这些操作。这些技术同样可以用于支持网络带宽分配的频繁变化。Finally, dissatisfaction with the schedules produced by stride scheduling forskewed ticket distributions led to an improved hierarchical stride schedulingalgorithm that provides a tighter $O(\\lg n_c)$ bound on each client’s absolute error.Hierarchical stride scheduling is a novel recursive application of the basictechnique that achieves better throughput accuracy than previous schemes, andcan reduce response-time variability for some workloads. 最后，由于步进调度在票据分布极度不均时产生的调度结果令人不满意，促使我提出了一种改进的分层步进调度算法，该算法能够将每个客户端的绝对误差收紧到 $O(\\lg n_c)$的界限。分层步进调度是一种对基本技术的递归应用，能够比以往的方案实现更高的吞吐精度，并且可以降低某些工作负载的响应时间波动性。The remainder of this chapter presents lottery scheduling, multi-winner lotteryscheduling, stride scheduling, and hierarchical stride scheduling. Eachmechanism is described in a separate section that begins with a description ofthe basic algorithm, followed by a discussion of extensions that support dynamicoperations and irregular quantum sizes. Source code and examples are included toillustrate each mechanism. The chapter concludes by demonstrating that eachpresented mechanism is capable of serving as a substrate for the generalresource management framework presented in Chapter 2. Detailed simulationresults, performance analyses, and comparisons of the mechanisms are presentedin Chapter 4. 本章的其余部分将介绍彩票调度、多赢家彩票调度、步进调度以及分层步进调度。每种机制都在单独的小节中进行描述，首先介绍其基本算法，随后讨论支持动态操作和不规则时间片大小的扩展。每种机制都配有源代码和示例以便说明。最后，本章将展示这些机制都能够作为第2章所提出的通用资源管理框架的基础。第4章将详细呈现这些机制的仿真结果、性能分析以及相互之间的比较。3.1 Lottery SchedulingLottery scheduling is a randomized resource allocation mechanism for time-sharedresources. Each allocation is determined by holding a lottery that randomlyselects a winning ticket from the set of all tickets competing for a resource.The resource is granted to the client that holds the winning ticket. This simpleoperation effectively allocates resources to competing clients in proportion tothe number of tickets that they hold. This section first presents the basiclottery scheduling algorithm, and then introduces extensions that supportdynamic operations and nonuniform quanta. 彩票调度是一种用于时间共享资源的随机化资源分配机制。每次分配都通过举办一次彩票抽奖来决定，系统会从所有竞争该资源的票据中随机选出一张中奖票据，资源则分配给持有该中奖票据的客户端。这个简单的操作能够有效地根据各客户端持有票据的数量，按比例分配资源。本节将首先介绍基本的彩票调度算法，然后介绍支持动态操作和非均匀时间片的扩展。3.1.1 Basic AlgorithmThe core lottery scheduling idea is to randomly select a ticket from the set ofall tickets competing for a resource. Since each ticket has an equal probabilityof being selected, the probability that a particular client will be selected isdirectly proportional to the number of tickets that it has been assigned. 彩票调度的核心思想是从所有竞争某项资源的票据集合中随机选取一张票据。由于每张票据被选中的概率是相等的，因此某个客户端被选中的概率就与其所分配到的票据数量成正比。In general, there are clients competing for a resource, and each client hastickets. Thus, there are a total of $\\sum_{i=1}^{n_c} t_i$ tickets competingfor the resource. The probability that client will win a particular lottery issimply $t_i/T$ . After identical allocations, the expected number of wins forclient is $E[W_i] = n_a p_i$, with variance $\\sigma = n_a p_i (1 - p_i)$. Thus,the expected allocation of resources to clients is proportional to the number oftickets that they hold. Since the scheduling algorithm is randomized, the actualallocated proportions are not guaranteed to match the expected proportionsexactly. However, the disparity between them decreases as the number ofallocations increases. More precisely, a client’s expected relative error andexpected absolute error are both $O\\sqrt{n_a}$. Since error increases slowlywith $n_a$, accuracy steadily improves when error is measured as a percentage of$n_a$. 一般来说，系统中有若干客户端在竞争某项资源，每个客户端拥有一定数量的票据。因此，总共有 $\\sum_{i=1}^{n_c} t_i$ 张票据在竞争该资源。某个客户端赢得一次抽奖的概率就是 $\\frac{t_i}{T}$，其中 $T$ 为所有票据的总数。经过 $n_a$ 次相同的分配后，客户端 $i$ 的期望获胜次数为 $E[W_i] = n_a p_i$，其方差为 $\\sigma = n_a p_i (1 -p_i)$。因此，分配给各客户端的资源期望值与其持有的票据数量成正比。由于调度算法是随机化的，实际分配比例并不一定与期望比例完全一致。然而，随着分配次数的增加，二者之间的差距会逐渐缩小。更准确地说，客户端的期望相对误差和期望绝对误差都为$O(\\sqrt{n_a})$。由于误差随 $n_a$ 增长较慢，当以 $n_a$ 的百分比来衡量误差时，准确性会持续提高。 $\\sigma = n_a p_i (1 - p_i)$: 二项分布 回头研究下原理 One straightforward way to implement a lottery scheduler is to randomly select awinning ticket, and then search a list of clients to locate the client holdingthat ticket. Figure 3-1 presents an example list-based lottery. Five clients arecompeting for a resource with a total of 20 tickets. The thirteenth ticket israndomly chosen, and the client list is searched to determine the client holdingthe winning ticket. In this example, the third client is the winner, since itsregion of the ticket space contains the winning ticket. 实现彩票调度器的一种直接方法是随机选择一张获胜票，然后在客户端列表中查找持有该票的客户端。图3-1展示了一个基于列表的彩票调度示例。五个客户端正在争夺一个总共包含20张票的资源。第13号票被随机选中，然后在客户端列表中搜索，以确定持有获胜票的客户端。在这个例子中，第三个客户端获胜，因为它所占据的票据空间包含了获胜票。Figure 3-2 lists ANSI C code for a basic list-based lottery scheduler. Forsimplicity, it is assumed that the set of clients is static, and that clientticket assignments are fixed. These restrictions will be relaxed in subsequentsections to permit more dynamic behavior. Each client must be initialized viaclient_init() before any allocations are performed by allocate(). The allocate()operation begins by calling fast random() to generate a uniformly-distributedpseudo-random integer. Numerous techniques exist for generating random numbers.For example, the Park-Miller generator efficiently produces high-quality randomnumbers that are uniformly distributed between 0 and $2^{31} - 1$ [PM88, Car90].The random number produced by fast_random() is then scaled to reside in theinterval [0 , global_tickets-1], which will be referred to as the ticketspace. The scaled random number, winner, represents the offset of the winningticket in the ticket space. The ticket space is then scanned by traversing theclient list, accumulating a running ticket sum until the winning offset isreached. The client holding the ticket at the winning offset is selected as thewinner. 图3-2列出了一个基于列表的彩票调度器的 ANSI C 代码。为简化起见，假定客户端集合是静态的，且每个客户端的票据分配是固定的。后续章节将放宽这些限制，以实现更动态的行为。在进行任何分配之前，每个客户端必须通过 client_init() 进行初始化。allocate() 操作首先调用 fast_random()，生成一个均匀分布的伪随机整数。生成随机数的方法有很多，例如，Park-Miller 生成器可以高效地产生分布在 0 到 231−1 之间的高质量随机数 [PM88, Car90]。fast_random() 产生的随机数会被缩放到区间 [0, global_tickets-1]，这个区间被称为票据空间。缩放后的随机数 winner表示获胜票在票据空间中的偏移量。然后通过遍历客户端列表，累加票据总数，直到达到获胜偏移量。持有获胜票的客户端被选为赢家。 search的时候，是 $O(n)$复杂度Performing an allocation using the simple list-based lottery algorithm in Figure3-2 requires $O(n_c)$ time to traverse the list of clients. Variousoptimizations can reduce the average number of clients that must be examined.For example, if the distribution of tickets to clients is uneven, ordering theclients by decreasing ticket counts can substantially reduce the average searchlength. Since those clients with the largest number of tickets will be selectedmost frequently, a simple “move-to-front” heuristic can also be very effective. 使用图3-2中的简单基于列表的彩票算法进行分配时，需要 $O(n_c)$ 的时间来遍历客户列表。各种优化方法可以减少必须检查的客户的平均数量。例如，如果分配给各个客户的彩票数量不均匀，可以按照客户的彩票数从多到少进行排序，这样可以显著减少平均搜索长度。由于拥有最多彩票的客户会被选中的概率最高，一个简单的“移到前面”启发式方法也会非常有效。 这个地方并没有让其变的不公平, 只是降低了它的搜索复杂度 For large $n_c$, a tree-based implementation is more efficient, requiring only$O(\\lg n_c)$ opera- tions to perform an allocation. A tree-based implementationwould also be more appropriate for a distributed lottery scheduler. Figure 3-3lists ANSI C code for a tree-based lottery scheduling algorithm. Although manytree-based data structures are possible, a balanced binary tree is used toillustrate the algorithm. Every node has the usual tree links to its parent,left child, and right child, as well as a ticket count. Each leaf noderepresents an individual client. Each internal node represents the group ofclients (leaf nodes) that it covers, and contains their aggregate ticket sum. Anallocation is performed by tracing a path from the root of the tree to a leaf.At each level, the child that covers the region of the ticket space whichcontains the winning ticket is followed. When a leaf node is reached, it isselected as the winning client. 对于较大的 $n_c$（客户数量），基于树的实现更加高效，只需要 $O(\\lg n_c)$ 的操作即可完成一次分配。对于分布式彩票调度器来说，基于树的实现也更加合适。图3-3给出了一个基于树的彩票调度算法的 ANSI C 代码。虽然可以采用多种基于树的数据结构，这里为了说明算法，使用了平衡二叉树。每个节点都拥有常规的指向父节点、左子节点和右子节点的树链接，同时还包含一个彩票数量。每个叶子节点代表一个独立的客户。每个内部节点代表其所覆盖的客户（叶子节点）组，并包含这些客户的彩票总数。分配过程通过从树根到某个叶子节点的路径进行。在每一层，都会跟踪覆盖中奖票区域的子节点。当到达叶子节点时，该节点即被选为中奖客户。Figure 3-4 illustrates an example tree-based lottery. Eight clients arecompeting for a resource with a total of 48 tickets. The twenty-fifth ticket israndomly chosen, and a root-to- leaf path is traversed to locate the winningclient. Since the winning offset does not appear in the region of the ticketspace covered by the root’s left child, its right child is followed. The winningoffset is adjusted from 25 to 15 to reflect the new subregion of the ticketspace that excludes the first ten tickets. At this second level, the adjustedoffset of 15 falls within the left child’s region of the ticket space. Finally,its right child is followed, with an adjusted winning offset of 3. Since thisnode is a leaf, it is selected as the winning client. 图3-4展示了一个基于树的彩票调度的示例。八个客户正在竞争一个资源，总共有48张彩票。第25张彩票被随机选中，然后通过从根节点到叶节点的路径来找到中奖客户。由于中奖偏移量不在根节点左子节点所覆盖的彩票空间区域内，因此沿着右子节点继续查找。中奖偏移量从25调整为15，以反映新的子区域（排除了前10张彩票）。在第二层，调整后的偏移量15落在左子节点所覆盖的彩票空间区域内。最后，再沿着其右子节点查找，中奖偏移量调整为3。由于该节点是叶节点，因此它被选为中奖客户。3.1.2 Dynamic OperationsThe basic algorithms presented in Figures 3-2 and 3-3 do not support dynamicoperations, such as changes in the number of clients competing for a resource,and modifications to client ticket allocations. Fortunately, the use ofrandomization makes adding such support trivial. Since each random allocation isindependent, there is no per-client state to update in response to dynamicchanges. Because lottery scheduling is effectively stateless, a great deal ofcomplexity is eliminated. For each allocation, every client is given a fairchance of winning proportional to its share of the total number of tickets. Anydynamic changes are immediately reflected in the next allocation decision, andno special actions are required. 图3-2和图3-3中展示的基本算法并不支持动态操作，比如竞争资源的客户数量变化，以及客户的彩票分配修改。幸运的是，随机化的使用使得添加这类支持变得非常简单。由于每次随机分配都是独立的，无需针对动态变化去更新每个客户的状态。因为彩票调度本质上是无状态的，大量的复杂性被消除了。每次分配时，每个客户都根据其所持彩票数量占总数的比例，获得公平的中奖机会。任何动态变化都会在下一次分配决策中立即体现，无需进行特殊处理。Figure 3-5 lists ANSI C code that trivially extends the basic list-basedalgorithm to effi- ciently handle dynamic changes. The time complexity of theclient_modify(), client_leave(), and client_join() operations is $O(1)$.Figure 3-6 lists the corresponding extensions for the basic tree-basedalgorithm. These operations require $O(\\lg n_c)$ time to update the ticket sumsfor each of a client’s ancestors. The list-based client_modify() operation andthe tree-based node_modify() operation update global scheduling state only forclients that are actively competing for resources. 图3-5列出了扩展基本基于列表算法以高效处理动态变化的 ANSI C 代码。client_modify()、client_leave() 和 client_join() 操作的时间复杂度为 $O(1)$。图3-6则给出了对基本基于树算法的相应扩展。这些操作需要 $O(lg n_c)$ 的时间来更新每个客户祖先节点的彩票总数。基于列表的 client_modify() 操作和基于树的node_modify() 操作只会为那些正在积极竞争资源的客户更新全局调度状态。3.1.3 Nonuniform QuantaWith the basic lottery scheduling algorithms presented in Figures 3-2 and 3-3, aclient that does not consume its entire allocated quantum will receive less thanits entitled share. Similarly, it may be possible for a client’s usage to exceeda standard quantum in some situations. For example, under a non-preemptivescheduler, the amount of time that clients hold a resource can varyconsiderably. 在图3-2和图3-3中展示的基本彩票调度算法中，如果某个客户没有用完其分配的全部时间片（quantum），它实际获得的资源份额就会少于其应得份额。同样，在某些情况下，客户的使用量也可能超过标准时间片。例如，在非抢占式调度器下，客户持有资源的时间可能会有较大差异。Fractional and variable-size quanta are handled by adjusting a client’s ticketallocation to compensate for its nonuniform quantum usage. When a clientconsumes a fraction $f$ of its allocated time quantum, it is assigned transientcompensation tickets that alter its overall ticket value by $1/f$ until theclient starts its next quantum. This ensures that a client’s expected resourceconsumption, equal to $f$ times its per-lottery win probability , is adjusted by tomatch its allocated share. If $f&lt;1$, then the client will receive positivecompensation tickets, inflating its effective ticket allocation. If $f&gt;1$, then theclient will receive negative compensation tickets, deflating its effectiveallocation. 对于分数和可变大小的时间片，可以通过调整客户的彩票分配来补偿其不均匀的时间片使用情况。当客户只消耗了分配时间片的一部分 $f$ 时，会为其分配临时补偿彩票，使其总体彩票数在客户开始下一个时间片之前暂时变为 $1/f$ 倍。这确保了客户的预期资源消耗（即 $f$ 乘以其每次彩票获胜概率）能够调整为与其分配份额相匹配。如果$f &lt; 1$，客户会获得正补偿彩票，增加其有效彩票数；如果 $f &gt; 1$，客户会获得负补偿彩票，减少其有效彩票数。To demonstrate that compensation tickets have the desired effect, consider aclient that owns $t$ of the $T$ tickets competing for a resource. Suppose thatwhen the client next wins the resource lottery, it uses a fraction $f$ of itsallocated quantum. The client is then assigned $t/f-t$ transient compensationtickets, changing its overall ticket value to $t/f$. These compensation ticketspersist only until the client wins another allocation. 为了证明补偿彩票能够达到预期效果，假设某个客户拥有竞争某项资源的全部 $T$ 张彩票中的 $t$ 张。假设当该客户下一次赢得资源彩票时，只使用了分配给它的时间片的一部分 $f$。此时，该客户会被分配 $t/f - t$ 张临时补偿彩票，使其总体彩票数变为 $t/f$。这些补偿彩票仅在客户再次赢得资源分配之前有效。Without any compensation, the client’s expected waiting time until its nextallocation would be $T/t-1$ quanta. Compensation alters both the client’s ticketallocation and the total number of tickets competing for the resource. Withcompensation, the client’s expected waiting time becomes $(T+t/f-t)/(t/f) - 1$,which reduces to $fT/t -f$. Measured from the start of its first allocation tothe start of its next allocation, the client’s expected resource usage is $f$quanta over a time period consisting of $f+(fT/t -f) = fT/t$ quanta. Thus, theclient receives a resource share of $f/(fT/t) = t/T$, as desired. 如果没有任何补偿，客户从本次分配到下次分配的预期等待时间是 $T/t - 1$ 个时间片。补偿机制会同时改变客户的彩票分配和竞争该资源的彩票总数。采用补偿后，客户的预期等待时间变为 $(T + t/f - t) / (t/f) - 1$，这个表达式可以简化为 $fT/t - f$。从第一次分配开始到下一次分配开始，客户的预期资源使用量是 $f$ 个时间片，整个时间段总共包含 $f + (fT/t - f) = fT/t$ 个时间片。这样，客户获得的资源份额就是$f/(fT/t) = t/T$，达到预期目标。 这我们简单直观理解: 假设每次被选中的概率是$t/T$, 那么选择$T/t$ 次即可选中，则需要等待$T/t - 1$时间片.\\[\\begin{align}首次等待时间 &amp;= T/t - 1 \\\\预期等待时间 &amp;= 调整后的总时间片/调整后的份额 - 1 \\\\&amp;= \\frac{total\\_all\\_old(T) + task\\_new(1/f) - task_old(t)}{task\\_new(1/f)} - 1 \\\\&amp;= \\frac{T + \\frac{t}{f} - t}{\\frac{t}{f}} - 1 \\\\&amp;= \\frac{Tf + t - tf}{t} - 1 \\\\&amp;= \\frac{Tf}{t} + 1 -f -1 \\\\&amp;= \\frac{Tf}{t} -f\\end{align}\\] 而我来算下当前时间片占据 总时间片的比例，看其是否符合t/T 的比例:\\(\\begin{align}frac &amp;= \\frac{之间占用时间片 }{之前占用时间片 + 等待时间} \\\\&amp;= \\frac{f}{f + \\frac{Tf}{t} - f} \\\\&amp;= \\frac{t}{T}\\end{align}\\) 所以，该权重的补偿，是为了将等待时间缩短到合适的值，仅用临时于下一次该任务调度之前的权重补偿。 Note that no assumptions were made regarding the client’s resource usage duringits second allocation. Compensation tickets produce the correct expectedbehavior even when $f$ varies dynamically, since the client’s waiting time isimmediately adjusted after every allocation. A malicious client is thereforeunable to boost its resource share by varying $f$ in an attempt to“game” thesystem. 请注意，在客户第二次分配资源期间，并没有对其资源使用情况做任何假设。即使 $f$动态变化，补偿彩票也能产生正确的期望效果，因为客户的等待时间会在每次分配后立即进行调整。因此，恶意客户无法通过改变 $f$ 来“钻系统的空子”以提升其资源份额。Figure 3-7 lists ANSI C code for compensating a client that uses elapsedresource time units instead of a standard quantum, measured in the same timeunits. The per-client scheduling state is extended to include a new compensatefield that contains the current number of compensation tickets associated withthe client. The compensate() operation should be invoked immediately afterevery allocation; compensate(current, elapsed) should be added to the end of theallocate() operation. Compensation tickets are transient, and only persistuntil the client starts its next quantum. Thus, compensate() initially forgetsany previous compensation, and computes a new client compensation value based onelapsed. The client’s compensate field is updated, and the overall differencebetween the previous compensated ticket value and its new one is computed as netchange. Finally, the client’s ticket allocation is dynamically modified viaclient_modify(). 图3-7给出了用于补偿客户的 ANSI C 代码，这些客户使用的是实际消耗的资源时间单位，而不是标准时间片（quantum），两者都用相同的时间单位进行度量。每个客户的调度状态被扩展，增加了一个新的 compensate 字段，用于记录当前与该客户相关联的补偿彩票数量。每次资源分配后，都应该立即调用 compensate() 操作；也就是说，在 allocate()操作的末尾应添加 compensate(current, elapsed)。补偿彩票是临时性的，只在客户开始下一个时间片之前有效。因此，compensate() 操作会首先清除之前的补偿信息，并根据本次实际消耗的时间（elapsed）计算新的补偿值。随后，更新客户的 compensate 字段，并计算新的补偿彩票数与之前补偿彩票数之间的差值（net change）。最后，通过client_modify() 动态修改该客户的彩票分配。For example, suppose clients $A$ and $B$ have each been allocated 400 tickets.Client $A$ always consumes its entire quantum, while client $B$ uses onlyone-fifth of its quantum before yielding the resource. Since both $A$ and $B$have equal ticket assignments, they are equally likely to win a lottery whenboth compete for the same resource. However, client $B$ uses only $f = 1/5$ ofits allocated time, allowing client $A$ to consume five times as much of theresource, in violation of their 1 : 1 ticket ratio. To remedy this situation,client $B$ is granted 1600 compensation tickets when it yields the resource.When $B$ next competes for the resource, its total funding will be $400/f = 2000$ tickets. Thus, on average $B$ will win the resource lottery five times asoften as $A$ , each time consuming $1/5$ as much of its quantum as $A$,achieving the desired 1 : 1 allocation ratio. 例如，假设客户 A 和 B 各自分配了 400 张彩票。客户 A 总是用完它的整个时间片（quantum），而客户 B 在释放资源前只使用了时间片的五分之一。由于 A 和 B 分配的彩票数量相同，当他们同时竞争同一个资源时，中奖概率是一样的。然而，客户 B 实际只用掉了分配时间片的 $f = 1/5$，这导致客户 A 实际上消耗的资源是 B 的五倍，违背了他们 1:1 的彩票分配比例。为了解决这个问题，当客户 B 释放资源时，会给它补偿1600 张彩票。这样，当 B 下次竞争资源时，它的总彩票数就变成了 $400 / f = 2000$张。这样，平均来看，B 获得资源的次数会是 A 的五倍，但每次只消耗 A 的五分之一的时间片，从而实现了预期的 1:1 资源分配比例。3.2 Multi-Winner Lottery SchedulingMulti-winner lottery scheduling is a generalization of the basic lotteryscheduling technique. Instead of selecting a single winner per lottery, $n_w$winners are selected, and each winner is granted the use of the resource for onequantum. The set of $n_w$ consecutive quanta allocated by a single multi-winnerlottery will be referred to as a superquantum. This section presents the basicmulti-winner lottery algorithm, followed by a discussion of extensions fordynamic operations and nonuniform quanta. 多赢家彩票调度是对基本彩票调度技术的推广。它不是每轮只选出一个获胜者，而是选出$n_w$ 个获胜者，每个获胜者都获得一次时间片来使用资源。由单次多赢家彩票分配的连续 $n_w$ 个时间片被称为一个超级时间片（superquantum）。本节将介绍基本的多赢家彩票算法，并讨论对动态操作和非均匀时间片的扩展。3.2.1 Basic AlgorithmThe multi-winner lottery scheduling algorithm is a hybrid technique with bothrandomized and deterministic components. The first winner in a superquantum isselected randomly, and the remaining $n_w - 1$ winners are selecteddeterministically at fixed offsets relative to the first winner. These offsetsappear at regular, equally-spaced intervals in the ticket space [0, T -1],where T is the total number of tickets competing for the resource. Moreformally, the $n_w$ winning offsets are located at $(r + i \\frac{T}{n_w}) mod T$in the ticket space, where r is a random number and index $i \\in [0, n_w - 1]$yields the $i^{th}$ winning offset. 多赢家彩票调度算法是一种混合技术，结合了随机和确定性成分。在一个超级时间片（superquantum）中，第一个获胜者是随机选出的，其余的 $n_w-1$ 个获胜者则按照相对于第一个获胜者的固定偏移量以确定性方式选出。这些偏移量在彩票空间 ([0, T-1])内以规则、等间距出现，其中 $T$ 是参与资源竞争的彩票总数。更正式地说, $n_w$个获胜偏移量位于彩票空间中的 $(r + i \\frac{T}{n_w}) \\mod T$ 位置，其中 $r$是一个随机数，索引 $i \\in [0, n_w-1]$ 表示第 $i^{th}$ 个获胜偏移量。Since individual winners within a superquantum are uniformly distributed across the ticketspace, multi-winner lotteries directly implement a form of short-term, proportional-share fair-ness. Because the spacing between winners is $T/n_w$ tickets, a client with tickets is determin-istically guaranteed to receive at least $[n_w \\frac{t}{T}]$ quanta per superquantum. However, there areno deterministic guarantees for clients with fewer than $T/n_w$ tickets. 由于超级时间片（superquantum）内的各个获胜者在彩票空间中均匀分布，多赢家彩票调度直接实现了一种短期的、按比例分配的公平性。由于每两个获胜者之间的间隔为 $T /n_w$ 张彩票，拥有 $t$ 张彩票的客户可以确定性地保证在每个超级时间片内至少获得$\\left[ \\frac{n_w t}{T} \\right]$ 个时间片。然而，对于持有少于 $T / n_w$ 张彩票的客户，则无法做出确定性的保证。3.3 Deterministic Stride SchedulingStride scheduling is a deterministic allocation mechanism for time-sharedresources. Stride scheduling implements proportional-share control overprocessor-time and other resources by cross-applying and generalizing elementsof rate-based flow control algorithms designed for networks [DKS90, Zha91, ZK91,PG93]. New techniques are introduced to efficiently support dynamic operations,such as modifications to ticket allocations, and changes to the number ofclients competing for a resource.3.3.1 Basic AlgorithmThe core stride scheduling idea is to compute a representation of the timeinterval, or stride, that a client must wait between successive allocations. Theclient with the smallest stride will be scheduled most frequently. A client withhalf the stride of another will execute twice as quickly; a client with doublethe stride of another will execute twice as slowly. Strides are represented invirtual time units called passes, instead of units of real time such as seconds.Three state variables are associated with each client: tickets, stride, andpass. The tickets field specifies the client’s resource allocation, relative toother clients. The stride field is inversely proportional to tickets, andrepresents the interval between selections, measured in passes. The pass fieldrepresents the virtual time index for the client’s next selection. Performing aresource allocation is very simple: the client with the minimum pass is selected,and its pass is advanced by its stride. If more than one client has the sameminimum pass value, then any of them may be selected. A reasonable deterministicapproach is to use a consistent ordering to break ties, such as one defined byunique client identifiers.The only source of relative error under stride scheduling is due toquantization. Thus, the the relative error for any pair of clients is nevergreater than one, independent of $n_a$. However, for skewed ticket distributionsit is still possible for a client to have $O(n_c)$ absolute error, where $n_c$is the number of clients. Nevertheless, stride scheduling is considerably moreaccurate than lottery scheduling, since its error does not grow with the numberof allocations.Figure 3-11 lists ANSI C code for the basic stride scheduling algorithm. Forsimplicity, a static set of clients with fixed ticket assignments is assumed.These restrictions will be relaxed in subsequent sections to permit more dynamicbehavior. The stride scheduling state for each client must be initialized viaclient_init() before any allocations are performed by allocate(). Toaccurately represent stride as the reciprocal of tickets, a floating-pointrepresentation could be used. A more efficient alternative is presented thatuses a high-precision fixed-point integer representation. This is easilyimplemented by multiplying the inverted ticket value by a large integerconstant. This constant will be referred to as stride , since it represents thestride corresponding to the minimum ticket allocation of one.The cost of performing an allocation depends on the data structure used toimplement the client queue. A priority queue can be used to implementqueue_remove_min() and other queue operations in $O(\\lg n_c)$ time or better,where is the number of clients [CLR90, Tho95]. A skip list could also provideexpected time queue operations with low constant overhead [Pug90]. For small$n_c$ or heavily skewed ticket distributions, a simple sorted list is likely tobe most efficient in practice.TODO Math 二项分布 几何分布 二项分布与几何分布：从AP统计到实际应用 概率统计14——几何分布 参考链接 Lottery and Stride Scheduling: Flexible Proportional-Share Resource Management(原文)" }, { "title": "[arm] RMM", "url": "/posts/RMM/", "categories": "coco, RMM", "tags": "RMM", "date": "2025-09-03 15:31:00 +0800", "snippet": "overflowRMM 是一个系统软件（固件）器和 RME (hardware extension) 一起构成实现了ARM Confidential Compute Architecture(CCA) 用来提供受保护的可执行环境Realms.Confidential ComputingArmv8-A 通过建立了如下图的权限层次结构:该层次结构的逻辑是，让高权限等级去管理权限异常等级使用的资源...", "content": "overflowRMM 是一个系统软件（固件）器和 RME (hardware extension) 一起构成实现了ARM Confidential Compute Architecture(CCA) 用来提供受保护的可执行环境Realms.Confidential ComputingArmv8-A 通过建立了如下图的权限层次结构:该层次结构的逻辑是，让高权限等级去管理权限异常等级使用的资源，并且资源管理和访问权限是绑定在一起的。举个例子:EL2&amp;1 中 EL2 用来分配为EL1分配物理页，在 EL1&amp;0 中EL1为EL0分配虚拟机认为的物理页。并且低权限等级不能访问高权限等级的资源: guest user space(el0)不能访问 guest kernel (el1)的内存，guest kernel (el1) 不能访问host（el2）的内存。反过来则可以（例如, host 可以访问虚拟机的)这个是在纵轴层级上，而在横轴层级上，secure mode 比 non-secure mode 权限高,所以, secure world 可以访问Non-secure world的PAS，反之则不可以。(虽然non-secureworld的资源不受 secure world的管理)。其关系上更像是单侧隔离的关系。而ARM CCA 则打破了这种绑定, 可以让低权限的软件，去管理高权限的资源。(Non-secure world VMM 管理 Realm).这样，可以正好的整合进云场景中: 在云场景下，虚拟机资源的生命周期(创建，运行，迁移,销毁) 均由VMM控制，但是虚拟机实际上是客户的，其虚拟机中运行的程序，磁盘上的内容不想让host获取到。那么，就需要虚拟机运行到高权限的环境，但是其资源却需要低权限的VMM进行管理。RMM而怎么能让低权限软件访问高权限资源呢?提供接口。就像是用户态程序通过系统调用享用内核的为其创建的资源。例如各种类型的fd。虚拟机也可以通过电源管理接口 trap 到EL2，享用一些hyp的一些半虚拟化的服务等等。在CCA的架构中，接口由RMM实现。我们来看下:RMM 职责: Provide services that allow the Host to create, populate, execute anddestroy Realms. Provide services that allow the initial configuration and contents of aRealm to be attested. Protect the confidentiality and integrity of Realm state during the lifetimeof the Realm. Protect the confidentiality of Realm state during and following destructionof the Realm. 可以看到, RMM的职责除了转发 Host 对 Realm 生命周期的管理操作之外，还需要确保Realm 的机密性与完整性。另外，提供一个服用来验证Realm的初始配置和Realm的初始内容(这个是guest关心的)为了实现上面的职责, RMM 提供了如下接口: TO HOST: Realm Management Interface (RMI): for the creation, population, execution and destruction of Realm TO Realm Realm Services Interface (RSI): manage resources allocated to the Realm, and to request an attestation report PSCI: control power states of VPEs within a Realm. Granule" }, { "title": "schedule: overflow", "url": "/posts/sched/", "categories": "schedule", "tags": "sched", "date": "2025-09-02 22:00:00 +0800", "snippet": "调度子系统的任务:调度程序负责决定运行哪个程序，该程序运行多长时间。调度系统的责任很明确, 需要在不富裕的CPU上，合理的运行所有程序。目前的cpu架构决定,在一个core上, 同一时间只能有一个task运行, 所以调度子系统会决定当前cpu运行某个进程，并且让其他进程等待, 在合适的时机，将cpu上的进程调出，运行下一个合适的进程，依次循环。所以调度系统是建立在多任务的基础上构建, 我们可...", "content": "调度子系统的任务:调度程序负责决定运行哪个程序，该程序运行多长时间。调度系统的责任很明确, 需要在不富裕的CPU上，合理的运行所有程序。目前的cpu架构决定,在一个core上, 同一时间只能有一个task运行, 所以调度子系统会决定当前cpu运行某个进程，并且让其他进程等待, 在合适的时机，将cpu上的进程调出，运行下一个合适的进程，依次循环。所以调度系统是建立在多任务的基础上构建, 我们可以设想下, 如果将来某一天，体系架构从根本上变了, – CPU &gt;&gt; task number, Linus本人可能要执行rm -rf kernel/sched。schedule system type而这种多任务的调度系统分为两类: 非抢占式 抢占式非抢占式是指在前一个任务未主动退出之前，调度子系统不会将另一个该任务踢出，运行另一个任务。而我们日常生活中用到的OS都是抢占式(windows,linux,mac os)。这种调度系统会在进程正在运行时，打断该程序运行，调度另一个task到运行。由于本篇文章主要介绍Linux调度系统，所以，下文中调度均指抢占式调度。质量而衡量一个调度系统的质量，往往有两个: 效率 合理效率是指, 调度子系统本身所占用的cpu时间尽可能的少。而合理就是要满足目前的task 运行需求，或者说计算场景。前一个指标好理解可以定量，而后一个指标却是主观的。所以，向linux这种通用的操作系统需要满足用户绝大部分的需求。我们来简单举个例子:当前CPU上运行两个程序: MP3 player: 用于听歌 非常简单的死循环程序: 单纯用于费电…MP3程序的需求是每个10ms调度其一次，调度频率越稳定，其产生的音乐越稳定. 而”费电”程序只有一个原则, 尽量占用CPU, 让电脑的电量下降更快。那如果, 调度器不能按照这些程序的需求执行呢?那将导致让人非常恼火的卡顿，一度认为是电脑城老板的原因。而我们再举一个比较极端的例子:为了保证mp3 player 的低调度延迟, 在后续调整了调度策略，将调度点设置的很密集，因为调度器本身有性能损耗，导致挤压了放电程序的运行时间，从而导致性能降低。所以调度器总是会遇到各种各样的需求，但是总结来说，大部分的任务可以分为以下两类:任务种类开发者们根据这些task 运行需求大致分为两类: batch process: 大部分时间运行一些计算指令，不和外围设备交互。 interactive process: 这种类型的task 实际的运行时间较少，往往在执行一段时间后，会主动的调度走, 然后进行较长时间等待。这段时间内, task 将无需调度回来，只有当其等待的事件完成后，才有必要继续运行。 我们以上个章节提到的两个任务为例:费电程序，其代码一直执行死循环。其关注的是当前机器的费电效率，也就是吞吐。而由于调度所执行的上下文切换会带来性能损失(例如 flush tlb, cache replacement, 以及调度器本身的cost), 所以调度策略往往是尽量降低其调度频率。而像MP3 player, 其执行完”输出某个音符到设备后”, 主动让出调度，然后等待 10ms，再输出另外一个音符。其希望每隔10ms + 1 us，精准调度到该进程。其关注的是是调度延迟，所以调度策略往往是增加调度点。不幸的是，自古鱼和熊掌不可兼得。降低延迟和增加吞吐本身就是矛盾的。降低延迟往往意味着增加调度点check，或者更加频繁的调度来满足低延迟的需求，但是这样必然会降低整体性能，而降低吞吐。调度器需要在两者之间做平衡。另外，我们往往不能预测某个程序的究竟是哪种类型。例如一个数据库程序，其可能会执行某个排序算法大量消耗cpu，也可能在执行writeback 触发大量IO … 这时，需要调度系统更采用更”合理”的调度策略。Linux 调度系统概念我们再次回顾调度器的职责: 运行哪个进程 运行该进程多长时间Linux 通过为每个进程定义优先级用于辅助调度器决策运行哪个进程通过为每个进程分配时间片来决定 该进程在被调度后运行多长时间. 接下来我们分别看下这两个概念 NOTE 在引入CFS后, 将时间片的概念移除，取而代之的是vruntime的概念.优先级调度算法中最基本的一类是基于优先级的调度1. 更高的优先级不仅让调度系统优先调度, 并且其使用的时间片也更长.而Linux分为两种不同的优先级范围: 名称 范围 值和优先级关系 适用进程 nice [-20,20) 值越小，优先级越大 普通进程 实时优先级 [0, 99] 值越大，优先级越大 实时进程 另外上述描述的是静态优先级，用户可以通过内核接口进行配置。除此之外，内核会在静态优先级的基础上, 根据当前的上下文计算动态优先级。举个例子, 当发现当前进程切换无需flush tlb(例如: 用户进程切内核线程， 或者同一个进程的两个线程切换），可以将优先级稍微拉高，提升被调度的概率。我们将在描述 Linux schedule 历史演进中的具体的调度算法时，详细介绍下每个调度器的动态优先级计算。时间片时间片表明进程在被抢占时所占用的时间1. 调度策略必须规定一个默认的时间片，即使是CFS这样淡化时间片的调度算法也是同样。但是时间片的设置策略会大大影响调度器的行为: 设置过大: 导致系统调度延迟过高 设置过小: 导致系统吞吐减少所以, 科学的划分时间片是调度系统非常重要的优化点之一。而我们后续讲到的cfs，通过定义调度延迟和最小时间片，来根据系统负载动态分配时间片。参考链接 &lt;&lt;Linux 内核设计与实现&gt;&gt; The Rotating Staircase Deadline Scheduler O(1) scheduler for 2.4.19-rc1 Ingo Molnar and Con Kolivas 2.6 scheduler patches RSDL completely fair starvation free interactive cpu scheduler RSDM patch 郭健： Linux进程调度技术的前世今生之“今生” O(1)调度器：Linux2.6版本的核心算法" }, { "title": "How To study Linux kernel and become a committer", "url": "/posts/HOWTO-study-Linux-kernel/", "categories": "learn method", "tags": "learn method", "date": "2025-09-02 20:00:00 +0800", "snippet": "前言学习Linux内核究竟是为了什么 ? 从事内核开发者的人员相对较少, 方便找一份稳定的工作 ? 兴趣使然，对操作系统感兴趣，想走读些内核源码 ? 用户态性能分析自顶向下的需要 ?对于我个人来说，更倾向于1,2两点, 自己也从事Linux 内核,虚拟化一些年头了，但是对知识的掌握毫无体系可言，自己也在纠结如何学习 Linux内核。之前的学习方法往往是从BUG 定位中，掌握一些细节，然...", "content": "前言学习Linux内核究竟是为了什么 ? 从事内核开发者的人员相对较少, 方便找一份稳定的工作 ? 兴趣使然，对操作系统感兴趣，想走读些内核源码 ? 用户态性能分析自顶向下的需要 ?对于我个人来说，更倾向于1,2两点, 自己也从事Linux 内核,虚拟化一些年头了，但是对知识的掌握毫无体系可言，自己也在纠结如何学习 Linux内核。之前的学习方法往往是从BUG 定位中，掌握一些细节，然后在扩展相关的一小部分代码。这样的方法, 对解决BUG 没有什么问题。因为BUG定位，尤其是panic,softlockup/hardlockup, use-after-free, 内存踩踏. 以及一些其他的不难的BUG没有什么问题，因为这些问题的定位不会要求对整个的子系统有一个成体系的了解，而是往往要求对debug工具的熟练使用，以及对BUG触发点上那一点代码的深入了解。并且对于该BUG的修复往往upstream 已经有了，直接backport 回来即可(缺乏更深度的思考)。而如果要是提高下自己的目标呢?例如 : 成为内核某个子系统的 committer ? (甚至maintainer)其对自己掌握的内核知识，提出了更高的标准: 成体系了解该子模块(TopDown): 不再是某个点细节，而是很多很多的细节 需要对该子系统有比较深的见解，知道其为什么演进成当前的样子，当前的样子有什么样的问题 ? 有没有什么样的解决方法 ?基于如此高的要求, 需要对原有的学习方法进行重构。那重构成什么样呢 ?最近再看杨振宁的一些视频, 他在节目中夸夸而谈其认识的一些有名的物理作家的生平和性格, 以及一些研究方法。这是学术界最有成就的人所关注同行除了其学术论文之外的东西。并且这些内容在其为数不多的公开演讲中高频出现。所以，向优秀的同行学习，不仅是学习其学术成果，还要学习其自身的性格优点和学习方法。既然顶级大佬都在关注同行，我们普通人为什么不关注呢 ? NOTE 我们来思考一个问题，不同人差距很大，为什么会造成这种差距。而这种差距会随着时间越来越小，还是越来越大？ 首先来回答另一个问题, 如果不做出任何改变的话, 一定是越来越大。为什么会有初始差距, 为什么会越来越大。为什么有些人其从事过不同的领域，但是其都有不小的成就。原因就在于其效率高。 上面提到了影响效率的两个方面: 性格 学习方法 而性格即抽象又主观，而性格培养也需要方法，所以也可以概括为方法，所以整体来说，是 方法 影响了效率.所以，我们在工作之余，也需要花时间关注下优秀的开发者，如果有机会一定要和其多多交流。而做这些事情，不是为了感叹，其多么多么牛B，而是总结其 学习方法。Linux 学习方法这是本篇文章最想表述的内容，源于 chengjian的一篇文章1:他在写这篇文章时, 已经成为了内核的committer，但是他想成为maintainer (^ ^). 所以思考了自己和maintainer之间的差距，并如何弥补这部分差距。（非常推荐大家看这篇文章,因为这是比较优秀的工程师在其既有的学习方法上的改进（一定是有效的）)大概的总结就是: 自顶向下 先从了解一些操作系统相关的基础概念和原理入手, 对OS 有一个概念性的了解( 非常非常非常关键，十分推荐走读: Operating Systems: Three Easy Pieces) 在走读一些书籍或者博客，对Kernel 子系统有一个框架性的了解(抓总概) 结合书籍，博客， 源码 ， 抠细节. 多总结 多实践 多调试 多尝试修改 NOTE 这一点个人认为是非常重要的, 我们很多时候往往感叹，自己看了代码为什么会忘记. 即便是自己已经将每一行代码都看过，甚至将每一行代码都注释过。但是一段时间后，还是会忘, 并且忘的连渣都不剩。甚至看自己之前的注释都很陌生，回顾这部分内容都得话很长时间。 原因就是缺乏总结和实践。总结不是将自己看过的代码罗列，而是自顶向下的从一个模块出发，将每一处细节串联起一个网（树）。关于这个我们可以参考一些书籍，看起是如何总结一个子模块，以及其中的细节的。 但是和书籍不同，书籍往往是为入门者提供一扇窗户，而自己总结的目的更多的是帮助自己理清这部分知识, 以及其细节: 理解当前看的部分在整个子系统中的所起到的作用，和该子系统中的其他部分的关系，甚至和其他子系统的关系。关于这部分我们一定不要怕麻烦， 因为你现在不花时间，将来就会花更多的时间去重复该事情. 另外要学习画图总结，多学习别的优秀的工程师的画图方法。 我们通过读代码从而得到的认知不一定是正确的，而验证其真伪的一个方式就是调试, 所以，我们应该了解一些调试（可观测）的手段，在学习过程中去使用它。 另外，开发过模块的同学一定知道。自己写的代码，不用总结，也非常清楚其中的细节。而看别人写的代码就没有那么轻松。而内核是Linus 写的，但是其是开源的，我们可以任意修改！ 理解内核某部分的捷径就是去修改它 – apkm 2 (关于调试和修改，因为自己也缺少这方面的实践，暂不展开) 从古至今, 知将来 学习内核要有历史的眼光和发展的眼光1 那它历史上可不一定是这样子的，需要你去回顾它。 它以后也不一定还是它现在的样子，需要你去不断的关注。 这是kernel 学习的难点：厚重的历史。我们看Linux 内核源码是总是会被其各种各样的细节所绊住，很多细节，单看代码，很难搞懂。但是很多的细节代码，是在这几十年的历史中一点点的引入的。我们如何能先屏蔽到这些细节，自顶向下的学习呢。我目前的观点是结合书籍考古。 可以结合一些经典书籍，走读一些古老的代码（2.4, 2.6) 对内核的子模块有一个大体的认知和理解后，在走读新版本的内核，其中的细节我们要多多走读mail list 和 git log去了解更多细节。当然0.1 -&gt; 2.4内核也是有个演进的历史，我个人建议是将主要的一些演进了解下。(例如, 调度器的O(n)-&gt;O(1)-&gt;CFS的演进路线) 1中提到 主线合入的内核的特性, 往往都是通过不断的改进, 经过了 N 轮重构和 review 才合入的主线, 而越早期的讨论, 涉及的内容可能更多, 也更容易大家理解. 对于一些特性,不理解它为什么这么实现的时候, 去找当时社区邮件列表的讨论, 往往是最直接有效的途径. 最近在看匿名页和调度器，深有感触。在2.4 - 2.6内核演进过程中，很多子模块经过了大量的修改（重构），而在一个功能引入时，往往会有竞品(别的作者提出的patch），而这些内核开发者往往会在邮件列表中会详细讨论当前子系统的问题，以及这些patch所带来的影响，其优缺点，以及选择哪个patch的具体原因。以及这些patch如何进一步的改进。某些内容的营养价值很高，绝对的干货。但是 一个很大的问题是，这些信息是零散的，而且每个信息的价值（对于当前）是不同的。如何在茫茫邮件列表中找到这些干货是非常重要的。也是我这边之后所要探索的一个很重要的学习方法。 其他感悟 Operating Systems: Three Easy Pieces(操作系统导论) Linux Kernel Development(LKD: 内核设计与实现) Understanding the Linux Kernel(ULD: 深入理解Linux内核) END 最后，我想用一句话来勉励自己:开始往往不是最难的，最难的是重新开始。参考链接 Linux 内核特性演进史 – 成坚 «linux内核设计与实现»" }, { "title": "[arm] learn CCA", "url": "/posts/learn-CCA/", "categories": "coco, CCA", "tags": "learn CCA", "date": "2025-09-02 16:00:00 +0800", "snippet": "1. OverviewThis guide describes: The role of confidential computing in modern compute platforms The principles of confidential computing. How the Arm Confidential Compute Architecture (Arm CCA) ...", "content": "1. OverviewThis guide describes: The role of confidential computing in modern compute platforms The principles of confidential computing. How the Arm Confidential Compute Architecture (Arm CCA) enables confidentialcomputing in an Arm compute platform. 本指南介绍了： 机密计算在现代计算平台中的作用 机密计算的基本原理 Arm 机密计算架构（Arm CCA）如何在 Arm 计算平台上实现机密计算 After reading this guide, you will be able to: Define confidential computing Describe a complex system Chain of Trust Understand that a Realm is a protected execution environment introduced bythe Arm CCA Describe the difference between execution in a Realm and execution of aTrustZone Trusted Application Explain how Realms are created, managed, and executed on an implementationof the Arm CCA Explain how a Realm owner establishes trust in a Realm Understand use cases and usage scenarios for implementing Arm CCA 阅读本指南后，您将能够： 定义什么是机密计算 描述复杂系统中的信任链（Chain of Trust） 理解 Realm 是 Arm CCA 引入的一种受保护的执行环境 描述在 Realm 中的执行与 TrustZone 可信应用执行的区别 解释在 Arm CCA 实现中，Realm 是如何创建、管理和执行的 解释 Realm 拥有者如何在 Realm 中建立信任 理解实现 Arm CCA 的应用场景和用例 Before you beginThis guide assumes that you are familiar with the Arm Exception model and memorymanagement.If you are not familiar with these subjects, read our AArch64 Exception modeland AArch64 Memory management guides.Some of the operation of the Arm CCA refers to virtual machines andvirtualization. If you are not familiar with these concepts, refer to AArch64virtualization. If you are not familiar with Arm security concepts, seeIntroduction to security. 本指南假定您已经熟悉 Arm 异常模型和内存管理。 如果您不熟悉这些内容，请阅读我们的 AArch64 异常模型和 AArch64 内存管理指南。 Arm CCA 的部分操作涉及虚拟机和虚拟化。如果您不熟悉这些概念，请参考 AArch64 虚拟化相关资料。如果您不熟悉 Arm 的安全相关概念，请参阅安全性简介。2. What is Confidential Computing?Confidential Computing is the protection of data in use, by performingcomputation within a trustworthy hardware-backed secure environment. Thisprotection shields code and data from observation or modification by privilegedsoftware and hardware agents.Any application or Operating System executing in a Confidential Computingenvironment executes in isolation from any non-trusted agent in the rest of thesystem. Any data generated or consumed by the isolated execution cannot beobserved by other actors executing on that platform, without explicit permissionfrom the owner of the data. 机密计算是指通过在可信赖、由硬件支持的安全环境中进行计算，从而保护数据在使用过程中的安全。这种保护能够防止特权软件和硬件代理对代码和数据的观察或修改。 任何在机密计算环境中运行的应用或操作系统，都会与系统中所有非可信代理实现隔离。由隔离执行生成或使用的任何数据，除非数据所有者明确授权，否则其他在该平台上运行的实体无法观察到这些数据。由Arm CCA requirementsAn application developer wants to deploy workloads securely without having totrust the underlying software infrastructure, for example the hypervisor or coderunning in the Secure world.To support application developer workloads, a platform must provide thefollowing: An execution environment which provides isolation from all untrusted agents A mechanism to establish that the execution environment has been initializedinto a trustworthy state. Initialization requires the execution environmentto have its own Chain of Trust, independent from the Chain of Trust that theparallel untrusted environments in the platform use.In this guide, we explain how the Arm CCA fulfills these requirements throughhardware implementation and use of software. 应用开发者希望能够安全地部署工作负载，而无需信任底层的软件基础设施，例如虚拟机管理程序（hypervisor）或运行在安全世界中的代码。 为了支持应用开发者的工作负载，平台必须提供以下能力： 一个能够与所有不受信任代理实现隔离的执行环境 一种机制，用于确认该执行环境已经初始化为可信状态。初始化要求该执行环境拥有自己的信任链（Chain of Trust），并且该信任链要独立于平台中并行的不受信任环境所使用的信任链。 在本指南中，我们将解释 Arm CCA 如何通过硬件实现和软件的配合来满足这些要求。The relationship between Arm CCA and Arm RMEArm Realm Management Extension (RME) is an extension to the Arm A-profilearchitecture, introduced in version 9.2.The following sections describe thefeatures of the RME. The RME provides the hardware primitives necessary tosupport Confidential Compute on Arm.Arm CCA is a complete system architecture, including firmware components whichuse RME in order to deliver Confidential Compute functionality. Arm Realm Management Extension（RME）是 Arm A-profile 架构的扩展，首次引入在版本 9.2 中。以下部分将描述 RME 的功能。RME 提供了支持 Arm 上机密计算所需的硬件基础。 Arm CCA 是一个完整的系统架构，包括使用 RME 来提供机密计算功能的固件组件。AttestationWorkloads running inside Realms manage confidential data or run confidentialalgorithms.For a Realm Owner to be considered confidential, any workload needs to: Be sure it is running on a real Arm CCA platform rather than a simulation Know that it has been loaded properly and not been tampered with. Know that the overall platform, or the realm is not in a debug state thatcould leak its secrets.The process of establishing the trust in a platform is called Attestation. 在 Realm 中运行的工作负载会管理机密数据或执行机密算法。 对于 Realm 所有者来说，要确保其工作负载具备机密性，需要满足以下条件： 确认其确实运行在真实的 Arm CCA 平台上，而不是在仿真环境中 确认其已经被正确加载，且未被篡改 确认整个平台或该 Realm 没有处于可能泄露机密信息的调试状态 建立对平台信任的过程称为认证（Attestation）。Attestation is broken into key parts: Attestation of the platform Attestation of the initial state of the RealmThese parts combine to create attestation reports. A Realm can requestattestation reports at any time. You can then use reports to authenticate thevalidity of the platform and the code in the Realm.Platform Attestation involves establishing and evaluating the identity andconfiguration of the platform which hosts the Realm. This platform includes allhardware and firmware components which can impact Realm security. This createsrequirements on the hardware. 这里host不是名词，而是动词，表示承载, 持有 认证（Attestation）分为几个关键部分： 平台的认证 Realm 初始状态的认证 这些部分共同生成认证报告。Realm 可以在任何时候请求认证报告。您可以利用这些报告来验证平台的有效性以及 Realm 中代码的可信性。The hardware must be provisioned with an identity. The hardware must supportmeasurements of key firmware images such as the Monitor, the RMM, and firmwarefor any other controller in the platform that can materially impact securitysuch as a power controller.The Realm attestation token describes the initial configuration and contents ofthe Realm, including its memory contents and register state.Attestation of a Realm involves actors outside the CCA platform. Readers areencouraged to refer to the Remote Attestation Specification (RATS), whichprovides a model for reasoning about the roles and responsibilities of theseactors.The local platform, running the CCA workload, uses the RME as this guide and theRME guide (DEN0126) describe. 硬件必须具备身份标识，并且必须支持对关键固件镜像（如 Monitor、RMM，以及平台中任何可能对安全性产生实质影响的控制器固件，例如电源控制器）进行测量。 Realm 认证令牌描述了 Realm 的初始配置和内容，包括其内存内容和寄存器状态。 Realm 的认证过程涉及 CCA 平台之外的参与方。建议读者参考远程认证规范（RemoteAttestation Specification，RATS），该规范为这些参与方的角色和职责提供了推理模型。 本地平台在运行 CCA 工作负载时，会按照本指南和 RME 指南（DEN0126）的描述，使用RME3. Arm CCA ExtensionsAs What is Confidential Computing? describes, Arm CCA aenables you to deployapplications or Virtual Machines (VMs) while preventing access by moreprivileged software entities such as a hypervisor. However, it is theseprivileged software entities that typically manage resources like memory. In anon-CCA platform, a privileged software entity, for example a hypervisor, doeshave access to the memory of an application or VM.The Arm CCA enables the hypervisor to control the VM, but removes the right foraccess to the code, register state, or data that is used by that VM. Theseparation is enabled by creating protected VM execution spaces called Realms.A Realm has complete isolation from the normal world in terms of code executionand data access. Arm CCA achieves this separation through a combination ofarchitectural hardware extensions and firmware.Within the Arm CCA, the hardware extensions on an Arm Application PE are calledthe Realm Management Extension (RME). The firmware which manages Realms (the RMM)and the EL3 Monitor both make use of RME. We describe these elements in Arm CCAHardware Architecture and Arm CCA Software Architecture. 如《什么是机密计算？》中所描述的，Arm CCA 允许你部署应用程序或虚拟机（VM），同时防止更高特权的软件实体（如 hypervisor）访问这些应用程序或 VM。然而，在传统的非 CCA 平台上，例如 hypervisor 这样的特权软件实体通常会有权访问应用程序或 VM的内存。 Arm CCA 使得 hypervisor 可以控制 VM，但同时剥夺了它访问 VM 使用的代码、寄存器状态或数据的权限。这种隔离是通过创建受保护的 VM 执行空间，称为 Realms，来实现的。 每个 Realm 在代码执行和数据访问方面与正常世界完全隔离。Arm CCA 通过结合架构级硬件扩展和固件来实现这种隔离。 在 Arm CCA 中，Arm 应用处理器（Application PE）上的硬件扩展被称为 RealmManagement Extension（RME）。管理 Realms 的固件（RMM）和 EL3 Monitor 都会使用RME。我们在《Arm CCA 硬件架构》和《Arm CCA 软件架构》中详细介绍了这些组件。RealmsA Realm is an Arm CCA environment that can be dynamically allocated by theNormal world Host. The Host is the supervisory software that manages anapplication or Virtual Machine (VM). As described in Attestation, the initialstate of a Realm, and of the platform on which it executes, can be attested.Attestation enables the Realm owner to establish trust in the Realm beforeprovisioning any secrets to it. The Realm does not have to trust the Non-securehypervisor which controls it.The Host can allocate and manage resource allocation. The Host manages thescheduling of the Realm VM operation. However, the Host cannot observe or modifythe instructions executed by the Realm.Realms can be created and destroyed under Host control. Pages can be added orremoved through Host requests in a way that is similar to a hypervisor managingany other non-confidential VM.To run a CCA system, a Host needs to be modified. The Host continues to controlthe nonconfidential VMs but needs to communicate with the Arm CCA firmware, inparticular the Realm Management Monitor (RMM). The operation of the RMM isdescribed in Arm CCA Software Architecture. Realm 是由普通世界主机（Host）动态分配的 Arm CCA 环境。Host 是管理应用程序或虚拟机（VM）的管理软件。 如认证（Attestation）部分所述，Realm 的初始状态以及其运行的平台都可以进行认证。认证使 Realm 的所有者能够在向 Realm 配置任何机密信息之前，建立对 Realm 的信任。Realm 无需信任控制它的非安全 hypervisor。 参考SEV KS spec, 流程也是先进行attestation，然后将结果返回给guest owner时,guest owner在进行verify，从而决定要不要注入secert到VM. Host 可以分配和管理资源。Host 负责调度 Realm 虚拟机的运行。然而，Host 无法观察或修改 Realm 执行的指令。 Realm 可以在 Host 的控制下被创建和销毁。通过 Host 的请求，可以像 hypervisor 管理其他非机密虚拟机一样，向 Realm 添加或移除页面。 要运行 CCA 系统，Host 需要进行修改。Host 继续控制非机密虚拟机，但需要与 ArmCCA 固件（尤其是 Realm Management Monitor，RMM）进行通信。RMM 的运行方式在《Arm CCA 软件架构》中有详细介绍。Realm world and Root worldThe Armv8-A TrustZone extensions enable secure execution of code and isolationof data by having two separated worlds: The Secure world The Normal world.A world is combination of a security state of a PE and physical addressspace. The security state a PE is executing in determines which physicaladdress spaces (PAS) a PE can access. In the Secure state a PE can access Secureand non-Secure physical address spaces. In the Non-secure state it can onlyaccess the Non-secure physical address space. Normal world refers to combinationof Non-secure state and Non-secure physical address space. Armv8-A TrustZone 扩展通过将系统划分为两个独立的世界，实现了代码的安全执行和数据的隔离： 安全世界（Secure world） 普通世界（Normal world） “world” 是指处理器元素（PE）的安全状态与物理地址空间的组合。PE 当前的安全状态决定了它可以访问哪些物理地址空间（PAS）。在安全状态下，PE 可以访问安全和非安全的物理地址空间；在非安全状态下，PE 只能访问非安全物理地址空间。Normal world指的是非安全状态与非安全物理地址空间的组合。RME introduces two additional worlds: Root world refers to the combination of the Root security state and Rootphysical address space. A PE is in the Root security state when it isrunning in Exception level 3. The Root PAS is separate from the Secure PAS.This is a key difference to Armv8-A TrustZone, where Exception level 3 codedid not have a private address space and instead used the Secure PAS. Thelatter is still used by S_EL2/1/0. The Monitor runs in the Root world. Realm world comprises of Realm security state and Realm PAS. Realm statecode can execute at R_EL2, R_EL1 and R_EL0. The controlling firmware runningin the Realm world can access memory in the Normal world to enable sharedbuffers.The SCR_EL3.NS bit controls World switching in a non-RME PE. Exception level 3software sets NS = 0 when switching to the Secure world and sets NS = 1 whenswitching to the Normal world. World switching in an RME-implemented PE isextended through a new SCR_EL3.NSE bit.The following table shows how the bits control execution and access between thefour worlds: RME（Realm Management Extension）引入了两个新的世界： 根世界（Root world）：指的是根安全状态与根物理地址空间的组合。当 PE 运行在异常级别 3（Exception level 3）时，就处于根安全状态。根物理地址空间与安全物理地址空间是分开的。这与 Armv8-A TrustZone 的一个关键区别在于，TrustZone 下异常级别 3 的代码没有单独的地址空间，而是使用安全物理地址空间，而这个空间仍由S_EL2/1/0 使用。Monitor 运行在根世界中。 Realm 世界（Realm world）：包括 Realm 安全状态和 Realm 物理地址空间。Realm状态下的代码可以在 R_EL2、R_EL1 和 R_EL0 级别执行。运行在 Realm 世界中的控制固件可以访问普通世界的内存，以实现共享缓冲区。 在非 RME 的 PE 上，SCR_EL3.NS 位用于控制世界切换。异常级别 3 的软件在切换到安全世界时设置 NS = 0，切换到普通世界时设置 NS = 1。而在实现了 RME 的 PE 上，世界切换通过新增的 SCR_EL3.NSE 位进行扩展The following diagram shows the four worlds, and their relationship to theSCR_EL3 NS and NSE bits:The Root world enables trusted boot execution and switching between thedifferent worlds. The PE resets into the Root world.The Realm world provides an execution environment for VMs that is isolated fromthe Normal and Secure worlds. VMs require control from the Host in the Normalworld. To enable full control of Realm creation and execution, the Arm CCAsystem requires the RMM, which is part of the firmware that is required tomanage Realm creation and execution under requests from a Normal world Host.For more details, see Arm CCA Hardware Architecture and Arm CCA SoftwareArchitecture. 根世界（Root world）支持可信启动的执行，并负责在不同世界之间进行切换。处理器元素（PE）复位时会进入根世界。 Realm 世界为虚拟机（VM）提供了一个与普通世界和安全世界隔离的执行环境。虚拟机需要由普通世界的 Host 进行控制。为了实现对 Realm 创建和执行的全面管理，Arm CCA系统需要 RMM（Realm Management Monitor），它是固件的一部分，负责根据普通世界Host 的请求来管理 Realm 的创建和执行。 更多详细信息，请参见《Arm CCA 硬件架构》和《Arm CCA 软件架构》。Realm PlanesRealm Planes enable Realms to be subdivided into a set of execution environmentswhich share the same IPA to PA translations, but which can have different memoryaccess permissions. Each Plane has its own register context within the RealmExecution Context (REC). You could consider A REC as a Virtual ProcessingElement (vPE). Each Plane is an isolated execution context. Realm ManagementMonitor specification v1.1 describes the use of Planes, the specification can befound here RMM Specification v1.1.Realm Planes remove the need to have multiple Realms with connectedfunctionality and all the overhead this entails. All the functionality canreside in a single Realm with the Execution Context isolated in each Plane asrequired and this only requires one Realm being created and controlled by theRMM.Planes allow security services required by a Realm to be implemented inside theRealm itself, but isolated from the main guest OS.Realm Planes are partitioned at Realm creation. A Realm can have several Planeswithin the REC numbered from P0 to Pn. Realm 平面（Realm Planes）允许将一个 Realm 细分为一组执行环境，这些环境共享相同的 IPA 到 PA（中间物理地址到物理地址）转换，但可以拥有不同的内存访问权限。每个平面在 Realm 执行上下文（REC）中都有自己独立的寄存器上下文。你可以将 REC 看作是一个虚拟处理单元（vPE）。每个平面都是一个隔离的执行上下文。RealmManagement Monitor v1.1 规范详细描述了平面的使用，规范可在 RMM Specificationv1.1 中查阅。 Realm 平面的设计消除了需要多个具有相关功能的 Realm 及其带来的所有管理开销。所有功能都可以存在于单一的 Realm 内部，根据需要在每个平面中实现隔离的执行上下文，这样只需创建并由 RMM 控制一个 Realm 即可。 平面允许 Realm 所需的安全服务在 Realm 内部实现，并与主客户操作系统隔离。 Realm 平面在 Realm 创建时进行分区。一个 Realm 可以在 REC 中拥有多个平面，编号从 P0 到 Pn。The number of Planes is specified at Realm creation and fixed thereafter. Thefirst Plane (referred to as P0) has additional capabilities which allow it to: Control entry into other Planes (“Pn” is used as a shorthand to mean “anyPlane other than P0”) Handle exceptions taken from Pn Context switch register state of Pn Manage memory access permissions for Pn Configure traps taken from Pn Either emulate an interrupt controller for Pn, or pass through to Pninterrupts injected by the Host Issue RSI commands to interface with the RMMThe following diagram shows a simple use case for Realm Planes. In this example,the Realm contains two Planes: P1 contains the main guest OS, in this example based on Linux P0 includes a Virtual Trusted Platform Module (vTPM) emulation, which can beused to record the state of P1, including measurements of each of P1’s bootstages 平面的数量在 Realm 创建时指定，之后保持不变。第一个平面（称为 P0）具有额外的能力，可以： 控制进入其他平面（“Pn”用作“除 P0 以外的任意平面”的简写） 处理来自 Pn 的异常 切换 Pn 的寄存器状态 管理 Pn 的内存访问权限 配置来自 Pn 的陷阱（trap） 可以为 Pn 模拟一个中断控制器，或者将 Host 注入的中断直接传递给 Pn 向 RMM 发起 RSI 命令以进行接口通信 下图展示了 Realm 平面的一个简单用例。在这个例子中，Realm 包含两个平面： P1 包含主客户操作系统，本例中为基于 Linux 的系统 P0 包含虚拟可信平台模块（vTPM）仿真，可用于记录 P1 的状态，包括对 P1 各个启动阶段的测量 A REC executes in a single Plane at a time. Exception handlers control startingand yielding of Plane execution.There are two options for management of per-Plane memory access permissions: On platforms which implement FEAT_S2POE and FEAT_S2PIE, a single stage 2translation table can be used by all Planes within a Realm, with per-Planeaccess permissions being specified indirectly. Alternatively, each Plane can have its own stage 2 translation table, withthe RMM ensuring that IPA to PA translations are identical while per-Planeaccess permissions can be different. 一个 Realm 执行上下文（REC）在任意时刻只能在单一平面上执行。异常处理程序控制平面执行的开始和结束。 对于每个平面内存访问权限的管理，有两种选项： 在支持 FEAT_S2POE 和 FEAT_S2PIE 的平台上，可以使用一个单一的二级地址翻译表（stage 2 translation table）供 Realm 中所有平面使用，通过间接方式指定每个平面的访问权限。 或者，每个平面都可以拥有自己的二级地址翻译表，RMM 负责确保所有平面中的 IPA到 PA 具有相同的translation ，而每个平面的访问权限可以不同。 What is the difference between Arm TrustZone Extensions and Arm RME?All Arm A-Profile processors have can the Arm TrustZone architecture extensions.These extensions enable development of an isolated execution and dataenvironment. Elements like a Trusted Operating System (TOS) can service Trustedapplications, which execute in isolation, to service Secure requests from theRich OS that is running in the Normal world.The addition of virtualization to the Secure world in Armv8.4-A enable you tomanage multiple Secure Partitions in the Secure world. This feature can enableyou to apply multiple TOSs to a system. The Secure Partition Manager (SPM),executing at S_EL2, is the manager for the Secure Partitions. The SPM has asimilar functionality to the hypervisor in the Normal world.In operation, the Trusted OS is often part of a chain of trust. It is verifiedby higher privilege firmware. In some systems this is the SPM. This means thatthe TOS relies on the relationship with the higher privilege firmware developer. 所有 Arm A-Profile 处理器都支持 Arm TrustZone 架构扩展。这些扩展能够开发一个隔离的执行与数据环境。诸如可信操作系统（TOS）等组件可以为可信应用提供服务，这些应用在隔离环境中运行，以响应来自运行在普通世界中的丰富操作系统（Rich OS）的安全请求。 在 Armv8.4-A 中，安全世界（Secure world）加入了虚拟化功能，从而可以在安全世界中管理多个安全分区（Secure Partition）。这一特性使得系统能够部署多个可信操作系统（TOS）。安全分区管理器（SPM）在 S_EL2 级别执行，负责管理安全分区。SPM 的功能类似于普通世界中的虚拟机管理程序（hypervisor）。 在实际运行中，可信操作系统通常是信任链的一部分，它会被更高权限的固件进行验证。在某些系统中，这个更高权限的固件就是 SPM。这意味着 TOS 的安全性依赖于与更高权限固件开发者之间的信任关系。The following methods initiate the execution of the TOS: Rich OS yielding, where the Rich OS enters an idle loop and executes an SMCinstruction to call the TOS through the Monitor Interrupt targeted at the TOS. Secure type 1 interrupts execute the TOS. Asecure type 1 interrupt asserted during Normal world execution calls the TOSthrough the Monitor. 以下方法可以启动可信操作系统（TOS）的执行： 丰富操作系统（Rich OS）让出执行权：Rich OS 进入idle loop，并通过执行 SMC 指令，通过 Monitor 调用 TOS。 触发面向 TOS 的中断：Secure type 1 interrupt 会执行 TOS。在普通世界执行期间，如果触发了安全类型 1 的中断，会通过 Monitor 调用 TOS。 A Realm VM is different to a TOS or Trusted application because the Realm VM iscontrolled from the Normal world Host. In areas such as creation and memoryallocation, the Realm VM acts like any other VM being controlled from the Host.A difference between the Realm VM execution and the TOS execution is that theRealm does not have any physical interrupts enabled. All interrupts for theRealm are virtualized by the hypervisor and then signaled to the Realm throughcommands passed to the RMM. This means that a compromised hypervisor mightprevent execution of the Realm VM, so there is no guarantee of Realm execution. Realm 虚拟机（Realm VM）与可信操作系统（TOS）或可信应用不同，因为 Realm VM 是由普通世界的 Host 控制的。在创建和内存分配等方面，Realm VM 的行为与由 Host 控制的, 和其他虚拟机(running in Normal world)类似。 Realm VM 的执行与 TOS 的执行之间的一个区别是，Realm 不启用任何物理中断。所有针对 Realm 的中断都由虚拟机管理程序（hypervisor）进行虚拟化，然后通过传递给 RMM的命令通知 Realm。这意味着，如果 hypervisor 被攻破，可能会阻止 Realm VM 的执行，因此无法保证 Realm 一定可以正常运行。It is expected that the software entities which make use of each of Realms andTrustZone will differ in terms of the relationships between the software vendorand the platform owner.A TOS controls execution of Trusted applications that are used forplatform-specific services. The Trusted applications are developed duringthe platform development,by developers such as Silicone Providers (SiPs) andOriginal Equipment Manufacturers (OEMs). The Trusted applications are veryclosely tied to the chain of trust for the platform from the platform boot.Realm execution is intended to allow general developers to execute code on asystem without being involved in complex business relationships with thedevelopers in the compute system. Arm CCA enables Realms to be created anddestroyed on demand under the control of the Normal world host. Resources can beadded or retrieved from Realms dynamically. 预计使用 Realm 和 TrustZone 的软件实体，在软件供应商与平台所有者之间的关系方面会有所不同。 可信操作系统（TOS）负责控制用于平台专用服务的可信应用的执行。这些可信应用通常由平台开发过程中相关的开发者（如芯片供应商（SiPs）和原始设备制造商（OEMs））开发。可信应用与平台从启动开始的信任链紧密关联。 Realm 的执行旨在让普通开发者能够在系统上运行代码，而无需与计算系统中的开发者建立复杂的业务关系。Arm CCA 允许在普通世界主机的控制下按需创建和销毁 Realm，可以动态地向 Realm 添加或回收资源。Information security is often described in terms of the following threeprinciples: Confidentiality: data can only be observed by authorized users or processes. Integrity: data can only be modified by authorized users or processes. Availability: data or a service can be accessed in a timely manner byauthorized users or processes.A TOS can provide a guarantee of Confidentiality, Integrity and Availability.Arm CCA provides Realms with a guarantee of Confidentiality and Integrity, butnot Availability.The four-world environment provided by the Arm CCA system enables the completeseparation between the Secure world and Realm world. This allows the entitieswithin each of Secure world and Realm world to be independently deployed andmutually distrusting. 信息安全通常用以下三个原则来描述： 机密性：数据只能被授权的用户或进程访问。 完整性：数据只能被授权的用户或进程修改。 可用性：数据或服务能够被授权的用户或进程及时访问。 可信操作系统（TOS）能够为机密性、完整性和可用性提供保障。而 Arm CCA 为 Realm提供了机密性和完整性的保障，但不保证可用性。 Arm CCA 系统提供的四世界环境实现了安全世界（Secure world）和 Realm 世界之间的完全隔离。这使得安全世界和 Realm 世界中的实体能够独立部署，并且可以相互不信任。4. Arm CCA Hardware ArchitectureThis section describes the RME, which are the changes to PE architecture thatenable PEs to run Realms. 本节介绍 RME，即对处理器元素（PE）架构的改进，使 PE 能够运行 Realm。Realm world requirementsThe following diagram shows a complete view of how Realms fit within an Arm CCAsystem: 下图展示了 Realms 在 Arm CCA 系统中的整体结构与位置：The Realm world must be able to execute code, and access memory and trusteddevices, in complete isolation from all other non-Root worlds and devices.Similar to the other worlds, or security states, the Realm world has threeException levels, R-EL0, R-EL1, and R-EL2. Realm VMs run in R-EL1 and R-EL0. TheRealm Management Monitor (RMM) runs in R-EL2. The [Arm CCA Software Stack]describes the RMM.Isolation between worlds is enforced by hardware through faulting exceptions andby physical memory encryption.The following diagram illustrates that a Realm VM executes in the Realm world,but its resources are managed by the hypervisor executing in Normal world. Thismanagement is performed via requests sent by the hypervisor to the RMM. Realm 世界必须能够在与所有其他非 Root 世界和设备完全隔离的情况下执行代码、访问内存和可信设备。 与其他世界或安全状态类似，Realm 世界也有三个异常级别：R-EL0、R-EL1 和 R-EL2。Realm 虚拟机在 R-EL1 和 R-EL0 级别运行，Realm 管理监控器（RMM）在 R-EL2 级别运行。[Arm CCA 软件栈] 介绍了 RMM 的相关内容。 世界之间的隔离由硬件通过异常故障（faulting exceptions）和物理内存加密来强制实现。The Monitor is the gatekeeper between the separate worlds. It controls anypassage between Normal world, Secure world, and Realm world to ensure that theisolation between the worlds is maintained. The Monitor allows communication andcontrol where needed. Monitor 是各个世界之间的守门人。它负责控制普通世界、安全世界和 Realm 世界之间的所有通道，以确保各世界之间的隔离性得以保持。Monitor 在需要时允许通信和控制。Memory management for Arm CCAArm A-profile processors that implement the TrustZone Security Extensionspresent two Physical Address Spaces (PAS): Non-secure physical address space Secure physical address spaceThe Realm Management Extension add another two PAS: Realm physical address space Root physical address spaceEach location within physical memory can be made accessible via a singlePAS. This is illustrated in the following diagram 实现了 TrustZone 安全扩展的 Arm A-profile 处理器提供了两个物理地址空间（PAS）： 非安全物理地址空间 安全物理地址空间 Realm 管理扩展（RME）又增加了另外两个物理地址空间： Realm 物理地址空间 Root 物理地址空间 物理内存中的每个位置只能通过一个物理地址空间进行访问。下图对此进行了说明。Depending on the Security state in which a PE is executing, a subset of PAS areaccessible. The subset is shown in the following table. 根据 PE 执行时所处的安全状态，不同的物理地址空间（PAS）是可访问的。下表列出了可访问的物理地址空间的子集。The PAS via which each physical location is accessible is controlled by Rootsecurity state. This means that firmware executing in Root security state iseffectively able to transfer ownership of physical memory between each ofNon-secure, Secure and Realm worlds.To ensure that the isolation rules for all worlds are enforced, the physicalmemory access controls in the preceding table are enforced by the MemoryManagement Unit (MMU), downstream of any address translation. This process iscalled Granule Protection Check (GPC).The PAS assignment of every granule of physical memory is described in theGranule Protection Table (GPT). The Monitor in Exception level 3 can dynamicallyupdate the GPT which allows the physical memory to be moved between the worlds.Any access control violation results in a new type of fault, which is called aGranule Protection Fault (GPF). Root state controls the enablement of the GPC,the contents of the GPT and the routing of a GPF. 每个物理地址空间（PAS）能够访问物理内存的位置由根安全状态（Root security state）进行控制。这意味着在根安全状态下运行的固件实际上可以在非安全世界、安全世界和Realm 世界之间转移物理内存的所有权。 为了确保所有世界的隔离规则得到执行，前述表格中的物理内存访问控制由内存管理单元（MMU）在地址转换之后进行强制执行。这个过程称为粒度保护检查（GranuleProtection Check，GPC）。 物理内存中每个粒度（granule）的 PAS 分配情况都记录在粒度保护表（GranuleProtection Table，GPT）中。异常级别 3（Exception level 3）下的 Monitor 可以动态更新 GPT，从而实现物理内存在各个世界之间的移动。 任何访问控制违规都会导致一种新的故障类型，称为粒度保护故障（Granule ProtectionFault，GPF）。根安全状态负责 GPC 的启用、GPT 的内容以及 GPF 的路由。Resources belonging to a Realm must be in Realm-owned memory, meaning part ofthe Realm PAS, to ensure isolation. However, a Realm might need to access someresources held in Non-secure memory, for example to enable message passing. Thismeans that a Realm needs to be able to access physical addresses in both theRealm and Non-secure PASs.Within the EL3 translation regime, RME introduces an additional bit in thetranslation table descriptor, called NSE. Using NSE together with the existingNS bit, EL3 software can map any of Root, Non-Secure, Secure and Realm PAS.Within the Realm EL1&amp;0 translation regime and the Realm EL2&amp;0 translation regime,the NS bit allows software at R-EL2 to map either Realm PAS or Non-Secure PAS.Access to the different PAS is controlled by the state of the NS bit in theRealm stage 2 translation table. 属于 Realm 的资源必须位于 Realm 所有的内存中，即属于 Realm 物理地址空间（PAS），以确保隔离性。然而，Realm 可能需要访问存放在非安全内存中的某些资源，例如用于消息传递。这意味着 Realm 需要能够访问 Realm PAS 和非安全 PAS 中的物理地址。 在 EL3 翻译机制下，RME 在翻译表描述符中引入了一个新位，称为 NSE 位。结合 NSE位与现有的 NS 位，EL3 层的软件可以映射 Root、非安全、安全和 Realm 这四种 PAS。 在 Realm EL1&amp;0 和 Realm EL2&amp;0 的翻译机制下，NS 位允许 R-EL2 层的软件映射 RealmPAS 或非安全 PAS。对不同 PAS 的访问由 Realm 二级翻译表中的 NS 位状态进行控制。 NOTE 我们可以思考下guest和host共享的内存资源大概有哪些: DMA BUFFER PV features share memory virtio 并且向DMA buffer 往往是动态的(其实PV share memory也是), 所以就需要RMM (R-EL2)去切换 page table entry, 至于是更改了该page 所属的PAS还是创建了一个新page，更改映射，就看RMM怎么实现 The following diagram shows the position of the GPC in the logical sequence ofsteps which occur during translation of a Virtual Address (VA) to a PhysicalAddress (PA). In this diagram, TTD is Translation Table Descriptor and GPTD isGranule Protection Table Descriptor: 下图展示了在虚拟地址（VA）到物理地址（PA）转换过程中，GPC（粒度保护检查）在逻辑步骤中的位置。在图中，TTD 代表翻译表描述符（Translation Table Descriptor），GPTD 代表粒度保护表描述符（Granule Protection Table Descriptor）：The diagram shows the translation stages within an RME-based platform for thevirtual address to physical address translation.For more information on stage 1 and stage 2 translation see AArch64 MemoryManagement.RME adds the GPC after the translation process for stage 1 and stage 1 and 2translations The GPC checks all physical addresses and PAS against the GPT toallow memory access or create a fault. The GPT is held in Root memory to ensurethat it is isolated from all other worlds. The GPT can only be created andmodified by code running in the Root world, from the Monitor code or TrustedFirmware.Non-PE Requesters can also be included in this check if they are connected to aRequester side filter like a System MMU (SMMU). 该图展示了在基于 RME 的平台上，虚拟地址到物理地址转换过程中的各个翻译阶段。 关于一级和二级地址转换的更多信息，请参阅《AArch64 内存管理》。 RME 在一级和一级/二级地址转换过程结束后增加了粒度保护检查（GPC）。GPC 会根据粒度保护表（GPT）检查所有物理地址和物理地址空间（PAS），以决定是否允许内存访问或产生故障。GPT 存储在 Root 内存中，以确保它与其他世界隔离。只有在 Root 世界下运行的代码（如 Monitor 代码或可信固件）才能创建和修改 GPT。 如果非 PE 请求者（Non-PE Requesters）连接到类似于系统 MMU（SMMU）这样的请求端过滤器，也可以包含在此检查范围内。5. Arm CCA Software ArchitectureArm CCA platforms are created through a mix of hardware additions, such as RMEin the PEs, and firmware components, in particular the Monitor and RealmManagement Monitor. This section describes the software stack for an Arm CCAplatform. Arm CCA 平台是通过硬件扩展（如处理器元素中的 RME）与固件组件（尤其是 Monitor和 Realm Management Monitor）相结合来实现的。本节将介绍 Arm CCA 平台的软件栈。Software StackThe actions performed by software to manage a Realm can be divided into twoparts: Policy: decisions regarding which resources (memory; processor cycles;devices) to allocate to a Realm. This is the responsibility of thehypervisor. Mechanism: enacting a policy decision requires changing architecturalstate, such as modifying a translation table, or context-switching the stateheld in a processor register. 由软件执行的 Realm 管理操作可以分为两个部分： 策略（Policy）：关于为 Realm 分配哪些资源（如内存、处理器周期、设备）的决策。这部分由虚拟机管理程序（hypervisor）负责。 机制（Mechanism）：落实策略决策需要改变体系结构状态，例如修改翻译表或切换处理器寄存器中保存的上下文状态。When this action relates to a Realm and the hypervisor cannot be trusted toperform it, the RMM performs the action for the hypervisor.The RMM isolates Realms from each other through the stage 2 page tables in theRealm world.The RMM interfaces directly to the Monitor which also interfaces with the Secureworld and the Normal world. The Monitor, running in Exception level 3, has theplatform-specific code that must service all the Trusted functionality of thesystem. The RMM responds to a specific interface and has fully definedfunctionality to manage the requests from the Host and Realms. Because thisinterface is well-defined, the RMM can be generic code for all Arm CCA systems. 当某项操作涉及 Realm 且不能信任虚拟机管理程序（hypervisor）来执行时，RMM 会代替 hypervisor 执行该操作。 RMM 通过 Realm 世界中的二级页表实现对各个 Realm 之间的隔离。 RMM 直接与 Monitor 进行接口通信，而 Monitor 还与安全世界和普通世界进行交互。Monitor 运行在异常级别 3（Exception level 3），其中包含必须服务于系统所有可信功能的平台专用代码。RMM 响应特定的接口，并具备完善的功能来管理来自 Host 和Realm 的请求。由于该接口定义明确，RMM 可以作为所有 Arm CCA 系统的通用代码。The following diagram shows the complete Arm CCA platform running a confidentialRealm VM in the Realm world: 下图展示了在 Realm 世界中运行机密 Realm 虚拟机的完整 Arm CCA 平台：The RMM is the controlling software in the Realm world that reacts to requestsfrom the hypervisor in the Normal world to enable the management of the Realm VMexecution. The RMM communicates through the Monitor in Root world to controlRealm memory management functions for memory transition between NS PAS and RealmPAS. RMM 是 Realm 世界中的控制软件，负责响应来自普通世界 hypervisor 的请求，以实现对 Realm 虚拟机执行的管理。RMM 通过根世界（Root world）中的 Monitor 进行通信，以控制 Realm 的内存管理功能，实现 NS PAS（非安全物理地址空间）与 Realm PAS（Realm 物理地址空间）之间的内存转换。Realm Management MonitorThe RMM is the Realm world firmware that manages the execution of the Realm VMsand their interaction with the hypervisor in Normal world. The RMM operates inException level 2 in the Realm world, known as R_EL2.In the Arm CCA system, the RMM provides services to the Host, to enable the Hostto manage the Realms, and the RME also supplies services directly to the Realms.The Host services can be split into areas of policy and mechanics.For the Policy functionality, the Host owns all the policy decisions, includingthe following: When to create or destroy a Realm When to add or remove memory from a Realm When to schedule a Realm in or out RMM 是 Realm 世界中的固件，负责管理 Realm 虚拟机的执行以及与普通世界hypervisor 的交互。RMM 在 Realm 世界的异常级别 2（R_EL2）下运行。 在 Arm CCA 系统中，RMM 向主机（Host）提供服务，以使 Host 能够管理 Realm，同时RME 也直接向 Realm 提供服务。 Host 服务可以分为策略（policy）和机制（mechanics）两部分。 在策略功能方面，Host 拥有所有策略决策权，包括如下内容： 何时创建或销毁一个 Realm 何时向 Realm 添加或移除内存 何时调度 Realm 的运行或暂停 The RMM supports the host Policies by providing the following functionality: Services to manipulate Realm page tables, which are used in creation ordestruction and the addition or removal of Realm memory Management of Realm context. This is context save and restore used inscheduling. Interrupt support PSCI call interception. This is power management requests. The RMM alsoprovides services to Realms, primarily attestation and cryptographicservices. RMM 通过以下功能来支持主机（Host）的策略决策： 提供操作 Realm 页表的服务，用于 Realm 的创建或销毁，以及内存的添加或移除 管理 Realm 上下文，包括在调度时进行上下文的保存与恢复 支持中断处理 拦截 PSCI 调用（即电源管理请求） The RMM also upholds the following security primitives for the Realms: The RMM validates hosts requests for correctness The RMM isolates Realms from each otherThe RMM specification defines two interfaces: The Realm Management Interface (RMI) used by the Host The Realm Services Interface (RSI) used by the Realm. RMM 还为 Realms 提供以下安全基础机制： RMM 验证主机（Host）的请求是否正确 RMM 实现对各个 Realm 之间的隔离 RMM 规范定义了两个接口： 主机使用的 Realm 管理接口（RMI） Realm 使用的 Realm 服务接口（RSI） Each of these interfaces comprises a set of SMCCC-compliant commands.Arm CCA additionally defines an interface called the Realm Host Interface (RHI)used for communications between the guest and the hypervisor. The RMM is notused for the RHI communication.In the following sections, we look at each of these interfaces 每个接口都包含一组符合 SMCCC 标准的命令。 Arm CCA 还定义了一个称为 Realm 主机接口（RHI）的接口，用于客户机与虚拟机管理程序（hypervisor）之间的通信。RHI 通信不涉及 RMM。 在接下来的章节中，我们将分别介绍这些接口。Realm Management InterfaceThe RMI is the interface between the RMM and the Normal world Host.The RMI enables control of Realm management which includes creation, population,execution, and destruction of the Realms.The following diagram shows where the RMI is implemented between the Normalworld Host, Monitor, and the RMM: RMI 是 RMM 与普通世界 Host 之间的接口。 RMI 实现了对 Realm 管理的控制，包括 Realm 的创建、资源分配、执行和销毁等操作。 下图展示了 RMI 在普通世界 Host、Monitor 和 RMM 之间的具体实现位置：Realm Services InterfaceThe Realm Service Interface (RSI) is the interface between the Realm VM and theRMM.The RSI allows the Realm to request services including: Requesting an attestation report describing the CCA platform and the Realm’sinitial state Managing properties of the Realm’s address space, including sharing memorywith the Host Attesting and accepting devices which are assigned to the RealmThe following diagram shows the position of the RSI between the RMM and eachindividual Realm VM: Realm 服务接口（RSI）是 Realm 虚拟机与 RMM 之间的接口。 通过 RSI，Realm 可以请求以下服务： 请求认证报告，用于描述 CCA 平台和 Realm 的初始状态 管理 Realm 地址空间的属性，包括与 Host 共享内存 对分配给 Realm 的设备进行认证和接收 下图展示了 RSI 在 RMM 与各个 Realm 虚拟机之间的位置关系：Realm Host InterfaceThe Realm Host Interface (RHI) is the interface between the Realm and theuntrusted host hypervisor in the non-secure world.The RHI enables the host to request Realm management functions through a trustedinterface. Realm 主机接口（RHI）是 Realm 与非安全世界中的不可信主机虚拟机管理程序（hypervisor）之间的接口。 RHI 允许主机通过受信任的接口请求 Realm 管理功能。The RHI provides a standard interface for a Realm to request services from theHost, where those services do not require participation of the CCA firmware (RMMand EL3 Monitor). Examples of such services include: Early Provisioning of secrets to a Realm during guest boot. Discovery of Host-imposed constraints on the granularity that memory can beshared between Realm and Host. Retrieval of attestation evidence related to Realm-assigned devices. Theattestation token is stored by the Host and secured via hashes held by theRMM.The following diagram shows how RHI is used by the guest to request servicesfrom the Host. Although the RMM and EL3 Monitor are not participants in thiscommunication, they are part of the transport for RHI commands. RHI 为 Realm 向 Host 请求服务提供了标准接口，这些服务不需要 CCA 固件（即 RMM和 EL3 Monitor）的参与。此类服务示例包括： 在客户机启动期间，提前向 Realm 提供密钥等机密信息。 发现 Host 强加的内存共享粒度约束，即在 Realm 与 Host 之间共享内存时的限制。 获取与分配给 Realm 的设备相关的认证证据。认证令牌由 Host 存储，并通过 RMM 保存的哈希值进行保护。 下图展示了客户机如何通过 RHI 向 Host 请求服务。虽然 RMM 和 EL3 Monitor 不直接参与此类通信，但它们是 RHI 命令传输路径的一部分。 按照AMD SEV来看，Early Provisioning of secrets to a Realm VM, 是由VMM 和 guest owner进行交互，所以实际上是由guest 直接请求VMM, 但是VMM也会向RMM导入 导出证书等内容用户加密信道的建立 6. Device Assignment and Memory Encryption Contexts7. Confidential Compute in useThe need for Confidentiality in Compute systems spans many market segments.Confidential Compute enables shorter trust chain relationships for applicationand OS developers and will ease the certification load.The confidentiality afforded within a Realm VM enables different services to beisolated from the Normal world execution, while being controlled from the Normalworld for setup and execution 计算系统对机密性的需求涵盖了许多市场领域。机密计算（Confidential Compute）为应用和操作系统开发者提供了更短的信任链关系，也会减轻认证负担。 Realm 虚拟机所提供的机密性，使得不同服务能够与普通世界的执行环境隔离，同时又可以由普通世界进行配置和管理。7.1 Use casesThe following sections describe some of the use cases and how they are set up inan Arm Confidential Compute platform, with pointers to any examples, whereavailable.The use cases will describe what is being enabled and what it allows an end userto achieve while showing which CCA features the use case exercises. 接下来的章节将介绍一些在 Arm 机密计算平台上的使用场景，以及这些场景的具体设置方式，并在有示例时给出参考。 这些使用场景将说明实现了哪些功能，以及最终用户能够实现什么目标，同时展示该场景涉及了哪些 CCA 特性。Boot and attest a RealmBooting and attesting a Realm is the very basic task of initiating and booting aRealm on a platform. For CCA this should be from an attested and trustedplatform.Boot and attest use case enables an end user to attest that a Realm has beencreated correctly, on a trustworthy CCA platformThe CCA features that are exercised by this are: Realm creation Realm execution Remote attestation of CCA platform and Realm 启动和认证 Realm 是在平台上初始化并启动 Realm 的最基本任务。对于 CCA，这一过程应当在经过认证和可信的平台上进行。 启动和认证的使用场景使最终用户能够验证 Realm 是否在可信的 CCA 平台上被正确创建。 该场景涉及的 CCA 特性包括： Realm 的创建 Realm 的执行 CCA 平台和 Realm 的远程认证 The Boot and Attest use case is responsible for creating a Realm on an attestedplatform and then executing code within that Realm in isolation from theNon-secure and Secure worlds. The Realm is created under control from the Normalworld hypervisor using the RMI interface to create the isolated VM forexecution.This example use case should be used as a starting point for other CCA usecases. Although the Realm is a protected execution environment, its contents are,by definition, entirely nonconfidential due to being input from anon-confidential source media. Therefore, this should be used as a startingpoint for any further CCA use cases.The following link shows the Arm Learning Path for how to create a Realm in theArm Software stack: 启动和认证的使用场景负责在经过认证的平台上创建一个 Realm，并在该 Realm 中隔离地执行代码，使其与非安全世界和安全世界隔离。Realm 的创建由普通世界的虚拟机管理程序（hypervisor）通过 RMI 接口控制，以创建用于执行的隔离虚拟机。 此示例场景应作为其他 CCA 使用场景的起点。虽然 Realm 是一个受保护的执行环境，但由于其内容完全来自非机密的源介质，因此本质上并不具备机密性。因此，该场景应作为进一步 CCA 使用场景的基础。 以下链接展示了在 Arm 软件栈中创建 Realm 的 Arm 学习路径：Boot a Realm from an encrypted disk imageCloud service developers will want to provision each Realm with minimal firmwareto enable standard boot of a Linux based encrypted Root FileSystem (FS). Ownersof Cloud workloads must ensure their data and workloads are provisioned afterthe Root FS has been attested.Booting a Realm from an encrypted disc builds on the basic Realm creationdescribed in the section above and ensures the Realm is provisioned withconfidential information, from the encrypted contents of a disk image. 云服务开发者希望为每个 Realm 配置最小化的固件，以支持基于 Linux 的加密根文件系统（Root FS）的标准启动。云工作负载的所有者必须确保其数据和工作负载在根文件系统认证之后再进行配置。 从加密磁盘启动 Realm 是在前文所述的基本 Realm 创建流程基础上进行扩展，确保Realm 能够从磁盘镜像的加密内容中获取机密信息进行配置。The CCA features that are exercised in this use case are: The functionality described in Boot and attest a Realm A Boot Injection ProtocolArm provids an example of a boot injection reference software with the BootSynchronisation RHI (RHI-BS) interface. Implementors have the option to: use the Arm interface develop their own interface code.The reference software is developed from the Realm Host Interface specificationand requirements. 该使用场景涉及的 CCA 特性包括： “启动和认证 Realm”中描述的相关功能 启动注入协议（Boot Injection Protocol） Arm 提供了一个带有启动同步 RHI（RHI-BS）接口的启动注入参考软件示例。实现者可以选择： 使用 Arm 提供的接口 开发自己的接口代码 参考软件是根据 Realm 主机接口（RHI）规范和相关需求开发的。The protocol describes the trusted injection of the disk encryption keyfollowing Realm attestation to allow secrets to be exchanged between the Realmapplication and the Realm owner.Confidential Compute is not a single point solution. The operation of Secureboot of a Realm on a platform will require extensive cloud infrastructure toenable the platform to attest and execute the boot process. 该协议描述了在 Realm 认证完成后，如何可信地注入磁盘加密密钥，以便在 Realm 应用与 Realm 所有者之间安全交换密钥等机密信息。 机密计算并不是一种单点解决方案。在平台上安全启动 Realm 的操作，需要广泛的云基础设施支持，以实现平台的认证和启动过程的执行。Live CCA Firmware updateCloud vendors require the ability to update CCA firmware, such as the RealmManagement Monitor (RMM) or the Root Monitor in EL3, in a live CCA system,without having to reset the platform to reboot. This operation is required forall initial CCA platforms.Arm has developed reference software for this operation which is depends on theFirmware Activity Log from the RHI 1.0 specification and Live FirmwareActivation specification 1.0.All platforms will have their own Firmware update solutions so Arm referencesoftware demonstrating this operation considers the update process optional.Again, the deployment of the firmware update is dependant upon a wider cloudinfrastructure. 服务厂商需要能够在运行中的 CCA 系统中更新 CCA 固件（如 Realm 管理监控器 RMM 或EL3 中的 Root Monitor），而无需重置平台或重启系统。这一操作是所有初始 CCA 平台的基本需求。 Arm 针对该操作开发了参考软件，其实现依赖于 RHI 1.0 规范中的固件活动日志（Firmware Activity Log）以及 Live Firmware Activation 1.0 规范。 所有平台都会有各自的固件更新解决方案，因此 Arm 的参考软件将该操作的更新流程视为可选项。同样，固件更新的部署也依赖于更广泛的云基础设施。Per-Realm encryptionA cloud workspace owner will require confidentiality in their data when runningin a Realm and would prefer the reassurance of having isolation from all otherRealms running on that platform. Use the Memory Encryption Contexts (MEC) toenforce separate encryption keys being used for each Realm. For more informationon MEC see Memory Encryption ContextsPer Realm encryption will be mandatory in early CCA deployments to ensure theconfidentiality of separate workload memory.Implementation is dependent upon the underlying processors supporting the MECfeature (FEAT_MEC) corresponding to the architecture version v9.3. Any platformimplementing MEC must include an SMMU which is compliant with the SMMUarchitecture specification defined in release F.a or later.The software development must be compliant with the RMM specification v1.1 withthe MEC sections applied. 云工作空间的所有者在 Realm 中运行时需要确保其数据的机密性，并希望能够与平台上其他所有 Realm 保持隔离。可以使用内存加密上下文（Memory Encryption Contexts，MEC）来实现对每个 Realm 使用独立加密密钥的强制隔离。关于 MEC 的更多信息，请参阅《Memory Encryption Contexts》。 在早期 CCA 部署中，每个 Realm 的内存加密将是强制要求，以确保不同工作负载内存的机密性。 具体实现依赖于底层处理器支持 MEC 功能（FEAT_MEC），该功能对应于架构版本 v9.3。任何实现 MEC 的平台都必须包含符合 SMMU 架构规范 F.a 或更高版本的 SMMU。 软件开发必须符合 RMM v1.1 规范，并应用其中关于 MEC 的相关章节。Non-Realm workload Device Assignment (DA) on KVM HostWhen running a workload on a cloud there is a requirement to be able to use aPCIe-based DMA engine to access memory from a non-Realm source. The user must beable to assign a PCIe Trusted Device Interface to a workload that is running aRealm with at least one Plane as a Virtual Machine.The Realm must fully attest the TDI to confirm the identity and configuration ofthe device functionality before allowing the DMA to access the Realm (workload)memory.DA use cases require significant support and development to the whole Cloudinfrastructure and to the underlying platform with the integration of the PCIeelements before the use cases are able to guarantee full CCA operation. The CCAsupport for PCIe should include Root Complex Integrated Endpoint (RCiEP) devicesand any off-chip devices. Support for Security Protocol and Data Model (SPDM) isnot mandatory. 在云端运行工作负载时，需要能够使用基于 PCIe 的 DMA 引擎从非 Realm 来源访问内存。用户必须能够将 PCIe 可信设备接口（Trusted Device Interface, TDI）分配给在Realm 中运行、至少包含一个 Plane 的虚拟机工作负载。 Realm 必须对 TDI 进行完整认证，以确认设备功能的身份和配置，之后才允许 DMA 访问Realm（工作负载）内存。 DMA 相关的使用场景需要云基础设施和底层平台进行大量支持和开发，尤其是在集成PCIe 元素后，才能保证 CCA 功能的完整实现。CCA 对 PCIe 的支持应包括 RootComplex 集成端点（RCiEP）设备以及所有外部芯片设备。对于安全协议与数据模型（SPDM），并不强制要求支持。7.2 CCA usage scenariosThe above use cases are effectively building blocks to enable Arm-basedplatforms to provide the CCA Guarantee for workloads running on the Arm-basedcloud platforms. The following sections describe some potential areas in whichthe use cases can be implemented. This is not an exhaustive list but just a fewexamples where the RME and use cases can be used in wider system operations. 上述使用场景实际上是构建模块，使基于 Arm 的平台能够为在 Arm 云平台上运行的工作负载提供 CCA 保证。接下来的章节将介绍一些这些使用场景可以应用的潜在领域。这并不是一个详尽的列表，而只是展示了 RME 和相关使用场景在更广泛系统操作中的部分应用示例。Operating system vendor servicesThe use of Arm CCA allows for the execution of individual services in isolation from the Rich OSenvironment that has been validated (and potentially attested) by the OEM Chain of Trust. Thismeans that an OS vendor can develop services to be run as Realm VMs to ensure they can beexecuted within strict security requirements.The basic use of the Arm RME will ensure that all VMs are fully attested on atrusted platform that can also be regularly attested to ensure CCA Guaranteesare maintained. 使用 Arm CCA 可以让各个服务在经过 OEM 信任链验证（甚至认证）的Rich OS环境之外实现隔离运行。这意味着操作系统厂商可以开发以 Realm 虚拟机形式运行的服务，从而确保这些服务能够在严格的安全要求下执行。 Arm RME 的基本应用能够确保所有虚拟机都在可信平台上进行完整认证，并且该平台可以定期认证，以确保持续满足 CCA 的安全保证。Sensitive data servicesMany OS services collect and store sensitive or personal data during theirexecution. This data can be the target for malicious agents who mine and selldata. Arm CCA allows these services to be executed on any Arm CCA system inwhich the RME enables the data to be completely isolated from any Normal worldinterference. This can apply to areas such as: Address book data Healthcare information Browser history and cookies Auto-complete systems Calendar and email Clipboard dataArm CCA also allows isolation of personal data in AI models which can then allowcloud access to that data and more confidence in off-system analysis of personaldata. 许多操作系统服务在运行过程中会收集和存储敏感或个人数据。这些数据可能成为恶意行为者挖掘和出售的目标。Arm CCA 允许这些服务在任何 Arm CCA 系统上运行，在 RME 的支持下，数据能够完全隔离，避免普通世界的任何干扰。这适用于以下领域： 通讯录数据 医疗健康信息 浏览器历史和 Cookie 自动补全系统 日历和电子邮件 剪贴板数据 Arm CCA 还支持在 AI 模型中隔离个人数据，从而允许云端访问这些数据，并在进行系统外分析个人数据时提供更高的安全信心。User data confidentialityAs OS vendor information in Sensitive data services shows, you can also ensureisolation of user data from the Normal world without needing the manyrelationships with the OEMs and Trusted OS developers. For users, there is noability for the platform developer and OS or hypervisor developer to access userdata through privilege escalation or hierarchical privilege.This means that the user can trust their data on a large server system and beassured that their data cannot be accessed by the platform owner. For example,photos can never be seen, and personal works like music files can be stored andexecuted without the controlling OS or hypervisor, in the Normal world, beingable to access the files.This lack of access can both reassure the user and aid the platform developer.An example is a personally created music file that is stored and executed on aserver using a service like Soundcloud, If any copyright issues for the musicled to a legal challenge, the platform provider is not responsible for a dataleak, and can claim zero liability.Arm CCA also allows for any service or application developer to run their codeand data in an isolated Realm VM. Executing as a Realm VM allows the developerto have a higher level of trust in the execution of his code on any platformsupporting Arm CCA.The following sections show some examples: 如敏感数据服务中的操作系统厂商信息所示，用户数据可以与普通世界实现隔离，而无需与 OEM 和可信操作系统开发者建立复杂的信任关系。对于用户来说，平台开发者、操作系统或虚拟机管理程序开发者无法通过权限提升或层级权限访问用户数据。 这意味着用户可以信任自己的数据在大型服务器系统上的安全性，并确信平台所有者无法访问自己的数据。例如，照片永远不会被查看，个人作品如音乐文件可以被存储和播放，而普通世界中的控制操作系统或虚拟机管理程序都无法访问这些文件。 这种无法访问既能让用户更加安心，也有利于平台开发者。例如，个人创作的音乐文件存储并在服务器上通过 Soundcloud 等服务播放时，如果因版权问题发生法律纠纷，平台提供方无需为数据泄露负责，可以声明零责任。 Arm CCA 还允许任何服务或应用开发者在隔离的 Realm 虚拟机中运行自己的代码和数据。以 Realm 虚拟机方式运行，开发者可以更信任其代码在任何支持 Arm CCA 的平台上的执行。 以下章节将展示一些示例：Machine Learning modelsMachine Learning (ML) and Computer Vision (CV) models for learning and inferencewill contain IP that requires isolating to stop theft. With the additionalrequirement stop any chance of replacement when used for security functions likeface or other biometric recognition. Arm CCA allows the service to be managed,updated, and executed in any system without the need to trust the Rich OS orhypervisor that is being used.This also allows service developers to ensure that their own ML service is usedfor the biometric recognition without having to rely on an OS-based service thatthey may not have a trust relationship with.As Memory Encryption Contexts shows, the confidentiality enabled by Arm CCAenables data for AI models to be secure on a personal device which can beaccessed and analyzed by cloud services. The user can be confident that the datais secure on their platform and the access from the cloud is fully secured andattested. 用于学习和推理的机器学习（ML）和计算机视觉（CV）模型通常包含需要隔离以防止被盗的知识产权（IP）。此外，在用于安全功能（如人脸或其他生物识别）时，还需要防止模型被替换的风险。Arm CCA 允许这些服务在任何系统中进行管理、更新和执行，无需信任所用的丰富操作系统或虚拟机管理程序。 这也让服务开发者能够确保自己的机器学习服务被用于生物识别，而不必依赖于他们可能不信任的操作系统服务。 正如内存加密上下文（Memory Encryption Contexts）所展示的，Arm CCA 提供的机密性能够保障 AI 模型的数据在个人设备上的安全，并且可以被云服务安全访问和分析。用户可以确信其数据在本地平台上是安全的，同时云端的访问也是完全安全且经过认证的。Large Trusted services and applicationsIn many systems the development of the TrustZone enabled Secure world requireson-SoC memory which is required to be smaller in size than the Normal world DRAMmemory. Smaller memory makes it harder for trusted applications running in theSecure World to manage large applications – due to memory limitations, lack ofdynamic memory allocation and potential certification constraints.The mechanisms in place for excution of applications in Realm world means largeapplications can run in a Realm VM with full confidentiality using the RMEincluding DA and MEC. 在许多系统中，启用 TrustZone 的安全世界（Secure world）开发通常依赖于片上（SoC）内存，而这部分内存的容量往往远小于普通世界（Normal world）的 DRAM 内存。较小的内存容量使得在安全世界中运行的大型应用难以管理，原因包括内存限制、缺乏动态内存分配以及潜在的认证约束。 而在 Realm 世界中运行应用的机制，使得大型应用可以在 Realm 虚拟机中运行，并通过RME（包括 DA 和 MEC）实现完全的机密性。Bring Your Own DeviceWith Bring Your Own Device (BYOD), an organization allows users to run servicesand applications on their personal device. This means that users can accesssensitive company data such as email and work calendars.BYOD use is often allowed with the understanding that, if the device is lost orstolen, the device OS and all data are completely erased. The device is returnedto a factory initialized state, so that malicious actors cannot force access tothe company sensitive data. This can be an issue for the user, whose personaldata must also be erased.Arm CCA can allow Secure services to be executed on a BYOD device, without theneed to completely reset the device and erase all user data if the device islost or stolen. Realms can be managed externally, and only the Realm world needsto be reset. This configuration gives the BYOD device user more assurance whenusing organization services. 在“自带设备”（BYOD，Bring Your Own Device）场景下，企业允许用户在个人设备上运行服务和应用程序。这意味着用户可以访问公司的敏感数据，如电子邮件和工作日历。 通常，允许 BYOD 的前提是，如果设备丢失或被盗，设备操作系统及所有数据都将被彻底擦除，设备会恢复到出厂初始化状态，以防止恶意行为者强行访问公司的敏感数据。然而，这对用户来说也存在问题，因为他们的个人数据同样会被擦除。 Arm CCA 可以让安全服务在 BYOD 设备上运行，即使设备丢失或被盗，也无需完全重置设备和擦除所有用户数据。Realm 可以由外部进行管理，只需重置 Realm 世界即可。这种配置为 BYOD 设备用户在使用企业服务时提供了更多保障。" }, { "title": "[arm] learn RME", "url": "/posts/learn-RME/", "categories": "coco, RME", "tags": "RME", "date": "2025-08-26 14:55:00 +0800", "snippet": "1. OverviewThis guide introduces the Realm Management Extension (RME), an extension to thearchitecture. RME is the hardware component of the Arm Confidential ComputeArchitecture (Arm CCA) which als...", "content": "1. OverviewThis guide introduces the Realm Management Extension (RME), an extension to thearchitecture. RME is the hardware component of the Arm Confidential ComputeArchitecture (Arm CCA) which also includes software elements. RMEdynamically transfers resources and memory to a new protected addressspace that higher privileged software or TrustZone firmware cannot access.Because of this address space, Arm CCA constructs protected executionenvironments called realms.Realms allow a lower-privileged software, like an application or a VirtualMachine (VM), to protect its content. Realms also prevent execution from attacksusing software that runs at higher privilege levels, like an OS or a hypervisor.Higher-privileged software is still responsible for allocating and managing theresources that a realm uses. However, this higher-privileged software cannotaccess the contents of the realm or affect its execution flow.RME can also dynamically transfer memory to a protected address space forrealms. With RME, the memory available to TrustZone Software entities can bevaried dynamically. 本指南介绍了 Realm Management Extension（RME），这是 ARM 架构的一项扩展。RME是 Arm Confidential Compute Architecture（Arm CCA）的硬件组成部分，Arm CCA 还包括软件部分。RME 可以将资源和内存动态转移到一个新的受保护地址空间，而更高权限的软件或 TrustZone 固件无法访问这个地址空间。基于这一地址空间，Arm CCA 构建了称为 realm 的受保护执行环境。 realm 允许低权限软件（如应用程序或虚拟机 VM）保护自己的内容。realm 还能防止高权限软件（如操作系统或虚拟机管理器 hypervisor）发起的攻击影响其执行。高权限软件依然负责分配和管理 realm 所用的资源，但它无法访问 realm 的内容，也无法影响realm 的执行流程。 RME 还可以将内存动态转移到 realm 的受保护地址空间。借助 RME，TrustZone 软件实体可用的内存可以动态变化。 Realm 是RME 基于trustzone引入的一个新的 state, 用来运行一些低权限的软件,并保护自己的内容, 那为什么要新扩展一个state， 而不使用 secure state(虚拟化可以使用secure-EL2 introduce in ARMv8.4), 我们来思考下, trustzone 的 secure state, 有哪些问题? impact other SOFTWARE: secure state 可以访问所有 non-secure state 的软件， 所以低权限软件放到secure state 是危险的 另外 secure world 本身就是为了缩小该world的software, 来保证安全性,如果在该世界运行很多的软件, 会降低该world的安全性. other SOFTWARE impact this SOFTWARE 反过来说，secure world software 也会访问到虚拟机的内容. 总之, 重新从底层设计CCA的需求是，当前的secure state对于低权限的软件来说，隔离性还是不够的, 需要将这些软件，单独隔离起来. 需要再从CCA的需求侧再分析下 至于 dynamically move, 这个并不关键，因为该功能(GPC) 也为secure state实现了 This guide describes the key hardware features that RME introduces or changesand introduces you to the software architecture.You will learn about the following concepts: Understand the new Security states and Physical Address (PA) spaces in systems with RME Describe how a region of memory can be dynamically assigned between PA spaces Understand the system requirements for an RME-enabled system 本指南介绍了 RME 引入或变更的主要硬件特性，并带你了解相关的软件架构。 你将学习以下内容： 了解在采用 RME 的系统中新增的安全状态和物理地址（PA）空间； 说明如何将某个内存区域在不同物理地址空间之间动态分配； 了解支持 RME 的系统所需的系统要求。 This guide explains the following changes that RME introduces to the processorarchitecture: Additional Security states Additional Physical addresses Support for Granule Protection Checks, which allow granules of memory to bedynamically assigned to a physical address space 本指南解释了 RME 对处理器架构所引入的以下变化： 新增的安全状态； 新增的物理地址空间； 支持颗粒保护检查（Granule Protection Checks），该机制允许内存颗粒（granule）在不同物理地址空间之间动态分配。 Diversity and inclusion are important values to Arm. Because of this, we arereevaluating the terminology we use in our documentation. Older Armdocumentation uses the terms master and slave. 多样性和包容性是 Arm 非常重视的价值观。因此，我们正在重新审视在文档中使用的术语。Arm 早期的文档曾使用“主设备（master）”和“从设备（slave）”这些术语。 这是在叠甲么 This guide uses replacement terminology, as follows: The new term Requester is synonymous with master in older documentation The new term Subordinate is synonymous with slave in older documentation2. Security statesRME builds on the Arm TrustZone technology. TrustZone was introduced in Armv6and provides the following two Security states: Secure state Non-secure stateThe following diagram shows the two Security states in AArch64 with the softwarecomponents that are typically found in each Security state: RME 是在 Arm TrustZone 技术基础上构建的。TrustZone 在 Armv6 架构中首次引入，并提供了以下两种安全状态： 安全状态（Secure state） 非安全状态（Non-secure state）下图展示了在 AArch64 架构中这两种安全状态，以及每种安全状态下通常包含的软件组件： The architecture isolates software running in Secure state from software runningin Non-secure state. This isolation enables a software architecture in whichtrusted code runs in Secure state and is protected from code in Non-securestate.RME extends this model, and provides the following four Security states: Secure state Non-secure state Realm state Root stateThe following diagram shows the Security states in an RME-enabled PE, and howthese Security states map to Exception levels: 该架构将运行在安全状态（Secure state）下的软件与运行在非安全状态（Non-securestate）下的软件进行了隔离。这种隔离使得可信代码能够在安全状态下运行，并免受非安全状态下代码的影响和攻击。RME 对这一模型进行了扩展，提供了以下四种安全状态： 安全状态（Secure state） 非安全状态（Non-secure state） Realm 状态（Realm state） Root 状态（Root state） 下图展示了在支持 RME 的处理元（PE）中这些安全状态，以及这些安全状态与异常级别（Exception levels）之间的对应关系：Maintaining Secure state provides backwards compatibility with existingTrustZone use cases. These use cases can also be upgraded to take advantage ofnew features added by RME, like dynamic memory assignment.Realm state constructs protected execution environments called realms.Importantly, RME extends the isolation model introduced in TrustZone.The architecture provides isolation for the following states: Secure state from both Non-secure and realm states Realm state from both Non-secure and Secure statesThis isolation model provides a software architecture in which the software inSecure and realm states are mutually distrusting.With RME, Exception level 3 moves out of Secure state and into its own Securitystate called root. RME isolates Exception level 3 from all other Securitystates. Exception level 3 hosts the platform and initial boot code and thereforemust be trusted by the software in Non-secure, Secure, and realm states. Becausethese Security states do not trust each other, Exception level 3 must be in aSecurity state of its own. 保持安全状态（Secure state）能够向后兼容现有的 TrustZone 用例。这些用例也可以通过升级，利用 RME 新增的特性，例如动态内存分配。 Realm 状态（Realm state）用于构建称为 realm 的受保护执行环境。重要的是，RME 扩展了 TrustZone 引入的隔离模型。 该架构对以下状态提供了隔离： Secure 状态与 Non-secure 和 realm 状态之间的隔离 Realm 状态与 Non-secure 和 Secure 状态之间的隔离 这种隔离模型实现了一种软件架构，使得 Secure 和 realm 状态下的软件彼此互不信任。 在 RME 中，异常级别 3（Exception level 3）从 Secure 状态中分离出来，进入了一个称为 root 的独立安全状态。RME 将异常级别 3 与所有其他安全状态隔离开。异常级别3 负责托管平台和初始启动代码，因此必须被 Non-secure、Secure 和 realm 状态下的软件所信任。由于这些安全状态彼此不信任，异常级别 3 必须拥有自己的安全状态。Controlling the current Security stateA combination of the Exception level and SCR_EL3 registers controls the currentSecurity state.Exception level 3 is now its own root Security state. While in Exception level 3,the Security state is always root, and no other Exception level can be in rootstate.When in lower Exception levels such as Exception level 0, Exception level andException level 2, the NS and NSE fields in SCR_EL3 controls the Security state.The exception levels are shown in the following table: 异常级别（Exception level）与 SCR_EL3 寄存器的组合共同控制当前的安全状态（Security state）。 异常级别 3（Exception level 3）现在拥有自己的 root 安全状态。在异常级别 3 下，安全状态始终为 root，且没有其他异常级别可以处于 root 状态。 当处于较低的异常级别（如异常级别 0、异常级别 1 和异常级别 2）时，SCR_EL3 寄存器中的 NS 和 NSE 字段用于控制安全状态。各异常级别如下表所示： SCR_EL3.{NSE,NS} Security state 0b00 Secure 0b01 Non-Secure 0b10 - 0b11 Realm There is no encoding for root state. While in Exception level 3, the currentSecurity state is always root, regardless of the SCR_EL3.{NSE,NS} value.In Exception level 3, the current value of SCR_EL3.{NSE,NS} is used to controlsome operations. For example, when issuing a Translation Lookaside Buffer (TLB)invalidation instruction at Exception level 3 for a lower Exception level,SCR_EL3.{NSE,NS} controls which Security state the operation applies to. 没有针对 root 状态的编码。当处于异常级别 3 时，无论 SCR_EL3 的 NSE 和 NS 字段值如何，当前的安全状态始终是 root。 在异常级别 3 下，SCR_EL3 的 NSE 和 NS 字段的当前值用于控制某些操作。例如，当在异常级别 3 下为较低的异常级别发出 TLB（地址转换后备缓冲区）失效指令时，SCR_EL3的 NSE 和 NS 字段会决定该操作适用于哪个安全状态。Moving between Security statesThe principles of moving between Security states is inherited from TrustZone. Tochange Security state, execution must pass through Exception level 3, as shownin the following diagram: 在不同安全状态之间切换的原则继承自 TrustZone。要改变安全状态，必须经过异常级别3（Exception level 3），如下图所示：The changing Security state process in the diagram follows these steps: Execution starts in realm state and SCR_EL3.{NSE,NS} is set to 0b11. Thesoftware executes a Secure Monitor Call (SMC) instruction, which causes anexception to be taken to Exception level 3. The processor enters Exception level 3 and is now in root state becauseException level 3 is always in root state. However, SCR_EL3.{NSE,NS} stillhas the same Security state that the exception was taken from. Software inException level 3 changes SCR_EL3.{NSE,NS} to the corresponding value forthe required Security state and executes an Exception Return (ERET). The ERET causes exit from Exception level 3. When leaving Exception level 3,SCR_EL3. {NSE,NS} controls which Security state is entered. In this diagram,the Security state is Nonsecure.There is only one copy of the vector registers, the general-purpose registers,and most System registers. When moving between Security states it is theresponsibility of the software, not hardware, to save and restore registercontext. The software that saves and restores this register context is calledthe Monitor. 图中展示的安全状态切换过程遵循以下步骤： 执行从 realm 状态开始，SCR_EL3 的 NSE 和 NS 字段设置为 0b11。软件执行Secure Monitor Call（SMC）指令，这会导致异常被捕获并进入异常级别 3（Exception level 3）。 处理器进入异常级别3，此时安全状态为 root，因为异常级别 3 始终处于 root 状态。然而，SCR_EL3 的 NSE 和 NS 字段仍然保持着触发异常时的安全状态。异常级别 3中的软件将 SCR_EL3 的 NSE 和 NS 字段更改为所需安全状态对应的值，并执行异常返回（ERET）。 ERET 指令导致从异常级别 3 退出。当离开异常级别 3 时，SCR_EL3 的 NSE 和 NS字段决定进入哪种安全状态。在该图中，进入的是非安全状态（Nonsecure）。 向量寄存器、通用寄存器以及大多数系统寄存器都只有一份。在不同安全状态之间切换时，保存和恢复寄存器上下文的责任在于软件而不是硬件。负责保存和恢复这些寄存器上下文的软件被称为 Monitor（监控器）。 和 secure state &amp;&amp; non-secure state 切换一样. 3. Physical AddressesIn addition to two Security states, TrustZone provides the following twoPhysical Address (PA) spaces: Secure physical address space Non-secure physical address spaceThese separate PA spaces form part of the TrustZone isolation guarantee.Non-secure state cannot access an address in a Secure PA space. This isolationmeans that there are confidentiality and integrity guarantees for data belongingto Secure state. 这些独立的物理地址空间（PA 空间）构成了 TrustZone 隔离机制的一部分。非安全状态（Non-secure state）无法访问安全物理地址空间（Secure PA space）中的地址。这种隔离确保了属于安全状态的数据在保密性和完整性方面的保障。RME extends this guarantee to support the following PA spaces: Secure physical address space Non-secure physical address space Realm physical address space Root physical address spaceThe architecture limits which PA spaces are visible in each Security state. Thefollowing table shows the PA spaces that you can access in each Security state: 该架构限制了每种安全状态（Security state）下可见的物理地址空间（PA space）。下表展示了在每种安全状态下可以访问的物理地址空间： Physical address space Secure state Non-secure state Realm state Root state Secure PAS Y N N Y Non-Secure PAS Y Y Y Y Realm PAS N N Y Y Root PAS N N N Y 从这里可以看到, Realm PAS只能由 Realm state以及 Root PAS访问，并且 Realm state也不能访问 Secure PAS, 两者是相互隔离的In this table, Y means accessible and N means not accessible.When documentation refers to a physical address, prefixes are used to identifywhich address space is being referred to, for example: SP:0x8000 means address 0x8000 in the Secure PA space NSP:0x8000 means address 0x8000 in the Non-secure PA space RLP:0x8000 means address 0x8000 in the realm PA space RTP:0x8000 means address 0x8000 in the root PA spaceArchitecturally, each example is an independent memory location. This means thatSP:0x8000 and RTP:0x8000 are treated as different physical locations. All fourlocations can exist in an RMEenabled system although in practice, this isunlikely. 在此表中，Y 表示可访问，N 表示不可访问。 当文档提及物理地址时，会使用前缀来标识所指的地址空间，例如： SP:0x8000 表示安全物理地址空间中的地址 0x8000 NSP:0x8000 表示非安全物理地址空间中的地址 0x8000 RLP:0x8000 表示领域物理地址空间中的地址 0x8000 RTP:0x8000 表示根物理地址空间中的地址 0x8000 从体系结构上讲，每个示例都是独立的内存位置。这意味着 SP:0x8000 和 RTP:0x8000被视为不同的物理位置。在启用了 RME 的系统中，这四个位置都可以存在，尽管在实际应用中，这种情况并不常见。Virtual address spacesTo support the new Security states, RME introduces the following translationregimes for Realm state: Realm EL1&amp; 0 translation regime: This regime includes two virtual address (VA) regions, similar to theNon-secure EL1&amp;0 translation regime. This translation regime is subject tostage 2 translation. Realm Exception level 2 and 0 translation regime: This regime includes two VA regions, similar to the Secure Exception level 2and 0 translation regime. Realm Exception level 2 and 2 translation regime: This regime includes a single VA region, similar to the Secure Exceptionlevel 2 translation regime. The following diagram shows the realm state translation regimes: 为了支持新的安全状态，RME（Realm Management Extension）为 Realm 状态引入了以下地址转换机制： Realm EL1&amp;0 转换机制: 该机制包含两个虚拟地址（VA）区域，类似于非安全 EL1&amp;0 的转换机制。此转换机制会经过二阶段（stage 2）转换。 Realm 异常级别 2 和 0 转换机制： 该机制包含两个 VA 区域，类似于安全异常级别 2 和 0 的转换机制。 Realm 异常级别 2 和 2 转换机制： 该机制只包含一个 VA 区域，类似于安全异常级别 2 的转换机制。 下图展示了 Realm 状态下的地址转换机制：For all realm translation regimes, any address that translates to a Non-securephysical address is treated as execute-never. 对于所有 Realm 的地址转换机制，任何转换得到的非安全物理地址都会被视为“不可执行”（execute-never）。Root state translation regimeOnly Exception level 3 exists in root state. This means that there is a singletranslation regime for root state, which is the Exception level 3 translationregime. This translation regime existed before RME but was previously consideredpart of Secure state.RME contains the following changes to the Exception level 3 translation regime: Virtual addresses can translate to physical addresses in any of the fourphysical address spaces Any address that translates to a Non-secure, Secure, or realm physicaladdress is treated as execute-never MMU table walks can only access the root PA space When the MMU is disabled at Exception level 3, all output addresses are inthe root PA space 只有异常级别 3（Exception level 3）处于根状态（root state）。这意味着根状态只有一种地址转换机制，即异常级别 3 的转换机制。该转换机制在 RME 之前就已存在，但之前被认为是安全状态（Secure state）的一部分。 RME 对异常级别 3 的转换机制进行了如下修改： 虚拟地址可以被转换到四种物理地址空间中的任意一种 任何被转换到非安全、安全或 realm 物理地址的地址都会被视为“不可执行”（execute-never） MMU 的页表遍历只能访问根物理地址空间（root PA space） 当异常级别 3 下 MMU 被禁用时，所有输出地址都属于根物理地址空间 root state 只能运行 root PA space 中的代码 root state MMU只能访问 root PA space 中的 页表 Exception level 3 continues to use the Exception level 3 translation regime,which has a single VA range 异常级别 3 仍然使用异常级别 3 的地址转换机制，该机制只包含一个虚拟地址（VA）范围。 Controlling output PASWhen a virtual address is translated by the MMU, the output PA space iscontrolled by a combination of the following: Current Security state Current Exception level Translation tables System registersThe controls that are available to software vary depending on the translationregime. Now we will look at the controls in each Security state and translationregime. 当 MMU 对虚拟地址进行转换时，输出的物理地址空间由以下因素共同决定： 当前的安全状态（Security state） 当前的异常级别（Exception level） 转换表（Translation tables） 系统寄存器（System registers） 可供软件使用的控制方式会根据不同的地址转换机制而有所不同。接下来我们将分别介绍各安全状态和转换机制下的控制方式。Non-secure state translation regimesThe following diagram shows an overview of the Non-secure translation regimes:Non-secure state can only access the Non-secure PA space. Therefore, inNon-secure state there are no controls at stage 1 or stage 2 for controlling theoutput IPA space or PA space.In this diagram, TTD means Translation Table Descriptor. 非安全状态只能访问非安全物理地址空间（Non-secure PA space）。因此，在非安全状态下，无论是一级转换阶段（stage 1）还是二级转换阶段（stage 2），都没有用于控制输出 IPA 空间或 PA 空间的相关控制项。 在本图中，TTD 表示转换表描述符（Translation Table Descriptor）。Secure state translation regimesSecure state can access the Secure and Non-secure PA spaces, as shown in thefollowing diagram:For the Secure translation regime, the NS bit in the stage 1 translation tablesentries selects between two Intermedial Physical Address (IPA) spaces.There are controls at Exception level 2, VTCR_EL2, and VSTCR_EL2, to map each ofthose IPA spaces to a PA space. 对于安全地址转换机制，在一级转换表（stage 1 translation tables）项中的 NS 位用于在两个中间物理地址空间（IPA）之间进行选择。 在异常级别 2（Exception level 2），通过 VTCR_EL2 和 VSTCR_EL2 控制，可以将每个IPA 空间映射到一个物理地址空间（PA space）。 这个是很关键的, 因为EL2往往是用于虚拟化, 这样就让hyp 可以控制 是否虚拟化secure mode 的cap。并且也可以让 整个虚拟机PA都运行在 secure PA 中. 例如HuaWei VirtCCA. In Secure state, the controls are unchanged by RME and are described in thisguide for completeness. 在安全状态下，相关控制项未因 RME 而发生变化，为了完整起见，本指南仍对其进行了描述。 Realm state translation regimesRealm state can access the realm and Non-secure PA spaces, as shown in thefollowing diagram:The realm EL0/1 translation regime has a single realm IPA space. Therefore,there is no NS bit in the EL0/1 stage one translation table entries.The realm stage two TTDs include an NS bit, to map to either the realm orNon-secure PAS. This means that, unlike Secure state, realm state has per-pagecontrols at stage two.The realm EL2 translation regime and realm EL0/2 translation regime have stageone NS bits to control the output PA space.The NS bit in the translation table entries was introduced by TrustZone to allowSecure state to select the output PA space. In Secure state, the NS bit isencoded as follows: NS=0: Secure NS=1: Non-secureWith RME, the NS field is also used in realm EL0/1 stage two and realm EL2/0stage one translation tables, but is encoded as: NS=0: realm NS=1: Non-secure realm EL0/1 的地址转换机制只有一个 realm IPA 空间，因此在 EL0/1 的一级转换表项中没有 NS 位。 realm 的二级转换表描述符（stage two TTDs）包含一个 NS 位，用于映射到 realm 或Non-secure 物理地址空间（PAS）。这意味着，与 Secure state 不同，realm state 在二级转换阶段可以实现每页的控制。 这个是比较好用的, 可以独立控制 IPA page frame 的secure state. 这里还需要思考下: 为什么realm 可以由 stage 2 page table 配置 NS, 而 secure-EL2 不能(只能决定整个的stage 2是 secure 还是 Non-Secure) realm EL2 的转换机制和 realm EL0/2 的转换机制在一级转换表中有 NS 位，用于控制输出的物理地址空间。 NS 位最初由 TrustZone 引入，用于让 Secure state 选择输出的物理地址空间。在Secure state 下，NS 位的编码如下： NS=0：Secure NS=1：Non-secure 在 RME 中，NS 字段也用于 realm EL0/1 的二级转换和 realm EL2/0 的一级转换表中，其编码方式如下： NS=0：realm NS=1：Non-secure Root state translation regimesRoot state can access all PA spaces, as shown in the following diagram:The Exception level 3 stage one translation regime has two bits, NS and NSE, inthe translation table entries to control the output PA space. These encodingsare similar to the encodings that are used for SCR_EL3.{NSE,NS}, except thatthere is an encoding for root, as shown in the following table: 异常级别 3 的一级地址转换机制在转换表项中包含两个位：NS 和 NSE，用于控制输出的物理地址空间（PA space）。这些编码方式与 SCR_EL3 的 {NSE, NS} 字段所使用的编码方式类似，不过在这里还增加了一个用于 root 的编码，如下表所示：Only Exception level 1 exists in root state. Exception level 3 is only subjectto stage one translation.Impact on Translation Lookaside Buffers and cachesTLBs cache recently used translations. Translation Lookaside Buffer (TLB)entries need to record which translation regime an entry belongs to. During aTLB look-up, an entry can only be returned if the translation regime in theentry matches the requested translation regime. This prevents one Security statefrom using the TLB entries of another Security state. TLB（Translation Lookaside Buffer，翻译后备缓冲区）用于缓存最近使用过的地址转换。TLB 项需要记录每个条目所属的地址转换机制。在进行 TLB 查找时，只有当条目中的转换机制与当前请求的转换机制一致时，该条目才会被返回。这样可以防止一个安全状态（Security state）使用另一个安全状态的 TLB 条目。The following table shows an example simplified TLB, recording the translationregime:Similarly, caches lines need to record the associated PA space, as shown in thefollowing diagram:4. Granule Protection ChecksThis section describes Granule Protection Checks introduced by RME. GranuleProtection Checks enable the dynamic assigning of memory regions betweendifferent physical addresses spaces.This section teaches you about the following features: The structure of Granule Protection tables Fault reporting for Granule Protection Checks How regions transition between PA spacesAs described in Physical addresses, RME provides four physical address spaces.The following diagram shows these physical address spaces: 本节介绍由 RME 引入的粒度保护检查（Granule Protection Checks）。粒度保护检查支持在不同物理地址空间之间动态分配内存区域。 本节将讲解以下内容： GPT 的结构 GPC 的错误报告 区域如何在物理地址空间之间转换 如“物理地址”一节所述，RME 提供了四种物理地址空间。下图展示了这些物理地址空间：In theory, each PA space is separate and independent and could be fullypopulated. In practice, most designs have a single effective PA space for DRAMregions, using the PA spaces to partition the space into regions, as shown inthe following diagram: 理论上，每个物理地址空间（PA space）都是独立分离的，并且可以被完全填充。实际上，大多数设计只为 DRAM 区域使用一个有效的物理地址空间，并利用这些物理地址空间将整个空间划分为多个区域，如下图所示：For on-chip devices and memories, the memory system typically enforcesisolation. This isolation is provided either in the end peripheral or in theinterconnect. This configuration is referred to as completer side filtering,for example: On-chip ROM and SRAM, which is root-only and the interconnect enforces it.Example use cases include system boot. Generic Interrupt Controller (GIC). Transactions are routed to the GICregardless of PA space. GIC internally uses the security of the access tocontrol which state and configuration is accessible. For bulk memory, RME provides a mechanism to dynamically allocate pages todifferent PA spaces at runtime. For example, when starting a realm, ownershipof some memory is transferred from Non-secure state to realm state. When thatrealm is terminated the memory is reclaimed, and ownership returns to aNon-secure state. 对于片上设备和存储器，内存系统通常会强制实现隔离。这种隔离可以由终端外设或互连总线来提供。这种配置被称为“完成端过滤”（completer side filtering），例如： 片上 ROM 和 SRAM，这些仅属于 root 权限，且互连总线会强制实现隔离。典型用例包括系统启动。 这些应该是指, 不能配置的, 固定写死的 通用中断控制器（GIC）。无论物理地址空间如何，事务都会被路由到 GIC。GIC 会根据访问的安全性来控制可访问的状态和配置。 对于大容量内存，RME 提供了一种机制，可在运行时将页面动态分配到不同的物理地址空间。例如，在启动一个 realm 时，部分内存的所有权会从非安全状态转移到 realm状态。当该 realm 被终止时，内存会被回收，所有权重新归属到非安全状态。 In the system architecture, the physical address space region that isassigned is called the Resource PA space.The Granule Protection Checks in the MMU enables dynamic allocation of pagesto PA spaces. A set of Granule Protection Tables (GPTs) records for everylocation that is either of the following: Completer side filtered. In this allocation, the MMU permits all accessesand relies on memory system checks. These checks can be carried out ineither the interconnect or the peripheral. Allocated to a PAS: In this allocation, the MMU only permits access where the output physicaladdress space from VA to PA translation matches the PA space in the GPTs Where the physical address space does not match, the MMU blocks the accessand returns a Granule Protection Fault (GPF) Conceptually, the MMU after stage one and stage two translations performsGranule Protections Checks, as shown the following diagram: 在系统架构中，被分配的物理地址空间区域被称为资源物理地址空间（Resource PAspace）。 MMU 中的粒度保护检查（Granule Protection Checks）支持将页面动态分配到物理地址空间（PA spaces）。一组粒度保护表（Granule Protection Tables，GPTs）会为每个位置记录以下两种情况之一： 完成端过滤（Completer side filtered）：在这种分配方式下，MMU 允许所有访问，并依赖于内存系统的检查。这些检查可以在互连总线或外设中执行。 分配到某个物理地址空间（PAS）： 在这种分配方式下，MMU 只允许那些从 VA 到 PA 转换后输出的物理地址空间与GPTs 中记录的物理地址空间相匹配的访问。 如果物理地址空间不匹配，MMU 会阻止访问，并返回粒度保护错误（GranuleProtection Fault，GPF）。 从概念上讲，MMU 在完成一级和二级地址转换后，会执行粒度保护检查，如下图所示：In the diagram, stages are shown as serial however, the process is morecomplicated. In the following table, we show an example of an LDR instructionthat is executed in NS_EL1. For simplicity, we assume a single table level atstages 1 and 2: 在图示中，各个阶段被描绘为串行，但实际过程要更为复杂。下表展示了在 NS_EL1 下执行一条 LDR 指令的示例。为简化说明，我们假设第 1 阶段和第 2 阶段各只有一级表。 我们来总结下哪些地址需要做GPC: The PA used in stage 2 that is involved in the stage 1 translation from IPAto PA. The PA of stage 1 pgtable. The PA used in stage 2 that is involved in NS_EL1 IPA. The PA of NS_EL1. For a running system, most accesses reuse cached translations in the TLBs.However, the example highlights the interaction between the different stagesof translation and Granule Protection Checks. In Elision, we show that some parts of the process can be optimized. 对于正在运行的系统，大多数访问会复用 TLB（转换后备缓冲区）中的缓存转换。然而，该示例强调了不同转换阶段与粒度保护检查之间的相互作用。 在简化过程中，我们展示了其中一些环节可以进行优化。 Granule Protection TablesA set of tables, called Granule Protection Tables (GPT), configures which PAspace each granule is associated with. When the processor performs an access,the MMU walks the GPTs to determine whether the access is permitted.Granule Protection Checks (GPCs) are configured using the following Systemregisters: 一组称为颗粒保护表（Granule Protection Tables，GPT）的表用于配置每个颗粒（granule）关联的物理地址空间（PA space）。当处理器进行访问时，MMU 会遍历这些GPT 表，以确定该访问是否被允许。 颗粒保护检查（Granule Protection Checks，GPC）通过以下系统寄存器进行配置： GPCCR_EL3 to configure: Granule Protection Checks Enable Granule size: 4K, 16K, or 64K Size of protected region Elision enable GPTBR_EL3 to configure the PA of the GPT, which is in the root PA spaceGPTs have a two-level table structure, as shown in the following diagram:GPTBR_EL3 points to the base of the level 0 table. Each level 0 table entrycovers a 1GB region, and can be one of the following formats: Block Descriptor. The block is assigned to a specific PAS or be configuredto allow all PA spaces. Table Descriptor: The level 0 table entry is subdivided into granules representing theregion, with a level 1 GPT describing the mapping of those granules The descriptor gives the PA of the level 1 table. The table must be in theroot PA space. GPTBR_EL3 指向第 0 级表（level 0 table）的基地址。每个第 0 级表项覆盖一个 1GB区域，并且可以采用以下格式之一： 块描述符（Block Descriptor）：该块被分配给特定的物理地址空间（PAS），或者被配置为允许所有物理地址空间访问。 表描述符（Table Descriptor）： 第 0 级表项会进一步细分为代表该区域的颗粒（granule），并由第 1 级 GPT（level 1 GPT）描述这些颗粒的映射关系。 描述符中包含第 1 级表的物理地址（PA）。该表必须位于 root 物理地址空间（root PA space）内。 Each entry in a level 1 GPT is one of the following: Granules Descriptor Contains 16 GPI fields, with each GPI field describing one granule of PAspace Each granule can be independently assigned to a single PA space orconfigured to allow all PA spaces. This configuration delegatesresponsibility to check the legality of the access to a completer-sidefilter. Contiguous Descriptor is like a Granule Descriptor, but describes largerregions. Using larger regions can enable more efficient caching in the TLBs.Each entry in a L1 table describes 16 granules, with separate fields pergranule. The granule size is configurable through GPCCR_EL3 and matches thegranules sizes that are available for the translation tables.Because a level 1 table is fixed at 1GB but the granule size is variable andcovers the address range, the number of entries in a level 1 table also varies.Selecting a smaller granule size results in larger level 1 tables. The followingexample shows the level 1 tables sizes using a 4KiB and 64KiB granule size: 每个一级 GPT（level 1 GPT）表项可以是以下类型之一： 颗粒描述符（Granules Descriptor） 包含 16 个 GPI 字段，每个 GPI 字段描述一个物理地址空间的颗粒（granule）。 每个颗粒都可以独立分配到某一个物理地址空间，或者配置为允许所有物理地址空间访问。若配置为允许所有物理地址空间，则访问合法性的检查会委托给完成端（completer）侧的过滤器。 连续描述符（Contiguous Descriptor）类似于颗粒描述符，但用于描述更大的区域。使用更大的区域可以使 TLB（地址转换后备缓冲区）缓存更高效。 每个一级表项描述 16 个颗粒，每个颗粒有独立的字段。颗粒大小可通过 GPCCR_EL3 配置，并且与转换表支持的颗粒大小一致。 由于一级表固定覆盖 1GB 区域，而颗粒大小可变并决定覆盖的地址范围，所以一级表中的表项数量也会变化。选择较小的颗粒大小会导致一级表变得更大。以下示例展示了采用4KiB 和 64KiB 颗粒大小时的一级表大小：A GPTE refers to an entry in either a level 0 or level 1 GPT. GPI refers to thefields in a GPTE that describe the assigned PA space for a region of memory. GPI entries for level 0 entries are only used in block descriptors.The GPTBR_EL3.PPS defines the region that the GPT covers starting at address 0and extends to an upper bound. Any address beyond the range that GPTBR_EL3.PPSdefines, is treated as belonging to the Non-secure PA space. GPTE 指的是一级或零级 GPT 表中的一个表项。GPI 指的是 GPTE 中用于描述某个内存区域分配到的物理地址空间（PA space）的字段。 对于零级表项，GPI 字段仅用于块描述符（block descriptor）。 GPTBR_EL3.PPS 定义了 GPT 所覆盖的区域，从地址 0 开始，直到上限。任何超出GPTBR_EL3.PPS 所定义范围的地址，都被视为属于非安全物理地址空间（Non-secure PAspace）。Granule Protection Check faultsIf an access fails its Granule Protection Checks, a fault is reported.Collectively, these faults are referred to as Granule Protection Check (GPC)faults.The types of GPC fault are as follows: Granule Protection Fault (GPF): The GPT walk completed successfully, but theaccess was not permitted. GPT Walk Fault: The GPT walk failed to complete because of an invalid GPTentry. GPT address size fault: The GPT walk failed because of attempted access toan address beyond the configured range. Synchronous External abort on GPT fetch: The GPT walk failed because a readof a GPT entry returned an External abort.GPC faults are reported as one of the following exception types: Data Abort exception Instruction Abort exception GPC exception GPC exception is a new synchronous exception type introduced by RME. GPFs on accesses to the trace and Statistical Profiling Extension (SPE)buffers are handled differently. For more information, see Self-hosted traceand SPE. 如果一次访问未通过颗粒保护检查（Granule Protection Checks），则会报告一个异常。这些异常统称为颗粒保护检查（GPC）异常。 GPC 异常的类型如下： 颗粒保护异常（Granule Protection Fault, GPF）：GPT 遍历成功完成，但该访问不被允许。 GPT 遍历异常（GPT Walk Fault）：由于 GPT 表项无效，GPT 遍历未能完成。 GPT 地址大小异常（GPT address size fault）：由于试图访问超出配置范围的地址，GPT 遍历失败。 GPT 读取同步外部中止（Synchronous External abort on GPT fetch）：GPT 遍历时读取 GPT 表项返回了外部中止异常。 GPC 异常会以以下异常类型之一报告： 数据中止异常（Data Abort exception） 指令中止异常（Instruction Abort exception） GPC 异常（GPC exception） GPC 异常是一种由 RME 新增的同步异常类型。 对 trace 和统计分析扩展（Statistical Profiling Extension，SPE）缓冲区的访问发生 GPF 时，会有不同的处理方式。详情请参见“自托管 trace 和 SPE”相关内容。Granule Protection FaultsA Granule Protection Fault (GPF) is generated when the PA space returned by theVMSA VA to PA translation does not match the PA space that the granule isassigned to in the GPTs.For example, software attempts to access RLP:0x8000, but PA 0x8000 is allocatedto the Nonsecure PA space.GPFs can be reported as GPC exceptions, Instruction Abort exceptions, or DataAbort exceptions as summarized in the following table: 当 VMSA 的虚拟地址（VA）到物理地址（PA）转换所返回的物理地址空间（PA space）与GPT 表中该颗粒分配的物理地址空间不匹配时，会产生颗粒保护异常（GranuleProtection Fault, GPF）。 例如，软件试图访问 RLP:0x8000，但物理地址 0x8000 实际分配给了非安全物理地址空间（Nonsecure PA space）。 GPF 可以被报告为 GPC 异常、指令中止异常（Instruction Abort exception）或数据中止异常（Data Abort exception），具体总结如下表所示：The exception syndrome that is provided for Instruction Aborts and Data Aborts has beenextended to give information on GPFs.A GPT walk can fail to complete, and one of the following GPC faults is reported: GPT address size fault GPT walk fault Synchronous External abort on GPTE fetchArm expects these faults to be rare in system set up correctly. These faultswould typically represent an Exception level 3 software error or a loss ofconsistency, which is likely to be fatal.These fault types are always reported as GPC exceptions, and taken to Exceptionlevel 3. 指令中止异常（Instruction Abort）和数据中止异常（Data Abort）所提供的异常信息（syndrome）已扩展，以便能够提供有关颗粒保护异常（GPF）的信息。 GPT 遍历可能无法完成，此时会报告以下 GPC 异常之一： GPT 地址大小异常（GPT address size fault） GPT 遍历异常（GPT walk fault） GPT 表项读取同步外部中止（Synchronous External abort on GPTE fetch） Arm 预计这些异常在系统正确配置时非常罕见。出现这些异常通常意味着异常级别 3（Exception level 3）软件出错或一致性丢失，这通常是致命的。 这些异常类型始终作为 GPC 异常报告，并会被传递到异常级别 3。Transitioning a granule between physical address spacesA granule is moved between PA spaces by updating the GPTs. The architecturespecification includes the required sequences that software must follow. 通过更新 GPT 表，可以将一个颗粒（granule）在不同物理地址空间（PA space）之间迁移。架构规范中包含了软件必须遵循的相关操作流程。Impact on cachesAs part of transitioning a block or granule, Exception level 3 software ensuresthat copies of the location held in caches with the PA space are removed.To remove the location copies, RME introduces Point of Physical Aliasing (PoPA),a new conceptual point in the cache hierarchy. The PoPA is the point beyondwhich an access using any PA space uses the same copy in a cache or memory. Thefollowing diagram shows an example of PoPA: 在迁移一个块或颗粒的过程中，异常级别 3（Exception level 3）的软件需要确保与物理地址空间（PA space）相关的缓存中的该位置副本被移除。 为了移除这些副本，RME 引入了物理别名点（Point of Physical Aliasing，PoPA），这是缓存层次结构中的一个新概念点。PoPA 是指在该点之后，使用任何物理地址空间进行访问时，都会使用缓存或内存中的同一个副本。下图展示了 PoPA 的一个示例：As part of the transition flow, software cleans and invalidates the caches toPoPA. This ensures that no copies of the granule are held in caches with the oldPA space. Not all systems include caches beyond the PoPA. 在迁移流程中，软件会对缓存进行清理和失效操作，直到 PoPA（物理别名点）。这样可以确保缓存中不再保留属于旧物理地址空间（PA space）的颗粒副本。 并不是所有系统在 PoPA 之后都包含缓存。 这里的 PoPA 表示, 在该点后的cache层级中, 这些cache不再保存PA space 相关的信息. (例如 PoPA卡在L2, L3 中间，L3 不再保存 PA space相关字段), 这样有什么好处呢? 如果因为 move PA space 而 flush cache, flush 到 PoPA就行。 Impact on TLBsTLBs hold copies of recently used translations and can include the result ofGranule Protection Checks. When a granule is moved between PA spaces, softwaremust issue invalidate operations to remove any cached copies of the GPTEinformation. RME introduces new TLBI instructions for this purpose.The Arm architecture does not specify how TLBs are structured, and the structurecan vary between implementations. Possible approaches include one of or acombination the following: Caching the result of different stages separately Caching the result of multiple stages as a single entrySoftware does not need to be aware of which approach is implemented. TLB（地址转换后备缓冲区）会保存最近使用过的地址转换结果，并且可能包含颗粒保护检查（Granule Protection Checks）的结果。当一个颗粒在不同物理地址空间（PAspace）之间迁移时，软件必须发出失效操作，以清除缓存中的 GPTE 信息副本。为此，RME 引入了新的 TLBI 指令。 Arm 架构并未规定 TLB 的具体结构，不同的实现可能会有所不同。可能的实现方式包括以下一种或多种组合： 分阶段分别缓存转换结果 将多个阶段的结果作为单个表项进行缓存 软件无需关心具体采用了哪种实现方式。ElisionIn Granule Protection Checks, we saw an example sequence showing the GranuleProtection Checks during a table walk. To minimize the performance penalty ofthe additional checks, RME supports a mode in which some checks are elided.When elision is enabled (GPCCR_EL3.GPCP == 1), the MMU might not carry outGranule Protection Checks for reads of stage 2 Table Descriptors. All otheraccesses, including stage 1 descriptor fetches and fetches of stage 2 Block andPage Descriptors, occur as normal.The analysis of whether the elision is acceptable to a security model includes acombination of: The use and style of memory encryption The low probability of ciphertext being a valid translation table descriptor The correct implementation of physical address space checks for read-sensitivelocations 在颗粒保护检查（Granule Protection Checks）中，我们已经看到在表遍历过程中进行颗粒保护检查的示例流程。为了减少额外检查带来的性能损耗，RME 支持一种模式，可以省略部分检查。 当启用省略功能（即 GPCCR_EL3.GPCP == 1）时，MMU 可能不会对读取阶段 2 表描述符（stage 2 Table Descriptors）执行颗粒保护检查。其他访问，包括阶段 1 描述符的读取以及阶段 2 块描述符和页描述符的读取，仍会正常进行检查。 对于省略机制是否可以被安全模型接受，分析通常包括以下几个方面的综合考虑： 内存加密的使用方式和类型 密文碰巧成为有效转换表描述符的概率极低 对于读取敏感位置，物理地址空间检查的正确实现 " }, { "title": "[arm] RME(spec)", "url": "/posts/RME-spec/", "categories": "coco, RME", "tags": "RME", "date": "2025-08-26 14:55:00 +0800", "snippet": "B2.3.4 RME Device AssignmentThe term Assignable Device Interface refers to a portion of a device that can beindependently assigned to software executing in one of the Security states. Anassignable ...", "content": "B2.3.4 RME Device AssignmentThe term Assignable Device Interface refers to a portion of a device that can beindependently assigned to software executing in one of the Security states. Anassignable device interface can act as an independent requester and have its ownprivate memory-mapped resources.RME Device Assignment (RME-DA) is an RME system feature that enables the secureassignment of assignable device interfaces to the Realm Security state.There is the following related terminology:PCIe refers to devices that comply with the TEE Device Interface SecurityProtocol (TDISP [4]) as TEE-I/O capable devices. This specification refers tothem as TDISP-compliant devices.PCIe uses TEE Device Interface (TDI) to refer to an assignable device interfaceof TDISP-compliant devices. “可分配设备接口”（Assignable Device Interface）指的是设备中可以独立分配给在某个安全状态下运行的软件的一部分。可分配设备接口可以作为独立的请求方，并拥有其自己的私有内存映射资源。RME 设备分配（RME-DA）是 RME 系统的一项功能，它能够将可分配设备接口安全地分配给 Realm 安全状态。 相关术语如下： PCIe 将符合 TEE 设备接口安全协议（TDISP [4]）的设备称为 TEE-I/O capabledevices。本规范将其称为 TDISP-compliant devices。 PCIe 使用 TEE 设备接口（TDI）来指代 TDISP 兼容设备的可分配设备接口。TDISP defines both: A TEE Security Manager (TSM) which is a logical entity at the host thatenforces security policies. A Device Security Manager (DSM) which is a logical entity in the device thatenforces security policies on the device.TSM functionality in RME-DA is implemented within RMSD.SMMU for RME-DA [3] defines SMMU requirements for supporting the assignment ofTDIs to software executing in the Realm Security state.The RME system architecture defines memory system and PCIe Root Portrequirements for supporting the assignment of PCIe TDIs to the Realm Securitystate. TDISP 定义了以下两个实体： TEE 安全管理器（TSM），它是主机上的一个逻辑实体，用于强制执行安全策略。 设备安全管理器（DSM），它是设备中的一个逻辑实体，用于在设备上强制执行安全策略。 在 RME-DA 中，TSM 的功能由 RMSD 实现。 SMMU for RME-DA [3] 定义了 SMMU 支持将 TDI 分配给在 Realm 安全状态下运行的软件的相关要求。 RME 系统架构定义了内存系统和 PCIe Root Port将 PCIe TDI 分配给 Realm 安全状态的相关要求。B3.2.6 PCIe Root Port support for TDISPThis section defines requirements for an RME-DA Root Port (RP), in order tosecurely associate TDIs of TDISP-compliant devices with EL1 Realms, incompliance with PCIe TDISP [4].This specification uses the term outgoing for traffic that enters the RP fromits host interface and targets its PCIe hierarchy domain.This specification uses the term incoming for traffic that enters the RP fromits PCIe hierarchy domain.An RME-DA RP sets the TEE-IO Supported bit in the Device Capabilities Register.B3.2.6.1 Integrity and Data Encryption (IDE) supportThis section defines requirements for PCIe IDE [4] support in an RME-DA RP.An RME-DA RP supports all the following IDE features: At least one Selective IDE Stream. NUM_SEL_STR denotes the number of Selective IDE Streams supported. At least three Address Association registers per supported Selective IDEStream. The TEE-Limited Stream IDE capability.RME-DA requires Selective IDE Stream support for setting up IDE withTDISP-compliant devices, that can be located behind a PCIe switch or directlyattached to the RME-DA RP. This enables Requester ID (RID) checks to beperformed at the RP for all TDISP-compliant devices. Having three AddressAssociation registers per Selective IDE Stream allows TDISP-compliant devices toimplement the maximal number of BARs supported by PCIe for a TDI.An IDE stream is identified by an IDE Stream ID, and can be in IDE Insecurestate or IDE Secure state." }, { "title": "[arm] virtCCA", "url": "/posts/virtcca-paper/", "categories": "coco, virtCCA", "tags": "virtCCA", "date": "2025-08-25 14:55:00 +0800", "snippet": "ABSTRACTARM recently introduced the Confidential Compute Architecture (CCA) as part ofthe upcoming ARMv9-A architecture. CCA enables the support of confidentialvirtual machines (cVMs) within a sepa...", "content": "ABSTRACTARM recently introduced the Confidential Compute Architecture (CCA) as part ofthe upcoming ARMv9-A architecture. CCA enables the support of confidentialvirtual machines (cVMs) within a separate world called the Realm world,providing protection from the untrusted normal world. While CCA offers apromising future for confidential computing, the widespread availability of CCAhardware is not expected in the near future, according to ARM’s roadmap. Toaddress this gap, we present virtCCA, an architecture that facilitatesvirtualized CCA using TrustZone, a mature hardware feature available on existingARM platforms. Notably, virtCCA can be implemented on platforms equipped withthe Secure EL2 (S-EL2) extension available from ARMv8.4 onwards, as well as onearlier platforms that lack S-EL2 support. virtCCA is fully compatible with theCCA specifications at the API level. We have developed the entire CCA softwareand firmware stack on top of virtCCA, including the enhancements to the normalworld’s KVM to support cVMs, and the TrustZone Management Monitor (TMM) thatenforces isolation among cVMs and provides cVM lifecycle management. We haveimplemented virtCCA on real ARM servers, with and without S-EL2 support. Ourevaluation, conducted on micro-benchmarks and macro-benchmarks, demonstratesthat the overhead of running cVMs is acceptable compared to running normal-worldVMs. Specifically, in a set of real-world workloads, the overhead ofvirtCCA-SEL2 is less than 29.5% for I/O intensive workloads, while virtCCA-EL3outperforms the baseline in most cases.ACM Reference Format:Xiangyi Xu, Wenhao Wang B, Yongzheng Wu, Chenyu Wang, Huifeng Zhu, Haocheng Ma,Zhennan Min, Zixuan Pang, Rui Hou, and Yier Jin B. virtCCA: Virtualized Arm Confidential Compute Architecture with TrustZone.In Proceedings of ACM Conference (Conference’17). ACM, New York, NY, USA, 12pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn1 INTRODUCTIONConfidential computing is rapidly emerging as an indispensable technology in therealm of cloud computing. Its primary objective is to safeguard the sensitiveinformation of tenants from potential risks posed by untrustworthy or improperlyconfigured cloud service providers (CSPs). This paradigm shift towardsconfidential computing not only enhances data privacy and security but alsoinstills a greater sense of trust in cloud-based services. As a result, it isincreasingly becoming an integral part of modern cloud architectures.Recognizing the significance of confidential computing, leading chip companieshave stepped forward to offer support for trusted execution environments (TEEs)within their product offerings. For instance, Intel’s SGX [3] and TDX [19], AMD’s SEV [34], and IBM’s PEF [18] are all dedicated features that enables thecreation of isolated regions (i.e., enclaves) and confidential virtual machines(i.e., cVMs) exclusively used by tenantsARM processors have gained widespread popularity in mobile devices likesmartphones due to their energy efficiency and low power consumption. Notably,ARM was one of the pioneers in supporting TEEs, i.e., TrustZone, to ensure theprotection of sensitive data like passwords and fingerprints. TrustZone dividesthe system into two distinct worlds: the normal world and the secure world.While the normal world runs the feature-rich operating system (OS), the secureworld handles critical operations requiring enhanced security measures. To caterto the specific requirements of the secure world, customized trusted OSes likeOPTEE [30] and iTrustee [2] have been developed. These tailored trusted OSesensure the provision of secure and reliable services within the secure world.However, unlike AMD SEV, TrustZone does not fully support the cloud computingscenario, particularly in supporting a full-featured OS like Linux runninginside cVMs, while the lifecycle of cVMs is managed by the host hypervisor (e.g.,KVM).In recent years, ARM has made significant strides in the cloud computing market.To meet the need for confidential cloud computing, ARM has announced theConfidential Computing Architecture (CCA), a series of hardware and softwarearchitecture innovations available as part of the ARMv9-A architecture. CCAsupports the dynamic creation and management of Realms (ARM’s terminology forcVMs), opening confidential computing to all developers and various workloads.Motivations. As ARM’s latest innovation for confidential computing, CCApoints to a convincing future for confidential computing. However, CCAspecifications have still been under active evolution recently. Moreover, itusually takes years before the commodity chip is shipped, even after thehardware specifications become stable. It is expected that commercial productswith full CCA support will not be widely available in the near future. On theother hand, we spot that the CCA software supports, including the cVM guestkernel, KVM, and Realm Management Monitor (RMM), are evolving at a rapid pace [5,7, 8]. Unfortunately, these tremendous efforts and innovations are unlikely tocome in handy until real CCA hardware platforms are available in the market." }, { "title": "[mm:la57] switch to la57", "url": "/posts/la57-switch_mode/", "categories": "mm, la57", "tags": "mm, la57", "date": "2025-08-22 16:57:00 +0800", "snippet": "overview在intel sdm 4.1.2 Paging-Mode Enabling, 中提到: CR4.PAE and CR4.LA57 cannot be modified while either 4-level paging or 5-levelpaging is in use (when CR0.PG = 1 and IA32_EFEe.LME = 1 ). Attempt...", "content": "overview在intel sdm 4.1.2 Paging-Mode Enabling, 中提到: CR4.PAE and CR4.LA57 cannot be modified while either 4-level paging or 5-levelpaging is in use (when CR0.PG = 1 and IA32_EFEe.LME = 1 ). Attempts to do sousing MOV to CR4 cause a general-protection exception (#GP(0)). 不能在 long mode分页时开启修改 CR4.LA57以及CR4.PAE, 否则会触发 #GP 而kernel 又支持配置no5lvl, 来决定是否使能la57 feature: set no5lvl : 4-level page table not set no5lvl : 5-level page table那我们来看下, 内核是如何根据该参数切换的。内核启动 – startup64compressed/vmlinux.lds.S 会设置compressed 程序 efi的入口(ENTRY())，以及架构(OUTPUT_ARCH()), 引导器会根据该配置, 在进入ENTRY()之前，进入相应的cpu mode. 引导器可能是startup_32, 页可能是64bit bootloader#ifdef CONFIG_X86_64OUTPUT_ARCH(i386:x86-64)ENTRY(startup_64)#elseOUTPUT_ARCH(i386)ENTRY(startup_32)#endif例如，如果配置(i386:x86-64), cpu 就会进入long mode, 调用startup_64, 之前会通过esi传入boot_params参数, 而在startup_64函数中，则会调用paging_prepare()为切换CR4.LA57, 做好跳板:paging_preparestartup_64 调用paging_prepare()代码如下SYM_CODE_START(startup_64) ... /* * paging_prepare() sets up the trampoline and checks if we need to * enable 5-level paging. * * paging_prepare() returns a two-quadword structure which lands * into RDX:RAX: * - Address of the trampoline is returned in RAX. * - Non zero RDX means trampoline needs to enable 5-level * paging. * * RSI holds real mode data and needs to be preserved across * this function call. */ pushq %rsi movq %rsi, %rdi /* real mode address */ call paging_prepare popq %rsi而paging_prepare(), 需要做:准备一个32-bit code(兼容模式) 代码段用来切换 la57为什么要这样做呢? 原因是x86在进行paging mode切换时, 需要disable paging: ===Intel SDM 4.1.2 Paging-Mode Enabling === Software cannot transition directly between 4-level paging (or 5-level paging)and any of other paging mode. It must first disable paging (by clearing CR0.PGwith MOV to CR0), then set CR4.PAE, IA32_EFER.LME, and CR4.LA57 to the desiredvalues (with MOV to CR4 and WRMSR), and then re-enable paging (by settingCR0.PG with MOV to CR0). As noted earlier, an attempt to modify CR4.PAE,IA32_EFER.LME, or CR.LA57 while 4-level paging or 5-level paging is enabledcauses a general-protection exception (#GP(0)). 软件不能在四级页表（或五级页表）与其他分页模式之间直接切换。必须先通过将CR0.PG 清零（使用 MOV 指令写入 CR0）来禁用分页，然后再通过设置 CR4.PAE、IA32_EFER.LME 和 CR4.LA57 为所需的值（分别使用 MOV 指令写入 CR4 和 WRMSR 指令写入 MSR），最后重新启用分页（通过设置 CR0.PG 位）。如前所述，如果在已启用四级页表或五级页表的情况下试图修改 CR4.PAE、IA32_EFER.LME 或 CR4.LA57，会导致一般保护异常（#GP(0)）。 但是disable paging 时, 会退出IA-32e mode ===Intel SDM 10.8.5.4 Switching Out of IA-32e Mode Operation === To return from IA-32e mode to paged-protected mode operation operating systemsmust use the following sequence: Switch to compatibility mode. Deactivate IA-32e mode by clearing CR0.PG = 0. This causes the processor toset IA32_EFER.LMA = 0. The MOV CR0 instruction used to disable paging andsubsequent instructions must be located in an identity-mapped page. 然而, 在切回IA-32e mode时(enable paging)，会检查CS L-bit 是否设置，如果设置，就报告 #GP. === Intel SDM MOV—Move to/from Control Registers === Protected Mode Exceptions #GP(0) If an attempt is made to activate IA-32e mode and either the current CS hasthe L-bit set or the TR references a 16-bit TSS.所以, 我们来总结下，IA32e mode 的4-level paging 和 5-level paging切换需要做哪些事情: switching to compatibility mode disable paging set/clear CR0.LA57 enable paging好，我们来看下paging_prepare()是否做了这样的事情:paging_prepare## 满足如下条件, 使能la57 feature## 1. 编译选项## 2. 没有配置no5lvl## 3. cpuid满足(有该leaf, 并且有该feature)|-&gt; if IS_ENABLED(CONFIG_X86_5LEVEL) &amp;&amp; !cmdline_find_option_bool(\"no5lvl\") &amp;&amp; native_cpuid_eax(0) &gt;= 7 &amp;&amp; (native_cpuid_ecx(7) &amp; (1 &lt;&lt; (X86_FEATURE_LA57 &amp; 31))) \\-&gt; paging_config.l5_required = 1## 找一个跳板(从e820中, 先略过)|-&gt; paging_config.trampoline_start = find_trampoline_placement();## 将跳板代码copy进准备好的内存区域|-&gt; trampoline_32bit = (unsigned long *)paging_config.trampoline_start;|-&gt; memset(trampoline_32bit, 0, TRAMPOLINE_32BIT_SIZE);|-&gt; memcpy(trampoline_32bit + TRAMPOLINE_32BIT_CODE_OFFSET / sizeof(unsigned long), &amp;trampoline_32bit_src, TRAMPOLINE_32BIT_CODE_SIZE);# 好, 先在跳转完了，那接下来该准备页表了## 这里说明不需要切换paging mode|-&gt; if paging_config.l5_required == !!(native_read_cr4() &amp; X86_CR4_LA57) |-&gt; goto out## 需要切换到 5-level paging|-&gt; if paging_config.l5_required \\-&gt; trampoline_32bit[TRAMPOLINE_32BIT_PGTABLE_OFFSET] = \\ __native_read_cr3() | _PAGE_TABLE_NOENC;--&gt; else: \\-&gt; src = *(unsigned long *)__native_read_cr3() &amp; PAGE_MASK; \\-&gt; memcpy(trampoline_32bit + TRAMPOLINE_32BIT_PGTABLE_OFFSET / sizeof(unsigned long), (void *)src, PAGE_SIZE);在4-level 转换到5-level时，需要新增一级paging table, 该paging table只需要初始化index0 的 entry.在5-level转换到4-level时，理论上不需要新增paging table，而需要删除一级pagingtable，这里的做法时，获取原来的 PML5的第一个entry, 找到 PML4，然后copy 该PML4到trampoline paging.enter trampoline32SYM_CODE_START(startup_64) ... /* Save the trampoline address in RCX */ movq %rax, %rcx /* * Load the address of trampoline_return() into RDI. * It will be used by the trampoline to return to the main code. */ leaq trampoline_return(%rip), %rdi /* Switch to compatibility mode (CS.L = 0 CS.D = 1) via far return */ pushq $__KERNEL32_CS leaq TRAMPOLINE_32BIT_CODE_OFFSET(%rax), %rax pushq %rax lretqtrampoline_return: /* Restore the stack, the 32-bit trampoline uses its own stack */ leaq rva(boot_stack_end)(%rbx), %rsp参考链接 x86/boot/compressed: Enable 5-level paging during decompression stage 34bbb0009f3b7a5eef1ab34f14e5dbf7b8fc389c intel spec: 2.2 MODES OF OPERATION Figure 2-3. Transitions Among the Processor’s Operating Modes " }, { "title": "[js] js common webset", "url": "/posts/js-comm-webset/", "categories": "js", "tags": "js", "date": "2025-08-22 09:35:00 +0800", "snippet": "查找图标Font Awesome调色coolors", "content": "查找图标Font Awesome调色coolors" }, { "title": "Smmu Operation", "url": "/posts/smmu-operation/", "categories": "", "tags": "", "date": "2025-08-15 00:00:00 +0800", "snippet": "3.1 Software interfaceThe SMMU has three interfaces that software uses: Memory-based data structures to map devices to translation tables which areused to translate client device addresses. ...", "content": "3.1 Software interfaceThe SMMU has three interfaces that software uses: Memory-based data structures to map devices to translation tables which areused to translate client device addresses. Memory-based circular buffer queues. These are a Command queue for commandsto the SMMU, an Event queue for event/fault reports from the SMMU, and a PRIqueue for receipt of PCIe page requests. Note: The PRI queue is only present on SMMUs supporting PRI services. Thisadditional queue allows processing of PRI requests from devices separate fromevent or fault reports. A set of registers, for each supported Security state, for discovery andSMMU-global configuration. SMMU（系统内存管理单元）为软件提供了三种接口： 基于内存的数据结构，用于将设备映射到转换表，这些转换表用于转换客户端设备的地址。 基于内存的循环缓冲队列。这些队列包括用于向 SMMU 发送命令的命令队列、用于SMMU 向软件报告事件或故障的事件队列，以及用于接收 PCIe 页请求的 PRI 队列。 注意：PRI 队列仅在支持 PRI 服务的 SMMU 上存在。这个额外的队列允许将设备发起的 PRI 请求与事件或故障报告分开处理。 一组寄存器，每个受支持的安全状态都有一套 ，用于设备发现和 SMMU 全局配置。 The registers indicate the base addresses of the structures and queues, providefeature detection and identification registers and a global controlregister to enable queue processing and translation of traffic. WhenSecure state is supported, an additional register set exists to allowSecure software to maintain Secure device structures, issue commands on a secondSecure Command queue and read Secure events from a Secure Event queue**.In virtualization scenarios allowing stage 1 translation, a guest OS ispresented with the same programming interface and therefore believes it is incontrol of a real SMMU (albeit stage 1-only) with the same format of Command,Event, and optionally PRI, queues, and in-memory data structures.Certain fields in architected SMMU registers and structures are marked asIMPLEMENTATION DEFINED. The content of these fields is specific to the SMMUimplementation, but implementers must not use these fields in such a way that ageneric SMMUv3 driver becomes unusable. Unless a driver has extended knowledgeof particular IMPLEMENTATION DEFINED fields or features, the driver must treatall such fields as Reserved and set them to 0.An implementation only uses IMPLEMENTATION DEFINED fields to enable extendedfunctionality or features, and remains compatible with generic driver softwareby maintaining architected behavior when these fields are set to 0. 这些寄存器指示结构体和队列的基地址，提供功能检测和标识寄存器，以及用于使能队列处理和流量地址转换的全局控制寄存器。如果支持安全状态，还会有额外的一组寄存器，允许安全软件维护安全设备结构，通过第二个安全命令队列下发命令，并从安全事件队列读取安全事件。 在允许一级转换（stage 1 translation）的虚拟化场景下，客户操作系统（guest OS）会被呈现出同样的编程接口，因此它认为自己控制着一个真实的 SMMU（虽然只支持一级转换），其命令队列、事件队列（以及可选的 PRI 队列）和内存数据结构的格式都一致。 NOTE stage 1是指 IOVA-&gt;PA, 这里相当于满足guest的IOVA-&gt;PA的功能，所以，需要让guest感知IOMMU的存在，这里不太清楚SMMU这边是如何做的虚拟化，hardware or software??另外除了stage 1，还需要stage 2(GPA-&gt;HPA). 在架构定义的 SMMU 寄存器和结构体中，某些字段被标记为“实现自定义”（IMPLEMENTATION DEFINED）。这些字段的内容由具体的 SMMU 实现决定，但实现者不得以任何方式使用这些字段，导致通用的 SMMUv3 驱动无法正常工作。除非驱动对某些实现自定义字段或特性有扩展了解，否则驱动必须将所有此类字段视为保留（Reserved）并设为 0。 实现者仅在需要扩展功能或特性时才使用这些实现自定义字段，并且通过在这些字段设为0 时保持架构定义的行为，从而确保与通用驱动软件的兼容性。3.2 Stream numberingAn incoming transaction has an address, size, and attributes such as read/write,Secure/Non-secure, Shareability, Cacheability. If more than one client deviceuses the SMMU traffic must also have a sideband StreamID so the sources can bedifferentiated. How a StreamID is constructed and carried through the system isIMPLEMENTATION DEFINED. Logically, a StreamID corresponds to a device thatinitiated a transaction.Note: The mapping of a physical device to StreamID must be described to systemsoftware.Arm recommends that StreamID be a dense namespace starting at 0. The StreamIDnamespace is per-SMMU. Devices assigned the same StreamID but behind differentSMMUs are seen to be different sources. A device might emit traffic with morethan one StreamID, representing data streams differentiated by device-specificstate. 一次进入 SMMU 的事务包含地址、大小以及如读/写、安全/非安全、可共享性、可缓存性等属性。如果有多个客户端设备使用 SMMU，事务还必须带有一个边带的 StreamID，以便区分来源设备。StreamID 的构建方式和在系统中的传递方式是实现自定义的（IMPLEMENTATION DEFINED）。从逻辑上讲，StreamID 就对应于发起该事务的设备。 注意：物理设备和 StreamID 的映射必须告知系统软件。 Arm 推荐 StreamID 命名空间应从 0 开始，且尽可能密集分配。StreamID 的命名空间是每个 SMMU 独立的。即使不同 SMMU 下的设备分配了相同的 StreamID，也会被视为不同的来源。一个设备可以发出带有多个不同 StreamID 的流量，用于表示设备特定状态下区分的数据流。 NOTE 特定状态 是指/包括 secure state 么?StreamID is of IMPLEMENTATION DEFINED size, between 0 bits and 32 bits.The StreamID is used to select a Stream Table Entry (STE) in a Stream table,which contains per-device configuration. The maximum size of in-memoryconfiguration structures relates to the maximum StreamID span (see 3.3 Datastructures and translation procedure below), with a maximum of 2StreamIDSizeentries in the Stream table.Another property, SubstreamID, might optionally be provided to an SMMUimplementing stage 1 translation. The SubstreamID is of IMPLEMENTATION DEFINEDsize, between 0 bits and 20 bits, and differentiates streams of trafficoriginating from the same logical block to associate different applicationaddress translations to each.Note: An example would be a compute accelerator with 8 contexts that might eachmap to a different user process, but where the single device has commonconfiguration meaning it must be assigned to a VM whole.Note: The SubstreamID is equivalent to a PCIe PASID. Because the concept can beapplied to non-PCIe systems, it has been given a more generic name in the SMMU.The maximum size of SubstreamID, 20 bits, matches the maximum size of a PCIePASID. StreamID 的位宽是实现自定义的，可以在 0 到 32 位之间。 StreamID 用于在流表（Stream Table）中选择对应的流表项（STE），每个 STE 包含针对每个设备的配置信息。内存中配置结构的最大尺寸取决于 StreamID 的最大跨度（详见下文 3.3 数据结构和转换流程），流表最大可有 2^StreamIDSize 个条目。 另一个属性 SubstreamID（子流ID）可选地提供给实现了一级转换（stage 1translation）的 SMMU。SubstreamID 的位宽也是实现自定义的，范围为 0 到 20 位，用于区分同一逻辑块发出的不同数据流，从而为每个流关联不同的应用地址转换。 注意：例如，一个计算加速器有 8 个上下文，每个上下文可能映射到不同的用户进程，但该设备的通用配置意味着它必须整体分配给一个虚拟机。 注意：SubstreamID 等同于 PCIe 的 PASID。由于该概念可应用于非 PCIe 系统，因此在SMMU 中采用了更通用的命名。SubstreamID 的最大位宽为 20 位，与 PCIe PASID 的最大位宽一致。The incoming transaction flags whether a SubstreamID is supplied and this mightdiffer on a per-transaction basis. Both of these properties and sizes arediscoverable through the SMMU_IDR1 register. See section 16.4 System integrationfor recommendations on StreamID and SubstreamID sizing.The StreamID is the key that identifies all configuration for a transaction. AStreamID is configured to bypass or be subject to translation and suchconfiguration determines which stage 1 or stage 2 translation to apply. TheSubstreamID provides a modifier that selects between a set of stage 1translations indicated by the StreamID but has no effect on the stage 2translation which is selected by the StreamID only.A stage 2-only implementation does not take a SubstreamID input. Animplementation with stage 1 is not required to support substreams, therefore isnot required to take a SubstreamID input.The SMMU optionally supports Secure state and, if supported, the StreamID inputto the SMMU is qualified by a SEC_SID flag that determines whether the inputStreamID value refers to the Secure or Non-secure StreamID namespace. ANon-secure StreamID identifies an STE within the Non-secure Stream table and aSecure StreamID identifies an STE within the Secure Stream table. In thisspecification, the term StreamID implicitly refers to the StreamID disambiguatedby SEC_SID (if present) and does not refer solely to a literal StreamID inputvalue (which would be associated with two STEs when Secure state is supported)unless explicitly stated otherwise. See section 3.10.2 Support for Secure state.Arm expects that, for PCI, StreamID is generated from the PCI RequesterID sothat StreamID[15:0] == RequesterID[15:0]. When more than one PCIe hierarchy ishosted by one SMMU, Arm recommends that the 16-bit RequesterID namespaces arearranged into a larger StreamID namespace by using upper bits of StreamID todifferentiate the contiguous RequesterID namespaces, so that StreamID[N:16]indicates which Root Complex (PCIe domain/segment) is the source of the streamsource. In PCIe systems, the SubstreamID is intended to be directly providedfrom the PASID [1] in a one to one fashion.Therefore, for SMMU implementations intended for use with PCI clients, supportedStreamID size must be at least 16 bits. 每个进入的事务都会标记是否带有 SubstreamID，这一情况可能针对每个事务有所不同。这两个属性及其位宽都可以通过 SMMU_IDR1 寄存器进行查询。有关 StreamID 和SubstreamID 位宽的建议，见 16.4 节“系统集成”。 StreamID 是标识所有事务配置信息的关键。StreamID 可以被配置为绕过或参与转换，这决定了将应用哪一级（stage 1 或 stage 2）的地址转换。SubstreamID 作为修饰符，用于在由 StreamID 指定的一组 stage 1 转换中进行选择 ，但对 stage 2 转换无影响，stage 2 转换只由 StreamID 决定。 仅支持 stage 2 的实现不会接收 SubstreamID 输入。支持 stage 1 的实现不一定要支持子流，因此也不一定要接收 SubstreamID 输入。 SMMU 可选支持安全状态（Secure state），如果支持，则输入给 SMMU 的 StreamID 还要加上 SEC_SID 标志，以确定输入的 StreamID 属于安全还是非安全命名空间。非安全 StreamID 会在非安全流表中查找 STE，安全 StreamID 会在安全流表中查找STE 。本规范中，术语 StreamID 默认指的是经过 SEC_SID 区分后的 StreamID（如有），而不是单纯的 StreamID 输入值（在支持安全状态时，单一 StreamID 输入值会关联两个 STE，除非特别说明）。详见 3.10.2 节“安全状态支持”。 Arm 期望在 PCI 系统中，StreamID 由 PCI RequesterID 生成， 即 StreamID[15:0]== RequesterID[15:0] 。当一个 SMMU 下有多个 PCIe 层级时，建议将 16 位RequesterID 命名空间通过 StreamID 的高位区分，形成更大的 StreamID 命名空间，即用 StreamID[N:16] 区分不同的 Root Complex（PCIe 域/段）作为流的来源。在 PCIe系统中，SubstreamID 直接由 PASID 提供 ，一一对应。 NOTE StreamID[15: 0] == RequesterID[15:0] StreamID[N: 16] 区分不同的 Root Complex (Pcie domain/segment) SubstreamID &lt;— PASID 因此，对于面向 PCI 客户端的 SMMU 实现，支持的 StreamID 位宽至少应为 16 位。3.3 Data structures and translation procedureThe SMMU uses a set of data structures in memory to locate translation data.Registers hold the base addresses of the initial root structure, theStream table. An STE contains stage 2 translation table base pointers, andalso locates stage 1 configuration structures, which contain translation tablebase pointers. A Context Descriptor (CD) represents stage 1 translation ,and a Stream Table Entry represents stage 2 translation .Therefore, there are two distinct groups of structures used by the SMMU: Configuration structures, which map from the StreamID of a transaction (adevice originator identifier) to the translation table base pointers,configuration, and context under which the translation tables are accessed. Translation table structures that are used to perform the VA to IPA and IPA toPA translation of addresses for stage 1 and stage 2, respectively. SMMU 在内存中使用一组数据结构来定位转换数据。寄存器保存了初始根结构（即流表，Stream table）的基地址。一个流表项（STE）包含了二级转换表（stage 2 translationtable）的基地址指针，同时还定位了一级配置结构（stage 1 configurationstructures），这些结构包含了转换表的基地址指针。上下文描述符（ContextDescriptor, CD）表示一级转换（stage 1 translation），而流表项（Stream TableEntry, STE）表示二级转换（stage 2 translation）。 REGISTER\\-&gt; Stream Stable \\- STE \\- STE \\-&gt; stage 2 translation table \\-&gt; CD \\-&gt; 待补充!! 因此，SMMU 使用了两类不同的数据结构： 配置结构：用于根据事务的 StreamID（设备发起者标识符）映射到转换表的基地址指针、配置和上下文，从而访问转换表。 转换表结构：用于执行地址的转换，即一级转换（VA 到 IPA）和二级转换（IPA 到PA）。 The procedure for translation of an incoming transaction is to first locateconfiguration appropriate for that transaction, identified by its StreamID and,optionally, SubstreamID, and then to use that configuration to locatetranslations for the address used.The first step in dealing with an incoming transaction is to locate the STE,which tells the SMMU what other configuration it requires.Conceptually, an STE describes configuration for a client device in terms ofwhether it is subject to stage 1 or stage 2 translation or both. Multipledevices can be associated with a single Virtual Machine, so multiple STEs canshare common stage 2 translation tables. Similarly, multiple devices (strictly,streams) might share common stage 1 configuration, therefore multiple STEs couldshare common CDs. 对一个传入事务进行地址转换的过程，首先是根据该事务的 StreamID（以及可选的SubstreamID）定位到适合该事务的配置信息 , 然后使用这些配置信息去查找用于地址转换的数据结构。 处理一个传入事务的第一步，是定位其对应的流表项（STE），STE 告诉 SMMU 还需要哪些其他配置信息。 从概念上讲，STE 描述了某个客户端设备的配置，包括它是否需要进行一级（stage 1）或二级（stage 2）地址转换，或者两者都需要。多个设备可以关联到同一个虚拟机，因此多个 STE 可以共享同一套二级（stage 2）转换表。类似地，多个设备（严格来说是多个流）也可能共享同一套一级（stage 1）配置，因此多个 STE 也可以共享同一个上下文描述符（CD）。3.3.1 Stream table lookupThe StreamID of an incoming transaction locates an STE. Two formats of Streamtable are supported. The format is set by the Stream table base registers. Theincoming StreamID is range-checked against the programmed table size, and atransaction is terminated if its StreamID would otherwise select an entryoutside the configured Stream table extent (or outside a level 2 span). SeeSMMU_STRTAB_BASE_CFG and C_BAD_STREAMID.The StreamID of an incoming transaction might be qualified by SEC_SID, and thisdetermines which Stream table, or cached copies of that Stream table, is usedfor lookup. See section 3.10.1 StreamID Security state (SEC_SID). 传入事务的 StreamID 用于定位一个流表项（STE）。SMMU 支持两种流表（Stream table）格式，具体格式由Stream table base registers设置。传入的 StreamID 会根据已配置的表大小进行范围检查，如果 StreamID 会选中超出配置流表范围（或二级表范围）之外的条目，该事务将被终止。详见 SMMU_STRTAB_BASE_CFG 和 C_BAD_STREAMID。 传入事务的 StreamID 可能还会被 SEC_SID 修饰，这决定了查找时使用哪一张流表，或者其缓存副本。详见 3.10.1 节 StreamID 安全状态（SEC_SID）。3.3.1.1 Linear Stream TableA linear Stream table is a contiguous array of STEs, indexed from 0 by StreamID.The size is configurable as a 2ⁿ multiple of STE size up to the maximum numberof StreamID bits supported in hardware by the SMMU. The linear Stream tableformat is supported by all SMMU implementations. 线性流表（linear Stream table）是一个由 STE（流表项）组成的连续数组，通过StreamID 从 0 开始进行索引。其大小可以配置为 STE 大小的 2ⁿ 倍，最大可支持的大小由 SMMU 硬件支持的 StreamID 位数决定。所有的 SMMU 实现都支持线性流表格式。3.3.1.2 2-level Stream tableA 2-level Stream table is a structure consisting of one top-level table thatcontains descriptors that point to multiple second-level tables that containlinear arrays of STEs. The span of StreamIDs covered by the entire structure isconfigurable up to the maximum number supported by the SMMU but the second-leveltables do not have to be fully populated and might vary in size. This savesmemory and avoids the requirement of large physically-contiguous allocations forvery large StreamID spaces.The top-level table is indexed by StreamID[n:x], where n is the uppermostStreamID bit covered, and x is a configurable Split point given by SMMU_(*_)STRTAB_BASE_CFG.SPLIT. The second-level tables are indexed by up to StreamID[x -1:0], depending on the span of each table.Support for the 2-level Stream table format is discoverable using theSMMU_IDR0.ST_LEVEL field. Where 2-level Stream tables are supported, splitpoints of 6 bits, 8 bits and 10 bits can be used. Implementations support eithera linear Stream table format, or both linear and 2-level formats. 二级流表（2-level Stream table）是一种结构，由一个顶层表组成，顶层表中包含指向多个二级表的描述符，这些二级表又包含由 STE（流表项）组成的线性数组。整个结构所覆盖的 StreamID 范围是可配置的，最大可支持的范围由 SMMU 支持的 StreamID 位数决定，但二级表不需要全部填满，并且其大小可以不同。这样可以节省内存，并避免在非常大的 StreamID 空间下需要分配大块连续物理内存的问题。 顶层表通过 StreamID[n:x] 进行索引，其中 n 是所覆盖的最高位 StreamID 位，x 是由SMMU_(*_) STRTAB_BASE_CFG.SPLIT 配置的可调分割点。二级表则通过 StreamID[x-1:0]进行索引，具体取决于每个二级表所覆盖的范围。 是否支持二级流表格式可以通过 SMMU_IDR0.ST_LEVEL 字段进行探测。在支持二级流表的情况下，可以使用 6 位、8 位或 10 位的分割点（split point）。具体实现要么支持线性流表格式，要么同时支持线性和二级流表格式。SMMUs supporting more than 64 StreamIDs (6 bits of StreamID) must also supporttwo-level Stream tables.Note: Implementations supporting fewer than 64 StreamIDs might support two-levelStream tables, but doing so is not useful as all streams would fit within asingle second-level table.Note: This rule means that an implementation supports two-level tables when themaximum size of linear Stream table would be too big to fit in a 4KB page.The top-level descriptors contain a pointer to the second-level table along withthe StreamID span that the table represents. Each descriptor can also be markedas invalid.This example top-level table is depicted in Figure 3.2, where the split point isset to 8: 支持超过 64 个 StreamID（即 6 位 StreamID）的 SMMU，必须同时支持二级流表（two-level Stream tables）。 注意：支持少于 64 个 StreamID 的实现也可以支持二级流表，但这样做没有实际意义，因为所有流都可以放在一个二级表中。 注意：这条规则意味着，如果线性流表的最大尺寸太大，无法放入一个 4KB 页面时，实现就必须支持二级流表。 从这里可以看到, arm64支持 2-level stream tables 和intel支持 Scalable-mode RootEntry的目的是不同的, 虽然都是因为一个页面大小不够而扩展的. ARM: 为了扩展streamID intel: 是因为 root table entry 的 大小扩展了，导致之前一个页放不下256entry了, 所以也搞了2级 顶层描述符包含指向二级表的指针，以及该表所表示的 StreamID 范围。每个描述符也可以被标记为无效。 图 3.2 展示了这样一个顶层表的示例，其中分割点被设置为 8。 上面提到分割点由 SMMU_(*_) STRTAB_BASE_CFG.SPLIT 决定level1 stream table descriptor 所包含STE 的大小为 : 2L1STD.Span-1.L1STD.Span == 0表示invailed. L1STD.span所代表的大小也不能超过分割点规定的最大大小，需要满足 L1STD.span - 1 &lt;= SMMU_(*_) STRTAB_BASE_CFG.SPLIT In this example: StreamIDs 0-1023 (4 x 8-bit level 2 tables) are represented, though not allare valid. StreamIDs 0-255 are configured by the array of STEs at 0x1000 (each of whichseparately enables the relevant StreamID). StreamIDs 256-259 are configured by the array of STEs at 0x2F20. StreamIDs 512-767 are all invalid. The STE of StreamID 768 is at 0x4000. represented: 展现体现表示 StreamID 0-1023（对应 4 个 8 位的二级表）被展现，但并非所有 StreamID 都是有效的。 StreamID 0-255 由位于 0x1000 的 STE 数组进行配置（每个 STE 分别使能相应的 StreamID）。 StreamID 256-259 由位于 0x2F20 的 STE 数组进行配置。 StreamID 512-767 全部无效。 StreamID 768 的 STE 位于 0x4000。 A two-level table with a split point of 8 can reduce the memory usage comparedto a large and sparse linear table used with PCIe. If the full 256 PCIe busnumbers are supported, the RequesterID or StreamID space is 16-bits. However,because there is usually one PCIe bus for each physical link and potentially onedevice for each bus, in the worst case a valid StreamID might only appear onceevery 256 StreamIDs.Alternatively, a split point of 6 provides 64 bottom-level STEs, enabling use ofa 4KB page for each bottom-level table.Note: Depending on the size of the StreamID space, the L1 Stream table mightrequire allocation of a region of physically-contiguous memory greater than asingle granule. This table shows some example sizes for the amount of memoryoccupied by L1 and L2 Stream tables: 对于 PCIe，如果使用分割点为 8 的二级表，可以比使用大型且稀疏的线性表更有效地减少内存占用。如果支持全部 256 个 PCIe 总线号，则 RequesterID 或 StreamID 空间为16 位。然而，由于通常每个物理链路只有一个 PCIe 总线，并且每个总线可能只有一个设备，在最坏情况下，一个有效的 StreamID 可能只在每 256 个 StreamID 中出现一次。 另外，使用分割点为 6 时，会有 64 个底层 STE，从而可以为每个底层表使用一个 4KB页面。 注意：根据 StreamID 空间的大小，L1 流表可能需要分配大于单个粒度（granule）的物理连续内存区域。下表展示了 L1 和 L2 流表占用内存量的一些示例大小： SIDSIZE SPLIT L1 table size L2 table size 16 6 8KB 4KB 16 8 2KB 16KB 16 10 512B 64KB 24 6 2MB 4KB 24 8 512KB 16KB 24 10 128KB 64KB 3.3.2 StreamIDs to Context DescriptorsThe STE contains the configuration for each stream indicating: Whether traffic from the device is enabled. Whether it is subject to stage 1 translation. Whether it is subject to stage 2 translation, and the relevant translationtables. Which data structures locate translation tables for stage 1. STE（流表项）包含每个流的配置信息，指明: 设备发出的流量是否被使能； 是否需要进行一级（stage 1）地址转换； 是否需要进行二级（stage 2）地址转换，以及相关的转换表； 哪些数据结构用于定位一级转换表。 If stage 1 is used, the STE indicates the address of one or more CDs in memoryusing the STE.S1ContextPtr field.The CD associates the StreamID with stage 1 translation table base pointers (totranslate VA into IPA), per-stream configuration, and ASID. If substreams are inuse, multiple CDs indicate multiple stage 1 translations, one for eachsubstream. Transactions provided with a SubstreamID are terminated when stage 1translation is not enabled.If stage 2 is used, the STE contains the stage 2 translation table base pointer(to translate IPA to PA) and VMID. If multiple devices are associated with aparticular virtual machine, meaning they share stage 2 translation tables, thenmultiple STEs might map to one stage 2 translation table.Note: Arm expects that, where hypervisor software is present, the Stream tableand stage 2 translation table are managed by the hypervisor and the CDs andstage 1 translation tables associated with devices under guest control aremanaged by the guest OS. Additionally, the hypervisor can make use of separatehypervisor stage 1 translations for its own internal purposes. Where ahypervisor is not used, a bare-metal OS manages the Stream table and CDs. Formore information, see section 3.6 Structure and queue ownership. 如果使用了一级转换（stage 1），STE 会通过 STE.S1ContextPtr 字段指示一个或多个CD（上下文描述符）在内存中的地址。 CD（上下文描述符）将 StreamID 与一级转换表的基地址（用于将 VA 转换为 IPA）、每个流的配置信息以及 ASID 关联起来。如果使用了子流（substream），则会有多个 CD，分别对应每个子流的一级转换。当未启用一级转换时，带有 SubstreamID 的事务会被终止。 如果使用了二级转换（stage 2），STE 会包含二级转换表的基地址指针（用于将 IPA 转换为 PA）和 VMID。如果有多个设备关联到同一个虚拟机（即它们共享同一个二级转换表），那么多个 STE 可能会映射到同一个二级转换表。 注意：Arm 预期，在有虚拟机管理程序（hypervisor）存在的情况下，流表（Streamtable）和二级转换表由虚拟机管理程序管理，而与设备相关的 CD 和一级转换表则由客户操作系统（guest OS）管理。此外，虚拟机管理程序可以为自身内部用途使用独立的hypervisor 一级转换。如果没有虚拟机管理程序，则裸机操作系统负责管理流表和 CD。更多信息见第 3.6 节“结构和队列的所有权”。 这个有点优秀, 看似将队列可以直接透传给虚拟机. 还需要看下后面的章节去验证 When a SubstreamID is supplied with a transaction and the configuration enablessubstreams, the SubstreamID indexes the CDs to select a stage 1 translationcontext. In this configuration, if a SubstreamID is not supplied, behaviordepends on the STE.S1DSS flag: When STE.S1DSS == 0b00, all traffic is expected to have a SubstreamID and thelack of SubstreamID is an error. A transaction without a SubstreamID isaborted and an event recorded. When STE.S1DSS == 0b01, a transaction without a SubstreamID is accepted but istreated exactly as if its configuration were stage 1-bypass. The stage 1translations are enabled only for transactions with SubstreamIDs. When STE.S1DSS == 0b10, a transaction without a SubstreamID is accepted anduses the CD of Substream 0. Under this configuration, transactions that arrivewith SubstreamID 0 are aborted and an event recorded. 当一个事务携带 SubstreamID，并且配置允许使用子流（substreams）时，SubstreamID会用于索引 CD，从而选择一级（stage 1）转换上下文。在这种配置下，如果事务没有携带 SubstreamID，其行为取决于 STE.S1DSS 标志： 当 STE.S1DSS == 0b00 时，所有流量都必须带有 SubstreamID，缺少 SubstreamID 会被视为错误。没有 SubstreamID 的事务会被终止，并记录一个事件。 当 STE.S1DSS == 0b01 时，没有 SubstreamID 的事务会被接受，但会被视为一级旁路（stage 1-bypass）配置。只有带有 SubstreamID 的事务才启用一级转换。 当 STE.S1DSS == 0b10 时，没有 SubstreamID 的事务会被接受，并使用子流 0 的 CD。在这种配置下，带有 SubstreamID 0 的事务会被终止，并记录一个事件。 When stage 1 is used, the STE.S1ContextPtr field gives the address of one of thefollowing, configured by STE.S1Fmt and STE.S1CDMax: A single CD. The start address of a single-level table of CDs. The table is a contiguous array of CDs indexed by the SubstreamID. The start address of a first-level, L1, table of L1CDs. Each L1CD.L2Ptr in the L1 table can be configured with the address of a linear level two, L2, table of CDs. The L1 table is a contiguous array of L1CDs indexed by upper bits of SubstreamID. The L2 table is a contiguous array of CDs indexed by lower bits of SubstreamID. The ranges of SubstreamID bits that are used for the L1 and L2 indices are configured by STE.S1Fmt. 当使用一级转换（stage 1）时，STE.S1ContextPtr 字段给出了以下之一的地址，这些内容由 STE.S1Fmt 和 STE.S1CDMax 进行配置： 一个单独的 CD（上下文描述符）； 一个单级 CD 表的起始地址； 该表是一个由 CD 组成的连续数组，通过 SubstreamID 进行索引； 一级（L1）CD 表（L1CDs）的起始地址； L1 表中的每个 L1CD.L2Ptr 可以配置为指向一个线性二级（L2）CD 表的地址； L1 表是一个由 L1CDs 组成的连续数组，通过 SubstreamID 的高位进行索引；L2 表是一个由 CDs 组成的连续数组，通过 SubstreamID 的低位进行索引；用于 L1 和L2 索引的 SubstreamID 位范围由 STE.S1Fmt 进行配置。 The S1ContextPtr and L2Ptr addresses are IPAs when both stage 1 and stage 2 areused and PAs when only stage 1 is used. S1ContextPtr is not used when stage 1 isnot used.The ASID and VMID values provided by the CD and STE structures tag TLB entriescreated from translation lookups performed through configuration from the CD andSTEs. These tags are used on lookup to differentiate translation address spacesbetween different streams, or to match entries for invalidation on receipt ofbroadcast TLB maintenance operations. Implementations might also use these tagsto efficiently allow sharing of identical translation tables between differentstreams. 当同时使用一级和二级转换时，S1ContextPtr 和 L2Ptr 的地址为 IPA（中间物理地址）；当只使用一级转换时，这些地址为 PA（物理地址）。如果未使用一级转换，则不会使用S1ContextPtr。 CD 和 STE 结构中提供的 ASID 和 VMID 值会作为tag，标记由 CD 和 STE 配置进行地址转换查找时创建的 TLB 项。这些tag在查找时用于区分不同流之间的转换地址空间，或者在接收到广播 TLB 维护操作时用于匹配需要失效的条目。实现上也可能利用这些tag, 高效地在不同流之间共享相同的转换表。Figure 3.3 shows an example configuration in which a StreamID selects an STEfrom a linear Stream table, the STE points to a translation table for stage 2and points to a single CD for stage 1 configuration, and then the CD points totranslation tables for stage 1. 图 3.3 展示了一个示例配置，其中一个 StreamID 从一个线性流表中选择了一个 STE，STE 指向了一个用于二级转换的翻译表，并指向了一个单独的 CD，用于一级转换的配置，然后 CD 指向了用于一级转换的翻译表。Figure 3.4 shows a configuration in which an STE points to an array of severalCDs. An incoming SubstreamID selects one of the CDs and therefore theSubstreamID determines which stage 1 translations are used by a transaction. 图 3.4 展示了一种配置，其中一个 STE 指向一个包含多个 CD 的数组。传入的SubstreamID 会选择其中的一个 CD，因此 SubstreamID 决定了事务所使用的一级转换。Figure 3.5 shows a more complex layout in which a multi-level Stream table isused. Two of the STEs point to a single CD, or a flat array of CDs, whereas thethird STE points to a multi-level CD table. With multiple levels, many streamsand many substreams might be supported without large physically-contiguoustables. 图 3.5 展示了一个更复杂的布局，其中使用了多级流表。两个 STE 指向同一个 CD 或一个平铺的 CD 数组，而第三个 STE 指向一个多级 CD 表。通过多级结构，可以在不需要大块物理连续内存表的情况下，支持大量的流和子流。 和stream table一样，多级的好处就是省内存。 An incoming transaction is dealt with in the following logical steps: If the SMMU is globally disabled (for example when it has just come out ofreset with SMMU_CR0.SMMUEN == 0), the transaction passes through the SMMUwithout any address modification. Global attributes, such as memory type orShareability, might be applied from the SMMU_GBPA register of the SMMU. Or,the SMMU_GBPA register might be configured to abort all transactions. If the global bypass described in (1) does not apply, the configuration isdetermined: An STE is located. If the STE enables stage 2 translation, the STE contains the stage 2translation table base. If the STE enables stage 1 translation, a CD is located. If stage 2translation is also enabled by the STE, the CD is fetched from IPA spacewhich uses the stage 2 translations. Otherwise, the CD is fetched from PAspace. Translations are performed, if the configuration is valid. If stage 1 is configured to translate, the CD contains a translation tablebase which is walked. This might require stage 2 translations, if stage 2is enabled for the STE. Otherwise, stage 1 bypasses translation and theinput address is provided directly to stage 2. If stage 2 is configured to translate, the STE contains a translationtable base that performs a nested walk of a stage 1 translation table ifenabled, or a normal walk of an incoming IPA. Otherwise, stage 2 bypassestranslation and the stage 2 input address is provided as the outputaddress. A transaction with a valid configuration that does not experience a fault ontranslation has the output address (and memory attributes, as appropriate)applied and is forwarded. 对一个传入事务的处理逻辑步骤如下： 如果 SMMU 处于全局禁用状态（例如刚刚复位，SMMU_CR0.SMMUEN == 0），事务会直接通过 SMMU，地址不会被修改。此时可以通过 SMMU 的 SMMU_GBPA 寄存器应用一些全局属性（如内存类型或可共享性），或者将 SMMU_GBPA 配置为中止所有事务。 如果上述全局旁路（第1步）不适用，则确定配置流程： 定位一个 STE（流表项）。 如果 STE 使能了二级转换（stage 2），则 STE 包含二级转换表的基地址。 如果 STE 使能了一级转换（stage 1），则需要定位一个 CD（上下文描述符）。如果 STE 同时使能了二级转换，则 CD 从 IPA 空间（通过二级转换）获取；否则，CD 从 PA 空间获取。 如果配置有效，则执行地址转换操作。 如果一级转换被配置为启用，CD 包含转换表的基地址，需要遍历该表。如果 STE使能了二级转换，则遍历过程可能需要用到二级转换；否则，一级转换会旁路，输入地址会直接传递给二级转换。 如果二级转换被配置为启用，STE 包含转换表的基地址，会对一级转换表进行嵌套遍历（如果一级转换被启用），或者对输入的 IPA 进行普通遍历。否则，二级转换会旁路，输入地址会作为输出地址直接使用。 如果事务的配置有效，并且在转换过程中没有发生异常，则会应用输出地址（以及适当的内存属性），并将事务转发出去。 Note: This sequence illustrates the path of a transaction on a Non-securestream. If Secure state is supported, the path of a transaction on a Securestream is similar, except SMMU_S_CR0.SMMUEN and SMMU_S_GBPA control bypass.An implementation might cache data as required for any of these steps. Section16.2 Caching describes caching of configuration and translation structures.Furthermore, events might occur at several stages in the process that preventthe transaction from progressing any further. If a transaction fails to locatevalid configuration or is of an unsupported type, it is terminated with an abort,and an event might be recorded. If the transaction progresses as far astranslation, faults can arise at either stage of translation. The configurationthat is specific to the CD and STEs that are used determines whether thetransaction is terminated or whether it is stalled, pending software faultresolution, see section 3.12 Fault models, recording and reporting.The two translation stages are described using the VA to IPA and IPA to PAstages of the Armv8-A Virtualization terminology.Note: Some systems refer to the SMMU input as a Bus Address (BA). The term VAemphasizes that the input address to the SMMU can potentially be from the samevirtual address space as a PE process (using VAs).Unless otherwise specified, translation tables and their configuration fieldsact exactly the same way as their equivalents specified in the Armv8-ATranslation System for PEs [2].If an SMMU does not implement one of the two stages of translation, it behavesas though that stage is configured to permanently bypass translation. Otherrestrictions are also relevant, for example it is not valid to configure anon-present stage to translate. An SMMU must support at least one stage oftranslation.3.3.3 Configuration and Translation lookupFigure 3.7 illustrates the concepts that are used in this specification whenreferring to a configuration lookup and translation lookup.As described in 3.3.2 StreamIDs to Context Descriptors above, an incomingtransaction is first subject to a configuration lookup, and the SMMU determineshow to begin to translate the transaction. This involves locating theappropriate STE then, if required, a CD.The configuration lookup stage does not depend on the input address and is afunction of the: SMMU global register configuration. Incoming transaction StreamID. Incoming transaction SubstreamID (if supplied).The result of the configuration lookup is the stream or substream-specificconfiguration that locates the translation, including: Stage 1 translation table base pointers, ASID, and properties modifying theinterpretation or walk of the translation tables (such as translation granule) Stage 2 translation table base pointer, VMID and properties modifying theinterpretation or walk of the translation table. Stream-specific properties, such as the StreamWorld (the Exception Level, ortranslation regime, in PE terms) to which the stream is assigned. The translation lookup stage logically works the same way as a PE memory addresstranslation system. The output is the final physical address provided to thesystem, which is a function of the: Input address StreamWorld (Stream Security state and Exception level), ASID and VMID (whichare provided from the previous step). Figure 3.7 shows a PE-style TLB used in the translation lookup step. Arm expectsthe SMMU to use a TLB to cache translations instead of performing translationtable walks for each transaction, but this is not mandatory.Note: For clarity, Figure 3.7 does not show error reporting paths or CD fetchthrough stage 2 translation (which would also access the TLB or translationtable walk facilities). An implementation might choose to flatten or combinesome of the steps shown, while maintaining the same behavior.A cached translation is associated with a StreamWorld that denotes itstranslation regime. StreamWorld is directly equivalent to an Exception level ona PE.The StreamWorld of a translation is determined by the configuration that insertsthat translation. The StreamWorld of a cached translation is determined from thecombination of the Security state of an STE, its STE.Config field, its STE.STRWfield, and the corresponding SMMU_(*_)CR2.E2H configuration. See the STE.STRWfield in section 5.2 Stream Table Entry.In addition to insertion into a TLB, the StreamWorld affects TLB lookups, andthe scope of different types of TLB invalidations. An SMMU implementation is notrequired to distinguish between cached translations inserted for EL2 versusEL2-E2H.For the behavior of TLB invalidations, see section 3.17 TLB tagging, VMIDs,ASIDs and participation in broadcast TLB maintenance.A translation is associated with one of the following StreamWorlds:Note: StreamWorld can differentiate multiple translation regimes in the SMMUthat are associated with different bodies of software at different Exceptionlevels. For example, a Secure Monitor EL3 translation for address 0x1000 isdifferent to (and unaffected by) a Non-secure hypervisor EL2 translation foraddress 0x1000, as are NS-EL1 translations for address 0x1000. Arm expects thatthe StreamWorld configured for a stream in the SMMU will match the Exceptionlevel of the software that controls the stream or device.The term any-EL2 is used to describe behaviors common to NS-EL2, S-EL2, andRealm-EL2.The term any-EL2-E2H is used to describe behaviors common to NS-EL2-E2H,S-EL2-E2H, and Realm-EL2-E2H StreamWorlds.In the same way as in an Armv8-A MMU, a translation is architecturally unique ifit is identified by a unique set of {StreamWorld, VMID, ASID, Address} inputparameters.For example, the following are unique and can all co-exist in a translationcache: Entries with the same address, but different ASIDs. Entries with the same address and ASID, but different VMIDs. Entries with the same address and ASID but a different StreamWorld.Architecturally, a translation is not uniquely identified by a StreamID andSubstreamID. This results in two properties: A translation is not required to be unique for a set of transaction input parameters (StreamID, SubstreamID). Two streams can be configured to use the same translation configuration andthe resulting ASID/VMID from their configuration lookup will identify asingle set of shared translation cache entries. Multiple StreamID/SubstreamID configurations that result in identical ASID/VMID/StreamWorld configuration must maintain the same configuration where configuration can affect TLB lookup. For example, two streams configured for a stage 1, NS-EL1 with ASID == 3must both use the same translation table base addresses and translationgranule. When translating an address, any-EL2 and EL3 regimes use only one translationtable. CD.TTB1 is unused in these configurations. All other StreamWorlds useboth translation tables, and therefore CD.TTB0 and CD.TTB1 are both required.Only some stage 1 translation table formats are valid in each StreamWorld,consistent with the PE. Valid combinations are described in the CD.AA64description. Selecting an inconsistent combination of StreamWorld and CD.AA64(for example, using VMSAv8-32 LPAE translation tables to represent a VMSAv8-64EL3 translation regime) causes the CD to be ILLEGAL.Secure stage 1 permits VMSAv8-32 LPAE, VMSAv8-64 and VMSAv9-128 translationtables. Secure stage 2 is not supported for VMSAv8-32 LPAE translation tables.In this specification, the term TLB is used to mean the concept of a translationcache, indexed by StreamWorld/VMID/ASID and VA.SMMU cache maintenance commands therefore fall into two groups: Configuration cache maintenance, acting upon StreamIDs and SubstreamIDs. Translation cache maintenance (or TLB maintenance), acting on addresses, ASIDs,VMIDs and StreamWorld.The second set of commands directly matches broadcast TLB maintenance operationsthat might be available from PEs in some systems. The StreamWorld tag determineshow TLB entries respond to incoming broadcast TLB invalidations and TLBinvalidation SMMU commands, see section 3.17 TLB tagging, VMIDs, ASIDs andparticipation in broadcast TLB maintenance for details.3.3.4 Transaction attributes: incoming, two-stage translation and overrides3.3.5 Translation table descriptors3.4 Address sizes3.5 Command and Event queues3.6 Structure and queue ownership3.10 Security states supportThe Arm architecture provides support for two Security states, each with anassociated physical address space (PA space): Security state PA space Secure state Secure (NS == 0) Non-secure state Non-secure (NS == 1) The Realm Management Extension, FEAT_RME, introduces two new security states,each with an associated physical address space: Security state PA space Secure state Secure Non-secure state Non-secure Realm state Realm Root state Root ARM 支持两个安全状态，每个对应于一个 physical address space PA space 而RME又新增了两个安全状态，每个对应于一个 PA space 现在有四个安全状态，对应四个PA space 3.10.1 StreamID Security state (SEC_SID)StreamID Security state (SEC_SID) determines the Security state of theprogramming interface that controls a given transaction.The association between a device and the Security state of the programminginterface is a system-defined property.If SMMU_S_IDR1.SECURE_IMPL == 0, then incoming transactions have a StreamID,and either: A SEC_SID identifier with a value of 0. No SEC_SID identifer, and SEC_SID is implicitly treated as 0.If SMMU_S_IDR1.SECURE_IMPL == 1, incoming transactions have a StreamID, and aSEC_SID identifier. StreamID 的安全状态（SEC_SID）决定了控制某个事务的编程接口的安全状态。 设备与编程接口安全状态的关联是系统定义的属性。 如果 SMMU_S_IDR1.SECURE_IMPL == 0，则传入的事务带有 StreamID，并且： 有一个值为0的 SEC_SID 标识符，或 没有 SEC_SID 标识符，此时 SEC_SID 被隐式视为0。 如果 SMMU_S_IDR1.SECURE_IMPL == 1，则传入的事务带有 StreamID，并且带有SEC_SID 标识符。 SEC_SID Meaning 0 The StreamID is a Non-secure stream, and indexes into the Non-secure Stream table. 1 The StreamID is a Secure stream, and indexes into the Secure Stream table. In this specification, the terms Secure StreamID and Secure stream refer to astream that is associated with the Secure programming interface, as determinedby SEC_SID.The terms Non-secure StreamID and Non-secure stream refer to a stream that isassociated with the Non-secure programming interface, which might be determinedby SEC_SID or the absence of the SEC_SID identifier.Note: Whether a stream is under Secure control or not is a different property tothe target PA space of a transaction. If a stream is Secure, it means that it iscontrolled by Secure software through the Secure Stream table. Whether atransaction on that stream results in a transaction targeting Secure PA spacedepends on the translation table attributes of the configured translation, or,for bypass, the incoming NS attribute.For an SMMU with RME DA, the encoding of SEC_SID is extended to 2 bits, and hasthe following encoding: 在本规范中，术语“安全 StreamID”和“安全流”指的是与安全编程接口相关联的流，这种关联由 SEC_SID 决定。 absence: 缺乏，确实 术语“非安全 StreamID”和“非安全流”指的是与非安全编程接口相关联的流，这种关联可以由 SEC_SID 决定，也可以由 缺失 SEC_SID 标识符来决定。 注意：一个流是否处于安全控制之下，与该事务的目标物理地址空间是两个不同的属性。如果一个流是安全的，意味着它由安全软件通过安全流表进行控制。该流上的事务是否会访问安全物理地址空间，则取决于配置的转换表属性，或者在旁路情况下，取决于传入的NS 属性。 这段话描述的是，流是否处于安全控制，和该事物的target PA space 不是一一对应的。 而确定该事物是否安全，由 该事物对应的 Secure Stream Table决定。如果一个流是安全的，则其使用的是安全 Secure Stream Table, 而 Secure Stream Table 又可以通过translation table attr 来指向不同的PA space。而non-secure stream 则使用non-secure stream, 其只能指向 PA space (猜测). 那从这里看, SMMU 是不是CPU的逻辑很像么 ? CPU 通过 SCR_EL3.NS 来控制CPU el1, el2 的安全状态, 然后secure state和non-secure state 使用不同的页表，其中 secure-state 使用的页表可以指定NS attr 来决定本次访问的 PA space 现在的问题是, CPU侧可以通过 EL3的 secure monitor 来切换CPU的secure state,而SMMU主要是处理设备的IO请求，那怎么决定该设备是安全设备还是非安全设备呢? 另外, SMMU 实际上也是通过CPU来配置的，怎么能让CPU secure software来配置SMMU 的secure extention相关配置，而拒绝 non-secure software 的配置呢? 上面最后一段, 是接下来要详细了解的内容 对于带有 RME DA 的 SMMU，SEC_SID 的编码扩展为 2 位，编码如下： SEC_SID Meaning 0b00 Non-secure 0b01 Secure 0b10 Realm 0b11 Reserved Transactions with a SEC_SID value of Realm are associated with the Realmprogramming interface. 具有 Realm 值的 SEC_SID 的事务与 Realm 编程接口相关联。3.10.2 Support for Secure stateSMMU_S_IDR1.SECURE_IMPL indicates whether an SMMU implementation supports theSecure state. When SMMU_S_IDR1.SECURE_IMPL == 0: The SMMU does not support the Secure state. SMMU_S_* registers are RAZ/WI to all accesses. Support for stage 1 translation is OPTIONAL.When SMMU_S_IDR1.SECURE_IMPL == 1: The SMMU supports the Secure state. SMMU_S_* registers configure Secure state, including a Secure Command queue,Secure Event queue and a Secure Stream table. The SMMU supports stage 1 translation and might support stage 2 translation. The SMMU can generate transactions to the memory system, to Secure PA space(NS == 0) and Non-secure PA space (NS == 1) where permitted by SMMUconfiguration. SMMU_S_IDR1.SECURE_IMPL 指示 SMMU 实现是否支持安全状态（Secure state）。 当 SMMU_S_IDR1.SECURE_IMPL == 0 时： SMMU 不支持安全状态。 所有对 SMMU_S_* 寄存器的访问都是 RAZ/WI（Read-As-Zero/Write-Ignored，读为零/写入忽略）。 对 stage 1 地址转换的支持是可选的。 当 SMMU_S_IDR1.SECURE_IMPL == 1 时： SMMU 支持安全状态。 SMMU_S_* 寄存器用于配置安全状态，包括安全命令队列、安全事件队列和安全流表。 SMMU 支持 stage 1 转换，并且可能支持 stage 2 转换。 这里smmu 在支持 secure state 情况下, 非得支持 stage1 SMMU 可以根据配置，向内存系统发起事务，访问安全物理地址空间（NS == 0）和非安全物理地址空间（NS == 1）。 The Non-secure StreamID namespace and the Secure StreamID namespace are separatenamespaces. The assignment of a client device to either a Secure StreamID or aNon-secure StreamID, and reassignment between StreamID namespaces, issystem-defined.With the exception of SMMU_S_INIT, SMMU_S_* registers are Secure access only,and RAZ/WI to Non-secure accesses.Note: Arm does not expect a single software driver to be responsible forprogramming both the Secure and Non-secure interface. However, the twoprogramming interfaces are intentionally similar. 非安全 StreamID 命名空间与安全 StreamID 命名空间是两个独立的命名空间。将客户端设备分配到安全 StreamID 或非安全 StreamID，以及在这两个 StreamID 命名空间之间重新分配，都是由系统定义的。 除了 SMMU_S_INIT 之外，所有 SMMU_S_* 寄存器仅允许安全访问，对于非安全访问则是读为零/写入忽略（RAZ/WI）。 这两段话太关键了，回答了上面提出的疑问, 我们在回忆下上面提到的疑问: Q1: SMMU 如何分清楚哪些设备是安全设备，哪些设备是非安全设备 A: 通过SteamID, ssmu将 secure streamID和non-secure streamID 划分到两个独立的 命名空间中 Q2: 如何让 secure software 配置smmu的 secure 相关配置 A: SMMU_S_* 寄存器仅允许 secure access. 所以综上来说, SMMU 为支持 secure state, 做了两个非常重要的扩展: StreamID namespace : 通过namespace 将不同的StreamID 划分到不同的securesegment中, 扩展性很好，例如支持Realm后，可以再增加一个namespace. 根据不同的 secure access来访问不同的 SMMU_S_* 寄存器. 从而保证了配置的安全性 注意：Arm 并不期望由同一个软件驱动负责同时编程安全和非安全接口。不过，这两个编程接口在设计上是故意保持相似的。When a stream is identified as being under Secure control according to SEC_SID,see 3.10.1 StreamID Security state (SEC_SID), its configuration is taken fromthe Secure Stream table or from the global bypass attributes that are determinedby SMMU_S_GBPA.Otherwise, its configuration is taken from the Non-secure Stream table or fromthe global bypass attributes that are determined by SMMU_GBPA.The Secure programming interface and Non-secure programming interface haveseparate global SMMUEN translation-enable controls that determine whether bypassoccurs.A transaction that belongs to a Stream that is under Secure control can generatetransactions to the memory system that target Secure (NS == 0) and Non-secure(NS == 1) PA spaces. A transaction that belongs to a Stream that is underNon-secure control can only generate transactions to the memory system thattarget Non-secure (NS == 1) PA space. 当某个流根据 SEC_SID 被标识为处于安全控制之下（参见 3.10.1 StreamID 安全状态SEC_SID），其配置信息将来自于安全流表（Secure Stream table），或由 SMMU_S_GBPA决定的全局旁路属性。 否则，其配置信息将来自于非安全流表（Non-secure Stream table），或由 SMMU_GBPA决定的全局旁路属性。 安全编程接口和非安全编程接口分别拥有独立的全局 SMMUEN 转换使能控制，用于决定是否启用旁路。 属于安全控制流的事务，可以向内存系统发起目标为安全（NS == 0）和非安全（NS == 1）物理地址空间的事务。而属于非安全控制流的事务，只能向内存系统发起目标为非安全（NS == 1）物理地址空间的事务 。 Security state Permitted target PA spaces Secure Secure, Non-secure Non-secure Non-secure 3.10.2.1 Secure commands, events and configurationIn this specification, the term Event queue and the term Command queue refer tothe queue that is appropriate to the Security state of the relevant stream.Similarly, the term Stream table and Stream Table Entry (STE) refer to the tableor table entry that is appropriate to the Security state of the stream asindicated by SEC_SID.For instance: An event that originates from a Secure StreamID is written to the Secure Eventqueue. An event that originates from a Non-secure StreamID is written to theNon-secure Event queue. Commands that are issued on the Non-secure Command queue only affect streamsthat are configured as Non-secure. Some commands that are issued on the Secure Command queue can affect anystream or data in the system. The stream configuration for a Non-secure StreamID X is taken from the Xthentry in the Non-secure Stream table. Stream configuration for a Secure StreamID Y is taken from the Yth entry inthe Secure Stream table. The Non-secure programming interface of an SMMU with SMMU_S_IDR1.SECURE_IMPL ==1 is identical to the interface of an SMMU with SMMU_S_IDR1.SECURE_IMPL == 0.Note: To simplify descriptions of commands and programming, this specificationrefers to the Non-secure programming interface registers, Stream table, Commandqueue and Event queue even when SMMU_S_IDR1.SECURE_IMPL == 0.The register names associated with the Non-secure programming interface are ofthe form SMMU_x. The register names associated with the Secure programminginterface are of the form SMMU_S_x. In this specification, where reference ismade to a register but the description applies equally to the Secure orNon-secure version, the register name is given as SMMU_(S_)x. Where anassociation exists between multiple Non-secure, or multiple Secure registers andreference is made using the SMMU_(S_)x syntax, the registers all relate to thesame Security state unless otherwise specified.The two programming interfaces operate independently as though two logical andseparate SMMUs are present, with the exception that some commands issued on theSecure Command queue and some Secure registers might affect Non-secure state, asindicated in this specification. This independence means that:" }, { "title": "[arm] gic", "url": "/posts/learn_the_architecture_gicv3v4/", "categories": "arm_arch, gic", "tags": "gic", "date": "2025-08-13 20:39:00 +0800", "snippet": "Configuring the Arm GICThis section of the guide describes how to enable and configure aGICv3-compliant interrupt controller in a bare metal environment. For detailedregister descriptions see the A...", "content": "Configuring the Arm GICThis section of the guide describes how to enable and configure aGICv3-compliant interrupt controller in a bare metal environment. For detailedregister descriptions see the Arm Generic Interrupt Controller ArchitectureSpecification GIC architecture version 3.0 and 4. 本节指南介绍了如何在裸机环境下启用并配置符合 GICv3 标准的中断控制器。有关寄存器的详细描述，请参阅《Arm 通用中断控制器架构规范（GIC 架构版本 3.0 和 4）》。接下来我们将先介绍全局设置，然后介绍每个 PE 的专用设置。The configuration of Locality-specific Peripheral Interrupts (LPIs) issignificantly different to the configuration of Shared Peripheral Interrupts(SPIs), Private Peripheral Interrupt (PPIs), and Software Generated Interrupts(SGIs), and is beyond the scope of this guide. To learn more, refer to our guideArm CoreLink Generic Interrupt Controller v3 and v4: Locality-specificPeripheral Interrupts. 本地性专用外设中断（LPI）的配置与共享外设中断（SPI）、私有外设中断（PPI）以及软件生成中断（SGI）的配置有很大不同，超出了本指南的范围。如需了解更多，请参考我们的《Arm CoreLink 通用中断控制器 v3 和 v4：本地性专用外设中断》指南。Most systems that use a GICv3 interrupt controller are multi-core systems, andpossibly also multi-processor systems. Some settings are global, which meansthat affect all the connected PEs. Other settings are particular to a single PE. 大多数采用 GICv3 中断控制器的系统都是多核系统，甚至可能是多处理器系统。其中一些设置是全局性的，会影响所有连接的处理元件（PE）；而另一些设置则仅针对单个 PE。Let’s look at the global settings, and then the settings for each PE.Global settingsThe Distributor control register (GICD_CTLR) must be configured to enable theinterrupt groups and to set the routing mode as follows: 分发器控制寄存器（GICD_CTLR）必须进行配置，以启用中断分组并设置路由模式，具体如下： Enable Affinity routing (ARE bits): The ARE bits in GICD_CTLR control whetherthe GIC is operating in GICv3 mode or legacy mode. Legacy mode providesbackwards compatibility with GICv2. This guide assumes that the ARE bits areset to 1, so that GICv3 mode is being used. 启用亲和性路由（ARE 位）：GICD_CTLR 中的 ARE 位用于控制 GIC 处于 GICv3 模式还是兼容模式。兼容模式（Legacy mode）用于向后兼容 GICv2。本指南假设 ARE 位已设置为 1，即使用的是 GICv3 模式。 Enables: GICD_CTLR contains separate enable bits for Group 0, Secure Group 1and Non-secure Group 1: EnableGrp1S enables distribution of Secure Group 1 interrupts. EnableGrp1NS enables distribution of Non-secure Group 1 interrupts. EnableGrp0 enables distribution of Group 0 interrupts. 使能位：GICD_CTLR 包含用于 Group 0、安全 Group 1 和非安全 Group 1 的独立使能位： EnableGrp1S：使能安全 Group 1 中断的分发。 EnableGrp1NS：使能非安全 Group 1 中断的分发。 EnableGrp0：使能 Group 0 中断的分发。 Note Arm CoreLink GIC-600 does not support legacy operation, and the ARE bits arepermanently set to 1. permanently : ；永久的 ARM corelink GIC-600 不支持 legacy operation, 并且 ARE bits 永久的设置为1 Settings for each PEThis section covers settings that are specific to a single core or PE.Redistributor configurationEach core has its own Redistributor, as shown here:Figure 1. PE connected to redistributorThe Redistributor contains a register called GICR_WAKER which is used to recordwhether the connected PE is online or offline. Interrupts are only forwarded toa PE that the GIC believes is online. At reset, all PEs are treated as beingoffline. Redistributor 包含一个名为 GICR_WAKER 的寄存器，用于记录所连接的处理单元（PE）是在线还是离线状态。只有当 GIC 认为某个 PE 处于在线状态时，中断才会被转发给该PE。在复位时，所有的 PE 都被视为离线状态。To mark the connected PE as being online, software must: Clear GICR_WAKER.ProcessorSleep to 0. Poll GICR_WAKER.ChildrenAsleep until it reads 0.It is important that software performs these steps before configuring the CPUinterface, otherwise behavior can be UNPREDICTABLE. 软件在配置 CPU 接口前必须先执行这些步骤，否则可能会导致不可预期的行为。While the PE is offline (GICR_WAKER.ProcessorSleep==1), an interrupt that istargeting the PE will result in a wake-request signal being asserted. Typically,this signal will go to the power controller of the system. The power controllerthen turns on the PE. On waking, software on that PE would clear theProcessorSleep bit, allowing the interrupt that woke the PE to be forwarded. 当处理单元（PE）处于离线状态（GICR_WAKER.ProcessorSleep==1）时，若有中断目标指向该 PE，会产生一个唤醒请求信号（wake-request signal）。通常，这个信号会发送到系统的电源控制器。电源控制器随后会开启该 PE。在唤醒后，该 PE 上的软件会清除ProcessorSleep 位，从而允许唤醒该 PE 的中断被转发。CPU interface configurationThe CPU interface is responsible for delivering interrupt exceptions to the PEto which it is connected. To enable the CPU interface, software must configurethe following: CPU 接口负责将中断异常传递给其所连接的处理单元（PE）。要使 CPU 接口可用，软件需要进行以下配置： Enable System register access: The CPU interfaces (ICC_*_ELn) section describesthe CPU interface registers, and how they are accessed as System registers inGICv3. Software must enable access to the CPU interface registers, by settingthe SRE bit in the ICC_SRE_ELn registers. 启用系统寄存器访问： CPU 接口（ICC_*_ELn）部分描述了 CPU 接口寄存器，以及在 GICv3 中如何作为系统寄存器进行访问。软件必须通过设置 ICC_SRE_ELn 寄存器中的 SRE 位，来启用对 CPU 接口寄存器的访问。 Note Many recent Arm Cortex processors do not support legacy operation, and the SREbits are fixed as set. On these processors this step can be skipped. 许多新近的 Arm Cortex 处理器不再支持传统模式，SRE 位被固定为已设置状态。在这些处理器上，这一步可以跳过。 Set Priority Mask and Binary Point registers: The CPU interface contains thePriority Mask register (ICC_PMR_EL1) and the Binary Point registers(ICC_BPRn_EL1). The Priority Mask sets the minimum priority that an interruptmust have in order to be forwarded to the PE. The Binary Point register isused for priority grouping and preemption. The use of both registers isdescribed in more detail in Handling Interrupts. 设置优先级屏蔽和二进制位寄存器： CPU 接口包含优先级屏蔽寄存器（ICC_PMR_EL1）和二进制位寄存器（ICC_BPRn_EL1）。优先级屏蔽寄存器用于设置中断被转发到处理单元（PE）所需的最低优先级。二进制位寄存器则用于优先级分组和抢占。关于这两个寄存器的具体用法，会在“处理中断”部分进行详细介绍 Set EOI mode: The EOImode bits in ICC_CTLR_EL1 and ICC_CTLR_EL3 in the CPUinterface control how the completion of an interrupt is handled. This isdescribed in more detail in End of interrupt. 设置 EOI 模式： CPU 接口中的 ICC_CTLR_EL1 和 ICC_CTLR_EL3 寄存器的 EOImode 位用于控制中断完成后的处理方式。关于该内容会在“中断结束”部分进行更详细的说明。 Enable signaling of each interrupt group: The signaling of each interruptgroup must be enabled before interrupts of that group will be forwarded by theCPU interface to the PE. To enable signaling, software must write to theICC_IGRPEN1_EL1 register for Group 1 interrupts and ICC_IGRPEN0_EL1 registersfor Group 0 interrupts. ICC_IGRPEN1_EL1 is banked by Security state. Thismeans that ICC_GRPEN1_EL1 controls Group 1 for the current Security state. AtEL3, software can access both Group 1 enables using ICC_IGRPEN1_EL3. 启用各中断组的信号： 必须先启用每个中断组的信号，CPU 接口才会将该组的中断转发给处理单元（PE）。要启用信号，软件需要为 Group 1 中断写入 ICC_IGRPEN1_EL1 寄存器，为 Group 0 中断写入 ICC_IGRPEN0_EL1 寄存器。ICC_IGRPEN1_EL1 是按安全状态分组的，这意味着ICC_GRPEN1_EL1 控制当前安全状态下的 Group 1。在 EL3 层级，软件可以通过ICC_IGRPEN1_EL3 同时访问两个 Group 1 的使能寄存器。 PE configurationSome configuration of the PE is also required to allow it to receive and handleinterrupts. A detailed description of this is outside of the scope of thisguide. In this guide, we will describe the basic steps that are required for anArmv8-A compliant PE executing in AArch64 state. Routing controls: The routing controls for interrupts are in SCR_EL3 andHCR_EL2 of the PE. The routing control bits determine the Exception level towhich an interrupt is taken. The routing bits in these registers have anUNKNOWN value at reset, so they must be initialized by software. Interrupt masks: The PE also has exception mask bits in PSTATE. When thesebits are set, interrupts are masked. These bits are set at reset. Vector table: The location of the vector tables of the PE is set by theVBAR_ELn registers. Like with SCR_EL3 and HCR_EL2, VBAR_ELn registers have anUNKNOWN value at reset. Software must set the VBAR_ELn registers to point tothe appropriate vector tables in memory. To learn more about these steps, see the Learn the Architecture: Exception modelguide.SPI, PPI, and SGI configurationSo far, we have looked at configuring the interrupt controller itself. We willnow discuss the configuration of the individual interrupt sources.Which registers are used to configure an interrupt depends on the type ofinterrupt: SPIs are configured through the Distributor, using the GICD_* registers. PPIs and SGIs are configured through the individual Redistributors, using theGICR_* registers.These different configuration mechanisms are illustrated in the followingdiagram:Figure 2. Config registersFor each INTID, software must configure the following: Priority: GICD_IPRIORITYn, GICR_IPRIORITYn. Each INTID has an associatedpriority, represented as an 8-bit unsigned value. 0x00 is the highest possiblepriority, and 0xFF is the lowest possible priority. Running priority andpreemption describes how the priority value in GICD_IPRIORITYn andGICR_IPRIORITYn masks low priority interrupts, and how it controls preemption.An interrupt controller is not required to implement all 8 priority bits. Aminimum of 5 bits must be implemented if the GIC supports two Security states.A minimum of 4 bits must be implemented if the GIC support only a singleSecurity state. Group: GICD_IGROUPn, GICD_IGRPMODn, GICR_IGROUPn, GICR_IGRPMODn As describedin Security model, an interrupt can be configured to belong to one of thethree interrupt groups. These interrupt groups are Group 0, Secure Group 1 andNon-secure Group 1. Edge-triggered or level-sensitive: GICD_ICFGRn, GICR_ICFGRn For PPIs and SPI,the software must specify whether the interrupt is edge-triggered orlevel-sensitive. SGIs are always treated as edge-triggered, and thereforeGICR_ICFGR0 behaves as Read-As-One, Writes Ignored (RAO/WI) for theseinterrupts. Enable: GICD_ISENABLERn, GICD_ICENABLER, GICR_ISENABLERn, GICR_ICENABLERn EachINTID has an enable bit. Set-enable registers and Clear-enable registersremove the requirement to perform read-modify-write routines. Arm recommendsthat the settings outlined in this section are configured before enabling theINTID. Non-maskable: Interrupts configured as non-maskable are treated as higherpriority than all other interrupts belonging to the same Group. That is, anon-maskable Non-secure Group 1 interrupt is treated as higher priority thanall other Non-secure Group 1 interrupts. The non-maskable property is added in GICv3.3 and requires matching supportin the PE. Only Secure Group 1 and Non-secure Group 1 interrupts can be marked asnon-maskable. For a bare metal environment, it is often unnecessary to change settings afterinitial configuration. However, if an interrupt must be reconfigured, forexample to change the Group setting, you should first disable the interruptbefore changing its configuration.The reset values of most of the configuration registers are IMPLEMENTATIONDEFINED. This means that the designer of the interrupt controller decides whatthe values are, and the values might vary between systems.Arm GICv3.1 and the extended INTID rangesArm GICv3.1 added support for additional SPI and PPI INTIDs. The registers toconfigure these interrupts are the same as the original interrupt ranges, exceptthat they have an E suffix. For example: GICR_ISENABLERn - Enable bits for the original PPI range GICR_ISENABLERnE - Enable bits for the additional PPIs that are introduced inGICv3.1Setting the target PE for SPIsFor SPIs, the target of the interrupt must be configured. This is controlled byGICD_IROUTERn or GICD_IROUTERnE for the GICv3.1 extended SPIs. There is aGICD_IROUTERn register for each SPI, and the Interrupt_Routing_Mode bit controlsthe routing policy. The options are: GICD_IROUTERn.Interrupt_Routing_Mode == 0 The SPI is delivered to the PEA.B.C.D, which are the affinity co-ordinates specified in the register. GICD_IROUTERn.Interrupt_Routing_Mode == 1 The SPI can be delivered to anyconnected PE that is participating in distribution of the interrupt group. TheDistributor, rather than software, selects the target PE. The target cantherefore vary each time the interrupt is signaled. This type of routing isreferred to as 1-of-N. A PE can opt out of receiving 1-of-N interrupts. This is controlled by theDPG1S, DPG1NS and DPG0 bits in GICR_CTLR. Arm Generic Interrupt Controller (GIC) Architecture Specification, v3 and v4 GIC architecture version 3 and version 4 " }, { "title": "[arm] TrustZone", "url": "/posts/learn_the_architecture__trustzone/", "categories": "coco, trustzone", "tags": "trustzone", "date": "2025-08-13 09:39:00 +0800", "snippet": "What is TrustZone?TrustZone is the name of the Security architecture in the Arm A-profilearchitecture. First introduced in Armv6K, TrustZone is also supported in Armv7-Aand Armv8-A. TrustZone provi...", "content": "What is TrustZone?TrustZone is the name of the Security architecture in the Arm A-profilearchitecture. First introduced in Armv6K, TrustZone is also supported in Armv7-Aand Armv8-A. TrustZone provides two execution environments with system-widehardware enforced isolation between them, as shown in this diagram: TrustZone 是 Arm A-profile 架构中的安全架构名称。TrustZone 首次在 Armv6K 中引入，并且在 Armv7-A 和 Armv8-A 中也得到了支持。TrustZone 提供了两个执行环境，并在系统范围内通过硬件强制实现它们之间的隔离，如下图所示：The Normal world runs a rich software stack. This software stack typicallyincludes a large application set, a complex operating system like Linux, andpossibly a hypervisor. Such software stacks are large and complex. While effortscan be made to secure them, the size of the attack surface means that they aremore vulnerable to attack. 普通世界（Normal world）运行着丰富的软件栈。这个软件栈通常包括大量的应用程序、一个复杂的操作系统（如 Linux），以及可能存在的虚拟机管理器（hypervisor）。这样的软件栈庞大且复杂。尽管可以采取措施来提升其安全性，但由于攻击面较大，它们更容易受到攻击。The Trusted world runs a smaller and simpler software stack, which is referredto as a Trusted Execution Environment (TEE). Typically, a TEE includes severalTrusted services that are hosted by a lightweight kernel. The Trusted servicesprovide functionality like key management. This software stack has aconsiderably smaller attack surface, which helps reduce vulnerability to attack. 受信任世界（Trusted world）运行着更精简、更简单的软件栈，这被称为受信任执行环境（Trusted Execution Environment，TEE）。通常，TEE 包含由轻量级内核托管的若干受信任服务。这些受信任服务提供诸如密钥管理等功能。这样的软件栈攻击面要小得多，从而有助于降低遭受攻击的风险。 Note You might sometimes see the term Rich Execution Environment (REE) used todescribe the software that is running in the Normal world. 你有时可能会看到“富执行环境（Rich Execution Environment，REE）”这个术语，用来描述运行在普通世界（Normal world）中的软件。 TrustZone aims to square a circle. As users and developers, we want the richfeature set and flexibility of the Normal world. At the same time, we want thehigher degrees of trust that it is possible to achieve with a smaller and morerestricted software stack in the Trusted world. TrustZone gives us both,providing two environments with hardware-enforced isolation between them. TrustZone 旨在实现看似矛盾的需求。作为用户和开发者，我们既希望拥有普通世界所带来的丰富功能和灵活性，同时又希望获得受信任世界中通过更小、更受限制的软件栈所能实现的更高程度的信任。TrustZone 让我们两者兼得，提供了两个通过硬件强制隔离的执行环境。TrustZone for Armv8-MTrustZone is also used to refer the Security Extensions in the Armv8-Marchitecture. While there are similarities between TrustZone in the A profilearchitecture and the M profile architecture, there are also importantdifferences. This guide covers the A profile only. refer: 指代，取代 TrustZone 也用于指代 Armv8-M 架构中的安全扩展。虽然 A profile 架构中的TrustZone 与 M profile 架构中的 TrustZone 有一些相似之处，但也存在重要的区别。本指南仅涵盖 A profile 架构。Armv9-A Realm Management ExtensionThe Armv9-A Realm Management Extension (RME) extends the concepts supported byTrustZone. This guide does not cover RME, but you can find more information inthe Realm Management Extension Guide. Armv9-A 的 Realm 管理扩展（RME）扩展了 TrustZone 支持的相关概念。本指南不涉及RME，但你可以在《Realm 管理扩展指南》中获取更多信息。TrustZone in the processorIn this topic, we discuss support for TrustZone within the processor. Othersections cover support in the memory system and the software story that is builton the processor and memory system support. 在本主题中，我们将讨论处理器对 TrustZone 的支持。其他章节将介绍内存系统中的支持，以及基于处理器和内存系统支持构建的软件方案。Security StatesIn the Arm architecture, there are two Security states: Secure and Non-secure.These Security states map onto the Trusted and Normal worlds that we referred toin What is TrustZone? 在 Arm 架构中，存在两种安全状态：安全（Secure）和非安全（Non-secure）。这些安全状态分别对应于我们在“什么是 TrustZone？”中提到的受信任世界（Trusted world）和普通世界（Normal world）。 Note In Armv9-A, if the Realm Management Extension (RME) is implemented, then thereare two extra Security states. This guide does not cover the change introducedby RME, for more information on RME, see Realm Management Extension Guide. 在 Armv9-A 架构中，如果实现了 Realm 管理扩展（RME），那么还会有两个额外的安全状态。本指南不涉及 RME 所带来的变化，关于 RME 的更多信息，请参阅《Realm 管理扩展指南》。 At EL0, EL1, and EL2 the processor can be in either Secure state or Non-securestate, which is controlled by the SCR_EL3.NS bit. You often see this written as: 在 EL0、EL1 和 EL2 级别，处理器可以处于安全状态（Secure state）或非安全状态（Non-secure state），这一状态由 SCR_EL3.NS 位控制。你经常会看到这样的表述： NS.EL1: Non-secure state, Exception level 1 S.EL1: Secure state, Exception level 1EL3 is always in Secure state, regardless of the value of the SCR_EL3.NS bit.The arrangement of Security states and Exception levels is shown here: 无论 SCR_EL3.NS 位的取值如何，EL3 总是处于安全状态（Secure state）。安全状态和异常级别的对应关系如下图所示：Figure 1. Non-secure and Secure state Note Support for Secure EL2 was first introduced in Armv8.4 - A and support remainsoptional in Armv8-A.Switching between Security statesIf the processor is in NS.EL1 and software wants to move into S.EL1, how does itdo this? 如果处理器当前处于 NS.EL1，且软件希望切换到 S.EL1，该如何实现？To change Security state, in either direction, execution must pass through EL3,as shown in the following diagram: 无论是从非安全状态切换到安全状态，还是反向切换，都必须经过 EL3。如下图所示，只有通过 EL3，才能改变安全状态。 Figure 1. Change security stateThe preceding diagram shows an example sequence of the steps that are involvedin moving between Security states. Taking these one step at a time: 上面的图展示了在不同安全状态之间切换时涉及的步骤序列，下面我们一步一步来看： Entering a higher Exception level requires an exception. Typically, thisexception would be an FIQ or an SMC (Secure Monitor Call) exception. We look atinterrupt handling and SMCs in more detail later. EL3 is entered at the appropriate exception vector. Software that is running inEL3 toggles the SCR_EL3.NS bit. An exception return then takes the processor from EL3 to S.EL1. 进入更高的异常级别需要触发一个异常。通常，这个异常会是 FIQ 或 SMC（安全监控调用，Secure Monitor Call）异常。我们将在后文更详细地介绍中断处理和 SMC。 处理器通过相应的异常向量进入 EL3。在 EL3 运行的软件会切换 SCR_EL3.NS 位。 异常返回后，处理器从 EL3 进入 S.EL1。 There is more to changing Security state than just moving between the Exceptionlevels and changing the SCR_EL3.NS bit. We also must consider processor state. 实际上，切换安全状态不仅仅是切换异常级别和更改 SCR_EL3.NS 位，还需要考虑处理器的状态。There is only one copy of the vector registers, the general-purpose registers,and most System registers. When moving between Security states it is theresponsibility of software, not hardware, to save and restore register state. Byconvention, the piece of software that does this is called the Secure Monitor.This makes our earlier example look more like what you can see in the followingdiagram: 向量寄存器、通用寄存器以及大多数系统寄存器都只有一份。当在安全状态之间切换时，保存和恢复寄存器状态是软件的责任，而不是硬件的责任。按照惯例，负责这一工作的软件被称为安全监控器（Secure Monitor）。这样，我们之前的例子实际上更接近下图所示的流程。Figure 2. Secure MonitorTrusted Firmware, an open-source project that Arm sponsors, provides a referenceimplementation of a Secure Monitor. We will discuss Trusted Firmware later inthe guide. Trusted Firmware 是 Arm 赞助的一个开源项目，提供了安全监控器（Secure Monitor）的参考实现。我们将在本指南后面讨论 Trusted Firmware。A small number of registers are banked by Security state. This means that thereare two copies of the register, and the core automatically uses the copy thatbelongs to the current Security state. These registers are limited to the onesfor which the processor needs to know both settings at all times. An example isICC_BPR1_EL1, a GIC register that is used to control interrupt preemption.Banking is the exception, not the rule, and will be explicitly called out in theArchitecture Reference Manual for your processor. 只有少量寄存器会根据安全状态进行分组（banked）。这意味着这些寄存器有两份拷贝，处理器会自动使用当前安全状态对应的那一份。这类寄存器仅限于处理器需要始终同时知道两种设置的情况。例如，ICC_BPR1_EL1 是一个 GIC 寄存器，用于控制中断抢占。寄存器分组是特例而不是常规做法，并且会在你的处理器的架构参考手册中明确说明。When a System register is banked, we use (S) and (NS) to identify which copy weare referring to. For example, 当系统寄存器是分组（banked）时，我们会用 (S) 和 (NS) 来标识我们指的是哪一份。例如：ICC_BPR1_ EL1 (S) and ICC_BPR1_EL1 (NS). NOTE In Armv6 and Armv7 - A most System registers are banked by Security state, butgeneral- purpose registers and vector registers are still common. 在 Armv6 和 Armv7-A 架构中，大多数系统寄存器会根据安全状态进行分组（banked），但通用寄存器和向量寄存器仍然是共用的。 Virtual address spacesThe memory management guide in this series introduced the idea of multiplevirtual address spaces, or translation regimes. For example, there is atranslation regime for EL0/1 and a separate translation regime for EL2, shownhere: regime : 制度；规则 本系列的内存管理指南介绍了多虚拟地址空间（multiple virtual address spaces）或称为转换机制（translation regimes）的概念。例如，EL0/1 有一种转换机制，EL2 则有单独的转换机制，如下所示：Figure 1. Virtual address spacesThere are also separate translation regimes for the Secure and Non-securestates. For example, there is a Secure EL0/1 translation regime and Non-secureEL0/1 translation regime, which is shown here: 安全状态（Secure）和非安全状态（Non-secure）也分别拥有独立的转换机制。例如，存在安全 EL0/1 的转换机制和非安全 EL0/1 的转换机制，如下所示：Figure 2. Secure EL0/1 translation regime and Non-secure EL0/1 translation regime图2. 安全 EL0/1 转换机制与非安全 EL0/1 转换机制When writing addresses, it is convention to use prefixes to identify whichtranslation regime is being referred to: 在书写地址时，通常使用前缀来标识所指的转换机制： NS.EL1:0x8000 - Virtual address 0x8000 in the Non-secure EL0/1 translation regime S.EL1:0x8000 - Virtual address 0x8000 in the Secure EL0/1 translation regimeIt is important to note that S.EL1:0x8000 and NS.EL1:0x8000 are two differentand independent virtual addresses. The processor does not use a NS.EL1translation while in Secure state, or a S.EL1 translation while in Non-securestate. 需要注意的是，S.EL1:0x8000 和 NS.EL1:0x8000 是两个不同且独立的虚拟地址。处理器在安全状态下不会使用 NS.EL1 的转换机制，在非安全状态下也不会使用 S.EL1 的转换机制。Physical address spacesIn addition to two Security states, the architecture provides two physicaladdress spaces: Secure and Non-secure. 除了两种安全状态之外，Arm 架构还提供了两种物理地址空间：安全（Secure）和非安全（Non-secure）。While in Non-secure state, virtual addresses always translate to Non-securephysical addresses. This means that software in Non-secure state can only seeNon-secure resources, but can never see Secure resources. This is illustratedhere: 在非安全状态下，虚拟地址总是被转换为非安全物理地址。这意味着，处于非安全状态的软件只能访问非安全资源，无法访问安全资源。如下图所示Figure 1. Physical address spacesWhile in Secure state, software can access both the Secure and Non-securephysical address spaces. The NS bit in the translation table entries controlswhich physical address space a block or page of virtual memory translates to, asshown in the following diagram: 在安全状态下，软件可以访问安全和非安全两种物理地址空间。转换表项中的 NS 位用于控制虚拟内存的某个块或页应被转换到哪个物理地址空间，如下图所示：Figure 2. NS bit Note In Secure state, when the Stage 1 MMU is disabled all addresses are treated asSecure. 在安全状态下，如果一级 MMU 被禁用，所有地址都会被视为安全地址。 相当于在实模式下, 用物理地址访问，所有的地址都被看作安全地址 Like with virtual addresses, typically prefixes are used to identify whichaddress space is being referred to. For physical addresses, these prefixes areNP: and SP:. For example: 与虚拟地址类似，物理地址通常也使用前缀来标识所指的地址空间。对于物理地址，这些前缀是 NP: 和 SP:。例如： NP:0x8000 – Address 0x8000 in the Non-secure physical address space SP:0x8000 – Address 0x8000 in the Secure physical address spaceIt is important to remember that Secure and Non-secure are different addressspaces, not just an attribute like readable or writable. This means that NP:0x8000 and SP:0x8000 in the preceding example are different memory locations andare treated as different memory locations by the processor. 需要注意的是，安全（Secure）和非安全（Non-secure）是不同的地址空间，而不仅仅是类似“可读”或“可写”的一种属性。这意味着在前面的例子中，NP:0x8000 和 SP:0x8000是两个不同的内存位置，处理器也会将它们视为不同的内存位置。 Note It can helpful to think of the address space as an extra address bit on thebus. 可以将地址空间理解为总线上的一个额外地址位。 If the Armv9-A Realm Management Extension (RME) is implemented, the number ofphysical address spaces increases to four. The extra physical address spaces areRoot and Realm. Software running in Secure state can still only access theNon-secure and Secure physical address spaces. For more information on RME, seeRealm Management Extension Guide. 如果实现了 Armv9-A 的 Realm 管理扩展（RME），物理地址空间的数量会增加到四个。新增的物理地址空间是 Root 和 Realm。运行在安全状态下的软件仍然只能访问非安全和安全物理地址空间。关于 RME 的更多信息，请参阅《Realm 管理扩展指南》。Data, instruction, and unified cachesIn the Arm architecture, data caches are physically tagged. The physical addressincludes which address space the line is from, shown here: 在 Arm 架构中，数据缓存是按物理方式标记的。物理地址中包含了该缓存行所属的地址空间，如下所示：Figure 1. Data-cachesA cache lookup on NP:0x800000 never hits on a cache line that is tagged with SP:0x800000. This is because NP:0x800000 and SP:0x800000 are different addresses. 对 NP:0x800000 进行缓存查找时，永远不会命中标记为 SP:0x800000 的缓存行。这是因为 NP:0x800000 和 SP:0x800000 是不同的地址。This also affects cache maintenance operations. Consider the example data cachein the preceding diagram. If the virtual address va1 maps to physical address0x800000, what happens when software issues DC IVAC, va1 (Data or unified Cacheline Invalidate by Virtual Address) from Non-secure state? 这同样会影响缓存维护操作。以前面图中的数据缓存为例，如果虚拟地址 va1 映射到物理地址 0x800000，当软件在非安全状态下执行 DC IVAC, va1（按虚拟地址失效数据或统一缓存行）时，会发生什么？The answer is that in Non-secure state, all virtual addresses translate toNon-secure physical addresses. Therefore, va1 maps to NP:0x800000. The cacheonly operates on the line containing the specified address, in this case NP:0x800000. The line containing SP:0x800000 is unaffected. 案是，在非安全状态下，所有虚拟地址都会转换为非安全物理地址。因此，va1 映射到NP:0x800000。缓存只会对包含指定地址的缓存行进行操作，在本例中即 NP:0x800000。包含 SP:0x800000 的缓存行不会受到影响。Check your knowledgeIf we performed the same operation from Secure state, with va1 still mapping toNP:0x800000, which caches lines are affected? 如果我们在安全状态下执行相同的操作，va1 仍然映射到 NP:0x800000，那么哪些缓存行会受到影响？Like in the earlier example, the cache invalidates the line containing thespecified physical address, NP:0x800000. The fact that the operation came fromSecure state does not matter. 和前面的例子一样，缓存会失效包含指定物理地址 NP:0x800000 的那一行。操作来自安全状态这一事实并不会影响结果。Is it possible to perform a cache operation by virtual address from Non-securetargeting a Secure line? 是否可以在非安全状态下，通过虚拟地址对安全缓存行执行缓存操作？No. In Non-secure state, virtual addresses can only ever map to Non-securephysical addresses. By definition, a cache operation by VA from Non-secure statecan only ever target Non-secure lines. 不可以。在非安全状态下，虚拟地址只能映射到非安全物理地址。根据定义，通过虚拟地址在非安全状态下进行的缓存操作只能作用于非安全缓存行。For set/way operations, for example DC ISW, Xt, operations that are issued inNon-secure state will only affect lines containing Non-secure addresses. FromSecure state set/way operations affect lines containing both Secure andNon-secure addresses. 对于组/路操作（如 DC ISW, Xt），在非安全状态下发起的操作只会影响包含非安全地址的缓存行。而在安全状态下，组/路操作会影响包含安全和非安全地址的缓存行。This means that software can completely invalidate or clean the entire cacheonly in Secure state. From Non-secure state, software can only clean orinvalidate Non-secure data. 这意味着，只有在安全状态下，软件才能完全失效或清除整个缓存；在非安全状态下，软件只能清除或失效非安全数据。Translation Look aside BufferTranslation Look aside Buffer (TLBs) cache recently used translations. Theprocessor has multiple independent translation regimes. The TLB records whichtranslation regime, including the Security state, an entry represents. While thestructure of TLBs is implementation defined, the following diagram shows anexample: 转换后备缓冲区（Translation Lookaside Buffer，TLB）用于缓存最近使用的地址转换。处理器拥有多个独立的转换机制。TLB 会记录每个条目所对应的转换机制，包括安全状态（Security state）。虽然 TLB 的具体结构由实现决定，但下图展示了一个示例：Figure 1. Translation Lookaside Buffer (TLBs)When software issues a TLB invalidate operation (TLBI instruction) at EL1 or EL2,the software targets the current Security state. Therefore, TLBI ALLE1 fromSecure state invalidates all cached entries for the S.EL0/1 translation regime. 当软件在 EL1 或 EL2 层级下发起 TLB 失效操作（TLBI 指令）时，操作对象是当前的安全状态。因此，在安全状态下执行 TLBI ALLE1，会使 S.EL0/1 转换机制下所有缓存的条目失效。EL3 is a special case. As covered earlier in Security states, when in EL0/1/2the SCR_EL3.NS bit controls which Security state the processor is in. However,EL3 is always in Secure state, regardless of the SCR_EL3.NS bit. When in EL3,SCR_EL3.NS lets software control which Security state TLBIs operate on. EL3 是一个特殊情况。如前文所述，在 EL0/1/2 时，SCR_EL3.NS 位用于控制处理器所处的安全状态。然而，无论 SCR_EL3.NS 位的值如何，EL3 始终处于安全状态。当处于 EL3时，SCR_EL3.NS 位允许软件控制 TLB 失效操作作用于哪个安全状态。For example, executing TBLI ALLE1 at EL3 with: SCR_EL3.NS==0: Affects Secure EL0/1 translation regime SCR_EL3.NS==1: Affects Non-secure EL0/1 translation regimeSMC exceptionsAs part of the support for two Security states, the architecture includes theSecure Monitor Call (SMC) instruction. Executing SMC causes a Secure MonitorCall exception, which targets EL3. 作为对两种安全状态支持的一部分，Arm 架构引入了安全监控调用（Secure MonitorCall，SMC）指令。执行 SMC 会触发一个安全监控调用异常，该异常会进入 EL3 层级。SMC’s are normally used to request services, either from firmware resident inEL3 or from a service that is hosted by the Trusted Execution Environment. TheSMC is initially taken to EL3, where an SMC dispatcher determines which entitythe call will be handled by. This is shown in the following diagram: SMC 通常用于请求服务，这些服务可能由驻留在 EL3 的固件提供，也可能由受信任执行环境（Trusted Execution Environment）中的服务提供。SMC 指令首先会进入 EL3，在那里由 SMC 分发器（dispatcher）决定由哪个实体处理该调用。如下图所示：Figure 1. SMC dispatcherIn a bid to standardize interfaces, Arm provides the SMC Calling Convention(DEN0028) and Power State Coordination Interface Platform Design Document(DEN0022). These specifications lay out how SMCs are used to request services. 为了规范接口，Arm 提供了 SMC 调用约定（SMC Calling Convention，DEN0028）和电源状态协调接口平台设计文档（Power State Coordination Interface Platform DesignDocument，DEN0022）。这些规范详细说明了如何通过 SMC 请求服务。Execution of an SMC at EL1 can be trapped to EL2. This is useful for hypervisors,because hypervisors might want to emulate the firmware interface that is seen bya virtual machine. 在 EL1 层级执行 SMC 指令时，可以被捕获到 EL2。这对于虚拟机管理器（hypervisor）非常有用，因为 hypervisor 可能希望模拟虚拟机所看到的固件接口。 Note The SMC instruction is not available at EL0 in either Security state. Wediscuss exceptions later in Interrupts when we look at the interruptcontroller. 在任一安全状态下，EL0 都无法使用 SMC 指令。关于异常的内容，我们将在后续“中断”部分讨论中断控制器时再进行介绍。 Secure virtualizationWhen virtualization was first introduced in Armv7-A, it was only added in theNon-secure state. Until Armv8.3, the same was true for Armv8 as illustrated inthe following diagram: 在 Armv7-A 首次引入虚拟化时，虚拟化功能仅在非安全状态下实现。直到 Armv8.3 之前，Armv8 也是如此，如下图所示：Figure 1. Secure virtualizationAs previously described in Switching between Security states, EL3 is used tohost firmware and the Secure Monitor. Secure EL0/1 host the Trusted ExecutionEnvironment (TEE), which is made up of the Trusted services and kernel. 如前文《在安全状态之间切换》中所述，EL3 用于承载固件和安全监控器（SecureMonitor）。安全 EL0/1 运行受信任执行环境（TEE），该环境由受信任服务和内核组成。There was no perceived kkneed for multiple virtual machines in Secure state.This means that support for virtualization was not necessary. As TrustZoneadoption increased, several requirements became apparent: 最初，人们认为在安全状态下不需要多个虚拟机。这意味着不需要为安全状态提供虚拟化支持。随着 TrustZone 的广泛应用，几个新需求逐渐显现：Some trusted services were tied to specific trusted kernels. For a device tosupport multiple services, it might need to run multiple trusted kernels.Following the principle of running with least privilege, moving some of thefirmware functionality out of EL3 was required. The solution was to introducesupport for EL2 in Secure state, which came with Armv8.4-A, as you can see inthis diagram: 一些受信任服务与特定的受信任内核绑定。为了让设备支持多个服务，可能需要运行多个受信任内核。遵循最小权限原则，需要将部分固件功能从 EL3 移出。为了解决这些需求，Armv8.4-A 在安全状态下引入了对 EL2 的支持，如下图所示：Figure 2. Support for EL2 in Secure stateRather than a full hypervisor, S.EL2 typically hosts a Secure Partition Manager(SPM). An SPM allows the creation of the isolated partitions, which are unableto see the resources of other partitions. A system could have multiplepartitions containing Trusted kernels and their Trusted services. 安全 EL2（S.EL2）通常不会运行完整的虚拟机管理器（hypervisor），而是承载安全分区管理器（Secure Partition Manager，SPM）。SPM 允许创建隔离的分区，每个分区无法访问其他分区的资源。这样，系统可以拥有多个分区，每个分区都包含受信任内核及其受信任服务。A partition can also be created to house platform firmware, removing the need tohave that code that is run at EL3. 也可以创建一个分区来容纳平台固件，这样就不再需要让这些代码在 EL3 级别运行。 Enabling Secure EL2 When S.EL2 is supported, it can be enabled or disabled. Whether S.EL2 isenabled is controlled by the SCR_EL3.EEL2 bit: 当支持 S.EL2 时，它可以被使能或禁用。S.EL2 是否启用由 SCR_EL3.EEL2 位控制： 0: S.EL2 disabled, behavior is as on a processor not supporting S.EL2 1: S.EL2 enabled Stage 2 translation in Secure state In Secure state, the Stage 1 translation of the Virtual Machine (VM) can outputboth Secure and Non-secure addresses and is controlled by the NS bit in thetranslation table descriptors. This results in two IPA spaces, Secure andNon-secure, each with its own set of Stage 2 translation tables as you can seein the following diagram: 在安全状态下，虚拟机（VM）的一级地址转换（Stage 1 translation）可以输出安全和非安全地址，这由转换表描述符中的 NS 位进行控制。这样就产生了两个中间物理地址（IPA）空间：安全和非安全，每个空间都有自己的一套二级转换表（Stage 2translation tables），如下图所示：Figure 3. Stage 2 translation in Secure state Unlike the Stage 1 tables, there is no NS bit in the Stage 2 table entries. Fora given IPA space, all the translations either result in a Secure or Non-securephysical address, which is controlled by a register bit. The Non-secure IPAstranslate to Non-secure PAs and the Secure IPAs translate to Secure PAs. 与一级转换表不同，二级转换表（Stage 2 table）项中没有 NS 位。对于某个特定的IPA 空间，所有的地址转换结果要么都是安全物理地址，要么都是非安全物理地址，这由一个寄存器位进行控制。非安全 IPA 会被转换为非安全物理地址，安全 IPA 会被转换为安全物理地址。System architectureSo far in this guide, we have concentrated on the processor, but TrustZone ismuch more than just a set of processor features. To take advantage of theTrustZone features, we need support in the rest of the system as well. 到目前为止，本指南主要关注的是处理器，但 TrustZone 远不止是一组处理器特性。要充分利用 TrustZone 的功能，系统的其他部分也需要相应的支持。Here is an example of a TrustZone-enabled system: 下面是一个支持 TrustZone 的系统示例：Figure 1. System architectureCompleters: peripherals, and memoriesEarlier in Physical address spaces we introduced the idea of two physicaladdress spaces, Secure and Non-secure. The processor exports the address spacethat is being accessed to the memory system. The memory system uses thisinformation to enforce the isolation. 在前文“物理地址空间”部分，我们介绍了安全（Secure）和非安全（Non-secure）两种物理地址空间的概念。处理器会将当前访问的地址空间类型传递给内存系统，内存系统则利用这些信息来实现隔离。In this topic, we refer to bus Secure and bus Non-secure. Bus Secure means a busaccess to the Secure physical address space. Bus Non-secure means a bus accessto the Non-secure physical address space. Remember that in Secure state softwarecan access both physical address spaces. This means that the security of the busaccess is not necessarily the same as the Security state of the processor thatgenerated that access. 在本节中，我们提到总线安全（bus Secure）和总线非安全（bus Non-secure）。总线安全指的是对安全物理地址空间的总线访问，总线非安全则指对非安全物理地址空间的总线访问。请记住，在安全状态下，软件可以访问两种物理地址空间。这意味着总线访问的安全属性不一定与发起访问的处理器的安全状态一致。 Note In AMBA AXI and ACE, the AxPROT[1] signal is used to specify which addressspace is being accessed. Like with the NS bit in the translation tables, 0indicates Secure and 1 indicates Non-secure. 在 AMBA AXI 和 ACE 总线协议中，AxPROT[1] 信号用于指定当前访问的是哪个物理地址空间。类似于转换表中的 NS 位，值为 0 表示安全，值为 1 表示非安全。 In theory, a system could have two entirely separate memory systems, using theaccessed physical address space (AxPROT) to select between them. In practicethis is unlikely. Instead, systems use the physical address space like anattribute, controlling access to different devices in the memory system. 理论上，系统可以有两套完全独立的内存系统，通过访问的物理地址空间（AxPROT）来选择使用哪一套。但实际中，这种做法并不常见。通常，系统会将物理地址空间作为一种属性，用于控制对内存系统中不同设备的访问权限。In general, we can talk about two types of memories and peripherals, and buscompleters: 一般来说，我们可以将内存、外设和总线终端（completer）分为两类： TrustZone aware This is a device that is built with some knowledge of TrustZone and uses thesecurity of the access internally. An example is the Generic Interrupt Controller (GIC). The GIC is accessed bysoftware in both Secure and Non-secure state. Non-secure accesses are onlyable to see Non-secure interrupts. Secure accesses can see all interrupts. TheGIC implements uses the security of the bus transaction to determine whichview to present. 这类设备具备 TrustZone 相关的设计，能够在内部利用访问的安全属性。 一个例子是通用中断控制器（Generic Interrupt Controller, GIC）。GIC 可以被安全和非安全状态下的软件访问。非安全访问只能看到非安全中断，安全访问则可以看到所有中断。GIC 利用总线事务的安全属性来决定展示哪种视图。 Non-TrustZone aware This represents most completers in a typical system. The device does not usethe security of the bus access internally. 这类设备占据了大多数典型系统中的终端。它们在内部不会利用总线访问的安全属性。 An example is a simple peripheral like a timer, or an on-chip memory. Eachwould be either Secure or Non-secure, but not both. 例如一个简单的外设（如定时器）或片上内存（on-chip memory），它们要么是安全的，要么是非安全的，但不会同时支持两种状态。 Enforcing isolationTrustZone is sometimes referred to as a completer-enforced protection system.The requester signals the security of its access and the memory system decideswhether to allow the access. How is the memory system-based checking done? TrustZone 有时被称为“由终端强制保护的系统”（completer-enforced protectionsystem）。请求方会发出其访问的安全属性信号，内存系统则决定是否允许该访问。那么，内存系统是如何进行检查的呢？In most modern systems, the memory system-based checking is done by theinterconnect. For example, the Arm NIC-400 allows system designers to specifyfor each connected completer: 在现代系统中，内存系统的检查通常由互连（interconnect）完成。例如，Arm 的NIC-400 互连允许系统设计者为每个连接的终端（completer）指定如下类型： Secure Only Secure accesses are passed to device. Interconnect generates a fault forall Non-secure accesses, without the access being presented to the device. 只有安全访问会被传递到设备。互连会对所有非安全访问直接产生错误（fault），而不会将该访问传递给设备。 Non-secure Only Non-secure accesses are passed to device. Interconnect generates a faultfor all Secure accesses, without the access being presented to the device. 只有非安全访问会被传递到设备。互连会对所有安全访问直接产生错误，而不会将该访问传递给设备。 Boot time configurable At boot time, system initialization software can program the device as Secureor Non-secure. The default is Secure. 在系统启动时，初始化软件可以将设备配置为安全或非安全。默认配置为安全。 TrustZone aware The interconnect allows all accesses through. The connected device mustimplement isolation. 互连允许所有访问通过。连接的设备自身必须实现隔离机制。 For example:Figure 1. Implement isolation NOTE 这相当于将整个的ram device划分为 Trusted or unTrusted, 然后相应属性的访问只能访问对应属性的targetThis approach works well for either TrustZone-aware devices or those devicesthat live entirely within one address space. For larger memories, like off-chipDDR, we might want to partition the memory into Secure and Non-secure regions. ATrustZone Address Space Controller (TZASC) allows us to do this, as you can seein the following diagram: 这种方法对于支持 TrustZone 的设备，或者完全属于某一个地址空间的设备来说效果很好。但对于更大的内存，比如片外 DDR，我们可能希望将内存划分为安全和非安全区域。TrustZone 地址空间控制器（TrustZone Address Space Controller，TZASC）可以帮助我们实现这一目标，如下图所示：Figure 2. Partition memoryThe TZASC is similar to a Memory Protection Unit (MPU), and allows the addressspace of a device to split into several regions. With each region specified asSecure or Non-secure. The registers to control the TZASC are Secure access only,permitting only Secure software to partition memory. TZASC 类似于内存保护单元（Memory Protection Unit，MPU），允许将设备的地址空间划分为多个区域，每个区域都可以被指定为安全或非安全。用于控制 TZASC 的寄存器只能通过安全访问进行操作，这样只有安全软件才能对内存进行分区。An example of a TZASC is the Arm TZC-400, which supports up to nine regions. Note Off-chip memory is less Secure than on-chip memory, because it is easier foran attacker to read or modify its contents. On-chip memories are more securebut are much more expensive and of limited size. As always, we must balancecost, usability, and security. Be careful when deciding which assets you wantin off-chip memories and which assets need to be kept on-chip. 片外内存（off-chip memory）比片上内存（on-chip memory）安全性更低，因为攻击者更容易读取或篡改其内容。片上内存更加安全，但成本更高且容量有限。和往常一样，我们需要在成本、可用性和安全性之间进行权衡。在决定哪些资产存放在片外内存、哪些资产需要留在片上内存时要格外小心。 When the Armv9-A Realm Management Extension (RME) is implemented, memory can bydynamically moved between physical address spaces via the Granule ProtectionTable. For more information, see Introducing Arm’s Dynamic TrustZone technologyblog. 当实现了 Armv9-A 的 Realm 管理扩展（RME）后，可以通过颗粒保护表（GranuleProtection Table, GPT）在不同物理地址空间之间动态移动内存。更多信息请参见《Introducing Arm’s Dynamic TrustZone technology》博客。Bus requestersNext, we will look at the bus requesters in the system, as you can see in thefollowing diagram:Figure 1. Bus requesters in the systemThe A-profile processors in the system are TrustZone aware and send the correctsecurity status with each bus access. However, most modern SoCs also containnon-processor bus requesters, for example, GPUs and DMA controllers. 系统中的 A-profile 处理器是支持 TrustZone 的，并且每次总线访问都会携带正确的安全状态。然而，大多数现代 SoC 还包含非处理器类型的总线请求方，例如 GPU 和 DMA控制器。Like with completer devices, we can roughly divide the requester devices in thesystem into groups: TrustZone aware Some requesters are TrustZone aware, and like the processor, provide theappropriate security information with each bus access. Examples of thisinclude System MMUs (SMMUs) that are built to the Arm SMMUv3 specification. 有些请求方是支持 TrustZone 的，就像处理器一样，每次总线访问都会提供相应的安全信息。例如，符合 Arm SMMUv3 规范的系统 MMU（SMMU）就是这样的请求方。 Non-TrustZone aware Not all requesters are built with TrustZone awareness, particularly whenreusing legacy IP. Such requesters typically provide no security informationwith its bus accesses, or always send the same value. 并非所有的请求方都具备 TrustZone 感知能力，尤其是在复用旧版 IP 时。这类请求方通常不会在总线访问中提供安全信息，或者始终发送同一个固定值。 What system resources do non-TrustZone-aware requesters need to access? Basedon the answer to this question, we could pick one of several approaches: 非 TrustZone 感知请求方需要访问哪些系统资源？根据这个问题的答案，我们可以选择以下几种方法之一： Design time tie-off Where the requester only needs to access a single physical address space, asystem designer can fix the address spaces to which it has access, by tyingoff the appropriate signal. This solution is simple, but is not flexible. 如果请求方只需要访问单一的物理地址空间，系统设计者可以通过拉高（tie-off）相关信号，将其访问权限固定到某个地址空间。这种方案简单，但灵活性较差。 Configurable logic Logic is provided to add the security information to the requester’s busaccesses. Some interconnects, like the Arm NIC-400, provide registers thatSecure software can use at boot time to set the security of an attachedrequester accesses. This overrides whatever value the requester provideditself. This approach still only allows the requester to access a singlephysical address space but is more flexible than a tie-off. 可以通过逻辑电路为请求方的总线访问添加安全信息。有些互连（如 Arm NIC-400）提供寄存器，安全软件可以在启动时配置，设置某个连接请求方的访问安全属性，从而覆盖请求方自身提供的值。该方法仍然只能让请求方访问单一物理地址空间，但比设计时固定更灵活。 SMMU A more flexible option is an SMMU. For a trusted requester, the SMMU behaveslike the MMU in Secure state. This includes the NS bit in the translationtable entries, controlling which physical address space is accessed. 更灵活的方案是使用 SMMU（系统内存管理单元）。对于受信任的请求方，SMMU 的行为类似于安全状态下的 MMU，包括转换表项中的 NS 位，用于控制访问哪个物理地址空间。 NOTE (Bus requester information) &lt;&lt;AMBA AXI and ACE Protocol Specificatio&gt;&gt; Section A4.7 Access permissions中讲解了AXI总线上的事物的access attr, 其中包括secure mode相关属性, 如下 AXI provides access permissions signals that can be used to protect againstillegal transactions: ARPROT[2:0] defines the access permissions for read accesses AWPROT[2:0] defines the access permissions for write accesses. The term AxPROT refers collectively to the ARPROT and AWPROT signals. AxPROT[1]表示该属性. 那如果按照, 前面的知识来看, 在CPU 侧，是通过EL3 切换SCR_EL3.NS 来决定eret 后的世界是secure world or Non-secure world, 从而决定最终MMU(可能时MMU) 向AXI发送的AxPROT[1] 是什么值. 那像上文说的SMMU 是怎么控制的呢? 难道我们通过修改SMMU pgtable就可以让device访问secure world? 那么控制SMMU的软件是secure world还是Non-secure world，还是smmu像带有 TZASC 的mem一样，也分secure/Non-secure world resource, 只让CPU对应的secure mode来处理? M and R profile Arm processors(略)InterruptsNext, we will look at the interrupts in the system, as you can see in thefollowing diagram: 接下来，我们将查看系统中的总线请求者，如下图所示：Figure 1. Interrupts in the systemThe Generic Interrupt Controller (GIC), supports TrustZone. Each interruptsource, called an INTID in the GIC specification, is assigned to one of threeGroups: 通用中断控制器（GIC）支持 TrustZone。每个中断源（在 GIC 规范中称为 INTID）被分配到三个组中的一个： Group 0: Secure interrupt, signaled as FIQ Secure Group 1: Secure interrupt, signaled as IRQ or FIQ Non-secure Group 1: Non-secure interrupt, signaled as IRQ or FIQ 组 0：安全中断，以 FIQ 方式信号通知 安全组 1：安全中断，以 IRQ 或 FIQ 方式信号通知 非安全组 1：非安全中断，以 IRQ 或 FIQ 方式信号通知 This is controlled by software writing to the GIC[D|R]_IGROUPR&lt;n&gt; and GIC[D|R]_IGRPMODR&lt;n&gt;registers, which can only be done from Secure state. The allocationis not static. Software can update the allocations at run-time. 这个分组通过软件写入 GIC[D|R]_IGROUPR 和 GIC[D|R]_IGRPMODR 寄存器来控制，且只能在安全状态下进行。分配不是静态的，软件可以在运行时动态更新分配。For INTIDs that are configured as Secure, only bus Secure accesses can modifystate and configuration. Register fields corresponding to Secure interrupts areread as 0s to Non-secure bus accesses. 对于被配置为安全的 INTID，只有安全总线访问才能修改其状态和配置。对应安全中断的寄存器字段，对于非安全总线访问会读取为 0。For INTIDs that are configured as Non-secure, both Secure and Non-secure busaccesses can modify state and configuration. 对于被配置为非安全的 INTID，安全和非安全总线访问都可以修改其状态和配置。Why are there two Secure Groups? Typically, Group 0 is used for interrupts thatare handled by the EL3 firmware. These relate to low-level system managementfunctions. Secure Group 1 is used for all the other Secure interrupt sources andis typically handled by the S.EL1 or S.EL2 software. 为什么会有两个安全组？通常，组 0 用于由 EL3 固件处理的中断，这些中断与底层系统管理功能相关。安全组 1 用于所有其他安全中断源，通常由 S.EL1 或 S.EL2 软件处理。Handling interruptsThe processor has two interrupt exceptions, IRQ and FIQ. When an interruptbecomes pending, the GIC uses different interrupt signals depending on the groupof the interrupt and the current Security state of the processor: 处理器有两种中断异常：IRQ 和 FIQ。当有中断挂起时，GIC 会根据中断所属的组以及处理器当前的安全状态，使用不同的中断信号： Group 0 interrupt Always signaled as FIQ exception Secure Group 1 Processor currently in Secure state – IRQ exception Processor currently in Non-secure state – FIQ exception Non-secure Group 1 Processor currently in Secure state – FIQ exception Processor currently in Non-secure state – IRQ exception NOTE 当收到了一个不属于当前 secure-state的 interrupt, 都是FIQ, 否则为IRQ(Group 0 除外，因为Group 0都是FIQ) Group 0 — El3 Secure Group1 – Secure state Non-Secure Group1 – Non-Secure state 其目的是想让对应的Secure state 处理相应的interrupt，但是Secure state &lt;–&gt;Non-secure state的切换需要EL3参与Remember that Group 0 interrupts are typically used for the EL3 firmware. Thismeans that: 请记住，组 0 的中断通常用于 EL3 固件。这意味着： IRQ means a Group 1 interrupt for the current Security state. FIQ means that we need to enter EL3, either to switch Security state or tohave the firmware handle the interrupt. IRQ 表示当前安全状态下的组 1 中断。 FIQ 表示需要进入 EL3，要么是为了切换安全状态，要么是让固件处理该中断。以下示例展示了异常路由控制如何进行配置： The following example shows how the exception routing controls could beconfigured: 以下示例展示了异常路由控制可以如何配置：Figure 1. Exception routing controlsThe preceding diagram shows one possible configuration. Another option that iscommonly seen is for FIQs to be routed to EL1 while in Secure state. The TrustedOS treats the FIQ as a request to yield to either the firmware or to Non-securestate. This approach to routing interrupts gives the Trusted OS the opportunityto be exited in a controlled manor. 前面的图展示了一种可能的配置。另一种常见的选择是在安全状态下将 FIQ 路由到 EL1。此时，受信任操作系统（Trusted OS）会将 FIQ 视为让出执行权给固件或非安全状态的请求。这种中断路由方式，使得受信任操作系统能够以可控的方式退出。 优雅switch, 让trust OS 在退出前，做一些事情参考链接 Learn the architecture - TrustZone for AArch64 ARM CoreLink TZC-400 TrustZone Address Space Controller Technical Reference Manual r0p1 Rate this page:" }, { "title": "virtio notify", "url": "/posts/virtio-notify/", "categories": "virt, io_virt", "tags": "io_virt", "date": "2025-06-09 18:10:00 +0800", "snippet": "virtio feature : VIRTIO_RING_F_EVENT_IDX我们这里抽象下，先定义一个producer, consumer的模型, 将event_idx定义为head，将vring.idx定义为tail, 即consumer modify headproducer modify tail(和vring.idx一样，表示producer下次要存储数据的位置)整个逻辑如下:c...", "content": "virtio feature : VIRTIO_RING_F_EVENT_IDX我们这里抽象下，先定义一个producer, consumer的模型, 将event_idx定义为head，将vring.idx定义为tail, 即consumer modify headproducer modify tail(和vring.idx一样，表示producer下次要存储数据的位置)整个逻辑如下:consumer:while have_notify(): while head &lt; tail: handle data ring[head] head++ STORE headproducer:while get_put_data() as data: put data into ring[tail] t1 = get_tail_last_notify() t2 = tail tail++ if head in [t1, t2): send_notify() set_tail_last_notify(tail)整个的代码逻辑是: producer 其在收到notify后, 观测到head &lt; tail, 就会继续处理循环处理ring中的数据. consumer 假定producer一定满足1, 所以其在更新tail后，会观测, head 是否在本次更新后的tail，和上次tail的范围内[t1, t2) head &lt; t1: 说明 consumer 还在处理(x, t1) 范围内的数据，处理完(x, t1) 范围内的数据后, 按照1的原则，肯定还会继续处理[t1, t2] 范围内的数据。所以, 无需notify. head ∈ [t1, t2): 说明, consumer 在过去某个时刻追上了head, 追上后，又可能因 producer这边tail还没有更新, 已经退出loop，需要等待新的notify 后，才会继续处理. head == t2: 说明: consumer 发动秘术”一日千里”, 在produer执行完tall++后, 执行if head in [t1, t2)之前, 就已经把该ring[t2-1]处理完，并且更新完head-&gt;t2, 十分迅速, 无需notify. head &gt; t2: consumer发动锦囊: “无中生有”, producer可以不跟他玩了。(consumer 有BUG) 从上面流程, 可以看出，双方都需要观测对方更新的数据后, 再继续做判断, 也就是producer consumer(考虑下次的loop)STORE tail STORE headLOAD head LOAD tail像这种store-&gt;load操作即便是在x86-TSO内存模型下，也是允许乱序的:我们举个两个例子，分别来看下producer 和consumer 乱序，所带来的影响初始状态和之后的动作: initial: tail = 1 head = 0 get_tail_last_notify() = 1(说明上次notify后, consumer还没有处理) producer 侧动作: 再次向ring放入一个desc，更新tail producer out of order: producer consumertail(1)++/* but STORE inst * have not COMMIT, * still in write * buffer */if head(0) in [t1(1), t2) DON'T SEND NOTIFY get data ring[head(0)] head(0)++ if head(1) == tail(1) // stale data break_LOOPCOMMIT STORE tail consumer out of order producer consumer get_data_ring[head(0)] head(0)++ /* but STORE inst * have not COMMIT, * still in write * buffer */ if head(1) == tail(1) break_LOOPtail(1)++//head is staleif head(0) in [t1(1), t2) DON'T SEND NOTIFY COMMIT STORE head 最终的状态都是有问题的:tail = 2head = 1no pending notify这里，我们仅以kernel代码为例，查看kenrel 作为guest driver, 作为avail_ring生产者，以及used_ring的消费者，是如何添加memory_barrier的.kernel code avail vring producer @@ -308,9 +308,9 @@ bool virtqueue_kick_prepare(struct virtqueue *_vq) bool needs_kick; START_USE(vq);- /* Descriptors and available array need to be set before we expose the- * new available array entries. */- virtio_wmb(vq);+ /* We need to expose available array entries before checking avail+ * event. */+ virtio_mb(vq); old = vq-&gt;vring.avail-&gt;idx - vq-&gt;num_added; new = vq-&gt;vring.avail-&gt;idx; 该patch来自于: virtio: correct the memory barrier in virtqueue_kick_prepare() 是对ee7cd89(\"virtio: expose added descriptors immediately\")的fix, ee7cd89patch将virtio_mb 修改为virtio_wmb, 但是wmb的作用是保证store-store的顺序，而这里需要保证store-load的顺序, 即STORE(avail_idx)-LOAD(avail_event_idx)之间的顺序. 所以该patch，又改了回来 used vring consumer @@ -324,6 +331,14 @@ void *virtqueue_get_buf(struct virtqueue *_vq, unsigned int *len) ret = vq-&gt;data[i]; detach_buf(vq, i); vq-&gt;last_used_idx++;+ /* If we expect an interrupt for the next entry, tell host+ * by writing event index and flush out the write before+ * the read in the next get_buf call. */+ if (!(vq-&gt;vring.avail-&gt;flags &amp; VRING_AVAIL_F_NO_INTERRUPT)) {+ vring_used_event(&amp;vq-&gt;vring) = vq-&gt;last_used_idx;+ virtio_mb();+ }+ END_USE(vq); return ret; } 可以看到, 在STORE used_event后, 加了一个内存屏障，从而保证STOREused_event_idx - LOAD used_idx之间的顺序. " }, { "title": "一文搞懵IO虚拟化之 -- virtio", "url": "/posts/virtio/", "categories": "virt, io_virt", "tags": "io_virt", "date": "2025-06-05 09:21:00 +0800", "snippet": " overflow virtio: ABSTRACTION API virtio-vring virtqueue ops vring vring struct vring notify sample of handle VirtIO why vir...", "content": " overflow virtio: ABSTRACTION API virtio-vring virtqueue ops vring vring struct vring notify sample of handle VirtIO why virtio is so efficientoverflowvirtio 起源于 2008 年的 virtio: Towards a De-Facto Standard For Virtual I/O Devices该论文1,2, 而其诞生的背景是, Linux 内核作为guest支持高达8种虚拟化系统: Xen KVM VMware 的 VMI IBM 的 System p IBM 的 System z User Mode Linux lguest IBM 的遗留 iSeries而之后，可能还会出现新的系统。每个platform都希望拥有自己的块设备, 网络和控制台驱动程序, 有的时候还需要一个 boutique framebuffer, USB controller, host filesystemand virtual kitchen sink controller…另外，它们中很少有对驱动程序进行任何显著的优化，并且提供了很多重复的但是往往略有不同的功能机。更重要的是，no-one seemsparticularly delighted with their drivers, or having to maintain them.(大家对维护这个都没有热情). 所以，当时需要一个统一标准, 高效的半虚拟化设备来替代它们。而2006年KVM出现后, 需求又更加迫切，因为KVM当时还没有一个虚拟化设备模型。使用模拟设备性能非常受限。Rusty Russell 团队认为可以创建一个公通用，高效能在多种虚拟机和平台运行的virtio IO 机制.最终, 作者设计了两个完整的API: virtio-vring（传输层) NOTE: 往往定制化的传输机制会让自己的通用性更差: 针对某个hyperisor 或者架构 甚至经常为每一种设备单独定制 所以, virtio-vring的实现并不激进或者革命性。 Linux API for virtual I/O devices. device probing 提供feature negotiation来保证 device和driver之间的向前/向后兼容. device configuration virtio: ABSTRACTION API作者设计了一个抽象层: 通用的驱动程序 一系列函数指针函数指针如下://from 6.15.0-rc6struct virtio_config_ops { void (*get)(struct virtio_device *vdev, unsigned offset, void *buf, unsigned len); void (*set)(struct virtio_device *vdev, unsigned offset, const void *buf, unsigned len); u32 (*generation)(struct virtio_device *vdev); u8 (*get_status)(struct virtio_device *vdev); void (*set_status)(struct virtio_device *vdev, u8 status); void (*reset)(struct virtio_device *vdev); u64 (*get_features)(struct virtio_device *vdev); int (*finalize_features)(struct virtio_device *vdev); const char *(*bus_name)(struct virtio_device *vdev); // 老版本代码，下面的成员在virtqueue_ops int (*find_vqs)(struct virtio_device *vdev, unsigned int nvqs, struct virtqueue *vqs[], struct virtqueue_info vqs_info[], struct irq_affinity *desc); void (*del_vqs)(struct virtio_device *); void (*synchronize_cbs)(struct virtio_device *); int (*set_vq_affinity)(struct virtqueue *vq, const struct cpumask *cpu_mask); const struct cpumask *(*get_vq_affinity)(struct virtio_device *vdev, int index); bool (*get_shm_region)(struct virtio_device *vdev, struct virtio_shm_region *region, u8 id); int (*disable_vq_and_reset)(struct virtqueue *vq); int (*enable_vq_after_reset)(struct virtqueue *vq);};最初的驱动, 主要包括以下功能: features: get_features() finalize_features() features bit 举例: 指示网络设备是否支持校验和卸载的 VIRTIO_NET_F_CSUM 特性位。 具体的协商步骤如下: driver 调用 get_features() 获取devices 的feature driver 在上面的集合中选择自己版本支持的features driver call finalize_features() to writeback subset features to devices 如果需要renegotiate 只能reset设备. PCI configuration space: get() set() 配置空间内容和具体的虚拟设备强相关，另外，可能包含一些特定的配置字段,这些配置字段可能受features控制。例如，网络设备如果有VIRTIO_NET_F_MACfeatures bit(host 希望设备有特定的mac地址)，配置空间中才包含该配置字段. PCI configuration space: STATUS bits get_status() set_status() 该字段由guest来表示当前设备的探测状态。例如当达到VIRTIO_CONFIG_S_DEVICE_OK状态时，表示guest已经完成device features probe. 此时host可以评估guest可以支持哪些feature。 devices reset reset() 重置设备: configuration space 和 status. 另外, 当执行reset操作时，缓冲区不应该被覆盖，可以用来尝试在guest中恢复driver。 通过上面的接口设计，做到了configuration API和driver分离，另外和trasnport相关API也是一套独立的接口，所以其三者相互分离的。 原文: The explicit separation of drivers, transport and configura-tion represents a change in thinking from current implemen-tations. virtio-vringvirtqueue ops虽然configuration API 很重要, 但是对性能的关键部分是实际的IO机制。作者将其抽象为virtqueue. 而virtqueue的本质是一个由 driver (guest) produce buffer, 由devices(host) consume buffer的队列。每个buffer 可以由多个只读，或者可读写的离散的数据段组成的数组。virtqueue ops如下:struct virtqueue_ops { int (*add_buf)(struct virtqueue *vq, struct scatterlist sg[], unsigned int out_num, unsigned int in_num, void *data); void (*kick)(struct virtqueue *vq); void *(*get_buf)(struct virtqueue *vq, unsigned int *len); void (*disable_cb)(struct virtqueue *vq); bool (*enable_cb)(struct virtqueue *vq);}; add_buf: add a new buffer to avail queue, 其中data参数是一个token，当buffer已经被consume时返回该值，用来标识该buffer. （是不是有点乱，这和vring的并行consume有关, 下面会讲到) get_buf: gets a used buffer. len 用来指示 driver侧向buffer中填充了多少有效 数据. 而返回值则是返回的add_buf()的data参数(cookie). 上面也提到主要 的原因是 ` buffers are not necessarily used in order` kick: 在缓冲区被加入到队列时，用来notify 对方(host devices). 另外，可以添加 多个buffer后，在发一次kick。（batching) enable_cb,disable_cb: 启用禁用callback. disable_cb 这相当于禁用中断。driver 会为每一个virtiqueue注册一个callback, 而这些会在唤醒服务线程之前禁用掉这个回调(???啥服务线程1,todo). 从而减少vmm和guest的交互。 而enable_cb则表示开启中断（启用回调), 通常会driver处理完队列中的所有的待处理的请求后调用。(used queue) VRINGvring struct介绍完相关的API之后，我们来看下用于transport 具体的数据结构. 该数据结构分为三部分: descriptor array: 管理所有的descriptor avail ring: guest driver 用来指示哪些desc 已经准备好了，可以被 host device 使用 used ring: host device 用来指示哪些 desc 已经被used, 可以被 guest driver 获取数据，然后free.我们结合 virtio-pci configuration space，来看下vring在configuration space的哪个地方配置，还有其具体的数据结构: 配置空间中的comm configuration cap 中指示了virtio_pci_comm_cfg 结构在BAR空间中的位置和offset, 该数据结构用来指示每个virtqueue的相关信息，其中包含其中包括vring 的base address(queue_desc). NOTE 在支持多队列的场景下, virtio_pci_comm_cfg.queue_select 是一个可读写字段，写该字段相当于一个select 操作。例如将该字段写1，然后在对virtio_pci_comm_cfg.queue_desc执行写操作相当于配置virtqueue 1的 vring base address. queue_desc 指向的地址包含上面提到的三个数据结构。其虽然连续，但是为了优化cacheline，每个数据结构中间可能会有padding field. vring_avail, vring_used中都包含一个idx，但是没有head, tail区分。两者需要结合来表示整个队列的状态vring data transport external consume我们知道, 在guest mode中运行程序是有额外代价的，这个代价主要源于 host emulation,有些emulation 是异步的(一般的IO device emulation), 这些emulation的动作会放到非vcpu thread, 而有些emulation 是同步的, 常见的是 VM-EXIT, 这些VM-EXIT event有些是主动的，有些是被动的，但是均会让guest trap到host.而对于模拟设备的虚拟化尤其如此: WHY ? 我们看下图.对于CPU而言, 和IO 虚拟化相关的操作主要有几下几个: 访问 内存 中的ringbuffer，以及 ringbuffer 指向的相关数据. 通过MMIO PIO, 访问设备资源(一般是bar 指向的ioport, 或者 MMIO), 这些资源包括, 队列相关信息: ring.head,tail(ringbuffer base addr只会在初始化的时候配置) doorbell 设备向cpu notify(interrupt)而哪些会造成VM-exit呢? 准确的说，都有可能造成，但是一般的memory access可以控制(假如某个地址触发ept violation), KVM 建立映射之后，一般不会取消映射，也就是下次访问该地址所在的page不会再触发vm-exit.除非触发内核的某些内存管理功能, 如swap,ksm等。所以这些操作带来的影响很小.那剩下的就是 MMIO/PIO 访问 ring.head,tail, doorbell, interrupt, 其中doorbell和interrupt都属于notify, 这个没有办法避免(但是也可以优化，下个章节会讲)。那最终剩余ring.head, ring.tail能不能优化。一个很明显的方法，是将其转移到内存中。 NOTE 一般的物理设备都会将ring.head,tail 放到 device register上，不清楚其放在设备上的好处。在chatgpt过程中，其提到, 可能是一些缓存一致性和 order问题. 但是仔细想想, 缓存一致性可以用 Strong Uncacheable (UC) 的内存类型避免,虽然在执行atomic相关操作时(一般是多个cpu当作 producer 操作ring.head), 会造成比较严重的性能问题. (UC lock , another word bus lock, 总之会锁数据总线). 而至于乱序, 也可以靠内存屏障解决. 所以，有知道的大佬可以帮忙解答下.ok, 我们在来回顾virtio 的ring.idx: vring_desc.idx vring_avail.idx其均在内存中。那在整个的数据传输过程中, 只剩余两个方向的notify 会触发VM-exit了 !!vring notifynotify的目的是, 当自己作为 producer 产生了数据，需要让对方([device &lt;-&gt; driver])处理时, 通知对方来感知这一行为。对于consume 来说, 这是被动的。这里有一种主动的方式, 就是关闭notify, 由consume 侧一直循环观测 producer的行为，看其是否产生了数据。这种称为poll。对于两者而言, poll 的优点是延迟低, 但是需要消耗更多的计算资源.(如果不消耗大量的计算资源的话，可能就适得其反).而notify的好处是, 消耗较少的计算资源。但是坏处也很明显 : 延迟高. 并且会打断当前的执行流程。 NOTE 我们这里简单思考下: 在notify方式中，之所以消耗的计算资源少，是因为不使用计算资源来轮训 producer的状态, 将该计算资源分配给别的任务，所以当notify 来临时，会打断当前的执行流。而打断过程的上下文切换是延迟的一部分原因。另一部分是, 当前执行的上下文不允许被打断(常见的是关中断), 所以, 需要等待该上下文可以被打断时（开中断），再触发notify. 这样就造成了更大的延迟。无论在物理环境，还是在虚拟环境中, notify 有两个方向: driver-&gt;device : 设备特定 device-&gt;driver : interrupt但是两者的代价又不相同, 如下图:在物理机上，两个方向的notify 均由纯硬件逻辑实现, 所以其notify的传输速度非常快.而在虚拟机环境中, 两个方向的notify均需要 host 去模拟，另外更糟心的是两个方向的notify 均会造成vm exit。严重影响guest vcpu的执行效率. driver-&gt;device: MMIO write: vmexit to trap into host emulation device-&gt;driver: 在virtio提出时, 中断虚拟化未支持完全(hardware), 并不能在cpu处于guest mode (VMX Non-root operation)时，注入 virq, 但是又为 了保证尽量减少中断延迟，于是需要kick vcpu. 也就是强制打断该vcpu，使其产生 vmexit (一般的做法是send ipi to this cpu, 让vcpu因 receive external interrupt而 vmexit.所以, 基于这一差距, 作者设计出一套 notify-less(inspired by tickless)的优化。而在之后更新的virtio协议的更高版本, 也在持续优化这方面。sample of handle VirtIO我们下面主要展示下, 在实际的数据传输时，vring, desc array 中的 数据流动.在看图之前，我们先列举一些点: 初始状态 假设vring大小为5, 并且初始状态下: 所有的desc都是free的 vring_avail.idx = 0 vring_used.idx = 0 guest, host会自己保存一个idx，该idx主要用来自己作为消费者，上次”消费”到哪了: guest: last_used_idx = 0 host: last_used_idx = 1 region of data residency 在desc从vring desc freelist中移除后, desc会驻留在vring中，但是这里，我们额外抽象出三个区域, 用来表示当前数据处理到哪个阶段: guest driver从 vring desc freelist取出desc，并准备其buffer host driver 从avail vring 中获取到数据，并且正在将这些数据发送到IO后端 guest driver 从used vring中收到数据，并且正在唤醒 iowait 相关task 这样avail vring中保存的仅是HOST DEVICE未处理的数据, 而 used vring 保存的仅是GUEST driver未处理的数据. vring full &amp;&amp; vring empty 我们来思考下: vring full 在处理流程中需要谁来关心，另外，怎么判断整个的vring是 full状态. 思考中 个人理解的答案 A: 只有guest driver 其需要关心vring full, 因为其最终控制着 vring desc freelist 的申请和释放. 另外, 怎么判断vring是否满也显而易见, 就是看vring desc freelist 中是否还有free 的成员。 所以vring full并不是指avail vring full, 或者 used vring full, 而就是表示所有 的desc正在处理，没有归还到 vring desc freelist 中. vring empty 这里就不卖官司了. vring empty 需要落实到每个vring上(avail, used). 而且只有consumer角色需要关心这些: guest driver: used vring is empty ? host device: avail vring is empty ? 题外话，可以先略过 NOTE 这里先跑题说些别的: 队列是否empty ? 这个判断条件需要 driver/device 在设备正常工作后一直判断 … 所以这里有两种方式实现: poll… NOTIFY 我们知道，poll的好处是延迟低，但是cpu 消耗高。而notify的好处是 cpu消耗低，但是延迟稍微高一些. 但是在虚拟化场景下, notify 往往会造成vm-exit，从而带来很大的额外开销. 这里先剧透下, 在整个的IO transport 过程中，virtio 优化的非常彻底，只有notify 会造成vm-exit。所以，virtio 针对notify 也提出一些优化点. 总结成一个单词 notify-less(inspired by tickless) ok, 了解完上述点后，我们来看下面的图:这是一个初始状态图， 所有的desc都在 vring desc FREELIST中。 所有的idx(包括last_xxx_idx)都是0。 guest virtio driver收到blk 层的IO request, 从vring desc FREELIST中申请了一个desc a, 并初始化a 初始化好a后，将a 放到 avail vring中. 此时, avail_vring.idx=++0=1guest driver收到了大量的IO 请求，此时将 vring desc FREELIST 的desc都申请完了,此时vring 是 full 状态, 另外，guest driver 将所有的 desc均初始化完成,并存放到vring此时:i = 4while i--: avail_vring.idx++avail_vring.idx is 5 host driver 通过某种途径感知到 avail vring中可能有东西(poll,notify), 于是比较了下 [last_avail_idx(0), avail_idx(5)]发现确实有5个数据需要处理。 于是， 从avail vring中将所有的desc 拿出来处理（每个desc的处理可以并行执行),并将这些io request 转换成对后端的请求。 此时last_avail_idx 0-&gt;5. e,c,d 这三个请求率先完成, 将其存放到used vring中，此时 used_idx 0-&gt;3. b, a 这两个请求也完成了，将其也存放到 used vring 中, 此时 used_idx 3-&gt;5. guest driver也通过某种途径感知到 used vring中可能有东西(poll, notify(interrupt)),于是比较了下[last_used_idx(0), used_idx(5)], 发现确实有5个io request 已经完成，需要唤醒正在iowait的进程。 首先处理c，d两个数据。此时， last_used_idx 0-&gt;2 处理完c, d两个数据后，正要准备处理剩余的数据时, 和因c, d io request阻塞的进程均被唤醒，而且释放了该io buffer, 此时将desc 归还到 vring desc FREELIST中 继续处理剩余的b,a e三个io request, 此时 last_used_idx 2-&gt;5 guest 中因virtqueue中的io request 阻塞的进程都被唤醒，并将desc 全都归还到vring desc FREELIST中.至此, guest 请求的5个io 均完成。why virtio is so efficient其他笔记 avail Note that there is padding such as to place this structure on a page separatefrom the available ring and descriptor array: this gives nice cache behaviorand acknowledges that each side need only ever write to one part of thevirtqueue structure. suppress notifications Note the vring_used flags and the vring_avail flags: theseare currently used to suppress notifications. For example,the used flags field is used by the host to tell the guest thatno kick is necessary when it adds buffers: as the kick requiresa vmexit, this can be an important optimization, and theKVM implementation uses this with a timer for networktransmission exit mitigation. Similarly, the avail flags fieldis used by the guest network driver to advise that furtherinterrupts are not required (i.e., disable_cb and enable_cbset and unset this bit). 参考链接 virtio: Towards a De-Facto Standard For Virtual I/O Devices virtio 虚拟化系列之一：从 virtio 论文开始 what it is that makes the Qemu hardware emulation so slowTODO the virtqueue callback might disable further callbacks before waking a servicethread. service thread ?? what ?? " }, { "title": "Ebpf", "url": "/posts/ebpf/", "categories": "", "tags": "", "date": "2025-05-21 00:00:00 +0800", "snippet": "", "content": "" }, { "title": "Bpf Verify", "url": "/posts/bpf-verify/", "categories": "", "tags": "", "date": "2025-05-21 00:00:00 +0800", "snippet": "", "content": "" }, { "title": "Bpf Jit", "url": "/posts/bpf-jit/", "categories": "", "tags": "", "date": "2025-05-21 00:00:00 +0800", "snippet": "", "content": "" }, { "title": "Bpf Isa", "url": "/posts/bpf-ISA/", "categories": "", "tags": "", "date": "2025-05-21 00:00:00 +0800", "snippet": "", "content": "" }, { "title": "Bpf Overflow", "url": "/posts/bpf-overflow/", "categories": "", "tags": "", "date": "2025-05-20 00:00:00 +0800", "snippet": "BPF 起源BPF 起源于1992, Steven McCanne 和 Van Jacobso 发布了论文: &lt;&lt;The BSD PacketFilter: A New Architecture for User-level Packet Capture&gt;&gt;1, 该论文主要提供了一种包过滤的技术，比其当时的包过滤技术快20倍.1 abstract.该包过滤技术和原有的...", "content": "BPF 起源BPF 起源于1992, Steven McCanne 和 Van Jacobso 发布了论文: &lt;&lt;The BSD PacketFilter: A New Architecture for User-level Packet Capture&gt;&gt;1, 该论文主要提供了一种包过滤的技术，比其当时的包过滤技术快20倍.1 abstract.该包过滤技术和原有的包过滤技术最大的不同是:原有的包过滤技术, 过滤步骤发生在用户态, 也就是说，如果要对包进行过滤，需要将 所有 包传给用户态，然后，在用户态使用规则进行过滤。而 bpf 则将过滤这个步骤放到了内核态, 在内核态过滤后，将过滤之后的数据，传递给用户态。避免了不必要的数据copy.由上图所示, 假如我们在用户态需要获取port=3000的所有tcp报文, 在传统模式下，即便是port != 3000的报文也需要copy到用户态。但是在bpf模式下，bpf会在kernel侧进行过滤动作，让port = 3000的报文筛选出来，然后copy到用户态, 这样就避免了一些不必要的copy。bpf模式下，包过滤的整体过程如下: From 1网卡收到包后，先将数据包额外copy一份, copy的副本主要用于包过滤+传递到用户态.而原来的包则走协议栈。该数据包会先交给BPF的程序进行处理, 而该BPF程序的作用就是根据用户的规则过滤报文，然后如果匹配成功，再将该报文copy到用户空间。那这里就暴露了一个问题, BPF 是如何根据用户的规则来过滤呢 ? 我们可以想到的是，通过ioctl()等系统调用，传递过滤规则，但是这样太不灵活了。而BPF则是通过插庄代码的方式来实现。这样就相当于在内核态中编程过滤。大大提升了灵活性。但是问题又来了, 让用户态可以在内核态编程，这不就赋予了用户态无限的权力？可以access 系统中的任意资源 ?BPF 虚拟机相关链接 The BSD Packet Filter: A New Architecture for User-level Packet Capture eBPF (作者分析了bpf的起源) ebpf.io eBPF 的发展历程及工作原理 eBPF 运行原理和流程 – 极客时间 eBPF 核心技术与实战 的学习笔记其他 包含论文" }, { "title": "page request", "url": "/posts/page-request/", "categories": "pcie, ats", "tags": "pcie, ats", "date": "2025-05-06 11:00:00 +0800", "snippet": "10.4 Page Request ServicesThe general model for a page request is as follows: A Function determines that it requires access to a page for which an ATStranslation is not available. The F...", "content": "10.4 Page Request ServicesThe general model for a page request is as follows: A Function determines that it requires access to a page for which an ATStranslation is not available. The Function causes the associated Page Request Interface to send a PageRequest Message to its RC. A Page Request Message contains a page address anda Page Request Group (PRG) index. The PRG index is used to identify thetransaction and is used to match requests with responses. When the RC determines its response to the request (which will typically beto make the requested page resident), it sends a PRG Response Message back tothe requesting Function. The Function can then employ ATS to request a translation for the requested page(s). Function 判断其需要访问某个尚未有 ATS 转换的页面。 Function 促使其关联的页面请求接口（Page Request Interface）向其 RC 发送一个页面请求消息（Page Request Message）。页面请求消息包含一个页面地址和一个页面请求组（PRG）索引。PRG 索引用于标识该事务，并用于将请求与响应进行匹配。 当 RC 确定对该请求的响应（通常是使所请求的页面变为常驻内存）后，它会向发起请求的 Function 发送一个 PRG 响应消息（PRG Response Message）。 随后，Function 可以使用 ATS 请求所需页面的转换 A Page Request Message is a PCIe Message Request that is Routed to the RootComplex with a Message Code of 4 (0000 0100b). The mechanism employed at the RCto buffer requests is implementation specific. The only requirement is that anRC not silently discard requests. Page Request Message 是一种 PCIe Message Request，其会以 Message Code 4（00000100b）路由到 Root Complex。RC 端用于缓存请求的机制由具体实现决定。唯一的要求是，RC 不能悄无声息地丢弃请求。All Page Request Messages and PRG Response Messages travel in PCIe Traffic Class A Page Request Message or PRG Response Message with a Traffic Class otherthan 0 shall be treated as Malformed TLPs by the RC or endpoint that receivesthe same. Intermediate routing elements (e.g., Switches) shall not detect thiserror. Malformed TLPs（Malformed Transaction Layer Packets）是指格式错误的 PCIe 事务层数据包。 所有 Page Request Message 和 PRG Response Message 都在 PCIe Traffic Class 0 中传输。带有非 0 Traffic Class 的 Page Request Message 或 PRG Response Message，应当被接收该消息的 RC 或 endpoint 视为 Malformed TLPs。中间的路由元件（例如Switches）不应检测此类错误The Relaxed Ordering and ID-Based Ordering bits in the Attr field of PageRequest Messages and PRG Response messages may be used. The No Snoop bit in theAttr field is reserved. Page Request Message 和 PRG Response message 的 Attr 字段中的 Relaxed Ordering和 ID-Based Ordering 位可以使用。Attr 字段中的 No Snoop 位为保留位。The page request service allows grouping of page requests into Page RequestGroups (PRGs). A PRG can contain one or more page requests. All pages in a PRGare responded to en mass by the host. Individual pages within a PRG arerequested with independent Page Request Messages and are recognized as belongingto a common PRG by sharing the same PRG index. The last request of a PRG ismarked as such within its Page Request Message. One request credit is consumedper page request (not per PRG). page request service 允许将多个页面请求分组为 Page Request Groups（PRGs）。一个 PRG 可以包含一个或多个页面请求。host会对 PRG 中的所有页面统一响应。PRG 内的每个页面通过独立的 Page Request Message 请求，并通过共享相同的 PRG index 被识别为属于同一个 PRG。PRG 的最后一个请求会在其 Page Request Message 中被标记为最后一个。每个页面请求会消耗一个 request credit（而不是每个 PRG 消耗一个）。A PRG Response Message is a PCIe Message Request that is Routed by ID back tothe requesting Function. It is used by system software to alert a Function thatthe page request(s) associated with the corresponding PRG has (have) beensatisfied. The page request mechanism does not guarantee any request completionorder and all requests are inherently independent of all other concurrentlyoutstanding requests. If a Function requires that a particular request becompleted before another request, the initial request will need to completebefore the subsequent request is issued. It is valid for a Function tospeculatively request a page without ascertaining its residence state and/or toissue multiple concurrently outstanding requests for the same page. PRG Response Message 是一种 PCIe Message Request，它会通过 ID 路由回发起请求的Function。系统软件使用它来通知 Function，与对应 PRG 相关联的页面请求已经被满足。page request 机制不保证请求完成的顺序，所有请求本质上都是彼此独立的，与其他并发未完成的请求没有依赖关系。如果 Function 需要某个请求在另一个请求之前完成，必须等到初始请求完成后再发起后续请求。Function 可以在不确定页面是否常驻的情况下，提前发起页面请求，也可以对同一页面同时发起多个并发的请求，这都是允许的。A Page Request Interface is allocated a specific number of page request messagecredits. An RC (system software) can divide the available credits in any mannerdeemed appropriate. Any measures the host chooses to employ to ensure thatcredits are correctly metered by Page Request Interfaces (a Page RequestInterface is not using more than its allocation) is an implementation choice. APage Request Interface is not allowed to oversubscribe the available number ofrequests (doing so can result in the page request mechanism being disabled ifthe buffer limit is exceeded at the root). A Page Request Interface’s pagerequest allocation is static. It is determined when the Page Request Interfaceis enabled and can only be changed by disabling and then re-enabling theinterface. Page Request Interface 会被分配一定数量的 page request message credit。RC（system software）可以以任何认为合适的方式分配这些可用的 credit。host为确保Page Request Interface 正确计量 credit（即 Page Request Interface 没有超出分配额度使用 credit）所采取的任何措施，属于实现选择。Page Request Interface 不允许超额发起请求（如果超出 root 端的 buffer 限制，可能导致 page request 机制被禁用）。Page Request Interface 的 page request credit 分配是静态的：它在 PageRequest Interface 启用时确定，只有在禁用并重新启用接口时才能更改。10.4.1 Page Request MessageA Function uses a Page Request Message to send page requests to its associatedhost. A page request indicates a page needed by the Function. The Page RequestInterface associated with a Function is given a specific Page Requestallocation. A Page Request Interface shall not issue page requests that exceedits page request allocation. Function 使用 Page Request Message 向其关联的host发送页面请求。每个页面请求表示 Function 需要的一个页面。与 Function 关联的 Page Request Interface 会被分配一个特定的 Page Request allocation。Page Request Interface 不得发起超过其 pagerequest allocation 的页面请求。A page request contains the untranslated address of the page that is needed, theaccess permissions needed for that page, and a PRG index. A PRG Index is a 9-bitscalar that is assigned by the Function to identify the associated page request.Multiple pages may be requested using a single PRG index. When more than asingle page is to be associated with a given PRG, the Last flag in the PageRequest Record is cleared in all the requests except the last request associatedwith a given PRG (the flag is set in the last request). Page requests areresponded to en mass. No response is possible (except for a Response Failureerror) until the last request of a PRG has been received by the root. The numberof PRGs that a Function can have outstanding at any given time is less than orequal to the associated Page Request Interface’s Outstanding Page RequestAllocation. It is valid for a request group to contain multiple requests for thesame page and for multiple outstanding PRGs to request the same page. page request 包含所需页面的未转换地址、该页面所需的访问权限，以及一个 PRGindex。PRG index 是由 Function 分配的 9-bit 标量，用于标识相关的 page request。可以通过同一个 PRG index 请求多个页面。当需要将多个页面关联到同一个 PRG 时，Page Request Record 中的 Last 标志位在该 PRG 关联的所有请求中都被清除，只有在最后一个请求中该标志位才会被置位。page request 会被统一响应。在 root 收到某个PRG 的最后一个请求之前，不会有响应（除了 Response Failure 错误之外）。Function在任意时刻可拥有的未完成 PRG 数量小于等于其关联 Page Request Interface 的Outstanding Page Request Allocation。一个请求组包含对同一页面的多个请求，以及多个未完成的 PRG 同时请求同一页面，都是允许的。A Page Request Interface applies to the “main” Function and its enabled ShadowFunctions (where the “main” is the Function that contains both the Page RequestExtended Capability and the Shadow Function Extended Capability). All pagesrequest messages of a single PRG must have the same Requester ID (of either the“main” Function or one of its Shadows). Page Request Interface 适用于 “main” Function 及其已启用的 Shadow Function（其中 “main” 指同时包含 Page Request Extended Capability 和 Shadow FunctionExtended Capability 的 Function）。单个 PRG 的所有 page request message 必须具有相同的 Requester ID（可以是 “main” Function，也可以是其某个 Shadow Function的 Requester ID）。The first two DWs of a Page Request Message contain a standard PCIe messageheader. The second two DWs of the message contain page request specific datafields. Page Request Message 的前两个 DW（Double Word，双字）包含标准的 PCIe messageheader。该消息的后两个 DW 包含 page request 专用的数据字段。 R: Read Access Requested - This field, when Set, indicates that the requesting Function seeks read access to the associated page. When Clear, this field indicates that the requesting Function will not read the associated page. If R and W are both Clear and L is Set, this is a Stop Marker (see § Section 10.4.1.2.1 ). The R field must be Set for Page Requests with a PASID and that have the Execute Requested bit Set. 当该字段被置位时，表示请求的 Function 需要对关联页面的读访问权限。若该字段为清除状态，则表示请求的 Function 不会读取关联页面。如果 R 和 W 都为清除状态且 L 被置位，则该请求为 Stop Marker（参见第 10.4.1.2.1 节）。对于带有PASID 且设置了 Execute Requested 位的 Page Request，R 字段必须被置位。 W Write Access Requested - This field, when Set, indicates that the requesting Function seeks write access and/or zero-length read access to the associated page. When Clear, this field indicates that the requesting Function will not write to the associated page. Upon receiving a Page Request Message with the W field Set, the host is permitted to mark the associated page dirty. If R and W are both Clear and L is Set, this is a Stop Marker (see § Section 10.4.1.2.1 ). 当该字段被置位时，表示请求的 Function 需要对关联页面进行写访问和/或零长度读访问。若该字段为清除状态，则表示请求的 Function 不会向关联页面写入数据。 当收到 W 字段被置位的 Page Request Message 时，host 可以将关联页面标记为dirty。 L Last Request in PRG - This field, when Set, indicates that the associated page request is the last request of the associated PRG. A PRG can have a single entry, in which case the PRG consists of a single request in which this field is Set. When Clear, this field indicates that additional page requests will be posted using this record’s PRG Index. 当该字段被置位时，表示关联的 page request 是对应 PRG 的最后一个请求。一个PRG 可以只包含一个条目，此时该 PRG 仅由一个请求组成，并且该字段被置位。若该字段为清除状态，则表示还会有更多的 page request 使用该记录的 PRG Index被提交。 If R and W are both Clear and L is Set, this is a Stop Marker (see § Section 10.4.1.2.1 ). Page Request Group Index - This field contains a Function supplied identifier for the associated page request. A Function need not employ the entire available range of PRG index values. A host shall never respond with a PRG Index that has not been previously issued by the Function and that is not currently an outstanding request PRG Index (except when issuing a Response Failure, in which case the host need not preserve the associated request’s PRG Index value in the error response). 该字段包含由 Function 提供的、用于标识关联 page request 的标识符。Function不必使用全部可用范围的 PRG index 值。host 不应对 Function 尚未发出的，或者当前不是未完成请求的 PRG Index 进行响应（除非是在发出 Response Failure 时，此时 host 在错误响应中无需保留关联请求的 PRG Index 值）。 Page Address - This field contains the untranslated address of the page to be loaded. For pages larger than 4096 bytes, the least significant bits of this field are ignored. For example, the least significant bit of this field is ignored when an 8096-byte page is being requested. 该字段包含要加载页面的未转换地址。对于大于 4096 字节的页面，该字段的低位会被忽略。例如，当请求一个 8096 字节的页面时，该字段的最低位会被忽略。 10.4.1.1 PASID UsageThe PASID Extended Capability indicates whether a Function supports PASID TLPPrefixes (NFM) or OHC-A1 with PASID (FM), and whether it is enabled to send andreceive them. PASID Extended Capability 指示一个 Function 是否支持 PASID TLP Prefix（NFM）或OHC-A1 with PASID（FM），以及它是否已启用以发送和接收这些前缀。Functions that support PASID are permitted to send a PASID on Page RequestMessages. The PASID field contains the process address space of the page beingrequested and the Execute Requested and Privileged Mode Requested bits indicatethe access being requested. 支持 PASID 的 Function 可以在 Page Request Message 中发送 PASID。PASID 字段包含所请求页面的进程地址空间，Execute Requested 和 Privileged Mode Requested 位指示所请求的访问权限。If one Page Request Message in a PRG has a PASID, all Page Request Messages inthat PRG must contain identical PASID values. Behavior is undefined when thePASID values are inconsistent. 如果一个 PRG 中的某个 Page Request Message 有 PASID，那么该 PRG 中所有的 PageRequest Message 必须包含相同的 PASID 值。如果 PASID 值不一致，行为未定义。Functions that support PASID and have the PRG Response PASID Required bit Set(see § Section 10.5.2.3 ), expect that PRG Response Messages will contain aPASID if the associated Page Request Message had a PASID. For such PRG ResponseMessages, the Execute Requested and Privileged Mode Requested bits are reservedand the PASID field contains the PASID from the associated Page Request Message. 支持 PASID 并且设置了 PRG Response PASID Required 位（参见第 10.5.2.3 节）的Function，期望当相关的 Page Request Message 带有 PASID 时，PRG ResponseMessage 也包含 PASID。对于这样的 PRG Response Message，Execute Requested 和Privileged Mode Requested 位为保留位，PASID 字段包含来自关联 Page RequestMessage 的 PASID。10.4.1.2 Managing PASID Usage on PRG RequestsThere are rules for stopping and starting the use of a PASID. 对于停止和启动 PASID 的使用有相关规则。This section describes additional rules that apply to Functions that have issuedPage Request Messages in a PASID that is being stopped. No additional rules arerequired to start the usage of the Page Request Interface for a PASID. 本节描述了适用于在被停止的 PASID 中已经发出 Page Request Message 的 Function的附加规则。对于启动 PASID 的 Page Request Interface 的使用，不需要额外的规则。When stopping the use of a particular PASID, a Stop Marker Message may beoptionally used to avoid waiting for PRG Response Messages before the Functionindicates that the stop request for a particular PASID has completed. 当停止使用某个特定 PASID 时，可以选择性地使用 Stop Marker Message，以避免在Function 指示某个 PASID 的停止请求已完成之前，必须等待 PRG Response Message。To stop without using a Stop Marker Message, the Function shall: Stop queueing new Page Request Messages for this PASID. Finish transmitting any multi-page Page Request Messages for this PASID (i.e.,send the Page Request Message with the L bit Set). Wait for PRG Response Messages associated any outstanding Page RequestMessages for the PASID. Indicate that the PASID has stopped using a device specific mechanism. Thismechanism must indicate that a Stop Marker Message will not be generated. 按照你的要求（功能、专业名词不翻译），下面是你的英文内容的中文翻译： 如果在停止时不使用 Stop Marker Message，Function 应当： 停止为该 PASID 排队新的 Page Request Message。 完成对该 PASID 的所有多页 Page Request Message 的发送（即，发送带有 L 位设置的 Page Request Message）。 等待与该 PASID 所有未完成 Page Request Message 相关的 PRG Response Message。 通过设备特定的机制指示该 PASID 已经停止。该机制必须能够指示不会生成 StopMarker Message。 To stop with the use of a Stop Marker Message the Function shall: Stop queueing new Page Request Messages for this PASID. Finish transmitting any multi-page Page Request Messages for this PASID (i.e.,send the Page Request Message with the L bit Set). Internally mark all outstanding Page Request Messages for this PASID asstale. PRG Response Messages associated with these requests will return PageRequest Allocation credits and PRG Index values but are otherwise ignored.184 Indicate that the PASID has stopped using a device specific mechanism. Thismechanism must indicate that a Stop Marker Message will be generated. Send a Stop Marker Message to indicate to the host that all subsequentPage Request Messages for this PASID are for a new use of the PASID value.Note: Steps 4 and 5 may be performed in either order, or in parallel. 如果在停止时使用 Stop Marker Message，Function 应当： 停止为该 PASID 排队新的 Page Request Message。 完成对该 PASID 的所有多页 Page Request Message 的发送（即，发送带有 L 位设置的 Page Request Message）。 在内部将该 PASID 的所有未完成 Page Request Message 标记为 stale。与这些请求相关的 PRG Response Message 会返回 Page Request Allocation credit 和 PRGIndex 值，但除此之外会被忽略。 通过设备特定的机制指示该 PASID 已经停止。该机制必须能够指示会生成 StopMarker Message。 发送 Stop Marker Message，以向主机指示该 PASID 的所有后续 Page RequestMessage 都属于对该 PASID 值的新使用。 注：步骤 4 和 5 可以任意顺序执行，也可以并行执行。10.4.1.2.1 Stop Marker MessagesA Stop Marker Message indicates that a Function has stopped using the PageRequest Interface and has transmitted all pending Page Request Messages for aspecific PASID. Stop Marker Messages are strongly ordered with respect to PageRequest Messages and serve to push Page Request Messages toward the Host. Whenthe Host receives the Stop Marker Message, this indicates that all Page RequestMessages associated with the PASID being stopped have been delivered and thatany subsequent Page Request Message with the same PASID value are associatedwith a new incarnation of that PASID value.Stop Marker Messages do not have a response. They do not have a PRG Index and donot consume Page Request allocation (see § Section 10.5.2.5 ).The Stop Marker Message bit layout is shown in § Figure 10-25.A Stop Marker Message is encoded as a Page Request Message for which: In NFM, includes a PASID TLP Prefix. In FM, includes OHC-A1 with PASID. The Execute Requested and Privileged ModeRequested bits and the Last DW BW / 1st DW BE fields in the OHC-A1 areReserved. The L, W and R fields contain 1b, 0b and 0b respectively. The Untranslated Address field and upper bits of the PRG Index field are Reserved. The Marker Type field contains 0 0000b to indicate that this is a Stop Marker Message. The Traffic Class must be 0. The Relaxed Ordering attribute bit must be Clear. The ID-Based Ordering attribute bit may be Set.Behavior is undefined if a Stop Marker Message is received and any of the following are true: Marker Type not equal to 0 0000b. No PASID TLP Prefix is present (NFM). The PASID value does not match an outstanding stop request. An incomplete Page Request Message for the PASID is outstanding (i.e., forsome PRG Index, the most recently received Page Request Message did not havethe L bit Set).10.4.2 Page Request Group Response MessageSystem hardware and/or software communicate with a Function’s page requestinterface via PRG Response Messages. A PRG Response Message is used by a host tosignal the completion of a PRG, or the catastrophic failure of the interface. Asingle PRG Response Message is issued in response to a PRG, independent of thenumber of page requests associated with the PRG. There is no mechanism forindicating a partial request completion or partial request failure. If any ofthe pages associated with a given PRG cannot be satisfied, then the request isconsidered to have failed and the reason for the failure is supplied in the PRGResponse Message. The host has no obligation to partially satisfy a multi-pagerequest. If one of the requested pages cannot be made resident, then the entirerequest can, but need not, be discarded. That is, the residence of pages thatshare a PRG with a failed page request, but that are not associated with thefailure, is indeterminate from the Function’s perspective.There are four possible Page Request failures: The requested page is not a valid Untranslated Address. PASID support exists, the Page Request has a PASID, and either PASID usage isnot enabled for this request, the PASID value is not valid, or the ExecuteRequested bit is Set when R is Clear. 185 The requested page does not have the requested access attributes (includingExecute permission and/or Privileged Mode access when those bits are present) The system is, for an unspecified reason, unable to respond to the request.This response is terminal (the host may no longer respond to any pagerequests and may not supply any further replies to the Function until theFunction’s page request interface has been reset). For example, a requestthat violates a Function’s assigned request limit or overflows the RC’sbuffering capability may cause this type of failure. A Function’s response to Page Request failure cases 1, 2, and 3 above isimplementation dependent. The failure is not necessarily persistent, that is,a failed request may, in some instances succeed if re-issued. The range ofpossibilities precludes the precise specification of a generalized failurebehavior, though on a per Function basis, the response to a failure will bean implementation dependent behavior.All responses are sent to their associated Functions via PRG ResponseMessages. A Function must be capable of sinking multiple consecutive messageswithout losing any information. To avoid deadlock, a Function must able toprocess PRG Response Messages for all of the Function’s outstanding PageRequest Messages without depending on the Function sending or receiving anyother TLP. 186 A PRG Response Message is an ID routed PCIe message. The onlyPage Request Interface specific fields in this message are the Response Codeand PRG. All other fields are standard PCIe message fields. (Note: thesemessages are routed based on the ID in bytes 8 and 9; with bytes 4 and 5containing the host’s Requester ID.)Receipt of a PRG Response Message that contains a PRG Index that is notcurrently outstanding at a Function shall result in the UPRGI flag in thePage Request Extended Capability being Set, contingent upon the TLP otherwisebeing error free. Because of ambiguous language in earlier versions of thisspecification, it is permitted (though discouraged) to handle this case as anUnsupported Request (UR) or Unexpected Completion (UC) by the Functioncontaining the Page Request Extended Capability, but otherwise no other erroris permitted to be logged or signaled.10.4.2.2 PASID Usage on PRG ResponsesIf a Page Request has a PASID, the corresponding PRG Response Message mayoptionally contain one as well. If the PRG Response PASID Required bit is Clear,PRG Response Messages do not have a PASID.If the PRG Response PASID Required bit is Set, PRG Response Messages have aPASID if the Page Request also had one. The Function is permitted to use thePASID value from the prefix in conjunction with the PRG Index to match requestsand responses.When a PASID is attached to PRG Response Messages, the Execute Requested andPrivileged Mode Requested bits are Reserved and the PASID value is copied fromthe PASID value of the Page Request." }, { "title": "drain pasid", "url": "/posts/drain-pasid/", "categories": "virt, iommu", "tags": "virt, pcie", "date": "2025-04-22 21:51:00 +0800", "snippet": "PASID 简介PASID overflowPASID 全称Process Address Space ID, PASID 同 requester ID结合，共同确定该request所映射的地址空间。所以 PASID 和 ASID 类似, 均标识一个地址映射关系。但是ASID 用于标识CPU 侧的memory request，而PASID 则标识PCIe end point 的DMA（还有其...", "content": "PASID 简介PASID overflowPASID 全称Process Address Space ID, PASID 同 requester ID结合，共同确定该request所映射的地址空间。所以 PASID 和 ASID 类似, 均标识一个地址映射关系。但是ASID 用于标识CPU 侧的memory request，而PASID 则标识PCIe end point 的DMA（还有其他request，下面描述）.我们结合上图, 对比下两者:||ASID|PASID||—|—|—||memory request 发起者|CPU|PCIe EP||通过该ID查阅对象|TLB|IOMMU DMA remapping table||作用|让TLB中可以缓存多个地址空间映射|让IOMMU 可以为一个function 的request 使用不同的地址空间|每个 function 都有一组独特的 PASID 值。一个 function 使用的 PASID 值与任何其他function 使用的 PASID 值无关如上图所示 request1, request2, request3 来源于一个设备，Request ID相同.但是request1 和request2/3 的PASID 不相同，所以该地址使用的 request address-&gt;phyiscal address 的映射关系”数据库” 不同. 而request2，request 3 PASID 也相同。其两者均使用同一套映射关系进行地址映射。而request 4 和request 3 即使PASID 相同，但是该request来自于不同的function，所以两者使用不同的映射关系进行映射。总结PASID 用来标识PCIe EP 发出的memory request, 当该request到达iommu时，IOMMU会根据PASID查询该function并所属于该PASID的 remapping table, 最终得到访问的物理地址.上面我们提到，可以用PASID标识一个request，在PCIe协议中就是一个TLP，那么问题来了: TLP的种类有那么多，PASID可以标记哪些request ? 标记无非是在TLP中加字段，PASID在TLP中的哪个位置呢 ?PASID in PCIePASID requestPASID 可以作用与下面的request: Memory Requests(including AtomicOp Requests) with Address Type (AT) of Untranslated or Translated Request(个人认为还包括TranslationRequest) Address Translation Requests: ATS Invalidation Request Messages Page Request Messages PRG Response Messages PAISD Prefix关于各个字段的解释，见PCIe 6.0 spec 6.20.2.1 PASID TLP Prefix - Non-Flit Mode接下来，我们来看下PASID的生命周期PASID drainPASID 可以分配，释放，再分配。这里面比较关键的步骤是释放步骤，也就是将PASIDdrain.那释放一个PASID 有什么难点么?难点就在于，在释放期间，如何彻底的clean/flush 掉 在PCIe 拓扑中的和该PASID 相关的所有资源. 那PASID 的资源，可以在哪些地方呢?我们以下图展示:主要包括下面部分: IOMMU IOTLB in IOMMU : IOMMU 保存该 pasid的iotlb page request queue: page request queue中有该pasid相关的未处理的page request 链路上的各种request: to upstream: Memory Request Translation Request Page Request to downstream: Memory Read Completion Translation Completion (include translation fault) Page Request Response Device 上 Pending 的 request IOTLB in Device: 设备端保存的IOTLB Memory Request: 设备还未发出的Memory Request Page Request: 设备还未发出的Page Request 这里面比较重要的是各种request, 为什么这么说, 因为这些request 往往和其他组件有交互, 举个例子: 当device发出了 Translation Request 当IOMMU处理后，可能会更新其本地的IOTLB（存疑需确认). IOMMU 向 Device发出Translation Completion(当然有可能还是 表示 translation fault的消息). 设备更新本地IOTLB如果我们非常武断的在步骤2后，发出PASID drain动作，并认为在步骤3执行之前，该动作已经完成. 那么IOMMU 和 Device 中的IOTLB 将是stale的. 而随后如果software侧又分配了该PASID，则可能使用stale IOTLB 酿成严重后果。所以，我们需要一个规范，来规定，PASID drain 需要做什么动作，并且需要达到一个什么样的状态可以认为PASID drain 动作完成. 对于设备/iommu 两个角色来说，谁在这个过程中比较关键呢? 是设备，因为设备是这些request的发起者和接收者, 正说为做事有始有终,最保险的方式是设备端将所有pending的request全部处理完, 再将PASID drain。PCIe SPEC 6.20.1 Managing PASID Usage 章节中描述了 设备应该做的事情, 原文如下: Stopped queuing new Requests for this PASID. 不再发出新的request Completed all Non-Posted Requests associated with this PASID. 等待所有 Non-Posted Request 完成 Flushed to the host all Posted Requests addressing host memory in all TCs that were used by the PASID. The mechanism used for this is device specific (for example: a non-relaxed Posted Write to host memory or a processor read of the Function can flush TC0; a zero length read to host memory can flush non-zero TCs). 确保所有到host memory 的 Posted Request 完成 Optionally flushed all Peer-to-Peer Posted Requests to their destination(s). The mechanism used for this is device specific. 确保所有PTP Posted Request 完成 Complied with additional rules described in Address Translation Services (§ Chapter 10. ) if Address Translations or Page Requests were issued on the behalf of this PASID. Page Requests 相关请求要服从 Chatper 10 描述的规则. 总结下上面的规则: new memory request: 不发 链路上的 memory request: flush or wait, 总之就是等其complete. Page Request: 额外的规则Page Request 为什么如此特殊呢? 能不能按照上面的规则来？(no new and wait oldcomplete). 当然可以。但是可以做一些优化。我们先展示下Page Request正常的处理流程，在展示在处理Page Request过程中如果触发了PASID drain, 会有一些什么问题, 或者说可优化的点 ?***正常处理流程:重点是步骤3和4, 当PCIe EP 发送 Page Request Message 到IOMMU后，IOMMU会将该Message 的信息存放到Page Request Queue中，并notify to host (deliver interrupt).软件收到该notify后，会从Page Request Queue中获取Request，并建立IOVA-&gt;PA的映射。而如果在这个过程中发生了PASID Drain 呢?在步骤3后，发生PASID Drain(host software 发起), 此时如果PCIe EP一定要等PageRequest Response 的话，host software 就比较纠结，我是为其建立映射好，还是不建立映射好，如果为其建立映射，在设备发出Stop Request Completion 后，还得把这些映射销毁。如果不为其建立映射, 软件也不知道现在 PCIe EP是否正在执行 Stop Request(或者说是否不再发出新的request), 所以为了保险起见，软件一般会正确处理该Page Request，为其建立映射，在收到Stop Request Completion后，再销毁该映射。所以这就导致了一些不必要的处理。能不能有一些优化呢?PASID Drain with Page Request optimizationPCIe Spec 10.4.1.2 Managing PASID Usage on PRG Requests, 描述了这一优化。该章节起到了两种方式: 未优化: Stop queueing new Page Request Messages for this PASID. Finish transmitting any multi-page Page Request Messages for this PASID (i.e., send the Page Request Message with the L bit Set). Wait for PRG Response Messages associated any outstanding Page Request Messages for the PASID. Indicate that the PASID has stopped using a device specific mechanism. This mechanism must indicate that a Stop Marker Message will not be generated. 这种就是典型的no new and wait old completion 优化(use Stop Marker Message): Stop queueing new Page Request Messages for this PASID. Finish transmitting any multi-page Page Request Messages for this PASID(i.e., send the Page Request Message with the L bit Set). (3)Internally mark all outstanding Page Request Messages for this PASID asstale. PRG Response Messages associated with these requests will returnPage Request Allocation credits and PRG Index values but are otherwiseignored. (4)Indicate that the PASID has stopped using a device specific mechanism.This mechanism must indicate that a Stop Marker Message will be generated. (5)Send a Stop Marker Message to indicate to the host that all subsequentPage Request Messages for this PASID are for a new use of the PASID value. 首先设备端首先支持去 Mark 那些标记该PASID的为complete 的 Page RequestMessage为stale. 如果收到标记的这些request的PRG Response Message, 仅获取PRGindex value 并获取Page Request Allocation credits(待研究)并不会根据该 Response Message 更新其IOTLB. 然后，PCIe EP 就认为自己已经准备好drain 该PASID, 做了两件事情: stop message complete Send a Stop Marker Messages 并且手册中提到, 这两个操作是无顺序的，可以并行. Note: Steps 4 and 5 may be performed in either order, or in parallel. 这里比较关键的是步骤3, 将外面的PRG Response Request标记为Stale之后, Software就可以随意处理”Stale Page Request”（不为其建立映射了，随意回下就可以了). 但是这里有一个问题: host software 如何看到设备做完了第三步 ?Stop Marker Message 可以用来指示, Stop Marker Message, 本身就是一个Page Request只不过(L, W, R) 为(1, 0, 0), 该消息只是起到一个指示的作用, 无需 Response:当 Software 在收到Stop Marker后, 认为该设备已经stopped pasid. NOTE 这里有一个疑问 ? 步骤5也可以指示设备是否完成stopped pasid, 为什么非得要另加一个Stop MarkerMessage的机制 ?? WARN 以上内容，都是自己的理解，如有错误或者其他的想法，欢迎评论一起讨论TODO ATS Page Request credits ?? VT-d SMS Kernel Handle参考链接 PCIe 6.0 6.20 PASID" }, { "title": "[perftest] core-to-core-latency", "url": "/posts/Core-to-Core-latency/", "categories": "perftest", "tags": "perftest, core-to-core-latency", "date": "2025-04-09 11:50:00 +0800", "snippet": "使用方法core-to-core-latencyUSAGE: core-to-core-latency [OPTIONS] [ARGS]ARGS: &lt;NUM_ITERATIONS&gt; The number of iterations per sample [default: 1000] &lt;NUM_SAMPLES&gt; The number...", "content": "使用方法core-to-core-latencyUSAGE: core-to-core-latency [OPTIONS] [ARGS]ARGS: &lt;NUM_ITERATIONS&gt; The number of iterations per sample [default: 1000] &lt;NUM_SAMPLES&gt; The number of samples [default: 300]OPTIONS: -b, --bench &lt;BENCH&gt; Select which benchmark to run, in a comma delimited list, e.g., '1,3' 1: CAS latency on a single shared cache line. 2: Single-writer single-reader latency on two shared cache lines. 3: One writer and one reader on many cache line, using the clock. [default: 1] -c, --cores &lt;CORES&gt; Specify the cores by id that should be used, comma delimited. By default all cores are used --csv Outputs the mean latencies in CSV format on stdout -h, --help Print help information -b: bench 的类型 cas: CAS – compare and swap, 通过atomic swap 测试shared cache line 延迟 (下面会介绍代码细节) single-write single-reader latency: one writer and one reader on many cache line: 默认采用1:cas -c: 指定要测试的核心，默认测量所有核心 –csv: 将测量结果以csv格式输出.代码细节(由于对rust不了解，这里大概展示下，三个bench的相关代码)CASimpl super::Bench for Bench { // The two threads modify the same cacheline. // This is useful to benchmark spinlock performance. // NOTE // 两个threads修改同一个cacheliine // //- thread 1: pong thread // - swap(PING-&gt;PONG) //- thread 2: ping thread // - swap(PONG-&gt;PING) // - test duration of (PING-&gt;PONG-&gt;PING) time and // record result fn run( &amp;self, (ping_core, pong_core): (CoreId, CoreId), clock: &amp;Clock, num_round_trips: Count, num_samples: Count, ) -&gt; Vec&lt;f64&gt; { let state = self; crossbeam_utils::thread::scope(|s| { //创建pong线程 let pong = s.spawn(move |_| { core_affinity::set_for_current(pong_core); //等待ping线程到达该点 state.barrier.wait(); //一共测量num_round_trips * num_samples // num_samples: 表示进行几组测试 // num_round_trips: 表示每一组测试ping-&gt;pong-&gt;ping // 的次数 for _ in 0..(num_round_trips*num_samples) { while state.flag.compare_exchange(PING, PONG, Ordering::Relaxed, Ordering::Relaxed).is_err() {} } }); //创建ping线程 let ping = s.spawn(move |_| { core_affinity::set_for_current(ping_core); let mut results = Vec::with_capacity(num_samples as usize); //等到pong线程达到该点 state.barrier.wait(); //采集 num_samples 数据 for _ in 0..num_samples { let start = clock.raw(); //测量num_round_trips 组数据 for _ in 0..num_round_trips { while state.flag.compare_exchange(PONG, PING, Ordering::Relaxed, Ordering::Relaxed).is_err() {} } //计算一组测试的时间差 let end = clock.raw(); let duration = clock.delta(start, end).as_nanos(); //获取每次内存访问的延迟, 由于测量的是PING-&gt;PONG-&gt;PING，中间 //执行了两次内存操作, 所这里要/2, 获取每次内存访问的延迟 results.push(duration as f64 / num_round_trips as f64 / 2.0); } results }); pong.join().unwrap(); ping.join().unwrap() }).unwrap() }}" }, { "title": "[perftest] lat mem rd", "url": "/posts/lat-mem-bench/", "categories": "perftest", "tags": "perftest, lmbench", "date": "2025-04-09 11:50:00 +0800", "snippet": "使用方法整体命令:[-P &lt;parallelism&gt;] [-W &lt;warmup&gt;] [-N &lt;repetitions&gt;] [-t] len [stride...]参数解释: P: 并行运行线程数 t: 是否连续访问 W: warmup N: repetitions len: 访问数据块最大大小(该程序会循环测试，从较小的数据块开始测试，逐步 ...", "content": "使用方法整体命令:[-P &lt;parallelism&gt;] [-W &lt;warmup&gt;] [-N &lt;repetitions&gt;] [-t] len [stride...]参数解释: P: 并行运行线程数 t: 是否连续访问 W: warmup N: repetitions len: 访问数据块最大大小(该程序会循环测试，从较小的数据块开始测试，逐步 增加数据块大小，最高达到len大小) stride: 访问步长, 可以指定多个步长依次测试命令示例:numactl -C 0 -m 0 ./bin/x86_64-linux-gnu/lat_mem_rd -P 1 -N 5 -t 1024m 512 1024解释: numactl: -C 0: 将进程绑定在cpu 0 -m 0: 将进程绑定在 NUMA node 0 lat_mem_rd: -P 1: 单线程 -N 5: -t: 非连续访问 1024m: 访问数据块大小为1G 512 1024: 访问步长, 命令执行时，会依次测试512, 1024两个步长, 得出两组结果 (相当于对不同的步长进行bench test) 输出如下:\"stride=5120.00098 1.4130.00195 1.4130.00293 1.413...384.00000 88.165512.00000 88.201768.00000 88.2161024.00000 88.219 第一列表示访问的数据大小，单位为M 第二列表示延迟具体实现我们这里，只关注下，两种访存模式(-t or no -t) 的具体实现. 无论是那种模式，都需要将要访问的内存区域，按照stride 进行分割. 每个分割的区域只访问一个字节. 如下图所示:从上图可以看出，在walk每个区域时，只会访问该区域的第一个字节.而两种访问村模式，只是决定了对这段区域内的访存顺序。 no -t: 连续访问 -t: 随机访问(不是真随机, 而是尽量做到两次连续的内存访问，尽量最远(个人理解))但是，该程序的非连续访问访问是对这段内存的每个range，都要访问一遍，并且不能重复访问。接下来看下代码细节代码细节整体流程main## 参数解析时决定其初始化函数|-&gt; -t: fpInit = thrash_initialize def: fpInit = stride_initialize## range表示本轮的访存范围，从LOWER 增长到 len, 也就是例子中配置的 ## 1024m|-&gt; foreach range: for (range = LOWER; range &lt;= len; range = step(range)) |-&gt; loads() |-&gt; init struct mem_state |-&gt; width, len, max_len, line(赋值为stride) ## 表示访问了多少次内存 |-&gt; count = 100 * (state.len / (state.line * 100) + 1) ## 具体访存函数 |-&gt; benchmp() ## 保存最小值 |-&gt; save_minimum() ## gettime 根据保存的时间戳, 计算出本次访问的时间段 ## count 表示一轮访问了多少次内存, get_n()表示进行了多少轮访问 ## 所以综合来说, result 计算的是，一次访存所消耗的时间 |-&gt; result = (1000. * (double)gettime()) / (double)(count * get_n()); ## 打印 |-&gt; fprintf(stderr, \"%.5f %.3f\\n\", range / (1024. * 1024.), result);其中fpInit, 决定了访问内存的方式，我们主要关注下这部分:访存方式首先来看下，比较简单的，顺序访问:顺序访问voidstride_initialize(iter_t iterations, void* cookie){ struct mem_state* state = (struct mem_state*)cookie; size_t i; size_t range = state-&gt;len; size_t stride = state-&gt;line; char* addr; //==(1)== base_initialize(iterations, cookie); if (!state-&gt;initialized) return; addr = state-&gt;base; //==(2)== for (i = stride; i &lt; range; i += stride) { //该内存存储的是下一个要访问的地址 *(char **)&amp;addr[i - stride] = (char*)&amp;addr[i]; } *(char **)&amp;addr[i - stride] = (char*)&amp;addr[0]; state-&gt;p[0] = addr; mem_reset();} base_initialize, 不再展开，主要调用malloc 分配内存，初始化state中有关成员例如: nwords addr: 分配内存的首地址 nlines … 为了做到对每个区域的其中一个byte做一次访问(假设有n个区域，尽量做到在一轮测试中，只访问n次内存， 每次都落在一个range中). 在addr指向的内存区域中构建一个链表，遍历该链表一次，就做到了对每个range访问一次。 另外，从代码也可以看出来，将每个区域的首地址，串联成一个链表，并且按照该链表访问，地址单方向递增的。（除了最后一个区域) 非顺序访问thrash_initialize## 分配内存|-&gt; base_initialize()## 表示len不是按照pagesize对齐|-&gt; if state-&gt;len % state-&gt;pagesize: ## nwords 表示 组的个数 |-&gt; state-&gt;nwords = state-&gt;len / state-&gt;line ## 分配一个数组，并计算出具体的链表 ## (words[n] 的值为:访问n组之后，要访问的下一个组的具体地址) |-&gt; state-&gt;words = words_initialize(state-&gt;nwords, state-&gt;line) |-&gt; words = (size_t*)malloc() ## log2(max) |-&gt; for (i = max&gt;&gt;1, nbits = 0; i != 0; i &gt;&gt;= 1, nbits++); ## 下面解释 |-&gt; for (i = 0; i &lt; max; ++i) { /* now reverse the bits */ for (j = 0; j &lt; nbits; j++) { if (i &amp; (1&lt;&lt;j)) { words[i] |= (1&lt;&lt;(nbits-j-1)); } } words[i] *= scale; } |-&gt; for (i = 0; i &lt; state-&gt;nwords - 1; ++i) ## 根据上面计算的链表，构造实际的链表 |-&gt; *(char **)&amp;addr[state-&gt;words[i]] = (char*)&amp;addr[state-&gt;words[i+1]] |-&gt; *(char **)&amp;addr[state-&gt;words[i]] = addr |-&gt; state-&gt;p[0] = addr--&gt; else |-&gt; 暂不分析这里想要实现的效果是，将每个内存访问尽量分散.我们以 max = 8 为例, 得到的链表是:0-&gt;4-&gt;2-&gt;6-&gt;1-&gt;5-&gt;3-&gt;7而做到这样效果的代码主要是:for (j = 0; j &lt; nbits; j++) if (i &amp; (1&lt;&lt;j)) words[i] |= (1&lt;&lt;(nbits-j-1))这里将数拆分, 并通过nbits-j-1找到最远的值4 = log2(2)---n = log2(3-2)n = 2 具体不知道是哪个算法, 还需要看下具体算法参考链接 Lmbench测试集 — 延迟测试工具lat_mem_rd" }, { "title": "ats", "url": "/posts/ats/", "categories": "pcie, acs", "tags": "pcie, acs", "date": "2025-03-31 11:00:00 +0800", "snippet": "ATS Specification10.1 ATS Architectural OverviewMost contemporary system architectures make provisions for translating addressesfrom DMA (bus mastering) I/O Functions. In many implementations, it h...", "content": "ATS Specification10.1 ATS Architectural OverviewMost contemporary system architectures make provisions for translating addressesfrom DMA (bus mastering) I/O Functions. In many implementations, it has beencommon practice to assume that the physical address space seen by the CPU and byan I/O Function is equivalent. While in others, this is not the case. Theaddress programmed into an I/O Function is a “handle” that is processed by theRoot Complex (RC). The result of this processing is often a translation to aphysical memory address within the central complex. Typically, the processingincludes access rights checking to insure that the DMA Function is allowed toaccess the referenced memory location(s). 大部分现代系统架构对I/O Functions 的DMA (bus mastering)的地址转换作了规定. 在许多实现中, 通常的做法是假定CPU 和I/O Fucntions看到的是相同的物理地址空间. 而在其他情况下并非如此. 编程到 I/O Function 中的地址是一个由 Root Complex 处理的‘句柄’. 这种处理的结果通常是将其转换为 central complex 内的物理内存地址。通常，这个处理包括访问权限检查，以确保 DMA Function 被允许访问引用的内存位置。The purposes for having DMA address translation vary and include: Limiting the destructiveness of a “broken” or miss-programmed DMA I/O Function Providing for scatter/gather Ability to redirect message-signaled interrupts (e.g., MSI or MSI-X) to different address ranges without requiring coordination with the underlying I/O Function Address space conversion (32-bit I/O Function to larger system address space) Virtualization support 进行DMA address translation的不同的目的包括 限制”broken”或者误编程的DMA I/O Function的破坏性 提供给 scatter/gather使用 能够将message-signaled interrupts(例如:MSI/MSIx)重定向到不同的 地址范围,并且不需要和下层的I/O Function协调 地址空间的转换(将32-bit I/O Function转换成更大的系统地址空间) 虚拟化支持 destructiveness: 破坏性，毁灭性 Irrespective of the motivation, the presence of DMA address translation in the host system has certain performance implications for DMA accesses. 不管动机如何, 主机系统中的DMA address translation的存在会造成DMA access时有一定的性能影响 irrespective: 无论, 不考虑 Depending on the implementation, DMA access time can be significantly lengtheneddue to the time required to resolve the actual physical address. If animplementation requires access to a main-memory-resident translation table, theaccess time can be significantly longer than the time for an untranslatedaccess. Additionally, if each transaction requires multiple memory accesses(e.g., for a table walk), then the memory transaction rate (i.e., overhead)associated with DMA can be high. 根据实现情况,DMA access的时间会因为解析实际的物理地址而显著增长. 如果实现中需要访问main-memory-resident(主存中驻留)的地址转换表, 访问的时间可能比不经过翻译的访问时间长的多.此外,如果每个transaction需要 multiple memory access(例如 fora table work), 则和DMA相关的内存事务率(即开销) 可能会很高To mitigate these impacts, designs often include address translation caches inthe entity that performs the address translation. In a CPU, the addresstranslation cache is most commonly referred to as a translation look-asidebuffer (TLB). For an I/O TA, the term address translation cache or ATC is usedto differentiate it from the translation cache used by the CPU. 为了减轻这些影响, 设计时通常在执行地址转换的实体中包含地址转换缓存. 在CPU 中,地址转换缓存最常见之的是 translation look-aside buffer(TLB). 对于I/O TA, 使用术语address translation cache 或者ATC 来区分CPU使用的地址转换缓存 entity: 实体 While there are some similarities between TLB and ATC, there are importantdifferences. A TLB serves the needs of a CPU that is nominally running onethread at a time. The ATC, however, is generally processing requests frommultiple I/O Functions, each of which can be considered a separate thread. Thisdifference makes sizing an ATC difficult depending upon cost models and expectedtechnology reuse across a wide range of system configurations. 虽然对于TLB和ATC之间有一些相似点, 但是也有很大的不同之处. 一个翻译后备缓冲区（TLB）满足CPU的需求，CPU通常一次运行一个线程。而ATC通常处理来自于muliple I/OFunctions,他们中的每个都可以被认为是一个单独的thread. 这种差异使得根据成本模型和预期技术在各种系统配置中重复使用的情况下，确定地址转换缓存（ATC）的大小变得困难。The mechanisms described in this specification allow an I/O Device to participate in the translation process and provide an ATC for its own memory accesses. The benefits of having an ATC within a Device include: Ability to alleviate TA resource pressure by distributing address translation caching responsibility (reduced probability of “thrashing” within the TA) Enable ATC Devices to have less performance dependency on a system’s ATC size Potential to ensure optimal access latency by sending pretranslated requeststo central complex 本规范中描述的机制允许I/O设备参与地址转换过程，并为其自身的内存访问提供地址转换缓存（ATC）。在设备内部拥有ATC的好处包括: 在Device中有ATC的好处包括: 能够通过分配address trasnlation cache(ATC)责任来缓解TA资源方面的压力(减少TA中的”thrashing(抖动)”的可能性) 使ATC devices减少对系统ATC size的性能依赖 通过将预翻译的请求发送到 central complex ，确保最佳访问延迟的潜力。 This specification will provide the interoperability that allows PCIe Devices to be used in conjunction with a TA, but the TA and its Address Translation and Protection Table (ATPT) are treated as implementation-specific and are outside thescope of this specification. While it may be possible to implement ATS within other PCIe Components, this specification is confined to PCIe Devices and PCIe Root Complex Integrated Endpoints (RCiEPs). 本规范将提供互操作性，使PCIe设备能够与翻译代理（TA）一起使用，但TA及其地址转换和保护表（ATPT）被视为特定于实现的内容，不在本规范的范围之内。虽然可以在其他的PCIe 组件中实现ATS, 但是本规范仅限于PCIe Devices和PCIe Root Complex IntegratedEndpoints. (RCiEPs)Figure 10-1 illustrates an example platform with a TA and ATPT, along with a set of PCIe Devices and RC Integrated Endpoints with integrated ATC. A TA and an ATPT are implementation-specific and can be distinct or integrated components within a given system design. 图10-1举例说明了一个带有TA和ATPT的平台, 并带有一些PCIe Devices和RC IntegratedEndpoints with integrated ATC.TA和ATPT是 implementation-specific并且在给定的系统设计中是不同的或者集成的组件 总结: 在现代架构中, CPU 和 I/FUNCTIONS 可能因为IOMMU 组件做DMA address translation, 导致两者view 不同 DMA address translation 有很多好处，其中一点是虚拟化 DMA address translation 会带来很多额外的开销，就像CPU 的page table walk一样,所以本章节主要是定义了一套规范, 来实现类似于CPU 侧的TLB。具体的方法是: PCIe endpoint 硬件提供address translation cache 通过下面章节中描述的协议来完成该cache的同步 10.1.1 Address Translation Services (ATS) OverviewThe ATS chapter provides a new set of TLP and associated semantics. ATS uses arequest-completion protocol between a Device1 and a Root Complex (RC)to provide translation services. In addition, a new AT field is defined withinthe Memory Read and Memory Write TLP. The new AT field enables an RC todetermine whether a given request has been translated or not via the ATSprotocol. ATS这个章节提供了一组新的TLP和相关的概念.ATS在Device[1]和Root Complex(RC)之间使用一种request-completion协议来确保 translation services. 此外,在Memory Read和 Memory Write TLP中提供了一个新的AT字段. 这个新的AT字段使RC 确定给定的request是否通过ATS协议进行了translateFigure 10-2 illustrates the basic flow of an ATS Translation Request operation Figure 10-2 描述了ATS Translation Request主要的操作流程In this example, a Function-specific work request is received by asingle-Function PCIe Device. The Function determines through animplementation-specific method that caching a translation within its ATC wouldbe beneficial. There are a number of considerations a Function or software canuse in making such a determination; for example: Memory address ranges that will be frequently accessed over an extended period of time or whose associated buffer content is subject to a significant update rate Memory address ranges, such as work and completion queue structures, data buffers for low-latency communications, graphics frame buffers, host memory that is used to cache Function-specific content, and so forth 在这个示例中，single-function PCIe Device 接收到一个 Function-specific 工作请求。该 single-function 通过一种实现特定的方法确定在其 ATC 中缓存一个转换是有益的。在做出这种判断时，single-function 或软件可以考虑多个因素；例如： 该内存地址段在一段时间内被频繁访问,或者相关的buffer内容处于一个很高的更新频 率 该内存地址段是例如工作和完成队列数据结构,低延迟通信的data buffers, 图形帧缓 冲区用于缓存 Function-specific内容的内存等等 The Function generates an ATS Translation Request which is sent upstream through the PCIe hierarchy to the RC which then forwards it to the TA. An ATS Translation Request uses the same routing and ordering rules as defined in thisspecification. Further, multiple ATS Translation Requests can be outstanding at any given time; i.e., one may pipeline multiple requests on one or more TC. Each TC represents a unique ordering domain and defines the domain that mustbe used by the associated ATS Translation Completion. Function生成了一个ATS Translation Request, 该 Request发送到upstream,该过程通过PCIe 层级到RC, 该RC接下来将转发到TA. 该ATS Translation Request使用和本规范中定义的相同的routing 和 ordering 规则. 此外，在任何给定时间可以存在多个 ATS 转换请求；例如,一个 pipeline multiple requests可能需要一个或多个TC.每个TC代表一个唯一的顺序域, 并定义关联的ATS翻译完成必须使用的域即，可以在一个或多个传输通道（TC）上对多个请求进行流水线处理。每个 TC 代表一个唯一的排序域，并定义与之相关的 ATS 转换完成所必须使用的域。 further: 此外, 通常用于表示更进一步的内容、额外的信息或更深层次的解释 outstanding: 未解决的, 未处理的 总结: ATS 在Device 和 RC 之间定义了一个 request-competion 的一个协议, 另外,Memory Read/Write 相关的TLP 中也新增了一个AT字段 往往需要设备来判断哪些request需要用ATS。（往往是一些访问频繁的，或者对延迟要求较高的) 关于这些新增加的 ATS translation request, 其路由和排序规则和其他TLP一样。另外，TA这边会pipeline 多个 request，这些request 被 分到不同的TC中，每个TC是一个ordering domain. Upon receipt of an ATS Translation Request, the TA performs the following basicsteps: Validates that the Function has been configured to issue ATS TranslationRequests. Determines whether the Function may access the memory indicated by the ATSTranslation Request and has the associated access rights. Determines whether a translation can be provided to the Function. If yes, theTA issues a translation to the Function. ATS is required to support a variety of page sizes to accommodate a range of ATPT and processor implementations. Page sizes are required to be a power of two and naturally aligned. The minimum supported page size is 4096 bytes. ATS capable components A Function must be informed of the minimum translation or invalidate size it will be required to support to provide the Function an opportunity to optimize its resource utilization. The smallest minimum translation size must be 4096 bytes. The TA communicates the success or failure of the request to the RC whichgenerates an ATS Translation Completion and transmits via a Response TLPthrough a RP to the Function. An RC is required to generate at least one ATS Translation Completion per ATS Translation Request;i.e., there is minimally a 1:1 correspondence independent of the success or failure of the request. A successful translation can result in one or two ATS Translation Completion TLPs per request. The Translation Completion indicates the range of translation covered. An RC may pipeline multiple ATS Translation Completions; i.e., an RC may return multiple ATS Translation Completions and these ATS Translation Completions may be in any order relative to ATS Translation Requests. The RC is required to transmit the ATS Translation Completion using the same TC (Traffic Class) as the corresponding ATS Translation Request. The requested address may not be valid. The RC is required to issue aTranslation Completion indicating that the requested address is notaccessible. 当收到一个ATS Translation Request, TA 执行下面的主要步骤: 验证这个Function已经被配置为 可以提交 ATS Translation Requests. 确定 Function 是否可以访问 ATS Translation Request 指示的内存并具有相关的访问权限。 确定可以向该Function 提供 translation. 如果可以, TA 将为该Function 提交一个translation ATS 需要支持各种页面大小以适应一系列 ATPT(Address Translation and PageTable) 和处理器实现。 页面大小必须是2的幂,并自然对齐 支持的page size的最小值是4096字节. ATS capable 组件需要支持此最小页面的大小 必须告知功能它需要支持的最小translation/invalidate 大小，以便为function提供优化其资源利用的机会。最小的转换大小必须为 4096 字节。 TA需要告诉RC 该请求的结果是成功还是失败, 该RC会产生一个ATS TranslationCompletion并且通过RP(root port??) 向Fucntion发送一个 Response TLP RC需要为每个ATS Translation Request 生成至少一个ATS TranslationCompletion; 也就是说, 至少存在1:1的对应关系, 与request的成功失败无关 一个成功执行的translation可以为每个request回应一条或两条 ATSTranslation Completion. Translation Completion表明了 translation 的范围. RC 可以对多个 ATS 转换完成进行流水线处理；即，RC 可以返回多个 ATSTrnaslation Completion，这些 ATS Translation Completion 相对于 ATSTranslation Request 可以是任意顺序。 RC 必须使用与相应的 ATS Translation Request 相同的 TC来传输 ATSTranslation Completion. 请求的地址可能无效。RC 必须发出一个 Translation Completion，指示请求的地址不可访问。 When the Function receives the ATS Translation Completion and either updates itsATC to reflect the translation or notes that a translation does not exist. TheFunction proceeds with processing its work request and generates subsequentrequests using either a translated address or an untranslated address based onthe results of the Completion. Similar to Read Completions, a Function is required to allocate resource space for each completion(s) without causing backpressure on the PCIe Link. A Function is required to discard Translation Completions that might be “stale”. Stale Translation Completions can occur for a variety of reasons. 当一个Function收到了ATS Translation Completion并且更新了这个translation 对应的ATC或者发现这个translation不存在. (TA page table work broken). 该 Function 继续处理他的工作请求并且接下来产生的请求根据 Completion的结果使用已经翻译过的地址或者未翻译的地址 和Read Completions相似, Function需要在当前PCIe link不产生backpressure 的情况下, 为每个compleions分配resource space Function 需要丢弃已经”stale”(实效的)Translation Completions. Staletranslation Completion可能由于不同的原因产生 As one can surmise, ATS Translation Request and Translation Completion processing is conceptually similar and, in many respects, identical to PCIe Read Request and Read Completion processing. This is intentional to reduce design complexity and to simplify integration of ATS into existing and new PCIe-based solutions. Keeping this in mind, ATS requires the following: ATS capable components must interoperate with [PCIe-1.1] compliant components. ATS is enabled through a new Capability and associated configuration structure. To enable ATS, software must detect this Capability and enable the Function to issue ATS TLP. If a Function is not enabled, the Function is required not to issue ATS Translation Requests and is required to issue all DMA Read and Write Requests with the TLP AT field set to “untranslated”. ATS TLPs are routed using either address-based or Requester ID (RID) routing. ATS TLPs are required to use the same ordering rules as specified in this specification. ATS TLPs are required to flow unmodified through [PCIe-1.1] compliant Switches. A Function is permitted to intermix translated and untranslated requests. ATS transactions are required not to rely upon the address field of a memory request to communicate additional information beyond its current use as defined by the PCI-SIG. 可以推测，ATS 翻译请求和翻译完成的处理在概念上与 PCIe Read Request 和 Readcompletion 的处理相似，并且在许多方面是相同的。这种设计是有意为之，以降低设计复杂性并简化 ATS 在现有和新的基于 PCIe 的解决方案中的集成。考虑到这一点，ATS要求如下： 支持 ATS 的组件必须能够与符合 [PCIe-1.1] 标准的组件进行互操作。 ATS 需要通过一个新的 Capability 和相关的configuration structure 来启用该功能. 为了enable ATS, 软件必须识别这个 Capability 并且使能该 Function 来提交 ATS TLP. 如果一个Function 没有 enable, Function 不能提交 ATS Translation Request 并且提交的 DMA Read 和 Write Request 的TLP 中的AT field 需要设置成 “untranslated” ATS TLPs 可以通过 address-based 或者 Requester ID (RID) 路由 ATS TLPs 需要 未经修改的通过[PCIe-1.1] compliant Switches. Function 允许去混合 translated 和 untranslated request 要求 ATS translation 不依赖内存请求的地址字段传递当前PCI-SIG之外的额外信息 IMPLEMENTATION NODE Adress Range Overlap It is likely that the untranslated and translated address range will overlap, perhaps in their entirety. This is not arequirement of ATS but may be an implementation constraint on the TA so that memory requests will be properly routed. 未翻译和已翻译的地址范围很可能会重叠，甚至可能完全重叠。这并不是 ATS 的要求，但可能是 TA（翻译代理）在实现上的限制，以确保内存请求能够被正确路由。 In contrast to the prior example, Figure 10-3 illustrates an example Multi-Function Device. In this example Device, thereare three Functions. Key points to note in Figure 10-3 are: Each ATC is associated with a single Function. Each ATS-capable Function must be able to source and sink at least one of each ATS Translation Request or Translation Completion type. Each ATC is configured and accessed on a per Function basis. A Multi-Function Device is not required to implement ATS on every Function. If the ATC implementation shares resources among a set of Functions, then the logical behavior is required to be consistent with fully independent ATC implementations. 对比前一个例子, Figure 10_3 举例说明了一个Multi-Function Device. 在这个例子中的Device, 有三个Functions. Figure 10-3 需要注意的关键点如下: 每个ATC和一个单独的Function相关. 每个ATS-capable Function 必须能够去sourceand sick至少 ATS Translation Request 或 Translation Completion 一种 每个ATC都基于每个Function进行配置和访问. 一个 Multi-Function设备不需要在每个Function上 如果 ATC 实现在一组功能之间共享资源，则逻辑行为需要与完全独立的 ATC 实现一致。 Independent of the number of Functions within a Device, the following are required: A Function is required not to issue any TLP with the AT field set unless the address within the TLP was obtainedthrough the ATS Translation Request and Translation Completion protocol. Each ATC is required to only be populated using the ATS protocol; i.e., each entry within the ATC must be filledvia an ATS Translation Completion in response to the Function issuing an ATS Translation Request for a given address. Each ATC cannot be modified except through the ATS protocol. That is: Host system software cannot modify the ATC other than through the protocols defined in this specification except to invalidate one or more translations in an ATC. A Device or Function reset would be an example of an operation performed by software to change the contents of the ATC, but a reset is only allowed to invalidate entries not modify their contents. It must not be possible for host system software to use software executing on the Device to modify the ATC. 无论设备中有多少个 Function，都需要满足以下要求: Function不需要提交带有AT字段的TLP除非这个TLP中的address通过 ATS TranslationRequest 和Translation Completion 协议获取过 ATC必须仅通过 ATS 协议进行填充；也就是说，ATC 中的每个条目都必须通过 ATS翻译完成来填充，这是对 Function 发出的特定地址的 ATS 翻译请求的响应 每一个ATC 不能被修改,除非通过ATS protocol. Host 系统软件只能通过该规范中的定义的协议来修改ATC, 除非去invalidate 一个或多个ATC中的 translations. Device 或者Function Reset操作将会是一个由software 去改变ATC内容的例子,但是reset操作只能能允许去invalidate entries但是不能modify他们的内容 主机系统软件不能通过在设备上运行的软件来修改ATC。 When a TA determines that a Function should no longer maintain a translation within its ATC, the TA initiates the ATS invalidation protocol. The invalidation protocol consists of a single Invalidation Request and one or more InvalidateCompletions. 当TA判定某个功能不应再在其ATC中维护某个translation时，TA 会启动 ATSInvalidation protocol invailidation. invailidate protocol 由一个 InvalidationRequest 和一个或多个 Invalidate Completions组成As Figure 10-4 illustrates, there are essentially three steps in the ATSInvalidation protocol: 如10-4图所示, ATS Invalidate protocol 基本上包含三个步骤 The system software updates an entry in the tables used by the TA. After thetable is changed, the TA determines that a translation should be invalidatedin an ATC and initiates an Invalidation Request TLP which is transmitted fromthe RP to the example single-Function Device. The Invalidate Requestcommunicates an untranslated address range, the TC, and an RP unique tagwhich is used to correlate Invalidate Completions with the InvalidationRequest. system software更新了一个entry, 而这个entry恰好被TA使用. 在这个table被改变后,TA判定对应的translation应该在ATC中被 invailidated并且发起一个Invalidation Request TLP,该TLP从 RP传达到例子中的single-Function Device.该Invalidate Request 传递了一个 untranslated address range, TC, 和一个 RPunique tag, 该tag用来把 Invalidate Completions 和 Invalidation Request 关联起来 The Function receives the Invalidate Request and invalidates all matching ATCentries. A Function is not required to immediately flush all pending requestsupon receipt of an Invalidate Request. If transactions are in a queue waitingto be sent, it is not necessary for the Function to expunge requests from thequeue even if those transactions use an address that is being invalidated. 该function收到了Invalidate Request并且无效了所有对应的ATC entries. 在收到Invalidate Request 后, function 不需要立即flush 所有pending的 requests.如果一个 transactions 正在队列中等待发送, 则该function没有必要从队列中删除请求,即使这些事务使用的address正在被 invailidated A Function is required not to indicate the invalidation has completed until all outstanding Read Requests or Translation Requests that reference the associated translated address have been retired or nullified. A Function 必须确保，只有在所有引用相关翻译地址的未完成 Read Requests 或Translation Requests 都已完成或取消后，才能指示invalidation 已经完成。 A Function is required to ensure that the Invalidate Completion indication to the RC will arrive at the RC after any previously posted writes that use the “stale” address. Function需要保证 发送到RC的 Invalidate Completion 到达RC之前, 之前任何的posted writes都必须使用 “stale”(老的,旧的) 地址 When a Function has ascertained that all uses of the translated address arecomplete, it issues one or more ATS Invalidate Completions. 当该function 确定了所用这个translated address相关请求都已经complete, 他会提交1个或多个ATS Invalidate Completions An Invalidate Completion is issued for each TC that may have referenced therange invalidated. These completions act as a flush mechanism to ensure thehierarchy is cleansed of any in-flight transactions which may containreferences to the translated address. 为每个引用了无效范围的TC发出 Invalidate Completion. 这些 completions 充当了一个flush 机制来保证 hierarchy已经清除了任何可能包含translated address的正在处理的事物 The number of Completions required is communicated within each Invalidate Completion. A TA or RC implementation can maintain a counter to ensure that all Invalidate Completions are received before considering the translation to no longer be in use. 每个Invalidate Completion 都会包含Completions的数量.TA或者RC的实现中会包含一个计数器来确保在认为这个translation 没有人在使用之前, 所有的Invalidate Completions都已经收到. If more than one Invalidation Complete is sent, the Invalidate Completion sent in each TC must be identical in the fields detailed in Section 10.3.2 . 如果多个 Invalidate Complete 发出, 每个TC 发出的Invalidate Completion 需要和 Section 10.3.2中描述的字段保持一致 An Invalidate Completion contains the ITAG from Invalidate Request to enablethe RC to correlate Invalidate Requests and Completions. Invalidate Completion 中包含来自于 Invalidate Request 中的ITAG 来保证 RC可以将这些 Invalidate Request和 Completion 联系起来 10.1.2 Page Request Interface ExtensionATS improves the behavior of DMA based data movement. An associated Page Request Interface (PRI) provides additional advantages by allowing DMA operations to be initiated without requiring that all the data to be moved into or out of system memory be pinned. The overhead associated with pinning memory may be modest, but the negativeimpact on system performance of removing large portions of memory from the pageable pool can be significant. ATS 改善了基于 DMA 的数据移动行为。相关的页面请求接口（PRI）通过允许在不需要将所有数据固定到系统内存中的情况下启动 DMA 操作，提供了额外的优势。(感觉这里是说,不需要所有的内存请求的地址都是 present的). pinning memory相关的开销可能不明显,但是从pageable pool 中删除大量的内存对系统性能负面影响可能很大. Page able????PRI is functionally independent of the other aspects of ATS. That is, a devicethat supports ATS need not support PRI, but PRI is dependent on ATS’scapabilities. PRI 在功能方面是独立于 ATS 其他部分. 展开来说, 一个设备支持ATS 不一定支持PRI,但是 PRI 依赖 ATS的capabilitisIntelligent I/O devices can be constructed to make good use of a more dynamic memory interface. Pinning will always have the best performance characteristics from a device’s perspective-all the memory it wants to touch is guaranteed to be present. However, guaranteeing the residence of all the memory a device might touch can be problematic and force a sub-optimal level of device awareness on a host. Allowing a device to operate more independently (to page fault when it requires memory resources that are not present) provides a superior level of coupling between device and host.可以构建智能 I/O 设备以充分利用更动态的内存接口。从设备的全视角来看,Pining 总是会有更高的性能特性 -- 该设备想要touch 的内存需要保证present.但是, 保证设备可能touch的所有内存都present可能是有问题的,并且会对hostdevice awareness 水平强制处于 sub-optimal. 允许设备操作来更加独立(当他需要的内存资源不是present的时候,会触发一个page fault)在设备和主机之间提供了更高级别(更良好)的耦合. (降低了耦合性, 或者说host对于device不需要增加一些特殊的管理了)The mechanisms used to take advantage of a Page Request Interface are very device specific. As an example of a model in which such an interface could improve overall system performance, let us examine a high-speed LAN device. Such adevice knows its burst rate and need only have as much physical buffer space available for inbound data as it can receive within some quantum. A vector of unpinned virtual memory pages could be made available to the device, that thedevice then requests as needed to maintain its burst window. This minimizes the required memory footprint of the device and simplifies the interface with the host, both without negatively impacting performance.利用 Page Request Interface 的机制实现是非常特定于设备的. 让我们来参考一个高速的LAN 设备, 作为例子模型来展示像这样的一个接口可以提高系统整体的性能.这样的一个设备指导它的burst rate(突发速率,最大速率)并且只需要为在一定范围内它可以接受的数据提供尽可能多的物理buffer空间. 可以向设备提供一个不固定的虚拟内存页的vector(也就是所有的page不一定是pinned,可能不是present，可能present，但是指向的physical page不同)， 之后设备需要维护他的 burst windows. 这将最小化设备所需的内存占用，简化与主机的接口，并且两者不会产生性能方面的负面影响.The ability to page, begs the question of page table status flag management. Typical TAs associate flags (e.g., dirty and access indications) with each untranslated address. Without any additional hints about how to manage pages mapped to a Function, such TAs would need to conservatively assume that when they grant a Function permission to read or write a page, that Function will use the permission. Such writable pages would need to be marked as dirty before their translated addresses are made available to a Function.对于page来说，该 ability回避了page table status flag 管理的问题。典型的TA 会将flags(eg. dirty &amp;&amp; access 标志位)和每个未翻译的address联系起来。如果没有关于如何管理映射到function页面的额外的指示, 这些TAs需要谨慎的假设当他们授予一个function去读写一个page 权限, 这些function将会使用这些权限.(当function 去write, 那么认为该page就有write的权限).这些可写页面需要在translated address 提供给function之前, 需要标记成脏页This conservative dirty-on-write-permission-grant behavior is generally not a significant issue for Functions that do not support paging, where pages are pinned and the cost of saving a clean page to memory will seldom be paid. However, Functions that support the Page Request Interface could pay a significant penalty if all writable pages are treated as dirty, since such Functions operate without pinning their accessible memory footprints and may issue speculative page requests for performance. The cost of saving clean pages (instead of just discarding them) in such systems can diminishthe value of otherwise attractive paging techniques. This can cause significant performance issues and risk functional issues in circumstances where the backing store is unable to be written, such as a CD-ROM.对于不支持paging的function来说, 保守的 dirty-on-write-permission-grant行为通常不会是一个重要问题, 其中固定页面在保存clean page到内存中将花费很少代价. 但是支持 Page Request Interface 的function可能会付出比较大的代价,如果所有的可写入的page都被对待为dirty的话,因为这些function不会固定他们要访问的内存空间并且可能会有一些预测性质的 page request 来提高性能(预读).在无法写入后段存储(例如 CD-ROM) 的情景下, 可能会导致严重的性能问题以及可能会造成一些安全问题(risk functional issues)The No Write (NW) flag in Translation Requests indicates that a Function is willing to restrict its usage to only reading thepage, independent of the access rights that would otherwise have been granted.Translation Requests NO Write (NW) flag 表明function将其使用限制为仅读取页面，而与原本授予的访问权限无关。If a device chooses to request only read access by issuing a Translation Request with the NW flag Set and later determines that it needs to write to the page, then the device must issue a new Translation Request.如果 device 通过发出带有 NW flag 的 Translation Request 来提交只读请求 ,然而之后决定需要去写这个page, 这样的话,device必须发出一个新的 Translation RequestUpon receiving a Translation Request with the NW flag Clear, TAs are permitted to mark the associated pages dirty. It is strongly recommended that Functions not issue such Requests unlessthey have been given explicit write permission. An example of write permission is where the host issues a command to a Function to load data from a storage device and write that data into memory.当受到一个不带 NW flag的 Translation Request, TAs 允许去标记相关的page 为dirty . 强烈建议function 不要发出这样的请求,除非他们已经获得了明确的写入权限.写入权限的一个例子是, 当主机提交了给function一个 从storage device 中 load data的cmd并且将该数据写入内存10.1.3 Process Address Space ID (PASID)Certain TLPs can optionally be associated with a Process Address Space ID (PASID). This value is conveyed using the PASID TLP Prefix. The PASID TLP Prefix is defined in the Section 6.20 .某些TLPs 可以选择性的带有 Process Address Space ID (PASID).该值通过 PASID TLP Prefix(前缀)标识. PASID TLP Prefix 在Section 6.20 中定义The PASID TLP Prefix is permitted on: Memory Requests (including Untranslated AtomicOp Requests) with Untranslated Addresses Address Translation Requests Page Request Messages ATS Invalidation Requests PRG Response MessagesPASID TLP Prefix允许在下面的请求中使用:\t\t带有 Untranslated Adress Memory Requests(包括 Untranslated\tAtomicOp Requests)\t\t\tAddress Translation Requests\t\t\tPage Request Messages\t\t\tATS Invalidation Requests\t\t\tRPG Response Messages\tUsage of the PASID TLP Prefix for Untranslated Memory Requests is defined in Section 6.20 . This section describes PASIDTLP Prefix for the remaining TLPs.对于 Untranslated Memory Requests使用 PASID TLP Prefix在Section 6.20中定义. 该章节描述了其他 TLPs 中的PASID TLP PrefixWhen a Request does not have a PASID TLP Prefix, the Untranslated Address represents an address space associated with the Requester ID.When a Request has a PASID TLP Prefix, the Untranslated Address represents an address space associated with both the Requester ID and the PASID value.当一个Requset 不带有 PASID TLP Prefix, 这个 Untranslated Address 表示与 Requester ID相关的地址空间当一个Request 带有 PASID TLP Prefix, 这个 UntranslatedAddress 表示 与Requester ID和PASID 值 相关的地址空间(Requester ID + PASID --&gt; request)When a Response has a PASID TLP Prefix, the PASID value reflects the address space associated the corresponding Request.当一个 Response 带有 PASID TLP Prefix, 这个 PASID的值反映了与其相应的请求相关的地址空间Each Function has an independent set of PASID values. The PASID field is 20 bits wide however the effective width is constrained by the lesser of the width supported by the Root Complex (TA) and the width supported by the Function (ATC). Unused upper bits of the PASID value must be 0b.每个 function 都有一组独立的 PASID 值. PASID字段有20 bits宽, 但是有效的宽度会被 Root Complex (TA) 和function (ATC) 限制的更短.PASID的没有使用的高bits位必须是0For Endpoints in systems where a Virtual Intermediary (VI) is present, Untranslated.对于一个存在 Virtual Intermediary (VI) (VMM) 的Enpoints, 是 UntranslatedAddresses with an associated PASID are typically used to represent Guest Virtual Addresses (GVA) and Untranslated Addresses that are not associated with a PASID represent Guest Physical Addresses (GPA). The TA could be designed so that the VI manages the tables used to perform translations from GPA to Translated Addresses while the individual Guest Operating Systems manage tables used to perform translations from GVA to GPA. When translating an address with anassociated PASID, the TA performs both translations and returns the resulting Translated Address (i.e., GVA to GPA followed by GPA to Translated Address). The intermediate GPA value is not visible to the ATC.带有相关的 PASID 的address 通常用于标识 Guest Virtual Address (GVA),不带 PASID 的 Untranslated Address标识 Guest Physical Address (GPA).TA这样设计以便 VI 管理用于执行从GPA 到 Translated Address 页表,而独立的Guest 操作系统管理用于从GVA到 GPA的页表.当 转换一个带有相关 PASID的地址, TA 执行 两种translations 并且返回 Translated Address (也就是说: GVA --&gt; GPA 然后是 GPA --&gt; Translated Address).中间的GPA值对ATC不可见When an ATC invalidates a cached GPA mapping, it invalidates the GPA mapping and also invalidates all GVA mappings in the ATC. When the GPA invalidate completes, the VI can safely remove pages backing GPA memory range from a Guest Operating System. The VI does not need to know which GVA mappings involved the GPA mapping." }, { "title": "CHAPTER 2 Coherence Basics", "url": "/posts/coherence_basic/", "categories": "cache", "tags": "cache", "date": "2025-03-29 14:50:00 +0800", "snippet": "In this chapter, we introduce enough about cache coherence to understand howconsistency models interact with caches. We start in Section 2.1 by presentingthe system model that we consider throughou...", "content": "In this chapter, we introduce enough about cache coherence to understand howconsistency models interact with caches. We start in Section 2.1 by presentingthe system model that we consider throughout this primer. To simplify theexposition in this chapter and the follow- ing chapters, we select the simplestpossible system model that is sufficient for illustrating the important issues;we defer until Chapter 9 issues related to more complicated system models.Section 2.2 explains the cache coherence problem that must be solved and howthe possibility of incoherence arises. Section 2.3 precisely defines cachecoherence. 在本章中，我们介绍了足够的cache coherence相关内容，以理解consistency模型如何与缓存交互。 我们在Section 2.1中开始介绍我们在整个入门教材中考虑的系统模型。为了简化本章及后续章节的阐述，我们选择了一个足够简单的系统模型来说明重要问题；与更复杂系统模型相关的问题将推迟到Chapter 9讨论。 Section 2.2解释了必须解决的cache coherence问题以及不一致性可能出现的原因。 Section 2.3精确地定义了cache coherence。 2.1 BASELINE SYSTEM MODELIn this primer, we consider systems with multiple processor cores that sharememory. That is, all cores can perform loads and stores to all (physical)addresses. The baseline system model includes a single multicore processor chipand off-chip main memory, as illustrated in Figure 2.1. The multicore processorchip consists of multiple single-threaded cores, each of which has its ownprivate data cache, and a last-level cache (LLC) that is shared by all cores.Throughout this primer, when we use the term “cache,” we are referring to acore’s private data cache and not the LLC. Each core’s data cache is accessedwith physical addresses and is write-back. The cores and the LLC communicatewith each other over an interconnection network. The LLC, despite being on theprocessor chip, is logically a “memory-side cache” and thus does not introduceanother level of coherence issues. The LLC is logically just in front of thememory and serves to reduce the average latency of memory accesses and increasethe memory’s effective bandwidth. The LLC also serves as an on-chip memorycontroller. 在本入门教材中，我们考虑具有多个处理器核心共享内存的系统。也就是说，所有核心都可以对所有（物理）地址进行加载和存储。基础系统模型包括一个多核处理器芯片和芯片外的主存，如图2.1所示。多核处理器芯片由多个单线程核心组成，每个核心都有自己的私有数据缓存，以及一个由所有核心共享的最后一级缓存（LLC）。在整个入门教材中，当我们使用“缓存”这个术语时，我们指的是核心的私有数据缓存，而不是LLC。每个核心的数据缓存通过物理地址访问，并采用写回（write-back）机制。核心和LLC通过互连网络进行通信。尽管LLC位于处理器芯片上，但在逻辑上它是一个“内存侧缓存”，因此不会引入另一级的coherence问题。LLC在逻辑上位于内存前面，旨在减少内存访问的平均延迟并增加内存的有效带宽。LLC还充当片上内存控制器。This baseline system model omits many features that are common but that are notrequired for purposes of most of this primer. These features includeinstruction caches, multiple-level caches, caches shared among multiple cores,virtually addressed caches, TLBs, and coherent di- rect memory access (DMA).The baseline system model also omits the possibility of multiple multicorechips. We will discuss all of these features later, but for now, they would addunnec- essary complexity. 该基础系统模型省略了许多常见但对于本入门教材的大部分内容来说并不必要的特性。这些特性包括指令缓存、多级缓存、多个核心共享的缓存、虚拟地址缓存、TLB（转换后备缓冲区）以及一致性直接内存访问（DMA）。基础系统模型也不考虑多个多核芯片的可能性。我们将在后面讨论所有这些特性，但目前它们会增加不必要的复杂性。2.2 THE PROBLEM: HOW INCOHERENCE COULD POSSIBLY OCCURThe possibility of incoherence arises only because of one fundamental issue:there exist multiple actors with access to caches and memory. In modernsystems, these actors are processor cores, DMA engines, and external devicesthat can read and/or write to caches and memory. In the rest of this primer, wegenerally focus on actors that are cores, but it is worth keeping in mind thatother actors may exist. fundamental: 基本的，根本的 不一致性问题的可能性仅仅源于一个根本性问题：存在多个访问缓存和内存的主体。在现代系统中，这些主体包括处理器核心、DMA引擎以及可以读写缓存和内存的外部设备。在本入门教材的其余部分中，我们通常关注作为主体的核心，但值得记住的是，其他主体也可能存在。Table 2.1 illustrates a simple example of incoherence. Initially, memorylocation A has the value 42 in memory as well as both of the cores’ localcaches. At time 1, Core 1 changes the value at memory location A from 42 to 43in its cache, making Core 2’s value of A in its cache stale. Core 2 executes awhile loop loading, repeatedly, the (stale) value of A from its local cache.Clearly, this is an example of incoherence as the store from Core 1 has not notbeen made visible to Core 2 and consequently C2 is stuck in the while loop. 表2.1展示了一个不一致性的简单例子。最初，内存位置A在内存中以及两个核心的本地缓存中都有值42。在时间点1，核心1将其缓存中内存位置A的值从42更改为43，使得核心2缓存中的A值变得过时。核心2执行一个while循环，不断从其本地缓存中加载（过时的）A值。显然，这是不一致性的一个例子，因为核心1的存储操作没有对核心2可见，导致核心2陷入while循环中。To prevent incoherence, the system must implement a cache coherence protocolthat makes the store from Core 1 visible to Core 2. The design andimplementation of these cache coherence protocols are the main topics ofChapters 6–9. 为了防止不一致性，系统必须实现一个缓存一致性协议，使核心1的存储操作对核心2可见。这些缓存一致性协议的设计和实现是第6至第9章的主要内容。Table 2.1: Example of incoherence. Assume the value of memory at memorylocation A is ini- tially 42 and cached in the local caches of both cores. 表2.1：不一致性的例子。假设内存位置A的值最初为42，并被缓存到两个核心的本地缓存中。2.3 THE CACHE COHERENCE INTERFACEInformally, a coherence protocol must ensure that writes are made visible toall processors. In this section, we will more formally understand coherenceprotocols through the abstract interfaces they expose. Informally : 非正式的formally : 正式的 非正式地说，缓存一致性协议必须确保写操作对所有处理器可见。在本节中，我们将通过它们所暴露的抽象接口更正式地理解一致性协议。The processor cores interact with the coherence protocol through a coherenceinterface (Figure 2.2) that provides two methods: (1) a read-request method thattakes in a memory location as the parameter and returns a value; and (2) awrite-request method that takes in a memory location and a value (to be written)as parameters and returns an acknowledgment. 处理器核心通过一个一致性接口与一致性协议进行交互（如图2.2所示），该接口提供了两种方法： （1）读取请求方法，该方法将内存位置作为参数并返回一个值； （2）写入请求方法，该方法将内存位置和一个要写入的值作为参数，并返回一个确认信 息。There are many coherence protocols that have appeared in the literature and beenemployed in real processors. We classify these protocols into two categoriesbased on the nature of their coherence interfaces—specifically, based on whetherthere is a clean separation of coherence from the consistency model or whetherthey are indivisible. 在文献中出现并在实际处理器中使用的coherence协议有很多。我们根据其coherence接口的性质将这些协议分为两类——具体来说，是基于coherence与consistency模型之间是否存在明确的分离，或者它们是否是不可分割的。Consistency-agnostic coherence. In the first category, a write is madevisible to all other cores before returning. Because writes are propagatedsynchronously, the first category presents an interface that is identical tothat of an atomic memory system (with no caches). Thus, any subsystem thatinteracts with the coherence protocol—e.g., the processor core pipeline—can as-sume it is interacting with an atomic memory system with no caches present. Froma consis- tency enforcement perspective, this coherence interface enables a niceseparation of concerns. The cache coherence protocol abstracts away the cachescompletely and presents an illusion of atomic memory—it is as if the caches areremoved and only the memory is contained within the coherence box (Figure 2.2)—while the processor core pipeline enforces the orderings mandated by theconsistency model specification. agnostic : 通常指的是对某个特定的细节或差异不关心或不依赖consistency-agnostic coherence: 可以翻译为“与consistency无关的coherence”。这意味着这种coherence协议的设计和实现不依赖于特定的consistency模型，它们可以与多种consistency模型一起工作，而无需对这些模型的细节进行特殊处理。 Consistency-agnostic coherence。在第一类中，一次写操作在返回之前对所有其他核心可见。由于写操作是同步传播的，第一类提供的接口与atomic内存系统（没有缓存）相同。因此，任何与coherence协议交互的子系统，例如处理器核心流水线，都可以假设它正在与一个没有缓存的原子内存系统交互。从consistency执行的角度来看，这种coherence接口实现了关注点的良好分离。缓存一致性协议完全抽象掉了缓存，并呈现出原子内存的假象——就好像缓存被移除，只剩下内存包含在coherence框中（如图2.2所示）——而处理器核心流水线则执行consistency模型规范所要求的顺序。Consistency-directed coherence. In the second, more-recent category, writesare propagated asynchronously—a write can thus return before it has been madevisible to all processors, thus allowing for stale values (in real time) to beobserved. However, in order to correctly enforce consistency, coherenceprotocols in this class must ensure that the order in which writes areeventually made visible adheres to the ordering rules mandated by theconsistency model. Referring back to Figure 2.2, both the pipeline and thecoherence protocol enforce the orderings mandated by the consistency model. Thissecond category emerged to support throughput-based general-purpose graphicsprocessing units (GP-GPUs) and gained prominence after the publication of thefirst edition of this primer.1 Consistency-directed coherence。在第二类较新的范畴中，写操作是异步传播的——因此，写操作可以在尚未对所有处理器可见之前返回，这样就可能观察到过时的值（在实际时间上）。然而，为了正确执行一致性，这一类的coherence协议必须确保写操作最终可见的顺序符合一致性模型所要求的顺序规则。回到图2.2，流水线和coherence协议都执行了一致性模型规定的顺序。这第二类的出现是为了支持基于吞吐量的通用图形处理单元（GP-GPUs），并在本指南的第一版出版后获得了显著发展。The primer (and the rest of the chapter) focuses on the first class of coherenceprotocols. We discuss the second class of coherence protocols in the context ofheterogeneous coherence (Chapter 10). 本指南（以及本章的其余部分）主要关注第一类coherence协议。我们将在异构coherence的背景下（第10章）讨论第二类coherence协议。2.4 (CONSISTENCY-AGNOSTIC) COHERENCE INVARIANTSWhat invariants must a coherence protocol satisfy to make the caches invisibleand present an abstraction of an atomic memory system? 为了使缓存不可见并呈现原子内存系统的抽象，coherence协议必须满足哪些不变性？There are several definitions of coherence that have appeared in textbooks andin published papers, and we do not wish to present all of them. Instead, wepresent the definition we prefer for its insight into the design of coherenceprotocols. In the sidebar, we discuss alternative definitions and how theyrelate to our preferred definition. 在教科书和已发表的论文中出现了几种coherence的定义，我们不打算全部展示，而是介绍我们 prefer definition，因为它为coherence协议的设计提供了深刻的见解。在侧边栏中，我们讨论了其他定义以及它们与我们perfer definition 之间的关系。We define coherence through the single-writer–multiple-reader (SWMR) invariant.For any given memory location, at any given moment in time, there is either asingle core that may write it (and that may also read it) or some number ofcores that may read it. Thus, there is never a time when a given memory locationmay be written by one core and simultaneously either read or written by anyother cores. Another way to view this definition is to consider, for each memorylocation, that the memory location’s lifetime is divided up into epochs. In eachepoch, either a single core has read-write access or some number of cores(possibly zero) have read-only access. Figure 2.3 illustrates the lifetime of anexample memory location, divided into four epochs that maintain the SWMRinvariant. 我们通过单一写入者–多重读取者（SWMR）不变性来定义coherence。对于任何给定的内存位置，在任何给定的时刻，要么是由一个核心可以写入（也可以读取），要么是由若干核心可以读取。因此，永远不会出现一个内存位置可以被一个核心写入的同时，也被其他任何核心读取或写入的情况。另一种看待这个定义的方法是考虑，对于每个内存位置，其生命周期被划分为多个时期。在每个时期中，要么是一个核心具有读写访问权限，要么是若干核心（可能为零）具有只读访问权限。图2.3展示了一个示例内存位置的生命周期，分为四个保持SWMR不变性的时期。In addition to the SWMR invariant, coherence requires that the value of a givenmemory location is propagated correctly. To explain why values matter, let usreconsider the example in Figure 2.3. Even though the SWMR invariant holds, ifduring the first read-only epoch Cores 2 and 5 can read different values, thenthe system is not coherent. Similarly, the system is incoherent if Core 1 failsto read the last value written by Core 3 during its read-write epoch or any ofCores 1, 2, or 3 fail to read the last write performed by Core 1 during itsread-write epoch. 除了SWMR不变性之外，coherence还要求给定内存位置的值被正确传播。为了说明为什么值很重要，让我们重新考虑图2.3中的例子。即使SWMR不变性成立，如果在第一个只读时期内，核心2和核心5读取到了不同的值，那么系统就不是一致的。同样地，如果在其读写时期内，核心1未能读取到核心3最后写入的值，或者核心1、2或3中的任何一个未能读取到核心1在其读写时期内执行的最后一次写入，那么系统也是不一致的。Thus, the definition of coherence must augment the SWMR invariant with a datavalue invariant that pertains to how values are propagated from one epoch tothe next. This invariant states that the value of a memory location at thestart of an epoch is the same as the value of the memory location at the end ofits last read-write epoch. 因此，coherence的定义必须在SWMR不变性的基础上增加一个数据值不变性，该不变性涉及值如何从一个时期传播到下一个时期。这个不变性表明，一个时期开始时的内存位置的值与其上一个读写时期结束时的内存位置的值相同。There are other interpretations of these invariants that are equivalent. Onenotable ex- ample [5] interpreted the SMWR invariants in terms of tokens. Theinvariants are as follows. For each memory location, there exists a fixednumber of tokens that is at least as large as the number of cores. If a corehas all of the tokens, it may write the memory location. If a core has one ormore tokens, it may read the memory location. At any given time, it is thusimpossible for one core to be writing the memory location while any other coreis reading or writing it. 这些不变性还有其他等价的解释。其中一个显著的例子 [5] 是用令牌来解释SMWR不变性。这些不变性如下所述：对于每个内存位置，存在一个固定数量的令牌，至少与核心的数量一样多。如果一个核心拥有所有的令牌，它可以写入该内存位置。如果一个核心拥有一个或多个令牌，它可以读取该内存位置。因此，在任何给定时间，不可能出现一个核心正在写入内存位置的同时，任何其他核心正在读取或写入它的情况。 Coherence invariants Single-Writer, Multiple-Read (SWMR) Invariant. For any memory location A,at any given time, there exists only a single core that may write to A (andcan also read it) or some number of cores that may only read A. 单一写入者、多重读取者（SWMR）不变量。对于任何内存位置A，在任何给定的时刻， 只存在一个核心可以写入A（并且也可以读取它），或者有若干核心只能读取A。 Data-Value Invariant. The value of the memory location at the start of anepoch is the same as the value of the memory location at the end of the itslast read-write epoch. 数据值不变量。一个时期开始时的内存位置的值与其上一个读写时期结束时的内存位置的值相同。 2.4.1 MAINTAINING THE COHERENCE INVARIANTSThe coherence invariants presented in the previous section provide someintuition into how coherence protocols work. The vast majority of coherenceprotocols, called “invalidate protocols,” are designed explicitly to maintainthese invariants. If a core wants to read a memory location, it sends messagesto the other cores to obtain the current value of the memory location and toensure that no other cores have cached copies of the memory location in aread-write state. These messages end any active read-write epoch and begin aread-only epoch. If a core wants to write to a memory location, it sendsmessages to the other cores to obtain the current value of the memory location,if it does not already have a valid read-only cached copy, and to ensure thatno other cores have cached copies of the memory location in either read-only orread-write states. These messages end any active read-write or read-only epochand begin a new read-write epoch. This primer’s chapters on cache coherence(Chapters 6–9) expand greatly upon this abstract description of invalidateprotocols, but the basic intuition remains the same. intuition: 直觉, 直观理解, 在技术上下文中，它可以表示对某个概念或机制的基本理 解或感知。 前一节中介绍的 coherence invariants 提供了一些关于一致性协议如何工作的intuition。绝大多数的一致性协议，称为“无效化协议”，其设计明确旨在维护这些invariants。如果一个核心想要读取一个内存位置，它会向其他核心发送消息，以获取该内存位置的当前值，并确保没有其他核心在读写状态下缓存了该内存位置的副本。这些消息结束了任何活跃的读写时期，并开始一个只读时期。如果一个核心想要写入一个内存位置，它会向其他核心发送消息，以获取该内存位置的当前值（如果它尚未拥有有效的只读缓存副本），并确保没有其他核心在只读或读写状态下缓存了该内存位置的副本。这些消息结束了任何活跃的读写或只读时期，并开始一个新的读写时期。本教程中关于缓存一致性的章节（第6至9章）将大大扩展对无效化协议的这一抽象描述，但基本的intuition保持不变。2.4.2 THE GRANULARITY OF COHERENCEA core can perform loads and stores at various granularities, often rangingfrom 1–64 bytes. In theory, coherence could be performed at the finestload/store granularity. However, in practice, coherence is usually maintainedat the granularity of cache blocks. That is, the hardware enforces coherence ona cache block by cache block basis. In practice, the SWMR invariant is likelyto be that, for any block of memory, there is either a single writer or somenumber of readers. In typical systems, it is not possible for one core to bewriting to the first byte of a block while another core is writing to anotherbyte within that block. Although cache-block granularity is common, and it iswhat we assume throughout the rest of this primer, one should be aware thatthere have been protocols that have maintained coherence at finer and coarsergranularities. 一个核心可以在不同的粒度上执行加载和存储操作，通常范围从1到64字节。理论上，缓存一致性可以在最细的加载/存储粒度上进行。然而，在实践中，缓存一致性通常是在缓存块的粒度上维护的。也就是说，硬件在每个缓存块的基础上强制执行一致性。在实践中，SWMR invariant 很可能是，对于任何内存块，要么有一个单一的写入者，要么有若干个读取者。在典型系统中，一个核心不可能在写入一个块的第一个字节的同时，另一个核心在该块内的另一个字节进行写入。虽然缓存块粒度很常见，并且这是我们在本教程其余部分中假设的粒度，但需要注意的是，也存在在更细和更粗粒度上维护一致性的协议。" }, { "title": "CHAPTER 1 Introduction to Consistency and Coherence", "url": "/posts/consistency_and_coherence/", "categories": "cache", "tags": "cache", "date": "2025-03-29 10:42:00 +0800", "snippet": "1.2 COHERENCE (A.K.A., CACHE COHERENCE)Unless care is taken, a coherence problem can arise if multiple actors (e.g.,multiple cores) have access to multiple copies of a datum (e.g., in multiplecache...", "content": "1.2 COHERENCE (A.K.A., CACHE COHERENCE)Unless care is taken, a coherence problem can arise if multiple actors (e.g.,multiple cores) have access to multiple copies of a datum (e.g., in multiplecaches) and at least one access is a write. Consider an example that is similarto the memory consistency example. A student checks the online schedule ofcourses, observes that the Computer Architecture course is being held in Room152 (reads the datum), and copies this information into her calendar app in hermobile phone (caches the datum). Subsequently, the university registrar decidesto move the class to Room 252, updates the online schedule (writes to thedatum) and informs the students via a text message. The student’s copy of thedatum is now stale, and we have an incoherent situation. If she goes to Room152, she will fail to find her class. Examples of incoherence from the world ofcomputing, but not including computer architecture, include stale web cachesand programmers using un-updated code repositories. 除非小心处理，否则当多个执行者（例如，多个核心）可以访问数据的多个副本（例如，在多个缓存中）并且至少有一个访问是写操作时，可能会出现 coherence 问题。考虑一个类似于 memory consistency 问题的例子。一个学生查看在线课程表，发现计算机体系结构课程在152教室上课（读取数据），并将此信息复制到她手机上的日历应用程序中（缓存数据）。随后，大学注册主任决定将课程移至252教室，更新了在线课程表（写入数据）并通过短信通知学生。此时，学生的数据副本已经过时，导致了一个 incoherent 的情况。如果她去152教室，就找不到她的课。来自计算机世界的 incoherence 例子（但不包括计算机体系结构）包括过时的网页缓存和程序员使用未更新的代码库。Access to stale data (incoherence) is prevented using a coherence protocol,which is a set of rules implemented by the distributed set of actors within asystem. Coherence protocols come in many variants but follow a few themes, asdeveloped in Chapters 6–9. Essentially, all of the variants make oneprocessor’s write visible to the other processors by propagating the write toall caches, i.e., keeping the calendar in sync with the online schedule. Butprotocols differ in when and how the syncing happens. There are two majorclasses of coherence protocols. In the first approach, the coherence protocolensures that writes are propagated to the caches synchronously. When the onlineschedule is updated, the coherence protocol ensures that the student’s calendaris updated as well. In the second approach, the coherence protocol propagateswrites to the caches asynchronously, while still honoring the consistencymodel. The coherence protocol does not guarantee that when the online scheduleis updated, the new value will have propagated to the student’s calendar aswell; however, the protocol does ensure that the new value is propagated beforethe text message reaches her mobile phone. This primer focuses on the firstclass of coherence protocols (Chapters 6–9) while Chapter 10 discusses theemerging second class. 访问过时数据（即 incoherence）通过 coherence 协议来防止，coherence 协议是一组由系统中的分布式执行者实现的规则。Coherence 协议有多种变体，但遵循几个基本主题，这些主题在第6到第9章中进行了讨论。本质上，所有变体都通过将一个处理器的写操作传播到所有缓存，使其他处理器能够看到该写操作，也就是说，保持日历与在线课程表同步。但协议在同步发生的时间和方式上有所不同。Coherence 协议主要分为两大类。第一种方法中，coherence 协议确保写操作同步传播到缓存中。当在线课程表被更新时，coherence 协议确保学生的日历也被更新。第二种方法中，coherence 协议在遵循一致性模型的同时，异步地将写操作传播到缓存。Coherence 协议不保证在线课程表更新时，新值会立即传播到学生的日历；然而，协议确保在短信到达她手机之前，新值已经传播。本入门书着重于第一类 coherence 协议（第6到第9章），而第10章讨论了新兴的第二类。 1.3 CONSISTENCY AND COHERENCE FOR HETEROGENEOUS SYSTEMSModern computer systems are predominantly heterogeneous. A mobile phoneprocessor today not only contains a multicore CPU, it also has a GPU and otheraccelerators (e.g., neural net- work hardware). In the quest forprogrammability, such heterogeneous systems are starting to support sharedmemory. Chapter 10 deals with consistency and coherence for such heteroge-neous processors. predominantly: 主要的heterogeneous: 各种各样的 结合起来翻译：以异构为主的，这通常指的是由不同类型的组件（如不同种类的处理器 或加速器）组成的系统，其中异构性是其主要特征。 现代计算机系统主要是异构的。如今的手机处理器不仅包含多核 CPU，还包括 GPU和其他加速器（例如神经网络硬件）。为了实现可编程性，这样的异构系统开始支持共享内存。第10 章讨论了这种异构处理器的一致性和一致性问题。The chapter starts by focusing on GPUs, arguably the most popular acceleratorstoday. The chapter observes that GPUs originally chose not to support hardwarecache coherence, since GPUs are designed for embarrassingly parallel graphicsworkloads that do not synchronize or share data all that much. However, theabsence of hardware cache coherence leads to programmability and/orperformance challenges when GPUs are used for general-purpose work- loads withfine-grained synchronization and data sharing. The chapter discusses in detailsome of the promising coherence alternatives that overcome these limitations—inparticular, explaining why the candidate protocols enforce the consistencymodel directly rather than implementing coherence in a consistency-agnosticmanner. The chapter concludes with a brief discussion on consistency andcoherence across CPUs and the accelerators. promising : 有前景的，有前途的promising coherence alternatives: 有前景的一致性替代方案/有前途的一致性替代方案 本章首先关注 GPU，GPU 可以说是当今最流行的加速器。本章指出，GPU最初选择不支持硬件 cache coherence，因为 GPU设计用于尴尬并行的图形工作负载，这类工作负载通常不需要太多的同步或数据共享。然而，当GPU 被用于具有细粒度同步和数据共享的通用工作负载时，缺乏硬件 cache coherence会导致可编程性和/或性能方面的挑战。本章详细讨论了一些有前景的 coherence替代方案，这些方案克服了这些限制，特别是解释了为什么候选协议直接强制执行consistency 模型，而不是以与 consistency 无关的方式实现coherence。本章最后简要讨论了在 CPU 和加速器之间的 consistency 和 coherence问题。1.4 SPECIFYING AND VALIDATING MEMORY CONSISTENCY MODELS AND CACHE COHERENCEConsistency models and coherence protocols are complex and subtle. Yet, thiscomplexity must be managed to ensure that multicores are programmable and thattheir designs can be validated. To achieve these goals, it is critical thatconsistency models are specified formally. A formal specification would enableprogrammers to clearly and exhaustively (with tool support) understand whatbehaviors are permitted by the memory model and what behaviors are not. Second,a precise formal specification is mandatory for validating implementations. consistency models 和 coherence protocols 是复杂而微妙的。然而，必须管理这种复杂性以确保多核处理器的可编程性及其设计的可验证性。为了实现这些目标，至关重要的是要正式指定 consistency models。正式的规范将使程序员能够清晰而全面地（借助工具支持）理解内存模型允许和不允许的行为。其次，精确的正式规范对于验证实现是必不可少的。Chapter 11 starts by discussing two methods for specifying systems—axiomaticand operational—focusing on how these methods can be applied for consistencymodels and coherence protocols. Then the chapter goes over techniques forvalidating implementations— including processor pipeline and coherence protocolimplementations—against their specification. The chapter discusses bothformal methods and informal testing. formal: 正式的 第十一章首先讨论了两种用于指定系统的方法——公理化方法和操作性方法，重点介绍了如何将这些方法应用于 consistency models 和 coherence protocols。接着，本章探讨了验证实现的方法，包括处理器流水线和 coherence protocol 实现与其规范的对比验证。本章讨论了正式方法和非正式测试两方面的内容。1.5 A CONSISTENCY AND COHERENCE QUIZIt can be easy to convince oneself that one’s knowledge of consistency andcoherence is sufficient and that reading this primer is not necessary. To testwhether this is the case, we offer this pop quiz. 人们很容易说服自己认为对 consistency 和 coherence 的了解已经足够，不需要阅读这本入门书。为了测试是否真是如此，我们提供了一个小测验。 该部分为阅读本书之前, 自己的思考 该部分为走读完本书之后，总结书中的内容Question 1: In a system that maintains sequential consistency, a core mustissue coherence requests in program order. True or false? (Answer is in Section3.8) 处理器为了充分利用流水线，往往会做一些优化。例如write buffer。处理器会定义一套consistency 策略，例如x86 TSO 就定义了write-store 可以乱序，这都会导致处理器 issue conherence request 和program order 不同.Question 2: The memory consistency model specifies the legal orderings ofcoherence transactions. True or false? (Section 3.8) 个人认为是对的。本身其就是定义规则，然后cache coherence 实现时，需要遵守这些规则Question 3: To perform an atomic read–modify–write instruction (e.g.,test-and-set), a core must always communicate with the other cores. True orfalse? (Section 3.9) atomic read-modify-write 个人认为是在read时，去 exclusive 该cacheline, 然后阻塞总线上其他的 write message, 直到该cpu的write动作完成，所以不需要always communicate，例如: 如果该cache本身就是 exclusive的话，直接”lock” and modify就可以了。Question 4: In a TSO system with multithreaded cores, threads may bypass valuesout of the write buffer, regardless of which thread wrote the value. True orfalse? (Section 4.4) 个人认为是不对的. 首先当前线程肯定不会bypass write buffer, 其相当于bypass memory直接从write buffer中获取到值。 另外，另一个thread也可以从write buffer中load value。因为write buffer 会导致store动作延后，而如果该core上的另一个thread bypass memory, 相当于其看到该core没有out oforder，如下图 --------------------+---------------------core 0 thread 1 |core 0 thread 2--------------------+---------------------P.W | | | R2: read new valueR1 | | |M.W | P.W 表示按照程序顺序发生写操作的点 M.W 表示该写动作发生在全局（memory）view 上的点 在上图中R2相当于 bypass memory, 在其看来 core 0 thread 1 上的P.W 的动作比R1 要早Question 5: A programmer who writes properly synchronized code relative to thehigh-level language’s consistency model (e.g., Java) does not need to considerthe architecture’s memory consistency model. True or false? (Section 5.9) 如果java在每条指令中间都夹杂着内存屏障指令，就可以不用关心memory consistency，但显然这是不可能的Question 6: In an MSI snooping protocol, a cache block may only be in one ofthree coherence states. True or false? (Section 7.2) YES. 虽然感觉很可能不对。但认知就到这里。Question 7: A snooping cache coherence protocol requires the cores tocommunicate on a bus. True or false? (Section 7.6) communicate on bus 在cpu特别多，尤其是某些cpu路径很长的情况下，效率会很低，所以后续推出了 diectory-based conherence protocalsQuestion 8: GPUs do not support hardware cache coherence. Therefore, they areunable to enforce a memory consistency model. True or False? (Section 10.1). 如果没有支持cache coherence ，就意味着其没有办法snoop 到 CPU或者其他agent的对cache的行为。但是其也需要遵守一定的 coherence， 例如GPU 做两个动作，store A，store B。 其一定需要保证GPU在内存上的操作顺序是上面描述的顺序。(memory consistency是多个并行操作对象对一个share resource(memory) 的操作原则. 而cache conherence只是实现它的一个手段。Even though the answers are provided later in this primer, we encourage readersto try to answer the questions before looking ahead at the answers.1.6 WHAT THIS PRIMER DOES NOT DOThis lecture is intended to be a primer on coherence and consistency. We expectthis material could be covered in a graduate class in about ten 75-minuteclasses (e.g., one lecture per Chapter 2 to Chapter 11).For this purpose, there are many things the primer does not cover. Some ofthese include the following. 这堂课旨在作为coherence和consistency的入门课程。我们预计这些内容可以在研究生课程中用大约十节75分钟的课程来覆盖（例如，每章从Chapter2到Chapter 11一节课）。 出于这个目的，有许多内容在这本入门教材中没有涉及。其中一些包括以下内容。 Synchronization. Coherence makes caches invisible. Consistency can makeshared mem- ory look like a single memory module. Nevertheless, programmerswill probably need locks, barriers, and other synchronization techniques tomake their programs useful. Read- ers are referred to the Synthesis Lectureon Shared-Memory synchronization [2]. 同步。Coherence使缓存对程序员不可见。Consistency可以使共享内存看起来像是一个单一的内存模块。然而，程序员可能仍然需要使用锁、屏障和其他同步技术来使他们的程序有效。读者可以参考《Synthesis Lecture on Shared-MemorySynchronization》来获取更多信息。 Commercial Relaxed Consistency Models. This primer does not cover thesubtleties of the ARM, PowerPC, and RISC-V memory models, but does describewhich mechanisms they provide to enforce order. 商业化的放松一致性模型。本入门教材不涉及ARM、PowerPC和RISC-V内存模型的细节，但描述了它们提供的用于强制顺序的机制。 Parallel programming. This primer does not discuss parallel programmingmodels, methodologies, or tools. 并行编程。本入门教材不讨论并行编程模型、方法或工具。 Consistency in distributed systems. This primer restricts itself toconsistency within a shared memory multicore, and does not cover consistencymodels and their enforcement for a general distributed system. Readers arereferred to the Synthesis Lectures on Database Replication [1] and QuorumSystems [3]. 分布式系统中的一致性。本入门教材仅限于共享内存多核中的一致性，不涵盖一般分布式系统的一致性模型及其实施。读者可以参考《Synthesis Lectures on DatabaseReplication》和《Quorum Systems》来获取更多信息。 " }, { "title": "Memory Consistency Motivation And Sequential Consistency", "url": "/posts/memory-consistency-motivation-and-sequential-consistency/", "categories": "", "tags": "", "date": "2025-03-29 00:00:00 +0800", "snippet": "This chapter delves into memory consistency models (a.k.a. memory models) thatdefine the behavior of shared memory systems for programmers and implementors.These models define correctness so that p...", "content": "This chapter delves into memory consistency models (a.k.a. memory models) thatdefine the behavior of shared memory systems for programmers and implementors.These models define correctness so that programmers know what to expect andimplementors know what to provide. We first motivate the need to define memorybehavior (Section 3.1), say what a memory con- sistency model should do (Section3.2), and compare and contrast consistency and coherence (Section 3.3).We then explore the (relatively) intuitive model of sequential consistency (SC).SC is important because it is what many programmers expect of shared memory andprovides a foun- dation for understanding the more relaxed (weak) memoryconsistency models presented in the next two chapters. We first present thebasic idea of SC (Section 3.4) and present a formalism of it that we will alsouse in subsequent chapters (Section 3.5). We then discuss implementations of SC,starting with naive implementations that serve as operational models (Section3.6), a basic implementation of SC with cache coherence (Section 3.7), moreoptimized implementa- tions of SC with cache coherence (Section 3.8), and theimplementation of atomic operations (Section 3.9). We conclude our discussion ofSC by providing a MIPS R10000 case study (Sec- tion 3.10) and pointing to somefurther reading (Section 3.11).3.1 PROBLEMS WITH SHARED MEMORY BEHAVIORTo see why shared memory behavior must be defined, consider the exampleexecution of two cores1 depicted in Table 3.1. (This example, as is the case forall examples in this chapter, assumes that the initial values of all variablesare zero.) Most programmers would expect that core C2’s register r2 should getthe value NEW. Nevertheless, r2 can be 0 in some of today’s computer systems.Hardware can make r2 get the value 0 by reordering core C1’s stores S1 and S2.Locally (i.e., if we look only at C1’s execution and do not considerinteractions with other threads), this reordering seems correct because S1 andS2 access different addresses. The sidebar on page 18 describes a few of theways in which hardware might reorder memory accesses, including these stores.Readers who are not hardware experts may wish to trust that such reordering canhappen (e.g., with a write buffer that is not first-in–first-out).With the reordering of S1 and S2, the execution order may be S2, L1, L2, S1, asillustrated in Table 3.2. Sidebar: How a Core Might Reorder Memory Access This sidebar describes a few of the ways in which modern cores may reordermemory accesses to different addresses. Those unfamiliar with these hardwareconcepts may wish to skip this on first reading. Modern cores may reorder manymemory accesses, but it suffices to reason about reordering two memoryoperations. In most cases, we need to reason only about a core reordering twomemory operations to two different addresses, as the sequential execution (i.e.,von Neumann) model generally requires that operations to the same addressexecute in the original program order. We break the possible reorderings downinto three cases based on whether the reordered memory operations are loads orstores. Store-store reordering. Two stores may be reordered if a core has a non-FIFOwrite buffer that lets stores depart in a different order than the order inwhich they entered. This might occur if the first store misses in the cachewhile the second hits or if the second store can coalesce with an earlier store(i.e., before the first store). Note that these reorderings are possible even ifthe core executes all instructions in program order. Reordering stores todifferent memory addresses has no effect on a single-threaded execution. However,in the multithreaded example of Table 3.1, reordering Core C1’s stores allowsCore C2 to see flag as SET before it sees the store to data. Note that theproblem is not fixed even if the write buffer drains into a perfectly coherentmemory hierarchy. Coherence will make all caches invisible, but the stores arealready reordered. Load-load reordering. Modern dynamically scheduled cores may execute instruc-tions out of program order. In the example of Table 3.1, Core C2 could executeloads L1 and L2 out of order. Considering only a single-threaded execution, thisreordering seems safe because L1 and L2 are to different addresses. However,reordering Core C2’s loads behaves the same as reordering Core C1’s stores; ifthe memory references execute in the order L2, S1, S2, and L1, then r2 isassigned 0. This scenario is even more plausible if the branch statement B1 iselided, so no control dependence separates L1 and L2. Load-store and store-load reordering. Out-of-order cores may also reorder loadsand stores (to different addresses) from the same thread. Reordering an earlierload with a later store (a load-store reordering) can cause many incorrectbehaviors, such as loading a value after releasing the lock that protects it (ifthe store is the unlock operation). The example in Table 3.3 illustrates theeffect of reordering an earlier store with a later load (a store-load reordering). Reordering Core C1’s accesses S1 and L1 and Core C2’s accesses S2 and L2allows the counterintuitive result that both r1 and r2 are 0. Note thatstore-load reorderings may also arise due to local bypassing in the commonlyimplemented FIFO write buffer, even with a core that executes all instructionsin program order. A reader might assume that hardware should not permit some or all of these be-haviors, but without a better understanding of what behaviors are allowed, it ishard to determine a list of what hardware can and cannot do.This execution satisfies coherence because the SWMR property is not violated, soinco- herence is not the underlying cause of this seemingly erroneous executionresult.Let us consider another important example inspired by Dekker’s Algorithm forensuring mutual exclusion, as depicted in Table 3.3. After execution, whatvalues are allowed in r1 and r2? Intuitively, one might expect that there arethree possibilities: (r1, r2) = (0, NEW) for execution S1, L1, S2, then L2 • (r1, r2) = (NEW, 0) for S2, L2, S1, and L1 (r1, r2) = (NEW, NEW), e.g., for S1, S2, L1, and L2 Surprisingly, most real hardware, e.g., x86 systems from Intel and AMD, alsoallows (r1, r2) = (0, 0) because it uses first-in–first-out (FIFO) write buffersto enhance performance. As with the example in Table 3.1, all of theseexecutions satisfy cache coherence, even (r1, r2) = (0, 0).Some readers might object to this example because it is non-deterministic(multiple out- comes are allowed) and may be a confusing programming idiom.However, in the first place, all current multiprocessors are non-deterministicby default; all architectures of which we are aware permit multiple possibleinterleavings of the executions of concurrent threads. The illusion of de-terminism is sometimes, but not always, created by software with appropriatesynchronization idioms. Thus, we must consider non-determinism when definingshared memory behavior.Furthermore, memory behavior is usually defined for all executions of allprograms, even those that are incorrect or intentionally subtle (e.g., fornon-blocking synchronization algo- rithms). In Chapter 5, however, we will seesome high-level language models that allow some executions to have undefinedbehavior, e.g., executions of programs with data races.WHAT IS A MEMORY CONSISTENCY MODEL?The examples in the previous sub-section illustrate that shared memory behavioris subtle, giving value to precisely defining (a) what behaviors programmers canexpect and (b) what optimiza- tions system implementors may use. A memoryconsistency model disambiguates these issues.A memory consistency model, or, more simply, a memory model, is a specificationof the al- lowed behavior of multithreaded programs executing with sharedmemory. For a multithreaded program executing with specific input data, itspecifies what values dynamic loads may return. Unlike a single-threadedexecution, multiple correct behaviors are usually allowed.In general, a memory consistency model MC gives rules that partition executionsinto those obeying MC (MC executions) and those disobeying MC (non-MC executions). This parti- tioning of executions, in turn, partitions implementations. An MCimplementation is a system that permits only MC executions, while a non-MCimplementation sometimes permits non-MC executions.Finally, we have been vague regarding the level of programming. We begin byassuming that programs are executables in a hardware instruction setarchitecture, and we assume that memory accesses are to memory locationsidentified by physical addresses (i.e., we are not con- sidering the impact ofvirtual memory and address translation). In Chapter 5, we will discuss issueswith high-level languages (HLLs). We will see then, for example, that a compilerallocat- ing a variable to a register can affect an HLL memory model in a mannersimilar to hardware reordering memory references.3.3 CONSISTENCY VS. COHERENCEChapter 2 defined cache coherence with two invariants that we informally repeathere. The SWMR invariant ensures that at any time for a memory location with agiven address, either (a) one core may write (and read) the address or (b) zeroor more cores may only read it. The Data-Value Invariant ensures that updates tothe memory location are passed correctly so that cached copies of the memorylocation always contain the most recent version.It may seem that cache coherence defines shared memory behavior. It does not. Aswe can see from Figure 3.1, the coherence protocol simply provides the processorcore pipeline an abstraction of a memory system. It alone cannot determineshared memory behavior; the pipeline matters, too. If, for example, the pipelinereorders and presents memory operations to the coherence protocol in an ordercontrary to program order—even if the coherence protocol does its job correctly—shared memory correctness may not ensue.In summary: Cache coherence does not equal memory consistency. A memory consistency implementation can use cache coherence as a useful “black box.” 3.4 BASIC IDEA OF SEQUENTIAL CONSISTENCY (SC)Arguably the most intuitive memory consistency model is SC. It was firstformalized by Lam- port [12], who called a single processor (core) sequential if“the result of an execution is the same as if the operations had been executedin the order specified by the program.” He then called a multiprocessorsequentially consistent if “the result of any execution is the same as if theop- erations of all processors (cores) were executed in some sequential order,and the operations of each individual processor (core) appear in this sequencein the order specified by its program.” This total order of operations is calledmemory order. In SC, memory order respects each core’s program order, but otherconsistency models may permit memory orders that do not always respect theprogram orders.Figure 3.2 depicts an execution of the example program from Table 3.1. Themiddle ver- tical downward arrow represents the memory order (&lt;m) while eachcore’s downward arrow represents its program order (&lt;p). We denote memory orderusing the operator &lt;m, so op1 &lt;m op2 implies that op1 precedes op2 in memoryorder. Similarly, we use the operator &lt;p to de- note program order for a givencore, so op1 &lt;p op2 implies that op1 precedes op2 in that core’s program order.Under SC, memory order respects each core’s program order. “Respects” means thatop1 &lt;p op2 implies op1 &lt;m op2. The values in comments (/* … */) give the valueloaded or stored. This execution terminates with r2 being NEW. More generally,all executions of Ta- ble 3.1’s program terminate with r2 as NEW. The onlynon-determinism—how many times L1 loads flag as 0 before it loads the value SETonce—is unimportant.This example illustrates the value of SC. In Section 3.1, if you expected thatr2 must be NEW, you were perhaps independently inventing SC, albeit lessprecisely than Lamport.The value of SC is further revealed in Figure 3.3, which illustrates fourexecutions of the program from Table 3.3. Figure 3.3a–c depict SC executionsthat correspond to the three intuitive outputs: (r1, r2) = (0, NEW), (NEW, 0),or (NEW, NEW). Note that Figure 3.3c depicts only one of the four possible SCexecutions that leads to (r1, r2) = (NEW, NEW); this execution is {S1, S2, L1,L2}, and the others are {S1, S2, L2, L1}, {S2, S1, L1, L2}, and {S2, S1, L2, L1}. Thus, across Figure 3.3a–c, there are six legal SC executions.Figure 3.3d shows a non-SC execution corresponding to the output (r1, r2) = (0,0). For this output, there is no way to create a memory order that respectsprogram orders. Program order dictates that: S1&lt;pL1 S2&lt;pL2But memory order dictates that: L1&lt;mS2(sor1is0) L2&lt;mS1(sor2is0)Honoring all these constraints results in a cycle, which is inconsistent with atotal order. The extra arcs in Figure 3.3d illustrate the cycle.We have just seen six SC executions and one non-SC execution. This can help usun- derstand SC implementations: an SC implementation must allow one or more ofthe first six executions, but cannot allow the seventh execution." }, { "title": "svm", "url": "/posts/svm-overflow/", "categories": "amd_spec, svm", "tags": "svm", "date": "2025-03-03 13:30:00 +0800", "snippet": " 文章主要来自AMD sdm `15 Secure Virtual MachineoverflowSVM 提供了由硬件扩展，旨在实现高效, 经济的虚拟机系统。其功能主要分为virtualization support 和 security support, 概述如下: virtualization support memory gues...", "content": " 文章主要来自AMD sdm `15 Secure Virtual MachineoverflowSVM 提供了由硬件扩展，旨在实现高效, 经济的虚拟机系统。其功能主要分为virtualization support 和 security support, 概述如下: virtualization support memory guest/host tagged TLB External (DMA) access protection for memory Nested paging support interrupt virtualization Intercepting physical interrupt delivery virtual interrupts Sharing a physical APIC Direct interrupt delivery CPU virtualization: guest mode &amp;&amp; host mode switch intercept: ability to intercept selected instructions or events·in the guest securitry support Attestation: SKINIT Encrypted memory: SEV, SEV-ES Secure Nested Paging: SEV-SNP CPU Enabling SVM 虚拟化扩展指令 VMRUN VMLOAD VMSAVE CLGI VMMCALL INVLPGA执行需要EFER.SVME 被设置为1, 否则执行这些指令将会产生#UD.另外 SKINIT STGI指令执行需要 EFER.SVME 设置为1 CPUID Fn8000_0001_ECX[SKINIT] 设置为1否则执行也会产生#UD在使能SVM之前，如那件需要如下判断SVM是否能被enable.if (CPUID Fn8000_0001_ECX[SVM] == 0) return SVM_NOT_AVAIL;if (VM_CR.SVMDIS == 0) return SVM_ALLOWED;if (CPUID Fn8000_000A_EDX[SVML]==0) return SVM_DISABLED_AT_BIOS_NOT_UNLOCKABLE// the user must change a platform firmware setting to enable SVMelse return SVM_DISABLED_WITH_KEY;// SVMLock may be unlockable; consult platform firmware or TPM to obtain the key.VMCBVMCB中即包括了guest上下文，也包括了用于vmm控制guest的配置信息。主要包括: 在guest 中要拦截的一些列的instruction or event 各种控制位用于指定guest 的execution envirionment，或指示在运行来宾代码之前需要采取的特殊操作，以及 guest processor state (例如, 控制寄存器)VMCB位于内存中, 某些指令和事件会根据vmcb构建guest上下文，或者将guest上下文writeback回vmcb，完成host和guest之间的切换, 我们下面详细介绍VMRUN and #VMEXIT使用VMRUN指令操作数需指定一个VMCB地址, 即rAX, 在VMRUN时, 会从rAX指向的VMCB内存中，先将当前cpu状态保存下来，然后再将部分字段load到当前CPU的上下文，将全部cpu 状态load后，就相当于进入guest了。具体操作是: remember VMCB address(rAX) for next #VMEXIT save host state -&gt; VM_HSAVE_PA MSR load control information intercept vector TSC offset interrupt control EVENTINJ field ASID load guest state ES, CS… GDTR IDTR CRxxx 此时，cpu context 已经是guest execute command store in TLB_CONTROL IF (EVENTINJ.V) cause exception/interrupt in guest elsejump to first guest instruction 上面的某些信息，在这个过程中会替换掉当前host的上下文, 如GDTR. 但是某些字段是不属于cpu context的, 例如control information, 这部分就相当于专门的cpu cache。目的是: vmcb中的这些字段在guest中可能会频繁访问，例如, intercept INTR control field,在每次外部中断到来时, 都会使用该字段。为了加速, 虚拟机运行时的性能，在VMRUN指令执行时，会将VMCB中的大部分字段，加载到cpu内部的cache中.如下图:从上图可知: VMCB的cache以该VMCB在内存中的base physical address为tag, 在VMRUN时，会将VMCB load到memory 当intecept 某些events时，可能会从触发 #VMEXIT. 这时，会将VMCB cache writeback 到memory VMCB cache，并不是cache了 VMCB 全部, 包括: interrupt shadow Event injection: 事件注入相关信息，在 VMRUN 时, 获取一次，并在进入guest之前，注入该event, 在之后guest运行过程中，不再需要这个字段, 所以这个信息没有必要cache TLB Control: 和上同理 RFLAGS, RIP, RSP, RAX: CPU: CPU 上下文信息, 这些信息在#VMEXIT后，很可能会改变，并且这些字段是load 到cpu context的。所以没有必要cache.(猜测) 另外, 在VMRUN和#VMEXIT时，需要save，load host state。这些CPU也做了相应的类似于VMCB的cache。手册中的描述如下: Processor implementations may store only part or none of host state in thememory area pointed to by VM_HSAVE_PA MSR and may store some or all hoststate in hidden on-chip memory. Different implementations may choose to savethe hidden parts of the host’s segment registers as well as the selectors.For these reasons, software must not rely on the format or contents of thehost state save area, nor attempt to change host state by modifying thecontents of the host save area.大致的意思是, 处理器可能会store 部分或者不会store 由 VM_HSAVE_PA MSR 指向的 host state, 并可能将部分或全部主机状态store 在 on-chip memory中。不同的实现可能会选择保存主机段寄存器的隐藏部分以及选择器。因此，软件不能依赖于主机状态保存区域的格式或内容，也不能通过修改主机保存区域的内容来尝试更改主机状态。所以，guest host上下文切换，主要涉及 VMCB -- VM_HSAVE_PA 中包含的上下文内容的切换, 但是某些event只需要简单处理后，又继续返回guest执行。这样就没有必要切换一些寄存器。（使用guest的即可)另外在大部分的场景下，在多次VMRUN, #VMEXIT期间, VMCB中的很多字段并没有改变。为了加速guest, host的切换. SVM支持控制某些字段在VMRUN时才会load.上面提到的两种情况，amd通过如下方式解决: VMCB clean Bits VMLOAD, VMSAVEVMCB Clean Bits 该功能在amd spec 15.15 VMCB State Caching章节中有详细讲述首先该功能在VMCB 新增了 VMCB Clean field(VMCB offset 0C0h, bit 31:0),这些bit决定了在VMRUN时，需要load哪些register. 每个bit可能代表某个或某组寄存器。该bit设置为0时，需要处理器去load VMCB到cache。但是这些bit是hint, 因此processor可能会忽略掉厚谢被设置为1的bits，无条件的从VMCB 中load。另外，当clear bits 设置为0时，总是需要load。所以这样就需要vmm判断，在上次 #VMEXIT到本次VMRUN之间，有哪些字段改变了.从而使用VMCB clean field完成高效的guest/host切换。有一些场景需要VMCB clear field都被设置为0, 如下: 该guest第一次 run guest 被切到另一个cpu上运行 hypervisor将 guest VMCB 切到另一个物理地址上面提到过，VMCB cache时根据VMCB的physical address 作为tag去match. 当CPU发现VMRUN指定的 VMCB physical address 和 cache中所有的 条目都不匹配时，会将VMCB clean field 都当作zero.VMCB Clean field 具体字段如下:VMLOAD, VMCLEAN在VMRUN包括#VMEXIT过程中，即使VMCB Clean Bits都设置为0, cpu也不会将所有的字段全部load/save, 需要通过额外的指令 VMLOAD VMSAVE这些字段包括: FS, GS, TR, LDTR (including all hidden state) KernelGsBase STAR, LSTAR, CSTAR, SFMASK SYSENTER_CS, SYSENTER_ESP, SYSENTER_EIP这样做的目的是，为了快速的完成guest和host的切换, 来处理一些简单的event,虽然在实际的KVM代码中，并没有这样做。 NOTE 可以参照SVM的代码, svm_vcpu_enter_exit中关于vmload和vmsave指令的使用. kvm选择在VMRUN之前，无条件的执行VMLOAD guest vmcb，在#VMEXIT时，VMSAVE guest vmcb, VMLOAD host save area2 但是手册中并未找到关于MSR_VM_HSAVE_PA指向的host state的格式。所以, 这里猜测host state格式和vmcb相同.intercept关于cpu虚拟化中，比重最大的部分，就是intercept。host通过 intercept guest中的敏感行为，trap到host，然后由vmm进行emulate后，再次进入guest。intercept operation主要分为两类: Exception intercept: instruction intercept:当发生了intercept时，需要将VMEXIT的reasion，还有一些其他的信息传递到host, 这些信息被写在: EXITCODE: intercept的原因 EXITINTINFO: 当guest想要使用 IDT deliver interrupt or exception 时发生了intercept，这时需要有一个地方保存着该信息， 以便处理完该event之后，再次向guest注入 interrupt/exception EXITINFO1, EXITINFO2: 提供了某些intercept的额外信息intercept就意味着需要保存guest state，并切换到host state，那该从哪个点保存guest state呢?这个和host上触发exception or interrupt的需求是一样的，都需要保存一个上下文切换到另一个上下文, 而host上触发excp/intr 是发生在指令边界处. (在interrupt and exception context switch章节中介绍了host触发excp/intr的逻辑)而 intercept 的逻辑也是这样，也是在指令边际处来切换. 但是其和host 上切换逻辑不同的是: 不同点 host guest intercept 是否切换 根据当前cpu的状态 结合cpu状态以及vmcb control field 切换信息量 fewer register and non-visible state more register and non-visible state 切换信息方式 stack-&gt;cpu cache – vmcb – host state 所以综合来看，intercept的整体要复杂, 其代价更大, 所以现在虚拟化主要的优化方式，就是在硬件中emulate，减少vmm intercept.关于intercept的细节有很多。不同的intercept event的触发条件，相关control field，以及EXITCODE/EXITINFO 均不同，我们不再这里描述。memory内存虚拟化我们这里主要关注两部分 nested page Table TLBinterruptbackground这里的interrupt虚拟化，囊括了异常和中断，其中异常的处理要简单些，我们来比较下: 中断 异常 中断的产生源是可能是software，也可能是hardware 异常产生源是cpu 中断处理，需要cpu和apic的配合 异常完全是cpu自己的逻辑 cpu需要判断当前的状态来决定是处理中断还是pending中断 异常一旦发生，就需要立即处理 在一个中断将要被处理时，还有其他pending的中断, 这就意味着，硬件需要根据配置来将多个中断连续处理 当异常被处理之前，不会有pending的异常 从上面来看，中断处理涉及的组件更多，处理细节也更复杂. 所以针对中断优化要更多一些。在介绍SVM的中断虚拟化之前，我们以比较复杂的中断为例，看看硬件处理中断需要完成哪些步骤: IO Device 将中断发送到中断控制器 中断控制器和CPU进行交互(可屏蔽中断的处理逻辑), 根据配置依次向cpu deliver中断 CPU侧会根据自己的状态，在某些时刻接收中断 接收中断后, cpu根据IDT完成中断的处理。我们来设想下，如果我们要将hardware的中断直接注入到guest中(终极目标:完全bypasvmm)，那就意味着，中断处理链上的所有组件都要被虚拟化.我们接下来看下, hardware emulate 了哪些部分，是怎么emulate的。overflow为了能够给guest注入中断/异常，SVM支持event injection 机制(这里的event是对interrupt和exception的统称)。可以让cpu在执行VMRUN后，在进入guest之前，由硬件触发对event的处理流程（例如根据IDT进行上下文切换). 这样就避免了对 VMM 进行纯软件层面的模拟，大大减少了复杂度.而为了进一步优化中断的处理, SVM 先后引入了: virtual interrupt: 引入虚拟中断, 在guest状态下引入对虚拟中断的评估逻辑。 AVIC: 引入对apic的虚拟化, 引入apic虚拟化后，大大增加了guest中处理中断的能力。可以尽量避免vmm参与virtual interrupt emulate.event injection我们先来看下event inject的实现:event injection 具体实现是在VMCB中引入 `EVENTINJ`字段, VMM可以设置该字段的某些field，来完成event inject. 在guest 代码执行之前，插入了中断代码的执行。实现注入在guest来看是透明的, 正常发生的。但是有一些例外: Inject event 接受 intercept check. (但是如果在delivery 该inject event时，触发了第二个异常, 这个异常受限于exception intercept) 也就是说，当前VMCB中配置了 intercept #PF，如果本次注入的是#PF, 本次注入将不会被intercept, 但是如果 #PF 的IDT handler配置的有问题，则会产生#GP待考证异常, 该异常可能会被intercept injected NMI 将不会阻塞未来的 NMIs delivery 该限制是为了防止guest可以block host的NMI delivery 如果VMM注入了一个guest mode可能发生的 event(e.g.,在guest位于64-bit mode 时注入#BR),该event inject将会fail，并且guest state 将不会发生改变。本次VMRUN 也会立即退出，error code为VMEXIT_INVALID 使用vector 3 /4 来injecting exception(Type=3) 就像使用INT3 INTO 指令发起trap一样，处理器需要在dispatch to handler 之前，检查IDT 中的 DPL software interrupt 将不会在不支持NextRIP字段的被正确注入。(CPUID Fn8000_000A_EDX[NRIPS] = 1)VMM应该在NextRIP不支持的情况下，模拟对software interrupt 的event injection. ICEBP TODO EVENTINJ 字段: VECTOR: event IDT vector, 如果 TYPE 是2, 则忽略该字段(因为NMI是固定的vector) TYPE: 指定exception或者interrupt的类型. 支持的类型如下: value Type 0 INTR(external or virtual interrupt) 2 NMI 3 Exception 4 software interrupt EV (error code valid): 如果为1，则需要将error code push到stack上。 ERRORCODE: ^^ V(vaild): 表明该event是否要inject到guest.如果EVENTINJ配置的有问题, VMRUN 则会以error code VMEXIT_INVALID 退出，例如下面配置: TYPE设置了除上面展示的其他的值 指定TYPE=3(exception), 但是该vector不可能是expception(例如vector 2, 是NMI)总结event inject 只是将interrupt delivery by IDT的逻辑虚拟化到硬件了. VMM可以只注入一个vector，硬件自动完成，对这个vector的后续处理.SVM TODOevent inject 机制这对于异常来说这个机制已经足够了, 注入异常流程大概是:通过上面来看注入异常的出发点，是guest执行了某些指令, 因为某些原因#VMEXIT, host intercept后, 会来判断该条指令的emulate 需不需要inject exception，如果需要则inject exception. 这套流程，完全契合inject exception，而且很难找出优化的空间.因为其是完全串行，sync的处理流程。 NOTE 其实我们从另一个角度来思考，虽然异常注入的流程不好再优化，那其实可以优化异常产生, 也就是host可以更好的从硬件层面更好的emulate guest指令的执行环境, 使其更少的vmexit。（例如NPT, 引入npt后，大大减少了#PF异常的产生). 但这部分不是中断虚拟化的范畴。（当然，本身也没有那么严格的界限，但是为了避免混乱, 我们这里放到 CPU虚拟化的章节中介绍)而对于中断而言，event injection 完成的是整个链条中哪部分处理呢?从上图来看，event inject主要完成CPU 接收events(interrupt or expection)之后的一些流程,即handle interrupt vector(也就是主要完成从当前guest上下文-&gt;中断上下文切换).所以中断而言，离终极目标还有很长一段距离。virtual interruptvirtual interrupt是对event inject的进一步升级, 在guest中执行中断评估逻辑, guest有能力在 指令边界处识别并处理中断(virtual), 这就意味着，我们需要虚拟化出一套可以用于虚拟中断执行的资源。我们先来看cpu这边关于物理中断准备了哪些资源: CR8/TPR: 用来指定当前cpu可以处理的最低中断优先级, 如果pending的中断没有TPR大， CPU不处理该中断 CR8虽然是CPU的寄存器, 但是最终映射于LAPIC的CR8, 所以其中断是否向CPU发送，需要首先走LAPIC的中断评估逻辑。不过为了方便介绍下面的章节，我们暂将CR8作用于CPU 的处理流程里。 RFLAGS.IF: CPU 侧的中断屏蔽位，用来指示当前CPU要不要阻塞apic pending 的中断（也就是要不要 回应/ack lapic). interrupt shadows: interrupt shadows – a single-instruction windows duringwhich interrupts are not recognized. 例如: STI 指令（开中断）的下一条指令，仍然是不接收中断的。 SVM 新增了 virtual interrupt 运行的资源，用于只处理virtual interrupt，而不影响phyiscal interrupt, 如下图所示: NOTE AMD spec 中没有提到 vRFLAGS.IF,除了增加了用于virtual interrupt处理的资源，还增加了用于配置virtual interrupt的VMCB 字段以及其他字段.V_INTR_MASKING为了防止guest block INTR(physical interrupt), SVM 提供了一个VMCB control bit:V_INTR_MASKING, 这个control bit控制guest EFLAGS.IF和 TPR/CR8 的作用范围: 1: 作用于 virtual interrupt 0: 作用于virtual interrupt and physical interrupt分别来看下, 其具体的作用者:EFLAGS.IF if V_INTR_MASKING == 1: The host EFLAGS.IF at the time of the VMRUNis saved and controls physical interrupts whilethe guest is running. The guest value of EFLAGS.IF controls virtual interrupts only. else: EFLAGS.IF control VINTR and INTR CR8/TPRsvm 新增了 virtual TPR register – VTPR, 在VMRUN时，从VMCBload，并且在#VMEXIT时，writeback to VMCB, APIC TPR 仅控制physical interrupt，V_TPR 仅控制 virtual interrupt if V_INTR_MASKING == 1: Writes to CR8 affect only the V_TPR register. Reads from CR8 return V_TPR. else: Writes to CR8 affect both the APIC’s TPR and the V_TPR register. Reads from CR8 operate as they would without SVM. 上面所说的TPR virtualization 仅作用于通过访问CR8触发。但是在32-bitmode中, 没有CR8, 软件只能通过访问TPR，访问TPR只能使用传统MMIO的方式(xapic), vmm需要做一些emulate处理，大致流程如下: VMM 不映射 guest 的 APIC page address guest 访问该区域将产生#PF intercept VMM根据这个物理地址来确定，该地址属于apic，并且是TPR的offset，执行相关emulate代码为了提高 32 位模式下 TPR 访问的效率，SVM 通过一种 MOV TO/FROM CR8的替代编码（即带有 LOCK 前缀的 MOV TO/FROM CR0）使 32 位代码可以使用CR8。为了实现更好的性能，应该修改 32 位客户机以使用这种访问方法,而不是使用内存映射的 TPR。即使在 EFER.SVME 中禁用 SVM，这些 MOV TO/FROM CR8 指令的替代编码仍然可用。它们在 64 位和 32 位模式下均可使用。 INTERESTING!!!injecting virtual interruptvirtual interrupt 允许host将一个interrupt(#INTR)传递给guest. 当运行在guest中时，会执行和host一样的中断评估逻辑， 例如中断是否taken受(EFLAGS.IF 以及 vTPR值的影响). 所以virtual interrupt的引入，实际上是在每次指令的边界，添加了关于virtual interrupt的处理逻辑。inject细节如下: inject 流程 新增了三个用于存储virtual interrupt的字段: VIRQ : 指示是否有VINTR需要注入 V_INTR_PRIO : priority of VINTR V_INTR_VECTOR : vector of VINTR taken 条件: 如果下面条件满足, 处理器将taken virtual INTR interrupt: if V_IRQ == 1 &amp;&amp; V_INTR_PRIO &gt; V_TPR and if EFLAGS.IF == 1 and if GIF == 1 and if the processor not in an interrupt shadow VINTR vs INTR: 所以通过上面来看VINTR和INTR 在处理上的区别很小, 后者需要INTACK 来获取中断信息，而前者从V_INTR_VECTOR 中获取 external handle on VMEXIT: 上面提到只有在合适的时机，virtual interrupt 才会被taken，在处理器taken后，dispatch virtual interrupt时（through IDT), V_IRQ 在检查 intercept of virtual interrupt 之后，并在访问 IDT（中断描述符表）之前，V_IRQ 被清除。 在#VMEXIT时，会将V_IRQ writeback to VMCB, 允许VMM 来跟踪该virtual interrupt 有没有 taken. 另外在#VMEXIT时，处理器会clear到CPU缓存中的 V_IRQ 和 V_INTR_MASKING,所以virtual interrupt不会在VMM中pending other 在guest运行时，VMM可以通过使能 INTR intercept 来 intercept INTR physical interrupt的优先级永远高: Physical interrupts take priority over virtual interrupts, whether they are taken directly or through a #VMEXIT. V_IGN_TPR可以控制当前pending的virtual interrupt不受 TPR 影响: V_IGN_TPR field in the VMCB can be set to indicate that the currently pending virtual interrupt is not subject to masking by TPR. The priority comparison against V_TPR is omitted in this case. This mechanism can beused to inject ExtINT-type interrupts into the guest. 上面提到 GIF :global interrupt (GIF) 用来控制 interrupt and other event 是否可以被当前处理器taken。STGI 和 CLGI 指令用来set clear该bit.下面时GIF取值对于event taken的影响:总结:离终极目标，满血版中断虚拟化又进了一步。现在在guest中增加了中断的评估逻辑，使得guest可以去处理自己的中断（VINTR)。并且处理方式很像物理中断，中断评估逻辑可以发生在guest的几乎任何指令边界处。这个优化很关键，相当于打开了一个枷锁, 允许中断注入和中断taken异步发生我们较event inject来比较下: ~ event inject virtual interrupt when to delivery end of VMRUN like INTR, guest every inst boundary&lt;/br&gt;Meet the conditions for beingtaken conditions of taken NO CONDITIONS like INTR, &lt;/br&gt;need check EFLAGS.IF TPR, &lt;/br&gt;interrupt shadow… Q: 像INTR有什么好处? A: 因为这是vm的需求，其总要求运行起来要像 物理机一样。 所以硬件如果模拟的不像，就需要软件来模拟的像一些. 举个例子: 如果在VMRUN之前，如果vcpu处于interrupt shadow, event inject此时就不能注入 event，vmm还需要做好额外的intercept，尽量在intercept shadow刚关闭时，立即 退出guest，来inject event，来减小interrupt 延迟。 但是使用, virtual interrupt 考虑的就少很多了，不管啥样，直接在VMRUN之前注入 virtual interrupt， guest cpu 会自己判断interrupt shadow啥时候关闭，从而 更高效的 inject.那virtual interrupt 完成了，中断处理链条中的哪些部分呢?Advanced Virtual Interrupt ControllerAVIC 是AMD虚拟化中的重要的一个增强。其为guest的每个vcpu都提供了和LAPIC兼容的副本。基于这个副本，我们可以对apic中的很多功能做虚拟化。introduction参考链接 amd spec KVM: SVM: use vmsave/vmload for saving/restoring additional host state附录interrupt and exception context switchinterrupt, fault exception 和trap exception 其上下文切换都是发生在指令边界处，这样做的好处，是明确规定了上下文切换的点，让指令执行原子化，方便软件进行处理.但是三者的机制不太相同。分别来看:interrupt当APIC发出一个interrupt后(我们这里以maskable interrupt为例), cpu会在指令边际处检查是否有pending的中断, 会根据当前的cpu状态评估(interrupt window)，要不要接收该interrupt, 如果接收, 就向apic 要详细的中断信息，然后通过IDT进行上下文切换（当然不仅仅是IDT，还有其他desc中的信息，这里不赘述)faultfault一般是指令执行过程中，发现该指令执行的有问题，例如，#PF是在寻址过程中，发现page table walk 出现了问题，但是此时该指令还未执行完成，所以需要将cpu恢复到该指令执行之前的上下文，然后，deliver一个exceptiontraptrap的处理十分简单，如上图所示，trap的触发是通过trap指令，该指令的作用就是在该指令之后的位置，挖一个坑，该坑通往处理该trap的异常处理程序。当异常处理程序返回时，执行trap指令的下一条指令。" }, { "title": "sev api", "url": "/posts/amd-sev-api/", "categories": "coco, sev", "tags": "sev", "date": "2025-02-28 09:39:00 +0800", "snippet": "Platform Management APIPlatform Management API 由 platform owner 使用，用于配置/查询 platform-widedata。下面的章节主要包括: Platform Context: Which data are categorized as platform-wide data Ownership: who is the pl...", "content": "Platform Management APIPlatform Management API 由 platform owner 使用，用于配置/查询 platform-widedata。下面的章节主要包括: Platform Context: Which data are categorized as platform-wide data Ownership: who is the platform owner Non-volatile Storage: Persistently store platform-wide data Platform APIsPlatform ContextSEV fw 在 platform 的整个生命周期中维护了一个platform context. 该context包含了SEV API 所需的data 和 metadata.PlatForm Context (PCTX) Field:PCTX中主要包括: platform state 某些API会改变platform state. 并且某些API只能在特定的state下才能执行。并且Guest Management API 只能在INIT/WORKING platform states 下，才能执行: platform config keys: PDH, PEK, CEK, OCA。这些key有些是导入的，有些是生成的， 有些是固化在固件中的。platform API 负责去管理这些证书的生命周期. Guest Information: GUEST_COUNT: number of guest contexts currently managed by the fw. GUEST: guest contexts currently managed by the fw. Ownership一个platform 可能由外部实体拥有，也可能是self-owned。platform owner的所有权由一个以 OCA（所有者信任根）为根的证书链定义。OCA 签署 PEK。当platform不由外部实体拥有时，platform会生成自己的 OCA 密钥对.这有什么用呢? 下面是自己的理解。很可能不对。目前在整个AMD的信任链中，PSP(Platform Secure Processor) 是根据CET有一套完整的信任链。但是, Processor 之上的软硬件组件，还是需要platform 来保证. 虽然platform owner已经无法窥探到guest的真面目了, 但是作为guest owner来说，还是想要platform owner的一些其他的服务和特性。举个不恰当的例子。某用户最近想买 aliyun deepseek 一体机. 但是买不起一手的，只能买二手的。去闲鱼上一看, 我的天，这些同样牌子的aliyun deepseek 一体机，怎么长得都不一样。用户也不确定自己买回来的是真的 aliyun牌子的还是awaiyun的。那用户就可以通过OCA验证。设备再aliyun出厂时，导入了OCA的公钥。以及使用了OCA私钥，签署了PEK。这样用户就可以去做验证了。当然，如果客户不想买一体机，想买个裸机回来自己搭建，那platform owner就是他自己。这时，OCA就可以使用sev fw API 在这台机器生成。所以，总结来说，OCA就是用来验证platform owner的身份的真伪。Non-volatile Storage上面提到的PCTX某些信息的生命周期可能比物理机运行周期还长（关机不清除) 。所以，这些信息是存储在non-volatile storage中。包括: PDH key pair PDH certificate PEK key pair PEK certificate OCA public key OCA private key (only if self-owned) OCA certificate这些cert/key 在生成/导入后，立即加密存储.PlatForm APIs name State restrictions for executing state change INIT UNINIT INIT INIT_EX UNINIT INIT SHUTDOWN ANY UNINIT PLATFORM_RESET UNINIT NOT CHANGE(UNINIT) PLATFORM_STATUS ANY NOT CHANGE PEK_GEN INIT NOT CHANGE(INIT) PEK_CSR INIT, WOKRING NOT CHANGE PEK_CERT_IMPORT INIT NOT CHANGE(INIT) INIT overflow INIT cmd用于 platform owner 初始化platform。该命令会从 non-volation storage 中加载并初始化 platform context. 这一般是首先要执行的命令(除了 PLATFORM_STATUS 确定APIversion) action CEK 是是根据芯片的唯一值 派生(derived)出来的 如果没有OCA 证书，则self-signed(自签名）一个OCA cert. 这个新生成的证书也会写到non-volatile storage中. 没有PEK, 或者 OCA 刚刚生成, 生成PEK signing key 并且通过OCA &amp;&amp; CEK 签名。同样的，也写到 non-volatile storage 中 没有PDH 或者 PEK 刚刚被生成, 生成 PDH key. 通过PEK 签署 PDH 证书. 所有核上的SEV-related ASID 都被标记为invalid. 在active 任何vm之前，每个核心都需要执行WBINVD 指令. INIT_EX和上面命令相似, 只不过支持 NV_PADDR传入额外信息，暂略SHUTDOWN overflow All platform and guest state maintained by the firmware is securely deletedfrom volatile storage. PLATFORM_RESET overflow reset the non-volatile SEV related data invoking this command is useful when the owner wishes to transfer theplatform to a new owner or securely dispose（销毁) of the system. action delete persistent state from non-volatile storge PLATFORM_STATUS overflow used by the platform owner to collect the current status of the platform. action IF PSTATE.UNINIT OWNER, CONFIG.ES, CUEST_COUNT = 0 IF owned by an EXTERNAL OWNER OWNER = 1 else OWNER = 0 CONFIG flags == INIT command params PEK_GEN overflow 该命令用户生成一个新的PEK. 用来重新生成 identity of the platform 但其实在平台reset后，首次调用INIT命令时, PEK 会被重新生成，所以该命令不是必须的。 action deleted from volatile and non-volatile storage: PEK key pair PEK certificate PDH key pair PDH certificate OCA key pair (if the platform is self-owned) OCA certificate re-generate and store in non-volatile storage OCA signing key &amp;&amp; self-signed OCA cert PEK &amp;&amp; PEK cert PDH &amp;&amp; and signed by PEK 该命令相当于依次调用 SHUTDOWN PLATFORM_RESET INIT PEK_CSR overflow 可以结合 PEK_CERT_IMPORT 命令使用. 该命令会生成一个CSR(cert signrequest?). 该CSR中包含 platform information PEK public key 之后CA则会根据CSR中的infomration, key 签署一个证书 action 该命令主要是生成CSR CSR的格式和 SEV CERT 的格式相同，只不过 signatures 字段都是0 NOTE 但是这里有个疑问，SEV CERT 需要签署两次(OCA, CEK 双签署），那导出的CSR中有没有CEK 签名 PEK_CERT_IMPORT overflow 该命令结合PEK_CSR命令一起使用。CSR由 platform owner ca签署后会用其OCA签署 CSR。然后，再在platform侧，执行该命领将签署好的PEK和OCA倒入导入platform 但是需要注意的是, 这个过程需要在trusted envirment中执行。 action 该platform 必须是self-owned. 需要确保caller 已经通过PEK_GEN命令重新生成了PEK，所以该PEK没有被 任何owner签署 OCA 和 PEK 证书会被验证。验证过程包括以下几个步骤： The algorithms of the PEK and OCA must be supported The version of the PEK and OCA certificates must be supported The PEK certificate must match the current PEK The OCA signature on the PEK certificate must be valid OCA cert and PEK signature written into platform context(不用在执行INIT了）and non-volatile stroage PDH is regenerated and signed with the new PEK PDH_GEN overflow 该命令可以根据需要多次重新生成 PDH。请注意，如果其他实体正在使用当前的PDH 来建立用于加密数据或进行完整性校验的密钥，那么重新生成 PDH 会使任何正在进行的密钥协商操作失效。在这种情况下，其他实体必须获取新的 PDH，才能继续进行密钥协商。 PDH_CERT_EXPORT overflow这个命令用于获取当前平台的PDH。常用于导出到remote entities 来建立安全的传输通道（例如热迁移) actions 导出如下数据: PDH cert CEK cert PEK cert OCA cert SUMMARYplatform API 主要是管理平台的生命周期，例如设备生产后的各类证书生成，设备reset等等。但是PDH_CERT_EXPORT命令则可能用在虚拟机的生命周期的管理中，我们下面会看到GUEST Management API和platform Management 类似，guest Management API 用于管理 guestcontext, 从而作用于guest 生命周期管理.Guest Context如果说Platform Context 是platform 粒度，是全局的，则Guest Context则是VM粒度，是每个vm独有的，我们关注下面的字段 STATE: UNINIT LUPDATE: guest current being launched and plaintext data and VMCS save area are being imported &gt; 正在导入明文 LSECRET: The guest is currently being launched and ciphertext data are being imported. &gt; 正在导入密文 RUNNING: guest is fully launched or migrated in, and not being migrated out to another machine. &gt; 已经launched 或者迁入，并且没有在迁出 SUPDATE: The guest is currently being migrated out to another machine. RUPDATE: The guest is currently being migrated from another machine. SENT: The guest has been sent to another machine. 每个sev vm 在运行过程中都会经历一个 finite state machine(有限状态机). 固件只会在每个虚拟机的特定state下，才能执行某些命令 HANDLE: 用于唯一标识 guest ASID: 上面说过, memory controller中有个加密模块，其是识别TLB中的 ASID 作为密钥ID, 来索引相关密钥 ACTIVE: POLICY: 用于描述当前虚拟机的sev策略，例如SEV, SEV-SP VEK: The memory encryption key of the guest NONCE: 当前与该虚拟机关联的可信通道随机数 MS: The master secret current associated with this guest. ?? TEK: transport encryption key TIK: transport integrity key (热迁移时候会用到) LD: 该虚拟机的启动摘要上下文GUEST Management APIs name State restrictions for executing state change   LAUNCH_START 新创建 GSTATE(UNINIT-&gt; LUPDATE) PSTATE(-&gt;WORKING)   LAUNCH_UPDATE_DATA PSTATE.WORKING &amp;&amp; GSTATE.LUPDATE NOT CHANGE   LAUNCH_UPDATE_MEASURE PSTATE.WORKING &amp;&amp; GSTATE.LUPDATE GSTATE(LUPDATE-&gt;LSECRET)   LAUNCH_SECERT PSTATE.WORKING &amp;&amp; GSTATE.LSECRET NOT CHANGE   LAUNCH_FINISH PSTATE.WORKING &amp;&amp; GSTATE.LSECRET GSTATE.RUNNING NOT CHANGE LAUNCH_START overflow 该命令用于通过使用新的 VEK 对虚拟机内存进行加密，从而引导（初始化）一个虚拟机。 此命令会创建一个由 SEV 固件管理的虚拟机上下文，之后可以通过返回给调用者的HANDLE来 引用该上下文. action 如果HANDLE字段是0， 会生成一个新的VEK. 如果不是0，会检查下面字段: HANDLE is a valid guest GUEST[HANDLE].POLICY 和 传入的POLICY field 相同 POLICY.NOKS == 0 (需要共享) 如果上述检查过了，则会将HANDLE 指向的 VEK copy到the new guest context(相当于dup()) new guest handle written to HANDLE field init GCTX.LD version check 检查参数POLICY 字段中的API_MAJOR和API_MINOR是否满足要求: PLATFORM.API_MAJOR &gt; POLICY.API_MAJOR or PLATFORM.API_MAJOR == POLICY.API_MAJOR and PLATFORM.API_MINOR &gt;= POLICY.API_MAJOR (向下兼容) DH_CERT_PADDR: 如果其为0， 则 忽略如下字段: DH_CERT_LEN SECTION_PADDR SECTION_LEN NOTE 为什么要这样做呢? 因为PDH cert, 就是为了建立其安全的加密通道,而其建立安全加密通道的信息，就保存在SECTION 中, 下面会看到 params: LAUNCH_UPDATE_DATA overflow 使用VEK加密guest data action GCTX.LD: 被更新为 PADDR指向的明文。而明文被guest的VEK 加密为存放在PADDR处 LAUNCH_UPDATE_VMSA (和 SEV-ES相关，略) LAUNCH_MEASURE overflow 该命令返回launched guest’s memory pages 和 VMCB areas(SEV-ES). 测量的结果使用TIK 作为密钥，guest owner可以使用该测量结果验证launch 过程没有被干预 action GCTX.LD最终被封存为导入guest的所有明文的hash digest. launch measurement 最终被计算为: HMAC(0x04 || API_MAJOR || API_MINOR || BUILD || GCTX.POLICY || GCTX.LD ||MNONCE; GCTX.TIK) NOTE ||表示拼接. MNONCE 在这个过程中 fw 随机生成的 用GCTX.TIK加密 将计算结果写入 MEASURE 字段 params NOTE MNONCE 在这个过程中的作用, 就是用来验证完整性的, MEASURE_PADDR.MEASURE(HMAC)中存放密文, 而MEASURE_PADDR.MNONCE 中存放明文. 而HMAC中被加密的MNONCE 只有FW知道, 所以该字段用于验证`MEASURE_PADDR.MEASURE`完整性. LAUNCH_SECRET overflow inject a secret into guest, 在launch measurement 已经被guest owner收到并验证 通过后执行该cmd action verfiy MAC field HMAC(0x01 || FLAGS || IV || GUEST_LENGTH || TRANS_LENGTH || DATA || MEASURE; GCTX.TIK) 目的是验证该secret的注入方是否是GUEST owner. 通过什么验证呢? MEASURE: (只有GUEST OWNER 和 FW知道) DATA 是TRANS_PADDR 指向的密文 而 TRANS_PADDR 指向的密文，通过GCTX.TEK 加密 如果 FLAGS.COMPRESSED == 1, 生成的明文则会被解压缩，解压后的 结果会被写入GUEST_PADDR, 并通过 VM 的VEK 进行加密. parameters NOTE 该过程比较复杂，我们在这里做下小结: 该流程的目的是，guest owner将一个secret 通过 LAUNCH_SECRET 注入到guest中。 首先遇到的一个问题是，怎么确定该”secret” 是guest owner 传过来的，另外，怎么确定, 该”secret”有没有被篡改。 首先，确定一个数据有没有被篡改。SEV FW常用的方式是，在明文参数中放一个字段存储该数据, 另外在使用HMAC()将所有需要保证完整性的数据进行打包。 例如，通过对比Packet Header Buffer中的 IV和 MAC中解密后的IV可以判断，HMAC中的数据有没有被篡改 另外，怎么验证该secret 是guest owner传递过来的呢? （下面纯属猜测) 通过MEASURE字段，MEASURE字段只有guest owner和 SEV fw 知道。SEV fw会在执行该命令之前首先对该字段做验证. (如果真是这样, IV 字段有些多余) (GCTX.LD) field 保证数据的完整性已经做到，那还需要保证数据加密，不会外界获取。方法是通过TEK加密 TRANS_PADDR 中指向的数据。 传递到platform侧后, sev fw会将该密文通过TEK解密，然后通过VEK加密，最终数据在内存中被host 观测到的是通过VEK 加密过的。而在guest中，则可以通过SME机制获取到解密后的数据LAUNCH_FINISH overflow 该命令用于将guest state 置为 可以RUN 的状态. action zero following GCTX field: TEK, TIK, MS, NONCE, LD ATTESTATON overflow 该命令生成一个报告，其中包括通过LAUNCH_UPDATE_*命令传递的guest memory，以及VMSA的SHA-256 digset。该摘要于guest memory 在 LAUNCH_MEASURE中使用的 digset一致。 parameters SEND_START overflow 该命令用于热迁移前的准备工作（源端) action valiate IF GCTX.POLICY.NOSEND != 0, return error IF GCTX.POLICY.SEV == 0. PDH, PEK, CEK, ASK, ARK cert 才被认为有效 check API Version. GCTX.POLICY: PDH - PEK - OCA 将被会验证(验证过程没说, 是否验证到ARK?, 感 觉可能会，因为其已经传过来了) 如果guest policy required?? 验证PDH, PEK, OCA, CEK , ARK, ASK证书链. 重新生成 NONCE 通过NONCE, PDH_CERT(dst) 以及 PCTX.PDH(src) private key计算master secret，然后生成新的TEK, TIK传输密钥，然后根据下图对传输密钥重新封装，将封装后的结果写入WRAP_TK, WRAP_IV, WRAP_MAC GCTX.POLICY 被写在 POLICY field, 并且被TIK 摘要，并写到 POLICY_MAC 字段. parameters summary 该命令在热迁移过程中十分关键, 其工作主要分为四部分 验证dst 端 证书链 通过两边的PDH (dst public key, source private key) 以及NONCE 生成master secret. 通过master secret 派生KIK, KEK 生成TEK, TIK，IV, 并通过KEK 加密生成WRAP_TK, WRAP_IV, 然后对WRAP_TK 使用KIK 进行认证标签. 主要是为了达成 安全传输 TEK, TIK 的目的，为之后加密数据的传输做准备. SEND_UPDATE_DATA overflow 导出guest memory 到另一个platform action 新生成IV 通过GCTX.VEK解密（手册中没有写, 但是个人认为这个流程必须先用VEK解密) 将GUEST_PADDR 指向的数据，通过 GCTX.TEK 加密, 写入TRANS_PADDR中; 计算MAC HMAC(0x02 || FLAGS || IV || GUEST_LENGTH || TRANS_LENGTH || DATA;GCTX.TIK) 其中DATA 是TRANS_PADDR指向的密文 SEND_UPDATE_VMSA略SEND_FINISH overflow finalizes the send operational flow action The following fields of the guest context are zeroed: GCTX.TEK GCTX.TIK GCTX.MS GCTX.NONCE SEND_CANCEL overflow This command cancels the send operational flow action The following fields of the guest context are zeroed: GCTX.TEK GCTX.TIK GCTX.MS GCTX.NONCE RECEIVE_START overflow import a guest from one platform to anther 常见的使用方式: 在热迁仪目的端使用 在磁盘上恢复guest action create a new guest context 如果HANDLE字段为0，则生成一个新的VEK, 如果不是0, 则检查如下字段 HANDLE is a valid guest GUESTS[HANDLE].POLICY is equal to the POLICY field The MAC of the POLICY is valid POLICY.NOKS is zero 如果上述检查通过，HANDLE指向的guest的VEK 将copy到新的GCTX write new guest handle to HANDLE 通过NONCE 和 PDH_CERT(src) 以及PCTX.PDH private key(dst) 计算master secret 通过master secret 派生 KEK, KIK 以及参数中的WRAP_IV, WRAP_MAC来解密并 验证 WRAP_TK，从而得到TEK, TIK 使用 TIK 和 GCTX.POLICY 得到MAC，然后在和POLICY_MAC 字段进行对比验证 验证 GCTX.POLICY.API 和 POLICY.API_MAJOR. POLICY.ES 相关 parameters: summary 该命令主要工作: 创建新的GCTX 验证src端传过来的各类数据 导入从src端获取来的数据，从而建立起加密信道（主要是TEK) RECEIVE_UPDATE_DATA overflow import guest memory action verify data area though computing MAC HMAC(0x02 || FLAGS || IV || GUEST_LENGTH || TRANS_LENGTH || DATA;GCTX.TIK) 通过GCTX.TEK 和IV field 解密 data 将解密后的数据通过GCTX.VEK再次加密，并写入GUEST_PADDR指向的内存. parameters RECEIVE_UPDATE_VMSA(略)RECEIVE_FINISH overflow 结束 RECEIVE work flow actions 清空如下guest context 字段: GCTX.TEK GCTX.TIK GCTX.MS GCTX.NONCE GUEST_STATUS ACTIVATE overflow 该命令用于通知固件, VM 已经绑定到特定的ASID。随后固件将会将该虚拟机的VEK 加 载到 ASID 对应的 memory controller 的key slot 中。当guest 是RUNNING状态， 所有Cache Core Complexes 都可以执行该guest. DEACTIVATE overflow This command is used to dissociate the guest from its current ASID. Thefirmware will uninstall the guest’s key from the memory controller and disallow use of the ASID by all CCXs. DF_FLUSH overflow 在该命令用于deactivate 一个或多个guest后执行 在执行该命令之前需要先执行WBINVD 该命令用于 flush 每个core上的 data fabric write buffers OTHERS TODO " }, { "title": "sev && csv", "url": "/posts/amd-sev/", "categories": "coco, sev", "tags": "sev, csv", "date": "2025-02-28 09:39:00 +0800", "snippet": " 本文是对1的高仿，但是劣质版本，非常建议去阅读下huangyong的文章背景在云环境中有两类角色: platform owner : 云厂商 guest owner : 租用云厂商的用户云厂商负责提供云基础设施, 为用户构建出一套”属于自己的” 计算存储网络，同时需要保证云基础设施足够优质来吸引用户。而租户则是使用云厂商提供的云基础设施，来跑自己的业务，在云场景下，租户只需要关心云环...", "content": " 本文是对1的高仿，但是劣质版本，非常建议去阅读下huangyong的文章背景在云环境中有两类角色: platform owner : 云厂商 guest owner : 租用云厂商的用户云厂商负责提供云基础设施, 为用户构建出一套”属于自己的” 计算存储网络，同时需要保证云基础设施足够优质来吸引用户。而租户则是使用云厂商提供的云基础设施，来跑自己的业务，在云场景下，租户只需要关心云环境中的业务，而无需关心云基础设施的层面的问题，例如: 云主机的网络波动，硬件老化等等。而云环境下的安全也是platform owner的一个重要服务，而guest owner，只能选择信任云厂商提供的安全防护功能。但是, 总有防不住的时候，一旦platform owner的的防线被击穿，其上面运行的guest都会有风险，而由于host有足够的权限，并且其操作对于guest而言都是透明的，所以对于一些数据敏感的guest owner而言，这种安全风险不能接受。前面也提到, platform owner 也想提供更优质的服务来吸引用户。而安全防护也是一个很重要的服务。但是软件防护总是限制的, 而能不能通过在硬件层面来保证，在host 安全组件被击破后，还有一道硬件防线，可以防止guest不被攻击。AMD SEV 提供了该解决方案。简单来说，AMD SEV 主要提供了一个隔离方案，让host无法观测到guest的行为，同时提供给 guest 一些接口，可以让 guest 来验证，自己是跑在一个安全的AMD SEV 环境。另外, 对于提供sev功能的云厂商来说，既希望提供给用户这个功能，也希望能像传统虚拟机一样管理sev虚拟机的生命周期. 例如: 启动 关闭 迁移 快照overflow在介绍SEV之前, 我们先来看下传统虚拟机。在传统架构中，有两个角色, Hypervisor和guest, Hypervisor, 几乎可以访问虚拟机所有的位置的数据，还可以修改虚拟机运行上下文: MEMORY REGISTER DISK所以，在SEV中, 就是要限制Hypervisor的权限, 不能让host随意获取到guest中运行的数据，甚至不能恶意模块修改. 最直接的方法就是数据加密.而在SEV 引入之前，AMD CPU支持通过SME功能来加密内存, SEV 则是在原有SME功能基础上进行了进一步扩展, 通过虚拟机的VSID 作为keyid，来查找相关key，进行加密,内存加密部分我们放在 SME 章节中介绍.另外, guest需要一种方法可以确认，自己所处的环境是一个安全的, 可以被信任的环境。AMD是通过证书链认证实现的, 这部分内容我们在证书链章节中介绍。而对于hypervisor来说，其负责管理guest的生命周期，资源分配. 而由于sev 在guest 生命周期中附加了一些额外的操作，这些需要hypervisor 于sev fw交互, 所以需要一组API，这些API执行的时候需要一个通道. 这些内容我们放到API章节中介绍, 并且在该章节中，我们会介绍部分API.前面提到的主要是guest内存的加密，然而有这些是不够的, 仅能防住GUEST内存不被host看到，但是不能防止host篡改，为了达到这种防护，AMD引入了SEV-SNP, 来实现guest, host 内存隔离。而最后，我们也结合AMD SEV-SNP的原理，以及hygon CSV代码，来猜测hygon CSV3 内存隔离的实现.SMESecure Encrypted Virtualization (SEV) 功能允许在VM运行期间，透明的加解密内存，并且每个VM加解密时，可以使用他们独有的key(密钥)。实现方式是在memory control 中实现一个高性能的加密模块，该加密模块可以编程多个密钥，用来给不同的虚拟机使用。 cache in SME, SEV 在SME使能过程中，我们假设带C bit来访问内存，首先会从内存中获取秘文，然后由加密模块解密，最终递交给CPU。但是，cache中是如何存储的呢? 实际上，cache中存储的是明文。所以，假设我们多次访问一个内存，后续的访问则会从cache中直接获取，而并非从内存中获取秘文再解密。 但是，这实际上引入了一个问题，C-bit是存储在Page Table中。而我们完全可以建立两个映射，一个带有C bit, 一个不带有C bit，如下图所示: 这样的话，cache缓存的究竟是明文还是秘文呢? 假设我们都要存储的话，是不是会遇到重名的问题。 AMD工程师采用了一个非常聪明的做法, 将虚拟地址的最高位作用来标记C-bit，这样缓存一个地址的明文秘文就不会遇到重名问题. 但是我们设想下，在SEV中，该技巧还能不能奏效… A: 不能！为什么? 因为使用这种方法有代价，就是需要牺牲物理地址空间。C bit作为单bit牺牲掉物理地址的一个bit，造成物理地址空间范围减少一半，可以接受。但是VSID是多位(16-bit), 会大大减少虚拟机物理地址空间。我们设想下，假设物理地址空间为48bit.48-16 = 32, 相当于虚拟机变回了32位.在AMD后续的实现中，除了对内存加密外，还实现了其他额外的功能: SEV-ES : 寄存器加密 SEV-SNP : 将host和guest内存隔离KEYs证书链上面提到，虽然guest可以感知到自己的内存是加密的，但是如何保证hypervisor 不能解密呢? 换句话说，guest在一个对外完全封闭，对内四处漏风的环境内如何验证自己的环境符合一定的安全需求.举个例子, 我们去买一个手机, 手机厂家说, 这手机遥遥领先, 满载跑起来温度不超过50度。作为聪明的消费者, 我们当然不信，于是我们打开B站权威（没有收钱）的up主的评测, 去验证手机厂家说的话是否属实。而guest就像是消费者，云厂商中的基础设施就像是手机厂家, 其几乎不信任基础设置中的任何组件。所以，需要有一个权威机构保证某个东西完全没有问题，这样guest可以完全信任该组件。并无忧无虑的和其通信。上图是SEV架构下，guest 信任者的示例图，在上图中, guest 除了AMD hardware and Firmware谁都不信。而AMD hardware and Firmware作为最底层的硬件, 而且用户对其足够信任(如果不信任,就不会买了), guest将其做为唯一的信任者, 这很合理，但是这又是很理想的情况。为什么呢? 需要大家思考几个问题: guest如何知道自己运行的环境就是`AMD`牌子的`haredware and Firmware`, 而不是`DMA`牌. guest owner 有时会和platform的安全处理器建立安全信道进行通信(例如导入磁盘密钥), 如何保证在 hyp参与的情况下，该信道仍然是安全的 SEV通过证书链机制，实现上面的需求，我们来看下具体的细节keys and certificate在整个的证书链中，包含很多的keys，这些key之间存在一个认证链，我们先把整个关系展示出来，在分别介绍: ASK, ARK: ask 是amd的信任根，其签名表示AMD的真实性. 使用ARK私钥对ASM公钥进行签名.ark 是一个中级密钥，使用ask私钥对cek进行签名. CEK: cek 用来对pek进行签名，从而将pek锚定到amd的信任根, 每个芯片都有一个唯一的cek，关于该密钥km spec中的描述如下: Each chip has a unique CEK which is derived from secrets stored in chip-unique OTP fuses. The lifetime of this key is the lifetime of the individual chip. OTP 熔丝是一种硬件技术，用于在芯片制造过程中或之后存储永久性数据。这种数据一旦写入，就无法修改或删除，因此可以用于生成独特的、不可复制的密钥。这种机制确保了每个芯片的 CEK 是唯一的，并为芯片的安全功能提供了一个信任基础。 所以, 将CEK公私钥封装到芯片内部，同时，又使用ASK私钥对CEK公钥签名生成证书，保存在AMD厂商，这样就相当于把该机器锚定了amd的信任链. 所以, CEK 是固件可信的起点(回答了第一个疑问) 那怎么验证cek是否有效呢?? 可以让硬件对使用cek私钥另一个公钥进行签名，生成证书，然后，使用厂商的cek证书中的公钥对其签署的证书，进行验签，如果验签成功，说明CEK没有问题，同时也能说明CEK签署的证书也没有问题。 那签署的是什么证书？有何作用? OCA: OCA证书是自签署的, OCA私钥用来签名PEK, 用来表明PEK是经过platform owner签署的. 该OCA密钥对以及证书生成的方式，我们放到下面的章节中介绍 PEK, PDH: PEK 是由固件创建，由CEK和OCA(下面介绍) 双签名，其作用是对PDH进行签名。 PDH 使用椭圆曲线Diffie-Hellman(ECDH)算法密钥。PDH主要用于SEV fw和其他外部实体（guest owner)协商一个住密钥，然后使用这个主密钥通过 key derivation function(KDF) 来建立起一个可信通道。 所以，使用该可信通道，就可以让Guest owner和set fw在穿过hypervisor的情况下, 安全的通信 other keysSEV 可以在不信任的环境中，建立一个可信的通信通道. 需要下面key来保证。 TIK, TEK: 用于在不安全信道中传输数据 TEK: AES-128 encryption key SEV fw 使用该key 加密所有的 confidentialinformation 在fw和外部实体(例如guest owner/anthor SEV fw)中传输 TIK: HMAC SHA-256 integrity key. SEV firmware 使用TIK来验证在 sev fw和外部entity之前传输的受保护的信息。 SEV firmware 在发送方操作流程中生成，由firmware 生成一个TEK/TIK, 固件会 从安全的熵源生成 TEK, TIK 。 SEV fw 在 launching and receiving 相关工作流中会导入该 WRAPPED TEK, TIK 下面会介绍 WRAPPER TEK KEK, KIK: 用于在不安全信道中传输 TIK, TEK KEK: AES-128 encryption key used to wrap TEK and TIK during session establishment KIK: HMAC-SHA-256 key used to wrap the TEK and TIK during session establishment(integrity) SEV fw 通过密钥协商协议中协商得到的主密钥（master secret）推导出 KEK, KIK（密钥加密密钥) VEK: AES-128 encryption key. VEK的作用是: 在guest运行时，加密guest 内存. SEV fw 从一个安全嫡中生成 VEK. 并且在热迁时，源端目的端应使用不同的VEK. 所以remote platform 应该自己重新生成一个VEK. NOTE 我们来思考下，VEK作为虚拟机内存加密使用的key，不用做对外传输（热迁时也不迁移该key). 而TEK, TIK 用作传输过程中传输”秘密数据”的加密通道的重要工具。其是在建立加密通道时创建的。然而我们在receive 端确认，TEK, TIK 是由 “真的” SEV fw生成的呢? 从信任链我们知道, 信任链的底端是PDH。用于加密 TEK, TIK的KEK 是通过mastersecret 派生的。而master secret 又是通过两方的 PDH 派生而来. 所以整个的链条延伸如下: SEV-fw -- gen --&gt; TIK, TEKSEV-fw -- PDH keys -- calc--&gt; share secret -- KDF --&gt; master secret --KDF--&gt; KEK,KIK --wrapper--&gt; TIK, TEK 我们来看下细节SEV fw establish TRUSTED channelSEV firmware 需要在他自己和remote party之间建立起安全信道. 其中有两个角色: server: SEV firemware issuing lanch and receive command client: guest owner 和其 SEV firmware issuing send command大致流程如下: client server 交换 ECDH public key, 同时client 生成一个随机数N，并发送给server client server 通过 对方的 ECHD public key和自己的 ECHD private key 生成 share secret, 然后在通过 KDF(share secret, N) 生成master secret. client 通过master secret 派生出 KEK, KIK, 并生成用户传输加密验证的密钥:TIK, TEK client 通过KEK 将 TIK, TEK wrap 加密，并发送给server server 端也通过 master secret 派生出相同的 KEK, KIK. 并使用该密钥解析WRAP_TK,得到TEK, TIK 之后，server client，就可以使用 TEK, TIK 进行安全通信.其实整个过程就是达到的目的是，即验证了对方的身份(PDH pubkey)，同时又将client端生成的用于安全传输的密钥(TEK,TIK)传递到server端.这里面牵扯到一个事情，就是双方都有一个密钥对的基础上，如何安全的传输数据.(例如 WRAP_TK数据), AMD SEV 采用一个通用的方法来实施，我们下面介绍:Data Protection执行data protection 需要下面输入参数: IV: 128-bit initialization vector K: A 128-bit encryption key(e.g., KEK) I: A 128-bit integrity protection key(e.g., KIK) M: A message to protect(e.g., WRAP_TK)SEV key api spec 规定，不同的message 不应该使用相同的 IV 和 K 保护。但是使用不同的K可以使用相同的IV(这条在SEND_START command中有用到)加密规则:C = AES-128-CTR(M; K, IV)MAC = HMAC-SHA-256(IV || C; I)client 需要发送 C, MAC, IV 到server端，以便server端用来恢复明文。API overflowchannel of [software, fw]对这些密钥的管理，以及vmm和guest VM memory 之间的安全数据的传输，是通过处理器中的SEV firmware处理。host hyperivor 和 sev fw之间通信是通过一些API3.同时guest有时需要外部能访问到非加密数据, 例如DMA，所以在guest中，某些memory operation 是不需要使用key加密的。如下图所示:在上图中，guest可以控制页表的c-bit来控制哪些页在访问时, 需要被加解密.在sev-snp中，这个行为会更复杂，我们先不关注。总之，driver 可以使用API来管理key，但是获取不到guest key。而运行在guest时，则会使用guest key在访存操作时，进行数据加解密。所以, 软件如果要配置 sev 功能，需要通过 sev 提供的一组API。API包括: Platform Management API: 用于platform owner配置平台和查询平台范围内的数据 Guest Management API: 在整个客户机生命周期中管理 Guest Context 而SEV driver 通过SEV fw给定的方式, 向fw发送命令请求。目前支持两种通信方式: Mailbox mode: 最初的固件 Ring Buffer Mode: 0.24+ 固件其中, Mailbox Mode是通过MMIO Register实现，而Ring Buffer Mode而是在内存中划定了一块ringbuffer，需要先通过 Mailbox方式下发 RING_BUFFER 命令进入。PLATFORM API USAGE FLOWS – Platform Provisioning 执行 FACTORY_RESET 恢复出厂设置. 厂商请求 初始化 SEV platform执行INIT 厂商请求 PEK 签署, platform 执行 PEK_CSR 生成 CSR 厂商生成 PEK cert, 并用 CA signing key 签署(OCA) platform 执行PEK_CERT_IMPORT进行 PEK_CERT，以及OCA CERT导入GUEST API USAGE FLOWS – LAUNCH GUEST该过程有guest owner参与, 而首先要做的, 是在guest owner(server)和 AMDsecure processor(client)之间建立加密通道. 以便后续guest owner 和 client侧传输数据(measure 等信息)1,2. guest owner 向 client请求 导出PDH 和其他相关证书, client通过PDH_CERT_EXPORT 导出. 生成LAUNCH_START session 相关信息 NOTE 见” SEV fw establish TRUSTED channel” 以及见 SEV KS API “LAUNCH_START” cmd主要包括: NONCEserver PDHWRAPPER TK, IV, MAC 将guest(固件)加载到内存中 根据DH key 和获取到的session info, 执行LAUNCH_START命令 调用ACTIVATE 命令，将ASID 和虚拟机（KEY）绑定 调用LAUNCH_UPDATE_DATA 加密 guest memory 将所有load内存都加密后，调用LAUNCH_MEASURE 对该内存进行测量 将测量结构发送给guest owner 认证platform 并验证测量结果 将磁盘密钥发送给guest host 通过LAUNCH_SECERT 将磁盘密钥注入到guest 调用LAUNCH_FINISH结束launch流程 在VMCB中使能 SEV-enable bit 来为该虚拟机开启SEV功能 调用VMRUN启动虚拟机，进入guestGUEST API USAGE FLOWS – LIVE migration该过程相当于source端为client，dest端为server。在source和dest之间建立起一个加密通道。 向server端请求PDH server端执行 PDH_CERT_EXPORT 命令导出PDH和其他相关证书 调用SEND_START, 该命令要求输入目的端 platform的证书链: ARK-&gt;ASK-&gt;CEK-- +--&gt; PEK | OCA--- 以及PDH SEV fw会使用证书链验证 PDH, 验证成功后，生成session 相关信息 将session相关信息发送给目的端，目的端通过 RECEIVE_START 命令加载 receive 民营没有这么复杂，只需要输入source端 PDH 以及source端生成的session info即可. dest端调用ACTIVATE source端调用SEND_UPDATE_DATA, 将source guest memory导入到一个内存中，注意该内存，被SEV FW 使用 VEK 解密，并使用TEK 加密, 并使用 TIK 摘要 dest 端 调用 RECEIVE_UPDATE_DATA, 将传过来的guest memory 导入到dest guestmemory 中, 注意该内存以被 SEV fw 通过 TEK 解密，并通过dest guest VEK 加密 dest，source端调用相应 FINISH 命令结束热迁移过程其他流程“远程”(近程)证明根据LAUNCH process 的流程中可知，在guest launch 过程中，guest owner 可以通过guest发送过来的相关证书，以及摘要来验证guest 是否在可信环境中。如果处于安全环境，guest owner 将指示 vmm 来执行后续操作。但是这个环境比较繁琐。于是, hygon 修改了SEV 的 ATTESTATOIN 命令，并且搞了一个单机版的工具，来验证guest是否处于安全环境验证。该工具在guest 用户态执行，通过调用VMMCALL指令，将userdata, mnonce, hash等数据传输到kvm侧。注意，此时kvm看到的是密文，通过 ATTESTATION 命令，将该内存传递到CSV FW中。CSV FW 填充report信息到该内存, 其中userdata 是 guest 传递过来的userdata，并通过PEK进行签名。此时，CEV fw 返回到kvm，kvm看到的仍是密文。而guest 调用VMMCALL 返回，看到的是明文。可以通过证书链验证PEK证书的合法性。并且通过PEK 证书对证书信息进行验签，和SIG1 进行比对，如果验签成功，说明该数据被 合法的csv fw 处理过. 证明了当前guest处于安全环境中。page isolateSEV-SNPRMPSEV SNP 使用RMP机制，来划分这部分内存的归属. 每个page对应一个 RMP entryRMP.Assigned字段会比表明这个配置是owned by guest, 还是 owned by host.关于page own: host cpu memory access 和iommu memory access 都将当前的执行者认为hypervisor. RMP.Assigned = 0 包括 guest owned page 和 AMD-SP owned page(e.g. RMP page)两种类型关于memory access check: 当 CPU (guest or host) 使用访存指令访问内存时, 通过MMU pagetable walk后, 获取到HPA, 将该HPA传递给SEV fw, 然后fw 会去查找RMP table, 查看该page owner和 本次访存的角色(guest host) 是否一致. 如果不一致: GUEST: trap #NPF HOST: trap #PF 当 设备通过 DMA 访存时，该 TLP 到达 root complex后，被IOMMU拦截, IOMMU 使用IOMMU pagetable 将 IOVA转换为 PA, 让后将该PA传递给 SEV fw, 同样的fw会查找RMPtable。 对于memory access 限制，只限于write access, 因为read access 有AES DEC保护。为了防止host更改guest映射(e.g., guest的本意是 GPA1-&gt;HPA1, 结果被hyp更改为GPA1-&gt;HPA2), guest memory access check会额外检查RMP.GPA是否和本次pagetable walk获取的GPA相同, 如果不相同，则认为guest受到攻击，触发NPF:PAGE VALIDATE而page own不是一成不变的. host 通过RMPUPDATE指令, guest 通过PVALIDATE指令可以修改page own.首先说明不同的page state: Hypervisor Page: owned by hyp。可被hyp读写, 不能被guest 使用c=1访问 （public page 下面会讲) Guest-Invalid: owned by guest, 但是未验证。不可被hyp写 . 不能被guest 使用c=1访问 Guest-Valid: owned by guest, 已经验证。不可被hyp写 . 可以被guest 使用c=1访问owner 变更: hyp page -&gt; Guest-Invalid : HOST 调用 RMPUPDATE. 常用于启动虚拟机，after launch guest mem. Guest-xxx -&gt; hyp page : HOST 调用 RMPUPDATE. 常用于关闭虚拟机. Guest-Invalid -&gt; Guest-validate : GUEST 调用PVLAIDATE, 用于将GPA固定为private page. Guest-validate -&gt; Guest-Invalid: GUEST 调用PVLAIDATE, 取消private page固定，为public page映射作准备 NOTE 这里牵扯到private page, public page的概念，我们下面会讲到。我们来看如果hyp 更改 guest GPA-&gt;HPA映射会发生什么? guest首先执行pvalidate 指令，将其GPA和HPA固定. hw 设置RMP[X].validate flag. Hypervisor 分配了一个新的page 准备 re-mapping攻击, 调用 RMPUPDATE将该page所在的RMP GPA ASID 都更新为和RMP[x]一样的。 hyp 更改了NPT,将 GPA(A) 映射到了物理地址[Y] 硬件检查RMP[Y].Validate = 0 说明物理地址Y没有被guest验证过. NOTE guest应该确保，其对每个GPA所在的page应该只执行一次PVALIDATE指令.由此可见, HOST 调用RMPUPDATE的作用是管理guest page的生命周期，为GUEST 分配具体的物理内存(另外，guest 销毁时, 也需要将其内存资源进行回收).在RMP中: HOST执行的RMPUPDATE负责更改大部分字段, 如果RMPUPDATE 更改了上面紫色部分，则会clear RMP.validate 字段 部分伪代码: IF ((OLD_RMP.ASID ^ NEW_RMP.ASID) || (OLD_RMP.GUEST_PA ^ NEW_RMP.GUEST_PA) || (OLD_RMP.PAGE_SIZE ^ NEW_RMP.PAGE_SIZE) || (OLD_RMP.ASSIGNED ^ NEW_RMP.ASSIGNED)) N = CPUID Fn8000001F_EBX[15:12] temp_RMP.VALIDATED = 0 temp_RMP.VMSA = 0 temp_RMP.PERMISSIONS[0] = FULL_PERMISSIONS temp_RMP.PERMISSIONS[1:(N-1)] = 0 GUEST执行PVALIDATE负责更改validate字段而GUEST 调用PVALIDATE的作用是声明HOST 分配的这个page 是属于当前的GPA。HOST不能擅自再通过调用 RMPUPDATE 更改映射关系.这个机制很关键，相当于把最终的控制权交给了guest，让guest有能力管理其GPA是否要固定。CSV3回忆 SEV-SNP 的机制, SEV-SNP 机制主要靠 RMP , 而其作用主要分为两部分: 确定 PAGE 的归属 将PAGE 和具体的GPA绑定，并且保证host无法更改好，基于上面两点，我们来看下hygon CSV3.CSV3 “RMP” – SMCR在SEV-SNP中 所有物理页是通过RMP table来决定page归属的。而在CSV3中，该方式有了一些更改（个人认为是优化）。 首先通过 kernel cmdline 可以配置csv_mem_percentage 来决定，可以 用于csv3 虚拟机的内存比例 在kernel引导早期，csv driver会在各个numa上, 根据csv_mem_percentage 预留内存，并使用cma 管理. 而随后，会调用两个csv cmd : SET_SMR, SET_SMCR, 个人理解，SMR 相当于告诉csv固件，这些内存将被 csv fw 管理. 而SMCR则是在 SMR中 isolate 一块内存, 用于管理使用。(个人认为相当于RMP)但是256M内存非常小，并不能以page的的粒度去管理， 下面我们看下其管理机制.CSV3 change to “GUEST-Invaild” – SET_GUEST_PRIVATE_MEMORY 所有cma所在的地址空间范围（包括各个cma之间的空洞），被划分为8192个空间。个人猜 测在 SMCR中，也划分为8192个object，每个 object 管理一个physical address space。所以，其管理的粒度 (假设1T内存的话，每个空间为128M) 而csv3 虚拟机分配内存时，每次分配的内存也是以相同的颗粒大小分配[1T, 128M] 通过上面的设计，可以猜测，SMCR的是多个page, 而非单个当执行完 SET_GUEST_PRIVATE_MEMORY , 命令后，该page就归属成当前的虚拟机，host无法再访问。所以该步骤相当于 SEV-SNP的hyp page-&gt; guest invalid page的page state的转变过程.而sev-snp的功能除了决定page归属，还应该确保，guest内存在使用过程中不被host更改，csv3如何做到呢?CSV3 “PVALIDATE” – CSV3_SECURE_CMD_ENC/DEC这里其实产生了一个矛盾，SMCR的管理粒度是一组page，而PVALIDATE的需求往往是一个page,那SMCR目前的设计就无法满足PVALIDATE的需求。确实是这样，然而csv3不用SMCR控制这些，那怎么做呢?CSV3 GPA-&gt;HPA的映射是靠NPT page table, 那么直接把 NPT page table isolate，不让Hyp 管理，由SEV fw 自己管理。所以NPT host就无法更改了。这样做是非常省事! Hygon csv3 不用在设计复杂的机制来验证guest 映射是否更改。而在guest中也不用处理 #VE 这种异常类型.所以，对于csv3虚拟机中，是有两套页表，一套是有CSV fw管理作为private 内存映射访问。而另一套是由 KVM 管理，用于public 内存映射访问。由于存在两套页表，guest在运行时，怎么决定使用哪套页表呢?我的猜测是先在 isolate by CSV fw管理的NPT中找，如果找不到，再从NO isolate的NPT页表中找. 再找不到就报 #NPF 给KVM.那么由于有public/private page 转换需求，就需要从guest 中支持执行某些命令来控制csv fw 建立/取消 isolate by CSV fw NPT mapping.在看这些之前，我们先看下，guest如何和csv fw 进行通信。CSV3 guest exec CSV FW command NOTE 可以想下AMD-SNP PVALIDATE 是不是需要和 SEV fw进行通信。个人认为不需要，因为RMP 是明文，CPU完全有能力修改这部分内存。所以其在guest中扩展了指令集。 但是CSV3不同。CSV3 隔离了这部分内存，CPU如果要支持必须和CSV fw进行通信.CSV3 并没有扩展 指令集，而是设置了一个规则，让guest 可以和SEV fw 进行加密通信。但是csv fw 作为一个 “PCI 设备”, 在host中可以看到，在guest中并没有该设备，guest又如何合其通信呢?如上图所示，通过一个secure call pages 和 csv fw通信。这两个page一个有NPT映射，一个没有NPT映射，有NPT映射的作为input page， 而没有NPT 映射的作为output page。guest首先将需要执行的命令写入output page, 然后读取 input page，由于INPUT page没有NPT映射，trap到KVM中，kvm调用 UPDATE_NPT命令通知 csv fw建立映射，而csv 观测到该page是 secure call page后，执行 该page中的cmd，将resp数据放到output page中，并将原来的input page接触映射，将output page 建立映射。(此时output page, 和input page的角色已经互换）而guest 从kvm中返回，可以从原来的output page（现在的input page）中读取到csv fw返回的数据。private public page无论是 SEV-SNP, 还是CSV3，都面临一个问题，guest memory 不能被host访问，但是作为DMA的page又往往需要host 访问。所以其设计出一套机制，将guest memory 划分为两部分 private memory share memory:其中public memory 作为memfd 的file pagecache管理，而 private memory 则用CMA进行预分配.（这个不展开)切换流程:切换主要包含三个流程: clear guest pgtable中的C bit. 向sev fw 发送 CSV3_SECURE_CMD_ENC 解除NPT映射。这时该gpa可以被host自定义映射 guest调用VMMCALL trap 到kvm，kvm最终建立该gpa到memfd pagecache的映射。做完上面三步, guest 访问该地址会将明文存放到share page中。但是原来的private page 并没有释放，所以对于这个gpa会有双倍的内存损耗 NOTE 可以思考下，为什么不能使用大页那么我们再来看下 public to private 流程:过程和private to public 相似，不过不同的是，其会调用invalidate pagecache相关接口，将public page 释放掉.参考链接 huangyong - AMD SEV基本原理 AMD MEMORY ENCRYPTION AMD SEV SNP Secure Encrypted Virtualization API Version 0.24 Secure VM Service Module for SEV-SNP Guests YouTube – SNP Live Migration with guest-memfd and mirror VM Git – linux-svsm 探究AMD SEV/SEV-ES远程证明过程——在EPYC 7302洋垃圾服务器上" }, { "title": "qemu coroutine", "url": "/posts/coroutine/", "categories": "qemu, coroutine", "tags": "qemu_coroutine", "date": "2025-02-25 11:00:00 +0800", "snippet": " Introduction Linux User Context Switch qemu coroutine 协程状态机 CREATE and INIT enter switch yield Use Case for QEMUIntroduction多线程和协程都可以用于并行编程，但是他们实现方式和使用场景有很大的...", "content": " Introduction Linux User Context Switch qemu coroutine 协程状态机 CREATE and INIT enter switch yield Use Case for QEMUIntroduction多线程和协程都可以用于并行编程，但是他们实现方式和使用场景有很大的区别，我们来对比下: 对比项 协程 多线程 实现方式 在用户态单线程中，完成上下文切换 内核态完成上下文切换 开销 开销较低 线程创建销毁，以及切换都需要进入内核态，开销较高 并发 只能在单个线程中来回切换完成并发 可以实现真正的并行处理（在多核cpu) 调度(切换) 协程类似于非抢占式调度，只能在主动切换 线程可以在任何时刻被中断和切换 程序复杂度 协程处理同步和资源共享较简单 多线程编程需要处理线程间的同步和资 源共享问题, 复杂度更高, 往往需要借助系统api(锁，信号量) 使用场景上: 协程 当应用程序主要是 I/O 密集型任务，如网络请求、文件操作等 当需要高并发但不需要并行计算 使用多线程的场景 当应用程序是 CPU 密集型任务，需要利用多核 CPU 的并行计算能力 当需要处理大量需要同时执行的计算任务时 协程比较适合那种需要wait的任务, 例如上面提到的I/O 密集型任务(qemu中的aio, 可以在协程中下发多个aio，然后等待io complete event)我们举个例子:在该图中, 有两个cpu core, A进程有3个thread, 其中thread1和thread2在cpu0上运行，thread 3 有四个协程，在cpu1上运行, task A 的计算负载可以分别落在cpu 0 和cpu 1上, 这也是多线程的很大的优点: 可以最大化的利用多核cpu的并行处理能力。thread1和thread2其靠kernel的任务抢占机制，来共享cpu 0, 在任何时间都有可能被对方抢占. 而thread3 中的各个协程则是 根据自己任务的完成情况，或者当前任务是否需要等待而主动选择调度。Linux User Context Switch我们需要思考下，context switch 完成哪些任务: init new task context like pthread_create() need init IP, SP(a new stack), Params context switch save… load… destroy如果用户态要完成context switch，需要处理好上面所列的三件事。而这些事情涉及的东西太底层了，如设置ip，如传参等等，所以libc中提供了ucontext系列接口来完成这些事情:ucontextucontext API API name 作用 getcontext(ucontext_t *ucp) 获取当前上下文, 保存到ucp中 setcontext(ucp) 切换到目标(ucp)上下文 makecontext(ucp, (*func)(), int argc, …) 用来modify ucp, 下面详述 swapcontext(oucp, ucp) saves current thread context in oucp and makes *ucp the currently active context. 在执行makecontext()之前，需要做一些准备工作: 调用 getcontext() 来init ucp, 需要为其分配stack, init ucontext_t.uc_stack 相关成员 ss_sp: 指向具体的堆栈地址 ss_size: 堆栈大小 ss_flags: 设置ucp-&gt;uc_link参数，根据是否设置ucp-&gt;uc_link 来确定func() 返回时，所执行的动作: NULL: 进程退出 隐式调用 setcontext(ucp-&gt;uc_link) 我们编写一个例子来演示下，该接口的使用方法和效果ucontext example 测试程序展开 #include &lt;stdio.h&gt;#include &lt;ucontext.h&gt;#include &lt;stdlib.h&gt;#define STACK_SIZE (4096 * 2)void print_current_stack(){ unsigned long stack_pointer; __asm__(\"movq %%rsp, %0\" : \"=r\"(stack_pointer)); printf(\"stack pointer(%lx)\\n\", stack_pointer);}void func(int a, int b){ printf(\"the co exec, sum(%d)\\n\", a+b); printf(\"print co stack \\n\"); print_current_stack(); return;}int main(){ int ret; char *stack = (char *)malloc(STACK_SIZE); ucontext_t uc, old_uc; int a, b = 0; printf(\"the new stack is %p\\n\", stack); printf(\"print main stack:\\n\"); print_current_stack(); getcontext(&amp;uc); uc.uc_stack.ss_sp = stack; uc.uc_stack.ss_size = STACK_SIZE; uc.uc_link = &amp;old_uc; while(1) { printf(\"main co a(%d) b(%d)\\n\", a, b); makecontext(&amp;uc, (void (*)(void))func, 2, a, b); printf(\"swap context\\n\"); swapcontext(&amp;old_uc, &amp;uc); printf(\"swap context end\\n\"); if (a++ == 3) break; b=b+2; } return 0;} 在main中jum构建一个循环，来在另一个上下文中调用func(), 并设置返回的context为调用者(main())的context，这样func()返回后，直接返回到main()的while的上下文, 继续执行循环。 输出示例 输出如下: the new stack is 0x9d22a0print main stack:stack pointer(7fff0939ee50)main co a(0) b(0)swap contextthe co exec, sum(0)print co stackstack pointer(9d4250)swap context endmain co a(1) b(2)swap contextthe co exec, sum(3)print co stackstack pointer(9d4250)swap context endmain co a(2) b(4)swap contextthe co exec, sum(6)print co stackstack pointer(9d4250)swap context endmain co a(3) b(6)swap contextthe co exec, sum(9)print co stackstack pointer(9d4250)swap context end 由上图可见，func()和main()运行在两个上下文，并且两个上下文切换示意图如下:另外，linux中还支持另外一组上下文切换的API – sigsetjmp, siglongjmpsigsetjmp, siglongjmp该系列函数一般用于实现C语言中的异常处理，如在信号处理流程中，跳转到其他的执行流程. 避免再次执行到异常代码.我们先来看下其API sigsetjmp(sigjmp_buf env, int savemask) 功能: 保存当前的上下文和信号掩码，以便以后可以通过 siglongjmp 恢复 参数: env: 保存上下文信息 savemask: 如果非0， 当前的信号掩码也会被保存 返回值: 调用者返回0 如果通过siglongjmp恢复，而返回siglongjmp传递的值 siglongjmp(sigjmp_buf env, int val) 功能: 恢复由 sigsetjmp 保存的上下文信息和信号掩码，并从 sigsetjmp 返回。 参数: env: 由sigsetjmp保存的环境信息 val: sigsetjmp 0: return 1 x(x != 0) : return x 没有返回值(因为已经跳走了) 看起来sigxxxjmp也可以实现上下文切换，但是该系列接口有个很大的问题，比较适合 recover,但不适合new。其不像ucontext接口, 可以通过makecontext()接口先new一个context, sigsetjmp只能保存当前的现场, 所以相当于只能先走到要切换的流程中埋好点，然后才能切换，很不方便.但是sigxxxjmp()对比makecontext()也有好处. 其更加轻量化. 它不会涉及完整的上下文切换,例如其可以设置不切换信号掩码，减少因系统调用而产生的切换损耗.而qemu中的协程实现主要有三种 ucontext + sigjmp: util/coroutine-ucontext.c sigaltstack: util/coroutine-sigaltstack.c coroutine-win32本文主要介绍第一种，由ucontext和sigjmp结合实现。其中，ucontext系列接口负责new context, 为sigjmp接口埋点, 而sigjmp 系列接口负责协程切换.接下来，我们来看下qemu实现:qemu coroutine协程状态机这是一个典型的由 leader 创建协程的状态机，进入协程上下文会做两种事: 埋sigxxxjmp跳转点, 为之后再次切换进协程做准备 work…协程运行期间，可能因为wait io等事件选择先切出协程(COROUTINE_YIELD),此时协程是suspend状态。等待协程处理完完整的事物后，会切出协程上下文，并置为terminal 状态.另外除了首次进入协程是使用ucontext接口, 剩余的协程/leader之间的切换，均使用sigxxxjmp系列接口，这样可以尽量减少因切换上下文带来性能损耗。整体流程整个流程如下图:CREATE and INITcreate流程主要是为协程准备好上下文环境, init 流程主要是在协程中打好跳转点, 流程包括: 为协程分配堆栈空间 使用makecontext(), swapcontext() 执行到一个新的上下文 在协程上下文中，埋 sig jmp的点 跳转回leader 上下文INIT流程只是为协程搭建了一个上下文，但是该上下文接下来要执行什么任务，需要leader指明，所以在切回leader上下文后，leader还需要为协程准备协程要执行的函数，以及函数参数(红底蓝字部分)enter在create &amp;&amp; INIT 章节中，我们介绍到首次进入协程是通过swapcontext()接口,而之后再次进入协程，就需要使用sigxxxjmp系列接口，本章节主要介绍第二种。而enter这个动作既有可能发生在leader上下文，也有可能发生在协程上下文,所以我们以下面的场景为例子，看下qemu是怎么处理的。 leader enter 协程A 协程A enter 协程B 假设协程A, 协程B 在处理过程中不会yield, 直接terminal.整个流程如下图:这样处理，会导致协程只能串行，不能嵌套执行。我们来想下为什么要这样做, 首先我们来看下，两者上下文切换次数: 串行执行 leader-&gt;A-&gt;leader-&gt;B-&gt;leader 切换4次 嵌套执行 leader-&gt;A-&gt;B-&gt;A-&gt;leader 切换4次 两者切换次数相同。所以这里的原因(猜测)很可能是，防止协程可能带来的同步问题（避免A上下文中嵌入B的上下文从而带来死锁)switch接下来，我们再来看下switch过程。switch过程比较简单。主要的函数是,qemu_coroutine_switch(), 函数原型:CoroutineAction qemu_coroutine_switch(Coroutine *from_, Coroutine *to_, CoroutineAction action);参数有三个: from: 切出的协程 to: 切入的协程 action: 本次操作的类型 COROUTINE_YIELD: 暂停from协程 COROUTINE_TERMINATE: 终止from协程 COROUTINE_ENTER: 进入to协程 我们以一个没有执行过yield协程生命周期来看下switch的细节:可以看到在执行qemu_coroutine_switch()时，action参数会作为siglongjmp(, action)传入，这样在另一个上下文中，会通过sigsetjmp()的返回值，获取到action, 而qemu_aio_coroutine_enter()会根据协程返回状态，来选择一些action: COROUTINE_TERMINATE: 销毁协程 COROUTINE_YIELD: 忽略，继续执行leader流程这里我们来总结下，不同的switch过程: leader-&gt;co ENTER: co-&gt;leader TERMINATE YIELD yieldyield是一个比较特殊的存在，因为yield动作时，还需要保存协程的现场，以便之后，再次切回协程。并且在协程yield切回leader后，leader会继续运行执行其他流程。等待该协程的等待的事件到来后，需要再次执行enter切换回该协程，如下图所示:Use Case for QEMU附录virtio-blk触发堆栈virtio_blk_handle_vq## 从avail ring中获取req=&gt; blk_io_plug()=&gt; while (virtio_blk_get_request())=&gt; virtio_blk_submit_multireq() =&gt; foreach request: ## 可能会merge submit =&gt; submit_requests() =&gt; init qemu iovc =&gt; blk_aio_pwritev/blk_aio_preadv=&gt; blk_io_unplug()blk_aio_pwritev=&gt; blk_aio_prwv(,,,,co_entry::blk_aio_write_entry, flags, cb:: virtio_blk_rw_complete,opaque) =&gt; init acb::BlkAioEmAIOCB =&gt; qemu_coroutine_create(co_entry, acb) =&gt; bdrv_coroutine_enter(blk_bs(blk), co)参考资料 huangyong – 深入理解qemu协程 _银叶先生 – 协程的原理与实现：qemu 之 Coroutine" }, { "title": "qpos -- amd spec", "url": "/posts/pqos/", "categories": "amd_sdm, qpos", "tags": "amd_sdm", "date": "2025-02-12 14:18:00 +0800", "snippet": " introduce overflow features features detect PQM CPUID (PQM) configuration PQM MSR Monitoring L3 Occupancy MBM ...", "content": " introduce overflow features features detect PQM CPUID (PQM) configuration PQM MSR Monitoring L3 Occupancy MBM BMEC AMBC PQE cat L3BE L3SMBE SDCIAE 相关链接 其他资讯 introduceamd PQOS ~= intel RDT amd PQOS 和 intel RDT 实在是太像了，cpuid，包括MSR命名格式几乎都一样。所以，之后的很多章节，我们仅把图片粘贴上，以便之后查找。overflowfeaturesPOS 包括两个主要的功能: PQOS Monitoring(PQM): monitoring the usage of shared resources L3 cache occupancy L3 cache bandwidth PQOS Enforcement(PQE): set and enforce limits on usage of shared resources L3 cache allocation L3 cache bandwidth features detect PQE, PQM 通过 CPUID Fn0000_0007_EBx[PQE, PQM] 来查看PQOS主要功能检测 : PQOS Versions PQOS相关的attribute和capablities也使用CPUID 报告, 但是某些早期CPU使用PQOS version,PQOS version 和具体的 Family/Model 关系如下: capablities &amp;&amp; attribute of PQOS Fn0000_000F: PQM Fn0000_0010: PQE 具体的细节，我们放到之后的章节中分别介绍。PQMCPUID (PQM) Fn0000_000F_EDX_x0: EDX[bit 1] L3CacheMon: 是否支持L3 monitoring EBX: RMID 最大值 Fn0000_000F_EDX_x1: 是否支持某些monitor的sub feature EAX non-zero: it indicates the size of the L3 monitoring counter, offset from 24 bits. 0: the counter size is determined by the PQOS version number 在PQM MSR章节中会介绍 EBX: counter scaling factor, 在通过QM_CTR 获取到counter值的时候，需要在乘 scaling factor 来获取实际的 cache occupancy/bandwidth(bytes) ECX: identifies the largest RMID for L3 monitoring EDX[bit 0]: L3 Occupancy EDX[bit 1]: L3 total bandwidth EDX[bit 2]: L3 local bandwidth Fn8000_0020_EBX_x0: 包含一些PQM和PQE的一些扩展功能: 其中BMEC和ABMC是PQM功能，其余都是PQE功能。 configurationPQM MSR PQR_ASSOC QM_EVTSEL 和intel rdt不同的是, qpos似乎支持一些Extended Event Q: 为什么要这么搞呢，按照道理EvtID[bit 0-7] 可以配置255个字段, 为什么要 弄一个类似于二维的字段呢 A: 不知道 QM_CTR 其中, 关于CNT_LEN 可以在 CPUID Fn0000_000F_EAX_x1[CounterSize]中获取到。具体值为(CounterSize + 24) 但是如果获取到CounterSize的值为0， 则需要根据PQOS Version 来判定: Monitoring L3 Occupancy如前面提到eventid为1，其他略， 和rdt cat 功能一样MBMMBM eventid 类型如下: read to Local NUMA Domain: 为什么需要两个eventid。 因为 evtid 2 获取 total evtid 3 获取 non-local 所以 local = evtid2 - evtid3 non-temporal 参考amd sdm 4.6.1.2 Move non-temporal, 大概的意思是说，操作的数据是非临时数据，短时间内不会访问到(非临时的意思是，这个数据比较稳定，现在store了一个值，短时间内不会被访问修改) 从上面来看，evtid 和 并且但是其多了一些更细粒度的控制eventid功能，主要有两个: BMEC AMBCBMECBMEC feature 允许软件配置指定的L3 transcation 被监控计数。其提供了一组n个MSRs,– QOS_EVT_CFG_n , 其用来指定对应的L3CacheBwMonEvt 对应的event id的BW type。例如QOS_EVT_CFG_1 对应与 L3CacheBwMonEvt_1, 其对应的EvtID是3，而QOS_EVT_CFG_1则可以配置指定EvtID 3 的 BW type, 如下图所示上图还展示了EvtID 2 默认配置为 Total L3 Bandwidth, EvtID 3 的默认配置为 LocalL3 Bandwidth CPUID BMEC 通过cpuid 获取如下信息 Fn8000_0020_EBX_x0[BMEC] (bit 3): 判定BMEC功能是否支持 Fn8000_0020_EBX_x3[EVT_NUM] (bit 7:0): 指示 QOS_EVT_CFG 中的 n CPUID Fn8000_0020_ECX_x3: 指示哪些类型的L3 tranaction 被counted QOS_EVT_CFG_n, n 值表示EVT_NUM ， 其在实现中 &gt;= 2 QOS_EVT_CFG_n MSR details 所以，该功能就是提供了一种方式，可以指定EvtID的BW type。可以是一个集合(QOS_EVT_CFG_n中多个bit置位)AMBCAMBC 就更加灵活, 支持指定一组counter , 这些counter 用来计数特定的RMID（COS）,BwType, counterID, 并且查询这些counter时，也仍然使用QM_EVTSEL, QM_CTR. 只不过在设置QM_EVTSEL时，按照另一种方式配置，下面会介绍。我们来看下具体细节: CPUID: Fn8000_0020_x5 EAX: 指示counter的大小以及counter的overflow bit EBX: 指示支持的ABMC counter的数量 ECX: 指示能不能在 QOS_ABMC_CFG 中的BwTSrc字段中填写COS 而非RMID L3_QOS_EXT_CFG 该MSR 用来使能ABMC 功能.( 当然还有一个PQE相关功能 SDCIAE, 在PQE 章节中会介绍) L3_QOS_AMBC_CFG MSR CfgEn: 如果使能，则表示本次会配置该register 分配到（作用于) CtrID中指定的counter, 反之则不会有任何配置 CtrEn: 如果使能，则表示本次会使能对 Bw Type field tracking. CtrID: 表明该寄存器作用与哪个counter IsCOS: 指示 BwSrc 是 COS 还是RMID BwSrc: COS/RMID BwType: 上图中展示了。 L3_QOS_ABMC_DEC MSR 读取该寄存器，获取的是 QOS_EVT_CFG 的配置的值, 所以其格式和 几乎QOS_ABMC_CFG MSR相同.但是有一个字段需要注意下: CfgErr: 如果是1， 则表示上一次配置是invalid，并且相应的counter 是not enabled. 这两个寄存器用来，获取/更新 当前 AMBC 配置. QM_EVTSEL - 通过下面方式选定: EventedEvtID: 12 EvtID: L3CacheAMBC RMID: 需要配置为counter id QM_CTR读取QM_EVTSEL 选定的 counter值PQEPQE 功能主要分为: CAT CDP L3 bandwidth allocation L3 slow memory bandwidth allocation SDCI allocation enforcementCPUID Fn0000_00010_EDX_x0 用来 指示 是否支持L3 AllocCPUID Fn0000_00010_x1_ECX_1 来指示是否支持哪些sub-feature: EAX: 指示 CBM_LEN EBX: 指示 L3 Cache Allocation Sharing Mask ECX: 指示是否支持一些增强功能，例如CDP EDX: 指示 COS_MAX我们分功能介绍cat这里我们不过多介绍CAT, 因为大部分和rdt一样，只不过CBM对应于L3_MASK.但是需要注意一点，CPUID Fn 0000_0010_EBX_x1[L3ShareAllocmask] 表示其他的一些function肯定会共享这些L3 cache。所以，在L3ShareAllocMask 中bit为1时，对应的cache即便是配置了各个CPU对应的CLOS 的L3_MASK 没有overlap，也仍然有其他的function和该cpu争抢cache。L3BE我们主要关注下这部分，其功能配置和rdt很不一样。AMD 的带宽控制，是允许软件配置一个最大的带宽限制(通过 L3BE MSR), 该值是一个absolute bw number:L3BE value ++ == limit BW + 1/8 GB/s, 所以跟rdt的比例throttle还不一样。我们看下具体细节 CPUID Fn8000_0020_EBX_x0L3BE: 指示L3BE是否支持 Fn8000_0020_x1: EAX[30:0]: BW LEN(下面会介绍) EDX[31:0]: L3BE feature支持的最大 COS number L3QOS_BW_CONTROL_n MSRs （用来配置 bw limit) U(unlimited): 当设置是，表明当前COS bw 不受限制，该MSR中的BW字段被忽略 BW: expressed in 1/8 GB/s increment 在某些时候，某个COS怎么也达不到设置的受限制，可能是由于下面原因: The specified limit may be greater than the maximum system bandwidth. 内存总带宽受限 The sum of the limits applied to all classes of service in the domain mayexceed the maximum bandwidth the system can deliver to that COS domain. 软件配置的所有其他cos limit 超过了系统能给该COS domain的最大带宽。 Multiple COS domains which share the same memory channels may demand moretotal bandwidth than the shared memory can supply. 内存通道受限 I/O or other system entities may consume a large fraction of systembandwidth and result in less bandwidth being available to the variousprocessor COS domains. I/O 设备抢占带宽 Large amounts of write traffic may affect the memory system’s ability todeliver read bandwidth. 写带宽影响了读带宽 手册中还举了一个例子, 大致为，有两个COSx, y. COSx 配置 limit A，COS y 配置Limit 2 * A. 此时总带宽为 2 * A, 带宽肯定不够分，此时分配给COS x A， COS y A， 而不是按照其比例，分配 (2/3)* A 给COS x，(4/3) * A 给 y.另外, 当CDP enable时，L3QOS_BW_CONTROL 只能用0, 2, 4, 6这样的index寄存器，例如 COS 0 -&gt; index 0 COS 1 -&gt; index 1 COS 2 -&gt; index 2所以，该寄存器只能用一半。这样的操作很迷, 不知道为啥。L3SMBEL3SMBE 配置和L3BE 很像，只不过是控制slow memory 带宽，不再赘述。SDCIAESmart Data Cache Injection 可以让I/O 设备访问L3 cache. 避免直接访问内存，这样可以减少对内存带宽的占用。而SDCIAE 功能则可以限制 SDCI 所占用的L3 cache的portion.该功能页很简单，如果开启了该功能，只允许SDCI 使用L3MASK[MAX_COS]指示的缓存，例如如果 MAX_COS 是15， 则SDCI 将根据 L3_MASK_15 分配缓存。相关链接 amd spec 19 Platform Quality of Service (PQOS) Extension其他资讯 AMD Publishes Platform QoS Patches For Next-Gen Processors" }, { "title": "rdt -- intel spec", "url": "/posts/rdt/", "categories": "intel_sdm, rdt", "tags": "intel_sdm", "date": "2025-02-10 15:00:00 +0800", "snippet": " introduce Backgroud MAIN features overflow object (abstract) concrete definition MONITOR CPUID (EAX 0FH) IA32_PQR_ASSOC (RMID Field) I...", "content": " introduce Backgroud MAIN features overflow object (abstract) concrete definition MONITOR CPUID (EAX 0FH) IA32_PQR_ASSOC (RMID Field) IA32_QM_EVTSEL and IA32_QM_CTR CONTROL CPUID (EAX 10H) IA32_PQR_ASSOC (CLOS Field) CAT cat configuration how cat work CDP MBA Recommended Other Link Lecture other blob Linux Foundation TO DO 相关链接introduceIntel RDT技术提供了一种硬件机制，对内存方面的共享资源(例如L2, L3 cache, memory bandwidth) 进行monitor 和 allocate.Backgroud资源分配无论在现实生活还是在计算机系统中, 一直有一个难题：如何将资源, 进行更合理的分配。而资源分配使用，无非就几种方式: share split isolate我们一方希望资源可以被共享，让资源的利用率最大化，另一方面，我们又希望自己有个独立的资源，用于更频繁的使用，并且不希望别人占用过多的其他公共资源。所以，我们又希望将资源split，然后isolate。两者在某种程度上是对立的.举一个例子。有一个村庄，每个人共享村庄里的土地资源，但是为了更方便的生活，大家为自己的家庭盖了房子。每个家庭情况不一样，有的家庭物品多，多的放不下，有的家庭物品少，大部分房间都是空的。这样做看起来空间利用率不是很高，但是大家找东西方便了，只在自己家找就行了。但是，某一天该城市发生了战争，每个村民物资种类不够，有的人种蔬菜，有的人种粮食，大家需要共享，但是从村南五环到村北五环太远了。于是村长想出了一个方案，大家都把自己富余物资收集到村委会，由村委会来调配物资，供大家吃饱穿暖。一开始村委会的策略比较简单，按需分配，大家需要多少就分多少。但是有些大聪明耍起了小聪明，每次找村委会要很多物资，吃不了放地库囤着。那大家肯定不乐意啊，于是大家商量了下，按人头分配，每个人分配到的物资是固定的。这样，某些村民饭量小，吃不了. 而有些村民饭量大, 吃不够…于是村长又开始新一轮的分配策略…而在计算机的内存子系统来看，也是类似的情况。(甚至与整个计算机体系，一直在share， isolate 中纠结演变), 支持多核的一个很重要的原因，就是为了让多个cpu共享内存, io 等其他资源。但是支持smp后，每个cpu core希望有一个很方便，独占的访问空间，例如l1, l2 cache. 随着smp core的增加，维护各个cache之间的一致性的代价又很大(尤其是在多numa场景下). 于是NUMA aware kernel会尽量避免跨numa调度。另外，为了更方便的维护多个numa之间的cache一致性，同时也为了让IO 设备共享cpu cache，又将L3 cache作为一个numa上的share cache…另外随着单个计算节点的资源越多，其上面的应用更多更复杂，并且随虚拟化技术兴起, 对资源隔离的需求也逐渐增大。而软件层面，Linux 中的cgroup 技术可以对很多资源做隔离和限制，例如，CPU, memory 使用量, block io 带宽 等等。但是，关于cache, memory bandwidth 等共享资源，由于没有硬件的辅助手段进行monitor 和control，一直是个空白。直到Intel推出了RDT技术。 关于cache和 memory bandwidth 如何在多个cpu core之间，以及cpu core 和 IO之间共享的，我们在这里简单列个图介绍下，关于更多细节. 请参考, CMU 关于 cache coherence 的相关课程6,7 在这个系统中 每个numa上有四个core，每个core独享l1,l2 cache ，每个numa共享l3 cache，每个numa有一个memorycontrol，链接各自的DRAM。而各个numa 之间，使用QPI 进行interconnect，用来维护cache coherence. 从这个图中可以看出，关于L3 cache, 其share scope 是一个numa node, 而memory bandwidth，则是所有cpu core。（虽然每个numa上的memory controlller 链接各自的DRAM， 但是其他numa仍然可以通过QPI，来请求该numa上的memory)MAIN featuresintroduce章节中提到Intel RDT 技术主要围绕两个功能展开:monitor,allocate(control),而monitor和control的既包括cpu agent（logic cpu core), 也包括 non-cpu agent(io设备DMA也和cpu之间共享cache和memory). 本文暂不介绍non-cpu agent 的相关内容. 之后介绍的所有的内容，都是基于cpu agent。所以本章节，我们将这两个功能展开，来看下，其提供的一些子功能。 monitor feature 简称 作用 Cache Monitor Technology CMT 监控L2/L3 cache的使用 Memory BandWidth Monitor MBM 监控带宽使用 control(allocate) feature 简称 作用 Cache Allocation Technology CAT 控制 L2, L3 cache 分配 Code and Data Prioritization CDP 在L2, L3中icache和dcache是共享的，该功能可以细粒度控制icache/dcache各自的占用 Memory BandWidth Allocation MBA 控制带宽分配 如前面章节提到的，RDT 其子功能也是围绕着cache，memory ndwidth 共享资源，提供提供监控和控制能力。overflow无论是monitor还是control，都得为其划分针对的”对象”。而这又引出了一系列的问题(正如前面所说, 我们仅关注cpu-agent): 如何规定对象形态? (per-cpu? per-timestamp? per-xxx?) 软件如何配置当前cpu上下文是属于哪个对象? 软件配置后，硬件如何识别当前访存指令属于哪个对象？该章节，我们围绕”对象”来看下，RDT的控制粒度，以及其大概的实现细节。object (abstract)RDT 为了让软件可以更灵活的配置，在application 和 logic core 之间定义了一个抽象层: TAGS。software 可以为logic core 定义不同的tags，然后可以为不同的application 分配不同的tags, 从而完成不同粒度的监控/控制。大体流程，如下图所示: monitor: 软件可以配置当前进程上下文，是属于哪个TAG，然后在切换到该进程时，切换到该TAG，而资源计数器，也是为每个tag定义了单独的计数器。当进程执行访存指令时，根据其TAG更新计数器。 control 一种简单的控制方法是isolate，即为每个对象划定一个资源范围。而RDT中的CAT, CDP就是这样实现的。从图中可以看到，每个TAG都规定了一个资源范围，其只能访问该范围内的资源。软件可以自由定义其TAG所属的范围，可以让不同的tag之间是isolate 状态（TAG0， TAG1），也可以让其之间又overlap（TAG0/TAG1 和 TAG2) cache和memory bandwidth的控制策略不同，cache使用的是isolate的策略，而memory control则是delay. 上面我们抽象的介绍了TAG, 下面我们来看下手册中的具体定义concrete definition针对monitor和control功能，RDT定义了不同的TAG. TAG 简称 FOR Resource Monitoring IDs RMIDs monitor Classes of Sevice CLOS resource software 可以通过MSR IA32_QM_EVTSEL 来配置当前logic cpu context所属的 CLOS/RMID接下来，我们来分章节来看下，RDT monitor/control 子功能所其他细节。MONITOR软件侧使用monitor 功能 的大致流程如下: 通过CPUID/ACPI 来获取RDT 支持的features，以及支持RMIDs的总数量 software 可以通过某种方式来配置当前的RMID 允许软件通过MSRs/MMIO 来收集monitor stats我们来看下具体实现: CPUID: EAX=0xf, ECS=0x0, 0x1 specfy RMID: IA32_PQR_ASSOC collect event stats: IA32_QM_EVTSEL, IA32_QM_CTRCPUID (EAX 0FH)CPUID.EAX[0x0F]有两个分支来展示RDT相关feature. 展开 RMIDs numbers: CPUID.(EAX=0FH, ECX=1H).EBX reports the highest RMID value of any resource type that supports monitoring in the processor. CPUID.(EAX=0FH, ECX=1H).ECX enumerates the highest RMID value that can be monitored with this resource type, see Figure 18-21. support Non-CPU features: CPUID.(EAX=0FH, ECX=1H).EAX[bit 9,10] bit 9: Non-CPU Agent Cache Occupancy Monitoring bit 10: Non-CPU Agent Memory L3 External Bandwidth Monitoring support EventType mask: CPUID.(EAX=0FH, ECX=1H).EDX[bit 0, 1, 2] 在IA32_QM_EVTSEL and IA32_QM_CTR列举各个Event Type对应的EventID值, 以及其对应的featureIA32_PQR_ASSOC (RMID Field)IA32_PQR_ASSOC我们在concrete definition章节介绍过,在MONITOR 相关功能中, 软件需要配置其RMID Field 来标记当前CPU上下文。IA32_QM_EVTSEL and IA32_QM_CTRSoftware通过设置IA32_QM_EVTSEL MSR，来选择要collect的RMID和EventID, 并通过IA32_QM_CTR进行query.eventid 值如下: CMT: L3 cache occupancy MBM: L3 Total External Bandwidth and L3 Local External Bandwidth 关于L3 local/total BW , 我们可以简单认为，某个CPU访存，触发了L3 cache miss, 如果其访存地址位于local memory, 记录到L3 local/total BW 中; 如果其访存地址位于remote memory, 则只记录到L3 total BW 中. 如果要继续深入理解L3 Total External Bandwidth/L3 local External Bandwidth 推荐走读CMU directory-based cache coherence 相关课程.6CONTROL其软件侧整体的使用流程方式和monitor功能很相似，我们来看下: 通过CPUID/ACPI 获取RDT支持的feature，以及支持的CLOS总数量 software 可以通过某种方式配置当前CLOS 允许软件通过MSR/MMIO 来进一步配置每个CLOS的资源CPUID (EAX 10H) CPUID.EAX[0x10].ECX[0]: 用来enum哪些sub-feature支持, eg: EBX[bit 0]: res EBX[bit 1]: l3 cat EBX[bit 2]: l2 cat EBX[bit 3]: mba CPUID.EAX[0x10].ECX[1]: 用来enum l3 cat CPUID.EAX[0x10].ECS[2]: 用来enum l2 cat我们这里只展示下, l3 cat: EDX: 表示 highest CLOS number. ECS[bit 2]: 表示是否支持l3 cdp ECX[bit 3]: 表示cbm中的bit是否支持不连续，例如0x5(101b)是不连续的，0x6(110b)是连续的。 EAX[bit 4-0]: cbm lenIA32_PQR_ASSOC (CLOS Field)CAT和MBA 都使用 CLOS 最为TAG。前面提到过，和RMID一样，软件可以通过配置 IA32_PQR_ASSOC MSR中的CLOS Field, 来决定当前CPU的CLOS.关于具体的feature，其他配置方法，我们分别来看下:CAT我们先来看下CAT可以达到的效果:在没有cat时，某些低优先级的 application会和高优先级application共享LLC，甚至低优先级应用在短时间内访问大量内存后，其会争抢到跟多的cache。但是有了cat之后，限制低优先级应用的cache使用量。cat configuration前面提到，IA32_PQR_ASSOC中的CLOS决定了要control的对象。而capacity bitmap(CBM)则决定了不同的CLOS分配的cache的区域。图中的每个bit可以认为一块cache资源，A 表示allocate给横轴的clos。 in default bitmask，所有clos都分配了全部的cache，大家共享所有cache。 in overlappd bitmask, cache of clos0 ∈ clos1 ∈ clos2 ∈ clos3 in isolated bitmap, clos0, clos1, clos2, clos3 都使用独立的cache。互不冲突，其中cache size of ( clos0 = 2clos1 = 4 clos2 = 4 clos3)cbm 配置根据cat作用的对象不同，cbm配置的MSR也不同 L3 CBM: IA32_L3_MASK_n MSR: “n” is the Class of Service, L2 CBM: IA32_L2_MASK_ncat 相关的msr如下图:cbm 是怎么作用于cat 功能的?how cat work我们很容易想到，是不是基于way做的，我们首先回顾下，cache的组相连7,8:图片中描述的是某牙膏厂某代i7 l1 data cache和另一张不知名cpu的cache图，用来更方便的看per-way cache, 具体参数如下(第一个图): B=64: 每个block(cache line data part)为大小64byte E=8: 一共有8-way S=64: 每个1-way有64 个block cache line 总大小: 64 * 64 * 8 = 32 kbyte每个地址可以分为三部分来索引cache. set index: 用来选择哪个way tag: 在每一way中选择具体的cache line block index: 在每个cache line中选择offset那么，当一个地址确定时，其way index，block index已经确定，而变数就是该way中的index. 该mask 主要用来确定，在该way中能够选择哪些 cacheline 来缓存该物理内存.类似于再确定一个sub-way.我们来看看手册中的描述: It is generally expected that in way-based implementations, one capacitymask bit corresponds to some number of ways in cache, but the specificmapping is implementation-dependent. In all cases, a mask bit set to ‘1’specifies that a particular Class of Service can allocate into the cachesubset represented by that bit. A value of ‘0’ in a mask bit specifies thata Class of Service cannot allocate into the given cache subset. In general,allocating more cache to a given application is usually beneficial to itsperformance.手册中的意思是，一般猜测认为，可能是way-based的实现方式（除了address中的set index,还使用 cbm 在确定一个sub-way。但是手册中提到，这个cbm和sub-way的关系是 implementation-dependent.下图中也展示了这个大致的实现:手册中关于该图片的解释: The currently running application’s Class of Service is communicated to thehardware through the per-logical- processor PQR MSR (IA32_PQR_ASSOC MSR).When the OS schedules an application thread on a logical processor, theapplication thread is associated with a specific CLOS (i.e., thecorresponding CLOS in the PQR) and all requests to the CAT-capable resourcefrom that logical processor are tagged with that CLOS (in other words, theapplication thread is configured to belong to a specific CLOS). The cachesu\\bsystem uses this tagged request information to enforce QoS. The capacitybitmask may be mapped into a way bitmask (or a similar enforcement entitybased on the implementation) at the cache before it is applied to theallocation policy. For example, the capacity bitmask can be an 8-bit mask andthe enforcement may be accomplished using a 16-way bitmask for a cacheenforcement implementation based on way partitioning.大概的意思是，当OS调度器调度到一个thread时, 会切换到属于该thread的PQR MSR, 表明该thread属于这个CLOS。之后对 CAT-capable 资源的所有访问，都会被 tag上该 CLOS. 而cache subsystem 会根据 该tag 来 enforce QoS。另外，cbm可能会映射成一个way bitmap.例如从上图来看，cbm 又8bit，但是每个cache set 却有16-way, 所以两者有一个对应关系。CDPCDP 包括L3 CDP 和 L2CDP, 我们以l3 CDP为例。CDP 的作用是在high level cache中(l2, l3) 隔离 icache,dcache。而前面提到的cat也是isolate cache的作用。所以，cdp 是基于cat 的扩展功能。其做法是，当使能了CDP后，cbm将被split成两组，一组用于data, 一组用于code, 而clos和cbm进行重映射，每个clos对应两个cbm. 对于第二个图，如果clos number 为5，则最后一个不成对的clos4 将不会被cdp使用。其使能方式比较简单，配置IA32_L3_QOS_CFG中的bit 0其工作方式和cat的工作方式相同。MBAMBA 功能主要是用来限制memory bandwidth，其和cat技术一样, 使用CLOS标记当前throttle对象。另外使用IA32_L2_QoS_Ext_BW_Thrtl_xxx来调整该clos 的delay value具体怎么调整细节，包括其达到的效果，在手册中没有找到答案 NOTE 关于这一部分，手册中描述的比较模糊，所以我们这里只是简单看下如何配置，之后了解更多细节之后再更新.Recommended Other LinkLecture CMU-15418 snoop/directory-based cache coherence5,6: 可以更系统了解cache一致性所面临的挑战，以及其目前的解决方案 Onur Mutlu 大神关于memory QoS 课程 – 9 L11: Memory Controllers: Service Quality and Performance L13a: Memory Controllers: Service Quality and Performance II other blob 苏南里公牛 rdt blob 4: 由浅入深的介绍了intel rdt的细节，以及源码实现（比我写的好太多) huataihuang rdt blob 3: 介绍了相关原理以及一些实验部分Linux FoundationIntel的FengHua Yu的演讲10,11, 由于没有英文字幕，英语不好的慎看，不过大佬口音还是很chinese的，比较容易听懂。 愿全世界都说中国话TO DO该文章中包括了rdt 技术的相关背景和一些原理的猜测部分，之后打算走读Qnur Mutlu9课程，并将这部分内容转移到课程笔记中, 使该文章仅保留intel sdm部分。相关链接 Intel® Resource Director Technology (Intel® RDT) Architecture Specification intel sdm Intel Resource Director Technology(RDT) – huataihuang 浅度剖析内核 RDT 框架 – 苏里南公牛 Snooping-Based Cache Coherence Directory-Based Cache Coherence [计算机体系结构] Cache Memory – houmin Cache 直接映射、组相连映射以及全相连映射 Onur Mutlu 课程(内含youtube链接，以及pdf) [youtube]Resource Allocation: Intel Resource Director Technology (RDT) by Fenghua Yu, Intel [PDF] Resource Allocation: Intel Resource Director Technology (RDT) by Fenghua Yu, Intel" }, { "title": "dirty tracking", "url": "/posts/dirty-tracking/", "categories": "live_migration", "tags": "live_migration", "date": "2025-02-08 22:20:00 +0800", "snippet": "图示", "content": "图示" }, { "title": "vfio", "url": "/posts/vfio/", "categories": "virt, vfio", "tags": "vfio", "date": "2025-01-25 17:00:00 +0800", "snippet": "vfioqemuvfio_realize=&gt; vfio_attach_device =&gt; VFIOIOMMUClass-&gt;attach_device : vfio_iommu_legacy_class_init : iommufd_cdev_attachvfio_iommu_legacy_attach...", "content": "vfioqemuvfio_realize=&gt; vfio_attach_device =&gt; VFIOIOMMUClass-&gt;attach_device : vfio_iommu_legacy_class_init : iommufd_cdev_attachvfio_iommu_legacy_attach_devicevfio_iommu_legacy_attach_device=&gt; groupid = vfio_device_groupid() =&gt; group_patch = readlink(\"/sys//sys/devices/pci{domain}:{bus}/{domain}:{bus}/{b:d.f}/iommu_group\") # eg: ../../../kernel/iommu_groups/11 =&gt; read/sscan (group_patch) -&gt; groupid # eg: 11=&gt; group = vfio_get_group =&gt; group-&gt;fd = open(\"/dev/vfio/{groupid}\") =&gt; ioctl(group-&gt;fd, VFIO_GROUP_GET_STATUS, &amp;status) =&gt; if ! status.flags &amp; VFIO_GROUP_FLAGS_VIABLE: =&gt; goto error =&gt; vfio_connect_container() =&gt; foreach_container: =&gt; ioctl(group-&gt;fd, VFIO_GROUP_SET_CONTAINER, &amp;container-&gt;fd) =&gt; vfio_ram_block_discard_disable(container, true) =&gt; if ioctl() is success: =&gt; vfio_kvm_device_add_group(group); =&gt; return =&gt; else: =&gt; continue # if have not fit container, need create new =&gt; fd = qemu_open(\"/dev/vfio/vfio\", O_RDWR, errp) =&gt; ret = ioctl(fd, VFIO_GET_API_VERSION) =&gt; Determine if the API version is supported =&gt; vfio_ram_block_discard_disable(container, true) =&gt; vioc = VFIO_IOMMU_GET_CLASS(bcontainer) =&gt; vfio_kvm_device_add_group(group) =&gt; vfio_kvm_device_add_fd(group-&gt;fd, &amp;err) =&gt; struct kvm_device_attr attr = {.group = KVM_DEV_VFIO_FILE, .attr = KVM_DEV_VFIO_FILE_ADD, .addr = &amp;fd} =&gt; if vfio_kvm_device_fd &lt; 0: # need create kvm_device fd =&gt; struct kvm_create_device cd = {.type = KVM_DEV_TYPE_VFIO} =&gt; kvm_vm_ioctl(kvm_state, KVM_CREATE_DEVICE, &amp;cd) =&gt; vfio_kvm_device_fd = cd.fd =&gt; ioctl(vfio_kvm_device_fd, KVM_SET_DEVICE_ATTR, &amp;attr) =&gt; vfio_address_space_insert(space, bcontainer) =&gt; memory_listener_register(&amp;bcontainer-&gt;listener, bcontainer-&gt;space-&gt;as)=&gt; vfio_get_device =&gt; kernelKVM_CREATE_DEVICE, KVM_DEV_TYPE_VFIOioctl(, KVM_CREATE_DEVICE, cd = {.type = KVM_DEV_TYPE_VFIO})流程kvm_vm_ioctl=&gt; case KVM_CREATE_DEVICE: kvm_ioctl_create_device =&gt; dev(struct kvm_device) = kzalloc() =&gt; ops = kvm_device_ops_table[type] =&gt; init dev-&gt;ops, dev-&gt;kvm =&gt; ops-&gt;create() =&gt; list_add_rcu(&amp;dev-&gt;vm_node, &amp;kvm-&gt;devices); =&gt; anon_inode_getfd(ops-&gt;name, &amp;kvm_device_fops, dev, O_RDWR | O_CLOEXEC);TYPE KVM_DEV_TYPE_VFIO OPS:static const struct kvm_device_ops kvm_vfio_ops = { .name = \"kvm-vfio\", .create = kvm_vfio_create, .release = kvm_vfio_release, .set_attr = kvm_vfio_set_attr, .has_attr = kvm_vfio_has_attr,};kvm_vfio_create=&gt; determine if VFIO device of this vm is exist =&gt; { // Only one VFIO \"device\" per VM list_for_each_entry(tmp, &amp;dev-&gt;kvm-&gt;devices, vm_node) if (tmp-&gt;ops == &amp;kvm_vfio_ops) return -EBUSY; }=&gt; kv(struct kvm_vfio *) = kzalloc()=&gt; INIT_LIST_HEAD(&amp;kv-&gt;file_list);=&gt; dev-&gt;private = kvKVM_SET_DEVICE_ATTR, attr = {group = KVM_DEV_VFIO_FILE, attr = KVM_DEV_VFIO_FILE_ADD}ioctl(, KVM_SET_DEVICE_ATTR, attr = {group = KVM_DEV_VFIO_FILE, attr = KVM_DEV_VFIO_FILE_ADD}流程:kvm_vfio_file_add" }, { "title": "iommu", "url": "/posts/iommu/", "categories": "virt, iommu", "tags": "iommu", "date": "2025-01-24 13:49:00 +0800", "snippet": "what is iommuIOMMU 顾名思义，就是让IO 的MMU, 可以允许IO request 进行类似于 CPU 侧MMU根据页表映射而完成地址转换的行为。而IO request 主要包含哪些呢? DMA request interrupt而iommu 的作用就是完成这些request的重映射, 如下图所示:CPU 侧 request 通过CPU Memory Managemen...", "content": "what is iommuIOMMU 顾名思义，就是让IO 的MMU, 可以允许IO request 进行类似于 CPU 侧MMU根据页表映射而完成地址转换的行为。而IO request 主要包含哪些呢? DMA request interrupt而iommu 的作用就是完成这些request的重映射, 如下图所示:CPU 侧 request 通过CPU Memory Management(MMU) 完成VA-&gt;PA的映射转换, 最终得到HPA。当然，不同的domain（进程）由于其页表建立的映射关系不同，访问的相同的VA可以得到PA可能不同。Device侧 request 通过 DMA Memory Management(IOMMU) 完成 设备发出的DMA地址-&gt;HPA 的转换.IOMMU可以为每个设备(不准确，我们先这样理解) 都分配独立的页表, 从而让不同设备使用相同的DMA地址最终访问到不同的HPA.DMA request 看似比较好理解, 无非是让原本IO request 访问的地址 A, 重映射到地址B。但是，真的有这么简单么? 我们知道，内存访问是带有属性的(e.g., access right, memory type..), 这些属性，我们可以通过关注TLB中存储的字段得到。而DMA remapping 也可以让这些属性发生改变.而中断重映射也是同理，通过中断重映射，不仅可以改变 address（也就是哪个dest cpu，哪个vector), 同样也会改变其attr，这个我们可以通过关注 MSI message (MSI data, MSI address)得到。所以综上所述，重映射就像kernel中的remap()动作一样，我们可以重新定义该虚拟地址对应的physical address, 同样，我们可以根据变动页表的某些字段，来改变访问该VA时的某些行为.IOMMU can do what上面提到，IOMMU主要的工作是重映射设备发出的IO request, 而有了这个功能之后，可以做什么呢?那既然, IOMMU 的功能和MMU比较类似，我们来思考下MMU(也就是分页)可以什么? 可能很多小伙伴就会马上回答到, 多进程! 没错，我们这里改成多实例。而要让多实例可以良好的共享内存资源，我们需要完成以下两点需求: 物理地址隔离 地址转换物理地址隔离比较好理解, 不能让一个实例可以无条件的访问另一个实例内存。这样很不安全。而地址转换为什么又是必须的呢?How IOMMU workIOMMUiommu kernel cmdline off force noforce biomerge merge panic nopanic forcesac allowdac nodac usedac soft pt noptstruct代码记录initpci_iommu_init=&gt; intel_iommu_init =&gt; dmar_table_init =&gt; if dmar_table_initialized == 0: =&gt; ret = parse_dmar_table() ## 为各个unit 定义callbak =&gt; define some callbak { struct dmar_res_callback cb = { .arg[ACPI_DMAR_TYPE_HARDWARE_UNIT] = &amp;drhd_count, .cb[ACPI_DMAR_TYPE_HARDWARE_UNIT] = &amp;dmar_parse_one_drhd, .cb[ACPI_DMAR_TYPE_RESERVED_MEMORY] = &amp;dmar_parse_one_rmrr, ... } } =&gt; dmar_table_detect(); =&gt; status = acpi_get_table(ACPI_SIG_DMAR, 0, &amp;dmar_tbl) =&gt; dmar = (struct acpi_table_dmar *)(tboot_get_dmar_table(dmar_tbl)) =&gt; ret = dmar_walk_dmar_table(dmar, &amp;cb) =&gt; dmar_walk_remapping_entries =&gt; foreach remapping struct: =&gt; cb-&gt;cb[iter-&gt;type](iter, cb-&gt;arg[iter-&gt;type]) ## ----- parse_dmar_table END =&gt; if ret &gt; 0: =&gt; dmar_table_initialized = 1 =&gt; dmar_dev_scope_init =&gt; dmar_register_bus_notifier =&gt; init_no_remapping_devices =&gt; init_dmars =&gt; init_iommu_pm_ops =&gt; foreach drhd =&gt; iommu_device_sysfs_add =&gt; iommu_device_register =&gt; iommu_pmu_register主要看下dmar_parse_one_drhddmar_parse_one_drhd=&gt; drhd = (struct acpi_dmar_hardware_unit *)header# new a dmaru## dmaru -&gt; | --dmaru-- | --header-- |=&gt; dmaru = kzalloc(sizeof(struct dmar_drhd_unit) + header-&gt;length)=&gt; dmaru-&gt;hdr = (void *)(dmaru + 1)# copy header=&gt; memcpy(dmaru-&gt;hdr, header, header-&gt;length)=&gt; init some dmaru member =&gt; reg_base_addr, segment, reg_size, include_all =&gt; devices ## 在这里仅仅是alloc, 并没有初始化 =&gt; dmar_alloc_dev_scope(start, end, cnt[oparam]) =&gt; if dev_scope-&gt;entry_type in ACPI_DMAR_SCOPE_TYPE_NAMESPACE ACPI_DMAR_SCOPE_TYPE_ENDPOINT ACPI_DMAR_SCOPE_TYPE_BRIDGE ## don't alloc for ## ACPI_DMAR_SCOPE_TYPE_IOAPIC ## ACPI_DMAR_SCOPE_TYPE_HPET =&gt; (*cnt)++ =&gt; kcalloc(sizeof(struct dmar_dev_cscope ) * *cnt) =&gt; alloc_iommu() =&gt; iommu(struct intel_iommu *) = kzalloc() =&gt; iommu-&gt;seq_id = ida_alloc_range() =&gt; iommu-&gt;name = f\"dmar{iommu-&gt;seq_id}\" =&gt; map_iommu(iommu, drhd) =&gt; iommu-&gt;reg_phys = drhd-&gt;reg_base_addr =&gt; iommu-&gt;reg_size = drhd-&gt;reg_size ## reserve drhd mmio address range =&gt; request_mem_region() =&gt; iommu-&gt;reg = ioremap() =&gt; iommu-&gt;cap = dmar_readq(iommu-&gt;reg + DMAR_CAP_REG) =&gt; iommu-&gt;ecap = dmar_readq(iommu_reg + DMAR_ECAP_REG) =&gt; ... =&gt; =&gt; dmar_register_drhd_unit()##" }, { "title": "host-bridge is pci device BUT NOT pci bridge", "url": "/posts/host-bridge/", "categories": "pcie, host_bridge", "tags": "pcie, host_bridge", "date": "2025-01-23 21:37:00 +0800", "snippet": "查看host-bridge配置空间[root@A06-R08-I134-73-919XB72 openEuler-2403]# lspci -xxx -s 00:00.000: [86 80] [00 2f] [40 05] [10 00] 02 [00 00 06] 00 00 [00] 00 [vendor] [device] [command]...", "content": "查看host-bridge配置空间[root@A06-R08-I134-73-919XB72 openEuler-2403]# lspci -xxx -s 00:00.000: [86 80] [00 2f] [40 05] [10 00] 02 [00 00 06] 00 00 [00] 00 [vendor] [device] [command] [status][revision id] [class_code] | | header type [ type 0]10: [00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0020: 00 00 00 00 00 00 00 00](bar) [00 00 00 00] [86 80] [00 00] [cardbus CIS ptr] [subsys [subsystem id] vendor id]30: 00 00 00 00 [90] 00 00 00 00 00 00 00 00 [01] 00 00 cap pointor interrupt pin40: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0050: 01 e0 ff c7 00 00 00 00 00 00 00 00 00 00 00 0060: 05 90 02 01 00 00 00 00 00 00 00 00 00 00 00 0070: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0080: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0090: 10 e0 42 00 00 80 00 00 00 00 00 00 41 38 79 00a0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00b0: 00 00 00 00 9e 13 00 00 00 00 00 00 06 00 00 00c0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00d0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00e0: 01 00 03 00 08 00 00 00 00 00 00 00 00 00 00 00f0: 04 00 00 00 00 00 00 00 09 00 00 00 00 00 00 00传统方式读取pci配置空间host bridge的提供了两个io端口0xcf8(config address), 0xcfc(config_data)相关代码:pci_bus_read_config_##size pci_read raw_pci_read pci_conf1_readint raw_pci_read(unsigned int domain, unsigned int bus, unsigned int devfn, int reg, int len, u32 *val){ if (domain == 0 &amp;&amp; reg &lt; 256 &amp;&amp; raw_pci_ops) return raw_pci_ops-&gt;read(domain, bus, devfn, reg, len, val); if (raw_pci_ext_ops) return raw_pci_ext_ops-&gt;read(domain, bus, devfn, reg, len, val); return -EINVAL;}//对于pci type 0/1来说static int pci_conf1_read(unsigned int seg, unsigned int bus, unsigned int devfn, int reg, int len, u32 *value){ unsigned long flags; if (seg || (bus &gt; 255) || (devfn &gt; 255) || (reg &gt; 4095)) { *value = -1; return -EINVAL; } raw_spin_lock_irqsave(&amp;pci_config_lock, flags); //将 config address写入 CF8 outl(PCI_CONF1_ADDRESS(bus, devfn, reg), 0xCF8); //从CFC中读取 switch (len) { case 1: *value = inb(0xCFC + (reg &amp; 3)); break; case 2: *value = inw(0xCFC + (reg &amp; 2)); break; case 4: *value = inl(0xCFC); break; } raw_spin_unlock_irqrestore(&amp;pci_config_lock, flags); return 0;}通过下面bpftrace命令抓取lspci -xxx -s 00:00.0是否会调用该函数:bpftrace -e 'kfunc:vmlinux:pci_conf1_read { printf(\"Device: %x:%x \\n\", args-&gt;bus, args-&gt;devfn); }'输出如下:Device: 0:0 0Device: 0:0 4Device: 0:0 8Device: 0:0 c...Device: 0:0 f0Device: 0:0 f4Device: 0:0 f8Device: 0:0 fc可以看到其一共读取了256 byte参考链接 Intel® Platform Innovation Framework for EFI PCI Host Bridge Resource Allocation Protocol Specification" }, { "title": "emulate wbinvd", "url": "/posts/wbinvd-emulate/", "categories": "kvm, wbinvd", "tags": "kvm", "date": "2025-01-22 13:05:00 +0800", "snippet": "related patch KVM: VMX: wbinvd exiting commit e5edaa01c4cea5f60c617fac989c6458df0ecc4e Eddie Dong Sun Nov 11 12:28:35 2007 +0200 KV...", "content": "related patch KVM: VMX: wbinvd exiting commit e5edaa01c4cea5f60c617fac989c6458df0ecc4e Eddie Dong Sun Nov 11 12:28:35 2007 +0200 KVM: VMX: Execute WBINVD to keep data consistency with assigned devices commit f5f48ee15c2ee3e44cf429e34b16c6fa9b900246 Sheng Yang Wed Jun 30 12:25:15 2010 +0800 v1 v2 v3 v4 v5 v6 kvm: x86: make kvm_emulate_* consistant commit 5cb56059c94ddfaf92567a1c6443deec8363ae1c Joel Schopp Mon Mar 2 13:43:31 2015 -0600 " }, { "title": "qemu brige migration", "url": "/posts/pci_bridge/", "categories": "qemu, pci_bridge", "tags": "qemu_pci_bridge", "date": "2025-01-21 23:02:00 +0800", "snippet": "PCI bridge dev vmstatestatic const VMStateDescription pci_bridge_dev_vmstate = { .name = \"pci_bridge\", .priority = MIG_PRI_PCI_BUS, .fields = (VMStateField[]) { VMSTATE_PCI_DEVICE(p...", "content": "PCI bridge dev vmstatestatic const VMStateDescription pci_bridge_dev_vmstate = { .name = \"pci_bridge\", .priority = MIG_PRI_PCI_BUS, .fields = (VMStateField[]) { VMSTATE_PCI_DEVICE(parent_obj, PCIBridge), SHPC_VMSTATE(shpc, PCIDevice, pci_device_shpc_present), VMSTATE_END_OF_LIST() }};而pci_bridge type为:static const TypeInfo pci_bridge_type_info = { .name = TYPE_PCI_BRIDGE, .parent = TYPE_PCI_DEVICE, .instance_size = sizeof(PCIBridge), .abstract = true,};#define TYPE_PCI_BRIDGE \"base-pci-bridge\"OBJECT_DECLARE_SIMPLE_TYPE(PCIBridge, PCI_BRIDGE)struct PCIBridge { /*&lt; private &gt;*/ PCIDevice parent_obj; /*&lt; public &gt;*/ /* private member */ PCIBus sec_bus; /* * Memory regions for the bridge's address spaces. These regions are not * directly added to system_memory/system_io or its descendants. * Bridge's secondary bus points to these, so that devices * under the bridge see these regions as its address spaces. * The regions are as large as the entire address space - * they don't take into account any windows. */ MemoryRegion address_space_mem; MemoryRegion address_space_io; PCIBridgeWindows *windows; pci_map_irq_fn map_irq; const char *bus_name;};其pci_bridge type的 instance_type为PCIBridge, 而 pci_bridge_dev_vmstate中，并没有保存太多信息，例如address_space_io, address_space_mem相关信息. 但是这些信息又必须迁移到目的端。那是怎么传递的呢?PCI bridge migrate data像PCIBridge中的信息，其实在PCI device的配置空间中都有相应的配置空间，所以只需要将PCI device配置空间传过去就可以了。pci_bridge_type_info.fields中有对parent obj的引用。通过pci bridge的TypeInfo，可以看到，其parent obj instance为 PCIDevice, 我们直接看对这个instance的VMSD:const VMStateDescription vmstate_pci_device = { .name = \"PCIDevice\", .version_id = 2, .minimum_version_id = 1, .fields = (VMStateField[]) { VMSTATE_INT32_POSITIVE_LE(version_id, PCIDevice), VMSTATE_BUFFER_UNSAFE_INFO_TEST(config, PCIDevice, migrate_is_not_pcie, 0, vmstate_info_pci_config, PCI_CONFIG_SPACE_SIZE), VMSTATE_BUFFER_UNSAFE_INFO_TEST(config, PCIDevice, migrate_is_pcie, 0, vmstate_info_pci_config, PCIE_CONFIG_SPACE_SIZE), VMSTATE_BUFFER_UNSAFE_INFO(irq_state, PCIDevice, 2, vmstate_info_pci_irq_state, PCI_NUM_PINS * sizeof(int32_t)), VMSTATE_END_OF_LIST() }};我们这里只关心PCI bridge, 来看下其VMStateInfostatic VMStateInfo vmstate_info_pci_config = { .name = \"pci config\", .get = get_pci_config_device, .put = put_pci_config_device,};其会在vmstate_save_state相关流程中调用put, 将[PCIDevice.config,PCIDevice.confgi + PCI_CONFIG_SPACE_SIZE] 传到目的端。然后在vmstate_load_state流程中 调用get接口。我们分别来看下:putstatic int put_pci_config_device(QEMUFile *f, void *pv, size_t size, const VMStateField *field, JSONWriter *vmdesc){ const uint8_t **v = pv; assert(size == pci_config_size(container_of(pv, PCIDevice, config))); qemu_put_buffer(f, *v, size); return 0;}比较简单，只是做了下size的校验, 然后将整个配置空间传过去我们来详细看下get:static int get_pci_config_device(QEMUFile *f, void *pv, size_t size, const VMStateField *field){ PCIDevice *s = container_of(pv, PCIDevice, config); PCIDeviceClass *pc = PCI_DEVICE_GET_CLASS(s); uint8_t *config; int i; assert(size == pci_config_size(s)); config = g_malloc(size); qemu_get_buffer(f, config, size); //==(1)== for (i = 0; i &lt; size; ++i) { if ((config[i] ^ s-&gt;config[i]) &amp; s-&gt;cmask[i] &amp; ~s-&gt;wmask[i] &amp; ~s-&gt;w1cmask[i]) { error_report(\"%s: Bad config data: i=0x%x read: %x device: %x \" \"cmask: %x wmask: %x w1cmask:%x\", __func__, i, config[i], s-&gt;config[i], s-&gt;cmask[i], s-&gt;wmask[i], s-&gt;w1cmask[i]); g_free(config); return -EINVAL; } } //==(2)== memcpy(s-&gt;config, config, size); //==(3)== pci_update_mappings(s); if (pc-&gt;is_bridge) { PCIBridge *b = PCI_BRIDGE(s); pci_bridge_update_mappings(b); } memory_region_set_enabled(&amp;s-&gt;bus_master_enable_region, pci_get_word(s-&gt;config + PCI_COMMAND) &amp; PCI_COMMAND_MASTER); g_free(config); return 0;}这里面主要有几个工作: 比较配置空间（这里不是比较所有的，而是根据cmask, wmask, w1cmask 来选择比较哪些字段) 如果发现上面的比较没有问题，将整个的配置空间s-&gt;configcopy到目的端的PCIDevice-&gt;config中 mr相关操作 释放old mr 注册新的mr 比较配置空间解释下几个mask含义: cmask: 表示要哪些配置空间是需要在load时，做compare check wmask: 表示这些地址在配置空间中的是 R/W 的 w1cmask: 表示这些地址在配置空间中是 Write 1 to Clear后面两个都表示该配置空间地址可写那么就意味着，这些是OS 可以配置的，所以这些信息不用compare.以bridge的io空间为例, 首先io window是R/W的，所以需要置位wmaskstatic void pci_init_mask_bridge(PCIDevice *d){ ... d-&gt;wmask[PCI_IO_BASE] = PCI_IO_RANGE_MASK &amp; 0xff; d-&gt;wmask[PCI_IO_LIMIT] = PCI_IO_RANGE_MASK &amp; 0xff; ... d-&gt;cmask[PCI_IO_BASE] |= PCI_IO_RANGE_TYPE_MASK; d-&gt;cmask[PCI_IO_LIMIT] |= PCI_IO_RANGE_TYPE_MASK;}但是其又置位cmask, 不知道原因。mr相关操作这个主要分为两部分: PCI Device 的bar空间 PCI Bridge 的 io / memory /pref memory window但是两者本质是相同的，都需要在 parent mr 上 overlap." }, { "title": "time", "url": "/posts/time/", "categories": "time", "tags": "time", "date": "2024-12-20 22:19:00 +0800", "snippet": "参考链接We Are Not Getting Any Younger: A New Approach to Time and Timers", "content": "参考链接We Are Not Getting Any Younger: A New Approach to Time and Timers" }, { "title": "apic timer", "url": "/posts/setup-apic-timer/", "categories": "timer, apic-timer", "tags": "apic-timer", "date": "2024-12-20 10:10:00 +0800", "snippet": "setup timerboot cpustart_kernel=&gt; time_init(); =&gt; choose early clocksource in [hpet, pm, pit] to calibrate_tsc ## 以hpet计算tsc 为例 ## 这里主要是因为hpet/pit/pm频率是确定的，但是tsc ## 频率...", "content": "setup timerboot cpustart_kernel=&gt; time_init(); =&gt; choose early clocksource in [hpet, pm, pit] to calibrate_tsc ## 以hpet计算tsc 为例 ## 这里主要是因为hpet/pit/pm频率是确定的，但是tsc ## 频率不确定, 需要用其他的clocksource来计算下 =&gt; cpu_khz = hpet_calibrate_tsc(); =&gt; ((tsc_delta) * 1000000000L) / hpet_delta * =&gt; enable early timer =&gt; setup_irq(0, &amp;irq0) # ifndef CONFIG_SMP =&gt; time_init_gtod()=&gt; rest_init =&gt; kernel_thread(init, NULL, CLONE_FS | CLONE_SIGHAND); =&gt; smp_prepare_cpus =&gt; setup_boot_APIC_clock ## want to use APIC clock =&gt; disable irq =&gt; calibration_result = calibrate_APIC_clock() =&gt; setup_APIC_timer(calibration_result); =&gt; enable irqtime_init_gtod/* * Decide what mode gettimeofday should use. */void time_init_gtod(void){ char *timetype; if (unsynchronized_tsc()) notsc = 1; if (cpu_has(&amp;boot_cpu_data, X86_FEATURE_RDTSCP)) vgetcpu_mode = VGETCPU_RDTSCP; else vgetcpu_mode = VGETCPU_LSL; //有hpet 没有tsc ，优先使用hpet if (vxtime.hpet_address &amp;&amp; notsc) { //timetype -- [timer, clocksource] timetype = hpet_use_timer ? \"HPET\" : \"PIT/HPET\"; //如果使用hpet timer if (hpet_use_timer) //HPET_T0_CMP 表示下一次要到期的cycle //hpet_tick 表示 一次tick所占用的cycle //两者相减代表上次到期的cycle vxtime.last = hpet_readl(HPET_T0_CMP) - hpet_tick; else vxtime.last = hpet_readl(HPET_COUNTER); vxtime.mode = VXTIME_HPET; do_gettimeoffset = do_gettimeoffset_hpet;#ifdef CONFIG_X86_PM_TIMER /* Using PM for gettimeofday is quite slow, but we have no other choice because the TSC is too unreliable on some systems. */ } else if (pmtmr_ioport &amp;&amp; !vxtime.hpet_address &amp;&amp; notsc) { timetype = \"PM\"; do_gettimeoffset = do_gettimeoffset_pm; vxtime.mode = VXTIME_PMTMR; sysctl_vsyscall = 0; printk(KERN_INFO \"Disabling vsyscall due to use of PM timer\\n\");#endif } else { timetype = hpet_use_timer ? \"HPET/TSC\" : \"PIT/TSC\"; vxtime.mode = VXTIME_TSC; } printk(KERN_INFO \"time.c: Using %ld.%06ld MHz WALL %s GTOD %s timer.\\n\", vxtime_hz / 1000000, vxtime_hz % 1000000, timename, timetype); printk(KERN_INFO \"time.c: Detected %d.%03d MHz processor.\\n\", cpu_khz / 1000, cpu_khz % 1000); vxtime.quot = (USEC_PER_SEC &lt;&lt; US_SCALE) / vxtime_hz; vxtime.tsc_quot = (USEC_PER_MSEC &lt;&lt; US_SCALE) / cpu_khz; vxtime.last_tsc = get_cycles_sync(); set_cyc2ns_scale(cpu_khz);}" }, { "title": "sev-snp", "url": "/posts/snp-sev/", "categories": "kvm, sev-snp", "tags": "kvm", "date": "2024-12-17 22:27:00 +0800", "snippet": "调用栈snp_set_memory_privateset_memory_encrypted=&gt; __set_memory_enc_dec(addr, numpages, true) =&gt; __set_memory_enc_pgtable =&gt; cpa_flush() =&gt; x86_platform.guest.enc_status_change...", "content": "调用栈snp_set_memory_privateset_memory_encrypted=&gt; __set_memory_enc_dec(addr, numpages, true) =&gt; __set_memory_enc_pgtable =&gt; cpa_flush() =&gt; x86_platform.guest.enc_status_change_prepare (amd_enc_status_change_prepare) =&gt; if (!enc) =&gt; snp_set_memory_shared =&gt; set_pages_state =&gt; __set_pages_state ## 修改页表c-bit =&gt; ret = __change_page_attr_set_clr(&amp;cpa, 1); =&gt; cpa_flush(&amp;cpa, 0); =&gt; x86_platform.guest.enc_status_change_finish(addr, numpages, enc); =&gt; if (enc) =&gt; snp_set_memory_private(vaddr, npages); =&gt; set_pages_state(vaddr, npages, SNP_PAGE_STATE_PRIVATE) =&gt; enc_dec_hypercall(vaddr, npages &lt;&lt; PAGE_SHIFT, enc)set_memory_decryptedset_memory_decrypted =&gt; __set_memory_enc_dec(addr, numpages, false)__set_pages_state=&gt; if op == SNP_PAGE_STATE_SHARED: ## set rmp validate_pages =&gt; 0 ## 这个地方必须重新做验证。否则这个page可以会被host利用。 =&gt; pvalidate_pages() ## 只考虑sev_cfg.ghcbs_inititalized 情况=&gt; ghcb = __sev_get_ghcb(&amp;state);=&gt; vmgexit_psc(ghcb, data) ## data: desc =&gt; memcpy(ghcb-&gt;shared_buffer, desc, min_t(int, GHCB_SHARED_BUF_SIZE, sizeof(*desc))); =&gt; foreach_entry =&gt; sev_es_ghcb_hv_call(ghcb, &amp;ctxt, SVM_VMGEXIT_PSC, 0, 0) ## 初始化exit_code 和 exit_info =&gt; ghcb_set_sw_exit_code(ghcb, exit_code) =&gt; ghcb_set_sw_exit_info_1(ghcb, exit_info_1) =&gt; ghcb_set_sw_exit_info_2(ghcb, exit_info_2); ## 设置ghcb pa 到msr =&gt; sev_es_wr_ghcb_msr(__pa(ghcb)) =&gt; native_wrmsr(MSR_AMD64_SEV_ES_GHCB, low, high); =&gt; VMGEXIT() ### ???? 实际上是VMGEXIT指令 rep + vmmcall 编码 =&gt; asm volatile(\"rep; vmmcall\\n\\r\")=&gt; __set_put_ghcb(&amp;state)=&gt; if op == SNP_PAGE_STATE_PRIVATE: ## 如果是要变为private， 需要对新映射的page，重新做验证 =&gt; pvalidate_pages()enc_dec_hypercallenc_dec_hypercall=&gt; foreach pfn (may be huge) =&gt; notify_page_enc_status_changed =&gt; kvm_sev_hc_page_enc_status =&gt; kvm_sev_hypercall3(KVM_HC_MAP_GPA_RANGE, pfn &lt;&lt; PAGE_SHIFT, npages, KVM_MAP_GPA_RANGE_ENC_STAT(enc) | KVM_MAP_GPA_RANGE_PAGE_SZ_4K) =&gt; vmmcallsev_handle_vmgexit(kvm)sev_handle_vmgexit=&gt; case SVM_VMGEXIT_PSC: =&gt; setup_vmgexit_scratch() =&gt; snp_begin_psc() =&gt; switch(entry_state.operation) =&gt; case VMGEXIT_PSC_OP_PRIVATE, VMGEXIT_PSC_OP_SHARED =&gt; vcpu-&gt;run-&gt;exit_reason = KVM_EXIT_HYPERCALL; =&gt; vcpu-&gt;run-&gt;hypercall.nr = KVM_HC_MAP_GPA_RANGE;csvreserve memoryearly_csv_reserve_mem csv_cma_reserve_mem |-&gt; csv_smr = memblock_alloc_node(nr_node_ids) |-&gt; for_each_node (node) |-&gt; size = csv_early_percent_memory_on_node(node) ## 每个numanode 一个 array |-&gt; struct cma_array *array; ## 然后整个 numa reserve 空间, 通过 1&lt;&lt;CSV_CMA_SHIFT 划分 ## (PUD_SHIFT)分割，方便csv预留 |-&gt; count = DIV_ROUND_UP(size, 1 &lt;&lt; CSV_CMA_SHIFT); |-&gt; cma_array_size = count * sizeof(*csv_cma) + sizeof(*array); |-&gt; array = memblock_alloc_node(cma_array_size, SMP_CACHE_BYTES, NUMA_NO_NODE); |-&gt; csv_contiguous_pernuma_area[node] = array; ## 调用cma相关接口预留内存 |-&gt; for(i = 0; i &lt; count; i++) |-&gt; csv_cma = &amp;array-&gt;csv_cma[i]; |-&gt; ret = cma_declare_contiguous_nid(0, CSV_CMA_SIZE, 0, 1 &lt;&lt; CSV_MR_ALIGN_BITS, PMD_SHIFT - PAGE_SHIFT, false, name, &amp;(csv_cma-&gt;cma), node); ## 比较每个csv_cma, 找到其最低的地址和最高的地址 -- [start, ## end] ## 并且记录最大的间隙 |-&gt; spanned_size = end - start; |-&gt; if (spanned_size &gt; max_spanned_size) -- max_spanned_size = spanned_size; ## 将start end 赋值到 csv_smr |-&gt; csv_smr[node].start = start |-&gt; csv_smr[node].start = end ## 设置 smr_entry_shift, 这里说明 smr 数量为 NUM_SMR_ENTRIES 8192 个 |-&gt; csv_set_smr_entry_shift(ilog2(max_spanned_size / NUM_SMR_ENTRIES - 1) + 1); |-&gt; smr_entry_shift = max_t(unsigned int, shift, MIN_SMR_ENTRY_SHIFT);declare SMR, SMCR 猜测全称 secure Secure Memory Region, Secure Memory Control Register```shmodule_init(sp_mod_init) sp_mod_init psp_pci_init sev_pci_init |-&gt; if (is_vendor_hygon() &amp;&amp; boot_cpu_has(X86_FEATURE_CSV3)) csv_platform_cmd_set_secure_memory_region(sev, &amp;error);csv_platform_cmd_set_secure_memory_region将csv_smr中的所有的region，通过 CSV3_CMD_SET_SMR 传递给 csv fw|-&gt; hygon_psp_hooks.sev_do_cmd(CSV3_CMD_SET_SMR, csv_smr)从 cma 中分配 1 « CSV_MR_ALIGN_BITS(28) 内存 ，256M##这里，如果是按照 AMDSEV的 RMP 设计, 假设一个RMP page 可以容纳256 entry那一共有##entry_num = (256 * 1024 / 4) * 256mem_size = entry_num * 4 / 1024 /1024 = 64G##也就是覆盖64G内存, 内存覆盖率远远不够##所以其单位就不是page_size， 而是 1 « smr_entry_shift##有 NUM_SMR_ENTRIES (8192) 区域。假设1T内存每个区域有 1 * 1024 * 1024 (M) / 8192 = 128M 内存##每个区域有 32K(8 page) SMCR256M / 8192 = 256 * 1024 (k) / 8192 = 32K|-&gt; cmd_set_smcr-&gt;base_address = csv_alloc_from_contiguous(1« CSV_MR_ALIGN_BITS)### csv3_set_guest_private_memory```shcsv3_set_guest_private_memory## 查看guest 内存所在的numanode|-&gt; for_each_set_bit(i, &amp;csv-&gt;nodemask, BITS_PER_LONG) node_set(i, nodemask)## 计算分配内存大小，注意，这里包含了页表大小## 引用这段代码注释:#### NPT secure memory size## ## PTEs_entries = nr_pages## PDEs_entries = nr_pages / 512## PDPEs_entries = nr_pages / (512 * 512)## PML4Es_entries = nr_pages / (512 * 512 * 512)## ## Totals_entries = nr_pages + nr_pages / 512 + nr_pages / (512 * 512) +## nr_pages / (512 * 512 * 512) &lt;= nr_pages + nr_pages / 256## ## Total_NPT_size = (Totals_entries / 512) * PAGE_SIZE = ((nr_pages +## nr_pages / 256) / 512) * PAGE_SIZE = nr_pages * 8 + nr_pages / 32## &lt;= nr_pages * 9#### 这里分配时，也是按照 smr_entry_shift 进行分配|-&gt; size = ALIGN((nr_pages &lt;&lt; PAGE_SHIFT), 1UL &lt;&lt; smr_entry_shift) + ALIGN(nr_pages * 9, 1UL &lt;&lt; smr_entry_shift);|-&gt; nr_smr = size &gt;&gt; smr_entry_shift;|-&gt; for(i = 0; i &lt; nr_smr; i++) |-&gt; smr = kzalloc(sizeof(*smr), GFP_KERNEL_ACCOUNT); ## 以 smr_entry_shift 颗粒分配连续内存 |-&gt; smr-&gt;hpa = csv_alloc_from_contiguous((1UL &lt;&lt; smr_entry_shift), nodemask_ptr, get_order(1 &lt;&lt; smr_entry_shift)); ## 记录该内存信息到一个数组 |-&gt; regions[count].size = (1UL &lt;&lt; smr_entry_shift); |-&gt; regions[count].base_address = smr-&gt;hpa; |-&gt; count++ ## 当数组大小超过一个PAGE_SIZE时, 将该区域 调用 SET_GUEST_PRIVATE_MEMORY ## 提交给 csv fw |-&gt; if (count &gt;= (PAGE_SIZE / sizeof(regions[0])) || (remainder == count)) { ## 封装命令 |-&gt; set_guest_private_memory-&gt;nregions = count; |-&gt; set_guest_private_memory-&gt;handle = sev-&gt;handle; |-&gt; set_guest_private_memory-&gt;regions_paddr = __sme_pa(regions); ## 提交命令 |-&gt; ret = hygon_kvm_hooks.sev_issue_cmd(kvm, CSV3_CMD_SET_GUEST_PRIVATE_MEMORY, set_guest_private_memory, &amp;argp-&gt;error);QEMUkvm_cpu_exec|-&gt; case KVM_EXIT_MEMORY_FAULT: |-&gt; kvm_convert_memory参考commit[PATCH 0/4] x86: Cleanup and extend computing computing APIhttps://lore.kernel.org/all/20220222185740.26228-1-kirill.shutemov@linux.intel.com/Add AMD Secure Nested Paging (SEV-SNP) Guest Supporthttps://lore.kernel.org/all/20220307213356.2797205-1-brijesh.singh@amd.com/commit dc3f3d2474b80eaee8be89f4c5eb344f10648f42Author: Brijesh Singh &lt;brijesh.singh@amd.com&gt;Date: Thu Feb 24 10:56:01 2022 -0600 x86/mm: Validate memory when changing the C-bit Add the needed functionality to change pages state from shared to private and vice-versa using the Page State Change VMGEXIT as documented in the GHCB spec." }, { "title": "kvm stats", "url": "/posts/kvm-stats/", "categories": "live_migration, kvm stats", "tags": "kvm-stats", "date": "2024-11-29 21:17:00 +0800", "snippet": "背景我们如果对一个只启动bios的虚拟机做热迁移，发现其实际迁移的数据量不大，如下:qemu 参数:# srcqemu-system-x86_64 -m 16G --nographic --enable-kvm --serial tcp:localhost:6666,server,nowait --monitor stdio# dst: qemu-system-x86_64 --incomi...", "content": "背景我们如果对一个只启动bios的虚拟机做热迁移，发现其实际迁移的数据量不大，如下:qemu 参数:# srcqemu-system-x86_64 -m 16G --nographic --enable-kvm --serial tcp:localhost:6666,server,nowait --monitor stdio# dst: qemu-system-x86_64 --incoming tcp:xxxx:9000 -m 16G --nographic --enable-kvm执行热迁移命令:(qemu) migrate -d tcp:xxxx:9000(qemu) info migrateglobals:store-global-state: ononly-migratable: offsend-configuration: onsend-section-footer: ondecompress-error-check: onclear-bitmap-shift: 18Migration status: completedtotal time: 3064 msdowntime: 10 mssetup: 6 mstransferred ram: 38349 kbytesthroughput: 102.73 mbpsremaining ram: 0 kbytestotal ram: 16794440 kbytesduplicate: 4198330 pagesskipped: 0 pagesnormal: 283 pagesnormal bytes: 1132 kbytesdirty sync count: 3page size: 4 kbytesmultifd bytes: 0 kbytespages-per-second: 1387941precopy ram: 38022 kbytesdowntime ram: 12 kbytes可以发现 transferrd ram : 37M normal : 283 pages duplicate : 4198330 pages其中normal 表示正常发送的页, duplicate 表示一些特殊处理的页，可能多个page只发送一份，或者只发送特征（如zero page，就无需发送实际数据). NOTE TODO: XXX我们在XXXX 中会介绍，在qemu read Page 时，会不会触发 mmu notify to kvm mmu page而VM在启动初期，guest大部分内存都不会访问，所以大部分内存在qemu看起来都是zero page。我们来看下这部分内存qemu是如何处理的。detailsQEMU migration save page热迁移save page 流程大致如下:qemu_savevm_state_iterate=&gt; foreach_savevm_state(se) =&gt; se-&gt;ops-&gt;save_live_iterate() =&gt; ram_find_and_save_block =&gt; get_queued_page() # for post copy # 从ramlist.blocks的bitmap中获取到dirty RAMBlocks =&gt; found = find_dirty_block() =&gt; if found # 将RAMBlocks-&gt;dirty_bitmap =&gt; ram_save_host_pageram_save_host_page =&gt; foreach dirtybitmap bit =&gt; migration_bitmap_clear_dirty ## deley clean log =&gt; migration_clear_memory_region_dirty_bitmap =&gt; memory_region_clear_dirty_bitmap =&gt; foeach memorylistener ## nodify kvm set WP or clear D =&gt; listener-&gt;log_clear() =&gt; test_and_clear_bit(page, rb-&gt;bmap); # rb = RAMBlock =&gt; rs-&gt;migration_dirty_pages-- =&gt; ram_save_target_page ## save page =&gt; control_save_page() ## control pages ## compress .... =&gt; if (save_compress_page()) return 1; ## ------(1)----- =&gt; save_zero_page() =&gt; len += save_zero_page_to_file() =&gt; if (is_zero_range(p, TARGET_PAGE_SIZE)) ## only transport FREW Bytes =&gt; len += HEADER =&gt; return 0; ## !!!! =&gt; if (len) =&gt; ram_counters.duplicate++; =&gt; ram_counters.transferred += len; =&gt; return 1 =&gt; else ## means not zero page =&gt; return -1; =&gt; XBZERLE handler... return =&gt; multifd handler ... return =&gt; ram_save_page() =&gt; save_normal_page() =&gt; save_page_header() =&gt; if async ## XBZLER related =&gt; qemu_put_buffer_async =&gt; else ## ------(2)------ ## !!!!!! transport HOLE page !!!! =&gt; qemu_put_buffer(TARGET_PAGE_SIZE) =&gt; ram_counters.transferred += header_bytes =&gt; ram_counters.transferred += TARGET_PAGE_SIZE; ## !!!! =&gt; ram_counters.normal++;判断zero page的的方法很简单，就是判断该页中的每一个byte是不是0，当然有很多加速的指令，我们下面会介绍。如果是zero page，则会发送很少量的数据（一些header数据），并且自增ram_counters.duplicate, 如果不是zero page(还有一些其他的，暂时不介绍), 则会发送header + 整个的page.那么，我们在热迁移之前想看下这些zero page有多少, 如果zero page很多，并且脏页速率很低，我们可以认为该机器比较适合热迁移，在各个节点资源调度时，可以优先处理这部分机器。我们来看下相关细节How to determine if a page is a zero page上面提到过，判断page是否是zero page的函数为is_zero_range, 大致流程为:is_zero_range()=&gt; buffer_is_zero() =&gt; __builtin_prefetch() =&gt; select_accel_fn() =&gt; if len &gt;= length_to_accel =&gt; return buffer_accel() ## else =&gt; return buffer_zero_intQEMU 会利用SIMD 对该操作做一些优化. 看下相关初始化代码:init_cpuid_cache=&gt; cpuid(1,...)=&gt; init `cache` var base on cpuid =&gt; sse2 --&gt; cache |= CACHE_SSE2 =&gt; avx --&gt; cache |= CACHE_SSE4 =&gt; avx2 --&gt; cache |= CACHE_AVX2 =&gt; avx512f --&gt; cache |= CACHE_AVX512F=&gt; cpuid_cache = cache=&gt; init_accel =&gt; if cache &amp; CACHE_SSE2 buffer_accel = buffer_zero_sse2; length_to_accel = 64; =&gt; if cache &amp; CACHE_SSE4 buffer_accel = buffer_zero_sse4; length_to_accel = 64; =&gt; if cache &amp; CACHE_AVX2 buffer_accel = buffer_zero_avx2; length_to_accel = 128; =&gt; if cache &amp; CACHE_AVX512F buffer_accel = buffer_zero_avx512; length_to_accel = 256;做简单性能测试:程序代码: 测试代码 #include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;immintrin.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/mman.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;#include &lt;time.h&gt;#include &lt;stdint.h&gt;#include &lt;stdbool.h&gt;unsigned long length;#define DATA 0char * my_mmap(void) { // 定义要分配的内存大小 // 使用 mmap 分配内存 length = 1024*1024*1024; length = length * 1; void* addr = mmap(NULL, length, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0); if (addr == MAP_FAILED) { perror(\"mmap failed\"); return NULL; } // 初始化随机数生成器 srand(time(NULL)); // 将内存初始化为随机数据 unsigned char* data = (unsigned char*)addr; for (size_t i = 0; i &lt; length; ++i) { //data[i] = rand() % 256; // 生成 0 到 255 的随机字节 data[i] = DATA; } // 打印前 16 个字节的内容作为示例 printf(\"Random data:\\n\"); for (size_t i = 0; i &lt; 16; ++i) { printf(\"%02x \", data[i]); } printf(\"\\n\"); return addr;}static bool buffer_zero_avx2(const void *buf, size_t len){ /* Begin with an unaligned head of 32 bytes. */ __m256i t = _mm256_loadu_si256(buf); __m256i *p = (__m256i *)(((uintptr_t)buf + 5 * 32) &amp; -32); __m256i *e = (__m256i *)(((uintptr_t)buf + len) &amp; -32); /* Loop over 32-byte aligned blocks of 128. */ while (p &lt;= e) { __builtin_prefetch(p); if (!_mm256_testz_si256(t, t)) { return false; } t = p[-4] | p[-3] | p[-2] | p[-1]; p += 4; } ; /* Finish the last block of 128 unaligned. */ t |= _mm256_loadu_si256(buf + len - 4 * 32); t |= _mm256_loadu_si256(buf + len - 3 * 32); t |= _mm256_loadu_si256(buf + len - 2 * 32); t |= _mm256_loadu_si256(buf + len - 1 * 32); return _mm256_testz_si256(t, t);}char *print_timestamp() { // 获取当前时间 time_t raw_time; struct tm *time_info; static char buffer[80]; // 获取当前时间 time(&amp;raw_time); // 将时间转换为本地时间 time_info = localtime(&amp;raw_time); // 格式化时间为字符串 strftime(buffer, sizeof(buffer), \"%Y-%m-%d %H:%M:%S\", time_info); // 打印时间戳 return buffer;}#define MY_DEB(...) do {printf(\"%s: \", print_timestamp()); printf(__VA_ARGS__);}while(0)static inline uint64_t ldq_he_p(const void *ptr){ uint64_t r; __builtin_memcpy(&amp;r, ptr, sizeof(r)); return r;}static boolbuffer_zero_int(const void *buf, size_t len){ if (len &lt; 8) { /* For a very small buffer, simply accumulate all the bytes. */ const unsigned char *p = buf; const unsigned char *e = buf + len; unsigned char t = 0; do { t |= *p++; } while (p &lt; e); return t == 0; } else { /* Otherwise, use the unaligned memory access functions to handle the beginning and end of the buffer, with a couple of loops handling the middle aligned section. */ uint64_t t = ldq_he_p(buf); const uint64_t *p = (uint64_t *)(((uintptr_t)buf + 8) &amp; -8); const uint64_t *e = (uint64_t *)(((uintptr_t)buf + len) &amp; -8); for (; p + 8 &lt;= e; p += 8) { __builtin_prefetch(p + 8); if (t) { return false; } t = p[0] | p[1] | p[2] | p[3] | p[4] | p[5] | p[6] | p[7]; } while (p &lt; e) { t |= *p++; } t |= ldq_he_p(buf + len - 8); return t == 0; }}int main(){ char *buffer; size_t len = 0; int i = 0; int j = 0; MY_DEB(\"mmap begin \\n\"); buffer = my_mmap(); MY_DEB(\"mmap end \\n\"); MY_DEB(\"random end\\n\"); for (i = 0; i &lt; 30; i++) { if (i % 10 == 0) { MY_DEB(\"i = %d\\n\", i); } for (j = 0; j &lt; length; j=j+4096) { buffer_zero_avx2(buffer + j, 4096); } } MY_DEB(\"buffer zero avx end\\n\"); for (i = 0; i &lt; 30; i++) { if (i % 10 == 0) { MY_DEB(\"i = %d\\n\", i); } for (j = 0; j &lt; length; j=j+4096) { buffer_zero_int(buffer + j, 4096); } } MY_DEB(\"buffer zero int\\n\"); while(1) sleep(10); return 0;} 该部分代码主要copy qemu的 buffer_zero_xxx, 并在Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHzcpu上进行测试. DATA设置为0，性能最差.我们主要以其为基准得到测试结果Random data:00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 002024-12-02 15:34:01: mmap end2024-12-02 15:34:01: random end2024-12-02 15:34:01: i = 0...2024-12-02 15:34:09: i = 902024-12-02 15:34:10: buffer zero avx end2024-12-02 15:34:10: i = 0...2024-12-02 15:34:20: i = 902024-12-02 15:34:22: buffer zero int令人意外的是，两者性能差距并不是很大，大概都是1s处理10G数据。不过无论是哪种方法，性能都不符合当前需求。能不能通过一种粗略的方式，来获取KVM 建立 ept 映射 内存的数量, 来粗略估计zero page数量。内核中提供了kvm stats 相关api，可以完成这个事情。我们先来看下kvm stats 相关历史.kvm statORG KVM PATCHKVM stat 其实在 kvm的第一个patch中就引入了，当时的kvm stats 比较简单，使用全局变量来进行全局的 kvm 事件统计, 并通过debugfs来和用户态交互。 全局变量 struct kvm_stat { u32 pf_fixed; u32 pf_guest; u32 tlb_flush; u32 invlpg; u32 exits; u32 io_exits; u32 mmio_exits; u32 signal_exits; u32 irq_exits;};struct kvm_stat kvm_stat; debugfs 相关 static struct kvm_stats_debugfs_item { const char *name; u32 *data; struct dentry *dentry;} debugfs_entries[] = { { \"pf_fixed\", &amp;kvm_stat.pf_fixed }, { \"pf_guest\", &amp;kvm_stat.pf_guest }, { \"tlb_flush\", &amp;kvm_stat.tlb_flush }, { \"invlpg\", &amp;kvm_stat.invlpg }, { \"exits\", &amp;kvm_stat.exits }, { \"io_exits\", &amp;kvm_stat.io_exits }, { \"mmio_exits\", &amp;kvm_stat.mmio_exits }, { \"signal_exits\", &amp;kvm_stat.signal_exits }, { \"irq_exits\", &amp;kvm_stat.irq_exits }, { 0, 0 }};static __init void kvm_init_debug(void){ struct kvm_stats_debugfs_item *p; debugfs_dir = debugfs_create_dir(\"kvm\", 0); for (p = debugfs_entries; p-&gt;name; ++p) p-&gt;dentry = debugfs_create_u32(p-&gt;name, 0444, debugfs_dir, p-&gt;data);} 以pf_fixed为例，其会在fixed page fault exception，相关处理流程中自增 FNAME(page_fault)=&gt; if write_fault # main use for dirty tracking =&gt; fixed = FNAME(fix_write_pf)=&gt; else # for emulate CR4.WP =&gt; fixed = fix_read_pf()=&gt; if fixed =&gt; ++kvm_stat.pf_fixed; 总结为下图: per-vcpu stat全局变量有个很大的问题，就是cacheline抖动。很主流的做法就是将全局变量”split”,例如percpu var, 让每个vcpu访问自己的变量。avi 的做法则是将kvm_stat, 放到kvm_vcpu结构中，每个vcpu thread访问自己的变量。下面是commit message:Make the exit statistics per-vcpu instead of global. This gives a 3.5%boost when running one virtual machine per core on my two socket dual core(4 cores total) machine.avi发现在numa(2 numa 4 core)架构上大概有3.5%的提升。具体改动: 删除struct kvm_stat kvm_stat全局变量定义，在kvm_vcpu中添加该成员: @@ -298,6 +314,8 @@ struct kvm_vcpu { int sigset_active; sigset_t sigset;+ struct kvm_stat stat;+ struct { int active; u8 save_iopl;...-struct kvm_stat kvm_stat;-EXPORT_SYMBOL_GPL(kvm_stat); 修改kvm_stats_debugfs_item定义，将data修改为offset 记录该stat.xxx在kvm_vcpu变量中的offset```diff+#define STAT_OFFSET(x) offsetof(struct kvm_vcpu, stat.x) static struct kvm_stats_debugfs_item { const char *name; u32 *data; int offset;struct dentry *dentry; } debugfs_entries[] = { { “pf_fixed”, &amp;kvm_stat.pf_fixed },… { “pf_fixed”, STAT_OFFSET(pf_fixed) },``` 修改debugfs file ops, 在read该debugfs file时，需要将per vm &amp;&amp; per vcpu的stat将加 代码细节 修改创建debugfsfile的接口，使用自定义的ops +DEFINE_SIMPLE_ATTRIBUTE(stat_fops, stat_get, stat_set, \"%llu\\n\"); static __init void kvm_init_debug(void) { struct kvm_stats_debugfs_item *p; debugfs_dir = debugfs_create_dir(\"kvm\", NULL); for (p = debugfs_entries; p-&gt;name; ++p)- p-&gt;dentry = debugfs_create_u32(p-&gt;name, 0444, debugfs_dir,- p-&gt;data);+ p-&gt;dentry = debugfs_create_file(p-&gt;name, 0444, debugfs_dir,+ (void *)(long)p-&gt;offset,+ &amp;stat_fops); } 统计时，遍历per vm，然后再遍历per vm的per vcpu, 相加得和 +static u64 stat_get(void *_offset)+{+ unsigned offset = (long)_offset;+ u64 total = 0;+ struct kvm *kvm;+ struct kvm_vcpu *vcpu;+ int i;++ spin_lock(&amp;kvm_lock);+ list_for_each_entry(kvm, &amp;vm_list, vm_list)+ for (i = 0; i &lt; KVM_MAX_VCPUS; ++i) {+ vcpu = &amp;kvm-&gt;vcpus[i];+ total += *(u32 *)((void *)vcpu + offset);+ }+ spin_unlock(&amp;kvm_lock);+ return total;+} extend stat to vcpu stat and vm statavi可能觉得, 有一些事件，是和vcpu无关的，应该以vm为颗粒记录，所以又提供了一个per vm stat的统计颗粒。实现方式和上一个patch类似. 代码细节 将之前stat相关符号，修改为vcpu_stat struct kvm_stat -&gt; struct kvm_vcpu_stat stat_fops-&gt;vcpu_stat_fops stat_get-&gt;vcpu_stat_get STAT_OFFSET-&gt;VCPU_STAT 在struct kvm中增加kvm_vm_stat数据结构 在kvm_stat_debugfs_item中增加 kind成员，用来标识，该item是per-vm，还是per-vcpu +enum kvm_stat_kind {+ KVM_STAT_VM,+ KVM_STAT_VCPU,+};+ struct kvm_stats_debugfs_item { const char *name; int offset;+ enum kvm_stat_kind kind; struct dentry *dentry; }; 增加KVM_STAT_VM4的相关fops +static u64 vm_stat_get(void *_offset)+{+ unsigned offset = (long)_offset;+ u64 total = 0;+ struct kvm *kvm;++ spin_lock(&amp;kvm_lock);+ list_for_each_entry(kvm, &amp;vm_list, vm_list)+ total += *(u32 *)((void *)kvm + offset);+ spin_unlock(&amp;kvm_lock);+ return total;+}++DEFINE_SIMPLE_ATTRIBUTE(vm_stat_fops, vm_stat_get, NULL, \"%llu\\n\"); 在创建debugfs file时，需要根据kvm_stat_kind, 来选择相应的ops: +static struct file_operations *stat_fops[] = {+ [KVM_STAT_VCPU] = &amp;vcpu_stat_fops,+ [KVM_STAT_VM] = &amp;vm_stat_fops,+}; statvoid kvm_init_debug(void) {@@ -1310,7 +1330,7 @@ static void kvm_init_debug(void) for (p = debugfs_entries; p-&gt;name; ++p) p-&gt;dentry = debugfs_create_file(p-&gt;name, 0444, debugfs_dir, (void *)(long)p-&gt;offset,- &amp;stat_fops);+ stat_fops[p-&gt;kind]); } 然后，在随后的patch中, 增加了mmu stats struct kvm_vm_stat {+\tu32 mmu_shadow_zapped;+\tu32 mmu_pte_write;+\tu32 mmu_pte_updated;+\tu32 mmu_pde_zapped;+\tu32 mmu_flooded;+\tu32 mmu_recycled; };@@ -66,6 +66,12 @@ struct kvm_stats_debugfs_item debugfs_entries[] = { \t{ \"fpu_reload\", VCPU_STAT(fpu_reload) }, \t{ \"insn_emulation\", VCPU_STAT(insn_emulation) }, \t{ \"insn_emulation_fail\", VCPU_STAT(insn_emulation_fail) },+\t{ \"mmu_shadow_zapped\", VM_STAT(mmu_shadow_zapped) },+\t{ \"mmu_pte_write\", VM_STAT(mmu_pte_write) },+\t{ \"mmu_pte_updated\", VM_STAT(mmu_pte_updatsed) },+\t{ \"mmu_pde_zapped\", VM_STAT(mmu_pde_zapped) },+\t{ \"mmu_flooded\", VM_STAT(mmu_flooded) },+\t{ \"mmu_recycled\", VM_STAT(mmu_recycled) }, \t{ NULL } };per vm kvm stat user api上面的改动，虽然减小了统计的颗粒(per vm, per vcpu)，但是展示给用户空间的api还是以全局的颗粒度。Janosch Frank 做了一些工作允许userspace通过debugfs获取每个vm的stat，同时保证了兼容性，也允许全局的统计。其具体做法, 为每一个vm在kvm下，增加一个目录，然后，里面创建和原来相同的文件，只不过访问其中的文件，只会获取当前vm的stat， 相关代码不再展开，简单画图表示:其中，为每个vm创建debugfs dir，然后在每个vm 的debugfs dir, 创建和全局的相同文件。访问该文件时，需要对其kvm以及stat entry进行选定，该流程通过debugfs_stat_data[]实现，该数据结构中主要定义两个成员 kvm: 找到其kvm offset: 用来定义get所对应的数据在 kvm_vcpu / kvm 中的offset那有的同学可能就会问了，由于debugfs_stat_data中只存了kvm，没有存kvm_vcpu, 所以没有办法display per-vcpu stat。确实是这样。所以这个patch只是实现了per vm stat display, 对于像pf_fixed, pf_guest这样的per cpustat, 目前的做法时，将所有vcpu的相加，作为整个vm 的stat display.per vcpu kvm stat user apiLuiz 为了支持trace-cmd guest host merge, 需要将tsc-offset, tsc-scaling-ratio和tsc-scaling-ratio-frac-bits等信息提供给用户态，这些信息并非事件统计，而是一个属性值, 而某些事件又是vcpu颗粒的，例如tsc-offset, tsc-scaling-ratio,这些信息必须以vcpu为颗粒统计.所以, Luiz在per-vm的dir下，又创建了vcpux debugfs dir，并在里面创建tsc相关的debugfsfile。新增文件路径如下:/sys/kernel/debug/kvm/66828-10/vcpu0/tsc-offset/sys/kernel/debug/kvm/66828-10/vcpu0/tsc-scaling-ratio/sys/kernel/debug/kvm/66828-10/vcpu0/tsc-scaling-ratio-frac-bits 代码较简单, 不再展示kvm stat basefd(略, 之后再补充)参考链接 KVM: Per-vcpu statistics https://lore.kernel.org/all/1198998638-22713-8-git-send-email-avi@qumranet.com/ commit: 1165f5fec18c077bdba88e7125fd41f8e3617cb4 Author: Avi Kivity avi@qumranet.com Date: 2007-04-19 后续patch: https://lore.kernel.org/all/1198998638-22713-9-git-send-email-avi@qumranet.com/ KVM: Extend stats support for VM stats commit: ba1389b7a04de07e6231693b7ebb34f5b5d1a3e6 Author: Avi Kivity Date: 2007-11-18 KVM: Create debugfs dir and stat files for each VM [MAIL: KVM: Create debugfs dir and stat files for each VM][MAIL_PER_VM_STAT] commit: 536a6f88c49dd739961ffd53774775afed852c83 Author: Janosch Frank frankja@linux.vnet.ibm.com Date: 2016-05-18 kvm: x86: export TSC information to user-space kvm: create per-vcpu dirs in debugfs KVM stat basedfd mail QEMU kvm stat based-fd mailnotecommit ba1389b7a04de07e6231693b7ebb34f5b5d1a3e6Author: Avi Kivity &lt;avi@qumranet.com&gt;Date: Sun Nov 18 16:24:12 2007 +0200 KVM: Extend stats support for VM stats This is in addition to the current virtual cpu statistics." }, { "title": "dirty-rate calc", "url": "/posts/dirty-rate/", "categories": "live_migration, dirty_rate", "tags": "dirty rate", "date": "2024-11-25 11:00:00 +0800", "snippet": "calc-dirty-rate 整体流程qmp_calc_dirty_rate =&gt; qemu_thread_create(,MIGRATION_THREAD_DIRTY_RATE, get_dirtyrate_thread,,) (get_dirtyrate_thread) =&gt; dirtyrate_set_state(,,DIRTY_RATE_STATUS_...", "content": "calc-dirty-rate 整体流程qmp_calc_dirty_rate =&gt; qemu_thread_create(,MIGRATION_THREAD_DIRTY_RATE, get_dirtyrate_thread,,) (get_dirtyrate_thread) =&gt; dirtyrate_set_state(,,DIRTY_RATE_STATUS_MEASURING) =&gt; calculate_dirtyrate() =&gt; switch(mode) =&gt; DIRTY_RATE_MEASURE_MODE_DIRTY_BITMAP =&gt; calculate_dirtyrate_dirty_bitmap =&gt; DIRTY_RATE_MEASURE_MODE_DIRTY_RING =&gt; calculate_dirtyrate_dirty_ring =&gt; other: =&gt; calculate_dirtyrate_sample_vm =&gt; dirtyrate_set_state(,,DIRTY_RATE_STATUS_MEASURED)dirty_bitmapcalculate_dirtyrate_dirty_bitmap## may execute log_start(), may be not=&gt; memory_global_dirty_log_start(GLOBAL_DIRTY_DIRTY_RATE, )=&gt; record_dirtypages_bitmap(&amp;dirty_pages ,true) =&gt; dirty_pages-&gt;start_pages = total_dirty_pages;## get start end=&gt; start_time = get_clock_get_ms(QEMU_CLOCK_REALTIME)=&gt; global_dirty_log_sync(GLOBAL_DIRTY_DIRTY_RATE, true);## WAIT... unit calc_time_ms + start_time expired=&gt; DirtyStat.calc_time_ms = dirty_stat_wait(config.calc_time_ms, start_time);## record timeend dirtypages=&gt; record_dirtypages_bitmap(&amp;dirty_pages, false); =&gt; dirty_pages-&gt;end_pages = total_dirty_pages;=&gt; do_calculate_dirtyrate =&gt; (end_pages-start_pages) * 1000 / calc_time_msdirty_ringcalculate_dirtyrate_dirty_ring=&gt; global_dirty_log_change(GLOBAL_DIRTY_DIRTY_RATE, true);=&gt; start_time = qemu_clock_get_ms(QEMU_CLOCK_HOST) / 1000;=&gt; vcpu_calculate_dirtyrate =&gt; vcpu_dirty_stat_collect(records, true); =&gt; foreach VCPU =&gt; record_dirtypages(,true) =&gt; dirty_pages[cpu-&gt;cpu_index].start_pages = cpu-&gt;dirty_pages ## WAIT... unit calc_time_ms + start_time expired =&gt; duration = dirty_stat_wait(calc_time_ms, init_time_ms); =&gt; global_dirty_log_sync(flag, one_shot); =&gt; vcpu_dirty_stat_collect =&gt; foreach VCPU =&gt; record_dirtypages(, false) =&gt; dirty_pages[cpu-&gt;cpu_index].start_pages = cpu-&gt;dirty_pages =&gt; foreach VCPU ## 计算每一个vcpu的脏页速率 =&gt; do_calculate_dirtyrate对于dirty_bitmap和dirty_ring两者统计dirty page number依据不一样: dirty_bitmap: total_dirty_pages dirty_ring: cpu-&gt;dirty_pagesquery_migrate.ram.dirty-pages-rateqmp_query_migrate =&gt; fill_source_migration_info =&gt; case MIGRATION_STATUS_ACTIVE =&gt; populate_time_info =&gt; populate_ram_info ... =&gt; info-&gt;ram-&gt;dirty_pages_rate = stat64_get(&amp;mig_stats.dirty_pages_rate); ... =&gt; migration_populate_vfio_info依据mig_stats.dirty_pages_rate我们来看几个流程:sync logglobal_dirty_log_sync =&gt; memory_global_dirty_log_sync =&gt; memory_region_sync_dirty_bitmap =&gt; foreach memorylistener =&gt; if listener-&gt;log_sync =&gt; view = address_space_get_flatview() =&gt; foreach flat range ## foreach mr =&gt; listener-&gt;log_sync() ## dirty bitmap (kvm_log_sync) =&gt; if listener-&gt;log_sync_global =&gt; listener-&gt;log_sync_global() ## dirty ring (kvm_log_sync_global) =&gt; if one_shot =&gt; memory_global_dirty_log_stop(flag)kvm_log_sync:kvm_log_sync=&gt; kvm_physical_sync_dirty_bitmap =&gt; foreach KVMSlot =&gt; kvm_slot_get_dirty_log() =&gt; kvm_vm_ioctl(s, KVM_GET_DIRTY_LOG, &amp;d); =&gt; kvm_slot_sync_dirty_pages() =&gt; cpu_physical_memory_set_dirty_lebitmap()kvm_log_sync_global:kvm_log_sync_global=&gt; kvm_dirty_ring_flush() =&gt; kvm_cpu_synchronize_kick_all =&gt; kvm_dirty_ring_reap =&gt; kvm_dirty_ring_reap_locked =&gt; if (cpu) =&gt; total = kvm_dirty_ring_reap_one(s, cpu); =&gt; foreach dirty_gfns[] =&gt; kvm_dirty_ring_mark_page =&gt; dirty_gfn_set_collected(cur) ## !!!!!! 更新dirty_pages =&gt; cpu-&gt;dirty_pages += count =&gt; else foreach_cpu =&gt; total += kvm_dirty_ring_reap_one(s, cpu); =&gt; ret = kvm_vm_ioctl(s, KVM_RESET_DIRTY_RINGS);=&gt; foreach KVMSlot =&gt; kvm_slot_sync_dirty_pages =&gt; cpu_physical_memory_set_dirty_lebitmap ## 这个指使用了dirty_ring_with_bitmap, 并且是last stage, ## 这种情况就说明，dirty_bitmap涉及的memory 并不多，不需要iter =&gt; if (s-&gt;kvm_dirty_ring_with_bitmap &amp;&amp; last_stage &amp;&amp; kvm_slot_get_dirty_log(s, mem)) =&gt; kvm_slot_sync_dirty_pages(mem); =&gt; kvm_slot_reset_dirty_pages两者foreach KVMSlot的方式不同，但是个人感觉，应该最终结果是一样的.并且最终都调用到了cpu_physical_memory_set_dirty_lebitmap.kvm_log_sync和kvm_log_sync_global不同的是，kvm_log_sync需要调用get_dirty_log来获取当前的dirty_bitmap, 而dirty_ring则调用kvm_dirty_ring_reap_locked来完成”sync”动作，并更新cpu-&gt;dirty_pages" }, { "title": "guestperf", "url": "/posts/guestperf/", "categories": "live_migration, test", "tags": "guestperf", "date": "2024-11-20 15:27:00 +0800", "snippet": "guestperf代码分析参考链接 https://blog.csdn.net/huang987246510/article/details/114379675?spm=1001.2014.3001.5501", "content": "guestperf代码分析参考链接 https://blog.csdn.net/huang987246510/article/details/114379675?spm=1001.2014.3001.5501" }, { "title": "kernel PML", "url": "/posts/kernel-pml/", "categories": "live_migration, kernel pml", "tags": "kernel_pml", "date": "2024-11-19 09:50:00 +0800", "snippet": "PML &amp;&amp; WPPML 和 WP 起到的作用是一样的，只不过PML可以达到一种batch WP的效果:PML index始终指向 next PML entry, 每次record PML时，PML index 会dec, 并且check --PML_index的值, 是否在[0, 511]范围之内. 当PML index=0时，如果此时再record PML, 则会触发-...", "content": "PML &amp;&amp; WPPML 和 WP 起到的作用是一样的，只不过PML可以达到一种batch WP的效果:PML index始终指向 next PML entry, 每次record PML时，PML index 会dec, 并且check --PML_index的值, 是否在[0, 511]范围之内. 当PML index=0时，如果此时再record PML, 则会触发--pml_index=0xffff, PML index 则不在 [0, 511], 此时会broken当前的write 操作，并VM-exit。PML table大小为4096, 每一个entry 大小为8-byte, 保存着 dirty page的PFN.其中，红色绿色，两次write 操作在执行时，不会触发VM-exit，而是在write操作过程中，如果需要dirty EPT entry, 则在PML中新增一个由 PML index指向的entry另外，如果该指向该page 的 EPTE 已经mask dirty, 如黑色wrte，则本次写操作，不会记录PML中。我们再来横向比较下 WP 和 PML: 比较项 WP PML How To Catch Write operation in VM clear EPTE w bit clear EPTE dirty bit How To non-Cache Write in VM set EPTE w bit set EPTE dirty bit When VM-exit write WP page PML buffer FULL The maximum number of dirtypages that can be captured between a VM-entry and a VM-exit 1 512 从上面对比图来看，PML 和WP很像 WP关注W flag, 而PML关注D flag, 两者是只关注EPTE中的一个flag write clean page to dirty 在这两种用法中都可能会造成额外的VM-exit用来catch该write operation。只不过 WP: 只能catch one PML: **catch 512** 所以上面的相同不同之处，就导致PML可以比较完美的嵌入到当前的WP dirty log框架中，并且由于其batch catch的特性，会提升系统性能(见mail list),大致有4%～5%的性能提升)。接下来主要来分析下patch:patch 分析首先一部分patch 是为了解决上面提到的WP和PMLtrack dirty page的方式差异(一个是关注D flag, 一个是关注W flag)。Deference of Track Dirty Page(WP vs PML) modify kvm_arch_mmu_write_protect_pt_masked-&gt;kvm_arch_mmu_enable_log_dirty_pt_masked 目前针对spte的clear dirty log 有两种方式, clear W flag(WP) or clear D flag(PML), 所以命名也应该微调下（不能再以wp的命名方式命名, 应该更通用写） @@ -1059,7 +1059,7 @@ int kvm_get_dirty_log_protect(struct kvm *kvm, \t\tdirty_bitmap_buffer[i] = mask; \t\toffset = i * BITS_PER_LONG;-\t\tkvm_arch_mmu_write_protect_pt_masked(kvm, memslot, offset,+\t\tkvm_arch_mmu_enable_log_dirty_pt_masked(kvm, memslot, offset, \t\t\t\t\t\t\t\tmask); \t} from patch1 avoid unnecessary PML record avoid unnecessary PML record details 在没有引入PML之前，shadow pgtable中的dirty bit is too “chicken rib”(鸡肋), 或者说毫无用途。但是引入了PML之后，dirty bit 就类似与W bit，用来”通知 VMX non-root operation”要不要去track该page, 所以我们需要在不需要track this page 的流程中，mark EPTE’s D flag. eg: @@ -2597,8 +2597,14 @@ static int set_spte(struct kvm_vcpu *vcpu, u64 *sptep, \t\t} \t} -\tif (pte_access &amp; ACC_WRITE_MASK)+\tif (pte_access &amp; ACC_WRITE_MASK) { \t\tmark_page_dirty(vcpu-&gt;kvm, gfn);+\t\t/*+\t\t * Explicitly set dirty bit. It is used to eliminate unnecessary+\t\t * dirty GPA logging in case of PML is enabled on VMX.+\t\t */+\t\tspte |= shadow_dirty_mask;+\t} 因为已经mark_page_dirty(), 所以没有必要在track this dirty-ed page 另外还有一个地方，就是在fast pf fix路径，但是作者认为该路径在开启PML之后，一般不会走到: @@ -2914,6 +2920,16 @@ fast_pf_fix_direct_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, \t */ \tgfn = kvm_mmu_page_get_gfn(sp, sptep - sp-&gt;spt); +\t/*+\t * Theoretically we could also set dirty bit (and flush TLB) here in+\t * order to eliminate the unnecessary PML logging. See comments in+\t * set_spte. But as in case of PML, fast_page_fault is very unlikely to+\t * happen so we leave it unchanged. This might result in the same GPA+\t * to be logged in PML buffer again when the write really happens, and+\t * eventually to be called by mark_page_dirty twice. But it's also no+\t * harm. This also avoids the TLB flush needed after setting dirty bit+\t * so non-PML cases won't be impacted. 理论上，我们也可以在这里设置脏位（并刷新TLB），以消除不必要的PML日志记录。 请参阅set_spte中的注释。但是，由于在PML的情况下，fast_page_fault发生的可 能性非常小，因此我们将其保持不变。这可能会导致在实际写入时，同一GPA被再次 记录在PML缓冲区中，最后可能会被mark_page_dirty两次调用。但这也没有什么害处。 这样做还可以避免在设置脏位后需要的TLB刷新，从而不会影响非PML情况。+\t */ \tif (cmpxchg64(sptep, spte, spte | PT_WRITABLE_MASK) == spte) \t\tmark_page_dirty(vcpu-&gt;kvm, gfn); 这里是作者懒么，显然不是，作者很勤奋的写了一大段comment来说明为什么不去做: 可能性很小，另外，如果在这里增加set dirty bit代码，并且流程真的走到了该分支，那就需要 TLB flush, 这个代价太大了，并且会影响非PML的情景。 那第一个场景不是也类似于该场景，需要flush tlb么？但是 set_spte触发频率高很多 set_spte很大概率会modify spte，大概率会走到flush tlb，所以，综合来看，增加的开销不大。 from patch3 更改原有set_memory_region的一些逻辑, 大概流程为: set_memory_region change details kvm_vm_ioctl=&gt; kvm_vm_ioctl_set_memory_region =&gt; kvm_set_memory_region =&gt; __kvm_set_memory_region =&gt; kvm_arch_commit_memory_region =&gt; if (change != KVM_MR_DELETE) kvm_mmu_slot_apply_flags(kvm, new)kvm_mmu_slot_apply_flags=&gt; if new flag &amp; KVM_MEM_READONLY ## 仍然需要 WP =&gt; kvm_mmu_slot_remove_write_access return## 开启DIRTY log=&gt; new-&gt;flags &amp; KVM_MEM_LOG_DIRTY_PAGES =&gt; if (kvm_x86_ops-&gt;slot_enable_log_dirty) =&gt; kvm_x86_ops-&gt;slot_enable_log_dirty(kvm, new); ## vmx_slot_enable_log_dirty ## handle pages of normal page size =&gt; kvm_mmu_slot_leaf_clear_dirty # 只查找 page table level =&gt; foreach rmmap in memslot-&gt;arch.rmap[PT_PAGE_TABLE_LEVEL - 1] # clear dirty means，all page table level in slot will record to PML =&gt; flush |= __rmap_clear_dirty(kvm, rmapp) =&gt; if flush =&gt; kvm_flush_remote_tlbs ## handle pages of large page ## WHY? ===(1)=== =&gt; kvm_mmu_slot_largepage_remove_write_access ## level &gt; PT_PAGE_TABLE_LEVEL =&gt; foreach every level i (&gt; PT_PAGE_TABLE_LEVEL) =&gt; foreach rmmap in memslot-&gt;arch.rmap[i - PT_PAGE_TABLE_LEVEL] =&gt; flush |= __rmap_write_protect(kvm, rmapp, false) =&gt; if flush =&gt; kvm_flush_remote_tlbs(kvm) =&gt; else =&gt; kvm_mmu_slot_remove_write_access(kvm, new) ## foreach level, &gt;= PT_PAGE_TABLE_LEVEL =&gt; foreach every level i =&gt; foeach rmapp in memslot-&gt;arch.rmap[i - PT_PAGE_TABLE_LEVEL] =&gt; flush |= __rmap_write_protect(kvm, rmapp, false) =&gt; if flush =&gt; kvm_flush_remote_tlbs(kvm)## 关闭DIRTY log=&gt; else =&gt; if (kvm_x86_ops-&gt;slot_disable_log_dirty) =&gt; kvm_x86_ops-&gt;slot_disable_log_dirty(kvm, new) ## vmx_slot_disable_log_dirty ## ===(2)=== =&gt; kvm_mmu_slot_set_dirty =&gt; foreach every level i =&gt; foreach rmapp in memslot-&gt;arch.rmap[i - PT_PAGE_TABLE_LEVEL] =&gt; flush |= __rmap_set_dirty(kvm, rmapp); =&gt; if flush =&gt; kvm_flush_remote_tlbs(kvm) 我们需要关注两个问题(也是两个和WP不同的点) 当遇到huge page 时，不能简单的clear dirty bit. 如果EPT-PMD如果映射了大页，对一个hugepage进行跨”PAGE_SIZE“访问，则会导致PML中只记录一个write操作. 并且和WP不同的是，由于PML是类似于”auto batch WP”, 这两次write操作，无法分别进行catch，(但是WP可以), 所以如果我们想mark_page_dirty(one normal pagesize page), 就不能让这两次write操作连续执行下去，所以，这里对大页进行了WP. 当然，进行了WP之后，还有一些其他的操作，加速对大页中小页的catch。不放在本文讨论。(之后在分析) ++++++++++++++++++++++++遗留问题++++++++++++++++ 如果是PML, 需要在disable log dirty时，mark all spte dirty. 目的是，避免vcpu再次record PML。 flush PML to dirtymap另外一个，主要的改动是, flush PML buffer to dirty_bitmap.主要分为. 主动flush 主动flush细节 是指vcpu运行时，在每次vm-exit时，主动flush pml buffer @@ -7335,6 +7474,16 @@ static int vmx_handle_exit(struct kvm_vcpu *vcpu) \tu32 exit_reason = vmx-&gt;exit_reason; \tu32 vectoring_info = vmx-&gt;idt_vectoring_info; +\t/*+\t * Flush logged GPAs PML buffer, this will make dirty_bitmap more+\t * updated. Another good is, in kvm_vm_ioctl_get_dirty_log, before+\t * querying dirty_bitmap, we only need to kick all vcpus out of guest+\t * mode as if vcpus is in root mode, the PML buffer must has been+\t * flushed already. 这个代码段的目的是刷新保存了修改过的客体物理地址（GPAs）的PML（ Page Modification Log）缓冲区。这样做的好处是可以使得dirty_bitmap （脏位图）more upgated，因为PML缓冲区中的信息会被同步到dirty_bitmap中。+\t */+\tif (enable_pml)+\t\tvmx_flush_pml_buffer(vmx);+ 这里相当于积极的flush PML， 这样做的好处是，能更大力度保证 dirty_bitmap的真实性。并且简化了代码，无论是不是EXIT_REASON_PML_FULL event，都会在这里flush。 那么我们在看下EXIT_REASON_PML_FULLcallbak 还需不需要额外处理: static int handle_pml_full(struct kvm_vcpu *vcpu){ unsigned long exit_qualification; trace_kvm_pml_full(vcpu-&gt;vcpu_id); exit_qualification = vmcs_readl(EXIT_QUALIFICATION); //==(1)== /* * PML buffer FULL happened while executing iret from NMI, * \"blocked by NMI\" bit has to be set before next VM entry. */ if (!(to_vmx(vcpu)-&gt;idt_vectoring_info &amp; VECTORING_INFO_VALID_MASK) &amp;&amp; cpu_has_virtual_nmis() &amp;&amp; (exit_qualification &amp; INTR_INFO_UNBLOCK_NMI)) vmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI); //==(2)== /* * PML buffer already flushed at beginning of VMEXIT. Nothing to do * here.., and there's no userspace involvement needed for PML. */ return 1;} 不是简单return。但是是为了处理 block nmi state, 当正在执行 iret NMI时，因为 PML full event, 触发了vm-exit, 此时，VM 已经是 INTR_INFO_UNBLOCK_NMI状态。但是处理 PML full event应该是透明的，guest 并不认为在iret-&gt;next instruction中间有fault, 认为当前cpu(vcpu)仍然是block nmi 的状态，所以, 我们需要重新设置 VM 为 block nmi state. NOTE 不知道IRET指令会不会set A/D flag return 1 表示PML full event已经处理完。无需退回到 userspace(qemu) 处理 notify flush 是指vcpu正在non-root operation, 其他cpu想要获取该vm最新的dirty_bitmap, 所以需要该vcpu sync this vcpu PML to dirty bitmap. kvm_vm_ioctl_get_dirty_log =&gt; if (kvm_x86_ops-&gt;flush_log_dirty) =&gt; kvm_x86_ops-&gt;flush_log_dirty(kvm); ## vmx_flush_log_dirty =&gt; kvm_flush_pml_buffers =&gt; kvm_for_each_vcpu(i, vcpu, kvm) =&gt; kvm_vcpu_kick(vcpu); 参考资料 KVM: VMX: Page Modification Logging (PML) support Kai Huang(28 Jan 2015) 843e4330573cc5261ae260ce0b83dc570d8cdc05 mail: KVM: VMX: Page Modification Logging (PML) support thp: kvm mmu transparent hugepage support Andrea Arcangeli aarcange@redhat.com (Jan 13 2011) 936a5fe6e6148c0b3ea0d792b903847d9b9931a1 Intel Page Modification Logging, a hardware virtualization feature: study and improvement for virtual machine working set estimationothersKVM: VMX: Page Modification Logging (PML) support patch0 commit(翻译):This patch series adds Page Modification Logging (PML) support in VMX.1) IntroductionPML is a new feature on Intel's Boardwell server platfrom targeted to reduceoverhead of dirty logging mechanism.&gt; PML是英特尔的Broadwell服务器平台上的一项新功能，旨在降低脏页日志记录机制的&gt; 开销。The specification can be found at:http://www.intel.com/content/www/us/en/processors/page-modification-logging-vmm-white-paper.htmlCurrently, dirty logging is done by write protection, which write protects guestmemory, and mark dirty GFN to dirty_bitmap in subsequent write fault. This worksfine, except with overhead of additional write fault for logging each dirty GFN.The overhead can be large if the write operations from geust is intensive.&gt; 目前，脏页日志记录是通过写保护来实现的，即对guest内存进行写保护，并在随后的&gt; write fault中将脏GFN标记到脏位图中。这种方法基本上可以正常工作，但每次记录&gt; 一个脏GFN都需要额外的写入故障，带来了较大的开销。如果guest的写入操作非常频繁，&gt; 这种开销可能会非常大。PML is a hardware-assisted efficient way for dirty logging. PML logs dirty GPAautomatically to a 4K PML memory buffer when CPU changes EPT table's D-bit from0 to 1. To do this, A new 4K PML buffer base address, and a PML index were addedto VMCS. Initially PML index is set to 512 (8 bytes for each GPA), and CPUdecreases PML index after logging one GPA, and eventually a PML buffer fullVMEXIT happens when PML buffer is fully logged.&gt; PML是一种硬件辅助的高效脏页日志记录方法。当CPU将EPT表的D位从0更改为1时，&gt; PML会自动将脏GPA记录到一个4K的PML内存缓冲区中。为此，在VMCS中添加了一个&gt; 新的4K PML buffer base address 和一个PML index。最初，PML index 被设置为512&gt; （每个GPA占用8个字节），然后CPU在记录一个GPA后会减少PML index，最终当PML buffer 被&gt; 完全记录时，会触发一个PML缓冲区满VMEXIT事件。With PML, we don't have to use write protection so the intensive write fault EPTviolation can be avoided, with an additional PML buffer full VMEXIT for 512dirty GPAs. Theoretically, this can reduce hypervisor overhead when guest is indirty logging mode, and therefore more CPU cycles can be allocated to guest, soit's expected benchmarks in guest will have better performance comparing tonon-PML.&gt; Theoretically : 理论上&gt;&gt; 使用PML后，我们不再需要使用写保护，因此可以避免频繁的write fault EPT violation 。&gt; 相反，只需要在PML缓冲区被完全记录时触发一次PML缓冲区满VMEXIT事件（对于512个脏GPA）。&gt; 理论上，这可以减少在脏页日志记录模式下运行的客户端的超级管理程序开销，从而可以将&gt; 更多的CPU周期分配给guest，因此预计在客户端的基准测试中将比非PML情况具有更好的性能。2) Designa. Enable/Disable PMLPML is per-vcpu (per-VMCS), while EPT table can be shared by vcpus, so we needto enable/disable PML for all vcpus of guest. A dedicated 4K page will beallocated for each vcpu when PML is enabled for that vcpu.&gt; \"由于PML是每个vCPU（每个VMCS）的，而EPT表可以由多个vCPU共享，因此我们需要为客&gt; 户端的所有vCPU启用/禁用PML。启用PML后，会为每个vCPU分配一个专用的4K页面。\"Currently, we choose to always enable PML for guest, which means we enables PMLwhen creating VCPU, and never disable it during guest's life time. This avoidsthe complicated logic to enable PML by demand when guest is running. And toeliminate potential unnecessary GPA logging in non-dirty logging mode, we setD-bit manually for the slots with dirty logging disabled.&gt; eliminate : 消除，排除&gt;&gt; 目前，我们选择在创建VCPU时始终启用PML，并且在客户端的整个生命周期中从不禁用它。&gt; 这避免了在客户端运行时按需启用PML的复杂逻辑。为了消除 non-dirty logging mode上可&gt; 能发生的不必要的GPA日志记录，我们手动设置了D位以禁用脏页日志记录的slot。b. Flush PML bufferWhen userspace querys dirty_bitmap, it's possible that there are GPAs logged invcpu's PML buffer, but as PML buffer is not full, so no VMEXIT happens. In thiscase, we'd better to manually flush PML buffer for all vcpus and update thedirty GPAs to dirty_bitmap.&gt; 当用户空间查询 dirty_bitmap 时，可能存在一些已经被记录在 vCPU 的 PML 缓冲区中的 &gt; GPA，但由于 PML 缓冲区还没有满，所以不会发生 VMEXIT。在这种情况下，我们最好手动&gt; 地对所有 vCPU 的 PML 缓冲区进行刷新，并将脏 GPA 更新到 dirty_bitmap 中。这样可以&gt; 确保 dirty_bitmap 中包含了所有的脏 GPA，即使它们还没有引起 VMEXIT。We do PML buffer flush at the beginning of each VMEXIT, this makes dirty_bitmapmore updated, and also makes logic of flushing PML buffer for all vcpus easier-- we only need to kick all vcpus out of guest and PML buffer for each vcpu willbe flushed automatically.&gt; 我们在每次 VMEXIT 的开始都会对 PML 缓冲区进行刷新，这使得 dirty_bitmap 更加及&gt; 时地反映了脏 GPA 的状态，并且简化了对所有 vCPU 的 PML 缓冲区进行刷新的逻辑 --&gt; 我们只需要让所有的 vCPU 从客户端中退出，PML 缓冲区就会被自动地刷新。commit message 后面的部分，主要展示了开启PML的性能损耗（大概在0.06%~0.45%之间).(0.06% 是在--nographic场景下)以及和传统的 WP dirty log 相比，大概有(4% ~ 5%) 的提升。(作者的话是说，noticeable performance gain)" }, { "title": "dirty-ring", "url": "/posts/dirty-ring/", "categories": "live_migration, dirty-ring", "tags": "dirty-ring", "date": "2024-11-12 23:20:00 +0800", "snippet": "示意图:参考资料 KVM: Dirty ring interface Peter Xu 2020 https://lore.kernel.org/all/20201023183358.50607-1-peterx@redhat.com/ ", "content": "示意图:参考资料 KVM: Dirty ring interface Peter Xu 2020 https://lore.kernel.org/all/20201023183358.50607-1-peterx@redhat.com/ " }, { "title": "dirty-bitmap", "url": "/posts/kvm-dirty_bitmap/", "categories": "live_migration, dirty-bitmap", "tags": "dirty-bitmap", "date": "2024-11-12 23:20:00 +0800", "snippet": "ORG PATCH我们来看下最初的KVM实现了哪些功能。最初的KVM代码，是基于shadow page table,支持了dirty_bitmap. 我们从几个方面看下dirty_bitmap实现: kernel data struct USER API lock Contention Analysiskernel data struct并支持了dirty_bitmap, 同样是定义在...", "content": "ORG PATCH我们来看下最初的KVM实现了哪些功能。最初的KVM代码，是基于shadow page table,支持了dirty_bitmap. 我们从几个方面看下dirty_bitmap实现: kernel data struct USER API lock Contention Analysiskernel data struct并支持了dirty_bitmap, 同样是定义在了memslot结构体中，每个slot一个dirty_bitmapstruct kvm_memory_slot { gfn_t base_gfn; unsigned long npages; unsigned long flags; struct page **phys_mem; unsigned long *dirty_bitmap;};该dirty_bitmap保存了该slot中所有物理页是否是dirty的。USE API提供了dev_ioctl方法KVM_GET_DIRTY_LOG:kvm_dev_ioctl=&gt; case KVM_GET_DIRTY_LOG =&gt; copy user param kvm_dirty_log =&gt; kvm_dev_ioctl_get_dirty_logkvm_dirty_log定义如下:struct kvm_dirty_log { __u32 slot; __u32 padding; union { void __user *dirty_bitmap; /* one bit per page */ __u64 padding; };}; slot: slot id padding: 为了让 dirty_bitmap 64bit 对齐? 还是为了之后有其他扩展 dirty_bitmap: copy kernel bitmap to thiskvm_dev_ioctl_get_dirty_log具体函数流程:kvm_dev_ioctl_get_dirty_log # get memslot by slot id=&gt; memslot = &amp;kvm-&gt;memslots[log-&gt;slot]; # copy kernel dirty_bitmap to user=&gt; copy_to_user(log-&gt;dirty_bitmap, memslot-&gt;dirty_bitmap, n) # set pte WP=&gt; spin_lock(&amp;kvm-&gt;lock)=&gt; kvm_mmu_slot_remove_write_access(kvm, log-&gt;slot);=&gt; spin_unlock(&amp;kvm-&gt;lock) # clear dirty_bitmap=&gt; memset(dirty_bitmap, 0, n) # flush tlb=&gt; foreach(vcpu) =&gt; vcpu_load() =&gt; tlb_flush(vcpu) =&gt; vcpu_put()主要的工作如下: 将kernel 中 对应memslot dirty_bitmap copy到 user oparam 将 slot 中所有shadow page table 都标记为 WP clear kernel dirty bitmap 因为2中会涉及到页表修改, 所以需要flush tlb这里会和page fault的流程有race. 我们先展开mmu page fault的流程handle_exception=&gt; spin_lock(&amp;vcpu-&gt;kvm-&gt;lock);=&gt; paging64_page_fault =&gt; paging64_fix_write_pf =&gt; mark_page_dirty() # shadow pte clear WP(mask writeable) # guest pte mask dirty =&gt; shadow_ent |= PT_WRITEABLE_MASK =&gt; guest_ent |= PT_DIRTY_MASK=&gt; spin_unlock(&amp;vcpu-&gt;kvm-&gt;lock);我们来看下该流程是否有问题。kvm_dev_ioctl_get_dirty handle_exception----------------------------------------------copy dirty bitmapspinlockset WPspinunlock spinlock mask_dirty to dirty bitmapclear_dirty_bitmap clear WP spinunlock这么看似乎有点问题. NOTE 但是这个逻辑似乎一直没有更改，不知道自己是否推断的有些问题 avi 在 https://lkml.org/lkml/2012/2/1/140 也似乎提到了这个问题，那当前的版本难道是，如果进入了kvm_dev_ioctl_get_dirty, vcpu没有running的？引入EPT引入EPT之后，其EPT page table， 就类似于shadown pgtable, 我们在get_dirty_log时, 需要向EPT pte set WP.kvm_vm_ioctl_get_dirty_log =&gt; down_write(&amp;kvm-&gt;slots_lock); =&gt; kvm_mmu_slot_remove_write_access =&gt; search_all_ept_pagetable =&gt; if in ept_pgtable in this slot =&gt; SET WP =&gt; kvm_flush_remote_tlbs() =&gt; memset(memslot-&gt;dirty_bitmap, 0, n) =&gt; up_write(&amp;kvm-&gt;slots_lock);这个过程中并没有引入任何锁!!!如果guest因为WP而触发EPT violation，调用路径如下：tdp_page_fault =&gt; spin_lock(&amp;vcpu-&gt;kvm-&gt;mmu_lock); =&gt; __direct_map =&gt; mmu_set_spte(, pte_access=ACC_ALL,) ## __direct_map中认为shadow pgtable有全部权限，对于EPT来说， ## guest权限由guest pgtable来管理即可。无需shadow pgtable(ept ## pgtable控制 =&gt; if pte_access &amp; ACC_WRITE_MASK ## 设置 PT_WRITEABLE_MASK(clear WP) =&gt; spte |= PT_WRITEABLE_MASK ## set bitmap =&gt; mark_page_dirty ## update spte =&gt; set_shadow_spte(, spte) =&gt; spin_unlock(&amp;vcpu-&gt;kvm-&gt;mmu_lock);过程和之前类似，就是不知道在kvm_vm_ioctl_get_dirty_log中为什么两者不再有临界区了。而在kvm_vm_ioctl_get_dirty_log中使用kvm-&gt;slots_lock down_write sem, 可能表示在此过程中，要modify dirty_bitmap, 但是为什么在mark_page_dirty上下文不需要呢?但是这个slots_lock可能在vcpu线程的多个上下文中会down_read(), 可能导致get_dirty_log的流程影响到vcpu线程, 典型如:__vcpu_run down_read(slots_lock) vcpu_enter_guest up_read(slots_lock) kvm_x86_ops-&gt;run() ## vmx_vcpu_run down_read(slots_lock) up_read(slots_lock)而在链接convert slotslock to SRCU 中，AVI提到，在没有引入srcu时，似乎在64 smp vm(比较多核的虚拟机)中会遇到拿锁很高的情况。_raw_spin_lock_irq | --- _raw_spin_lock_irq | |--99.94%-- __down_read | down_read | | | |--99.82%-- 0xffffffffa00479c4 | | 0xffffffffa003a6c9 | | vfs_ioctl | | do_vfs_ioctl | | sys_ioctl | | system_call | | __GI_ioctl | --0.18%-- [...] --0.06%-- [...] 40.57% qemu-system-x86 [kernel] [k] _raw_spin_lock_irqsave | --- _raw_spin_lock_irqsave | |--99.88%-- __up_read | up_read | | | |--99.82%-- 0xffffffffa0047897 | | 0xffffffffa003a6c9 | | vfs_ioctl | | do_vfs_ioctl | | sys_ioctl | | system_call | | __GI_ioctl | --0.18%-- [...] --0.12%-- [...]在这种情况下, vcpu 即便是idle的，也会大量消耗cpu。原因未知。(没有想通,为什么down_read会造成如此高的负载, 看代码也没有频繁的down_write代码路径)随着srcu在kernel中应用。KVM 可以用srcu 避免read操作所带来的开销。（但是这块没有太看懂，有很多疑惑, 我们先过下代码)srcu get_dirty_log关于 srcu 我们这里只关注下 kvm_vm_ioctl_get_dirty_log()实现:kvm_vm_ioctl_get_dirty_log =&gt; down_write(kvm-&gt;slots_lock) =&gt; new a dirty_bitmap: dirty_bitmap # 新创建一个dirty_bitmap =&gt; memset(dirty_bitmap) # 清零 =&gt; spin_lock(mm_lock) =&gt; kvm_mmu_slot_remove_write_access # 清 write access =&gt; spin_unlock(mm_lock) =&gt; new a memslots: slots =&gt; copy kvm-&gt;memslots =&gt; slots(new) # 新创建slots，让其copy old_slots =&gt; slots-&gt;memslots[].dirty_bitmap = dirty_bitmap # 赋值clear的dirty_bitmap =&gt; old_slots = kvm-&gt;memslots =&gt; rcu_assign_pointer(kvm-&gt;memslots, slots) # rcu update pointer =&gt; synchronize_srcu_expedited # wait grace period expedited =&gt; copy_to_user(old_slots-&gt;memslots[].dirty_map) =&gt; up_write(kvm-&gt;slots_lock)似乎，并没有删除slots_lock, 但是, 该操作，确实解决了上面说的dirty bitmap的丢失问题kvm_vm_ioctl_get_dirty_log VCPU0spinlockSET WPspinunlock spinlock mask_dirty to dirty bitmap rcu_deference(memslots) clear WP spinunlockget old memslots pointerrcu assign new kvm memslots with dirty_bitmap clearwait gpcopy old memslots to user因为上面使用了srcu，这样就导致，vcpu0的更新 dirty_bitmap一定会落到 old memslots pointer锁指向的 memslots中。这样做的好处是，无需将一些对memslots更改的流程放到临界区, 影响reader效率。但是由于是srcu，我们期望保证get_dirty_log效率，所以这里使用的是synchronize_srcu_expeditedinterface但是后来Peter Z 推了一个改动，似乎让synchronize_srcu_expedited的速度变慢了，这样就会导致, get log接口，会变慢，从而可能增加guest migration 线程处理脏页速度，以及downtime.avi 在link 中提到，可不可以用atomic clear 的方式，替代srcu。并且Takuya Yoshikawa 在link中测试了一个草稿，感觉效果还不错。我们来看下最终的patch改动srcu-less get_dirty_log=&gt; kvm_vm_ioctl_get_dirty_log =&gt; for_each dirty_bitmap[i] in memslot-&gt;dirty_bitmap =&gt; get a new dirty_bitmap =&gt; dirty_bitmap_buffer = dirty_bitmap + n / sizeof(long); =&gt; memset(dirty_bitmap_buffer, 0, n); =&gt; spin_lock(&amp;kvm-&gt;mmu_lock); =&gt; foreach every long in dirty_bitmap =&gt; continue if dirty_bitmap[] == 0 =&gt; mask = xchg(dirty_bitmap[],0) =&gt; dirty_bitmap_buffer[] = mask =&gt; kvm_mmu_write_protect_pt_mask(mask) { unsigned long *rmapp; while (mask) { # 获取first set bit # 找到其rmapp, 通过rmapp, 让spte WP rmapp = &amp;slot-&gt;rmap[gfn_offset + __ffs(mask)]; __rmap_write_protect(kvm, rmapp, PT_PAGE_TABLE_LEVEL); # clear first set bit /* clear the first set bit */ mask &amp;= mask - 1; } } # kvm_mmu_write_protect_pt_mask end =&gt; spin_unlock(&amp;kvm-&gt;mmu_lock);# mark_page_dirty流程mark_page_dirty=&gt; mark_page_dirty_in_slot =&gt; set_le_bit(gfn, bitmap)将srcu删除，替换成了atomic操作(xchg). 这样可以让 mark_page_dirty在不加mmu_lock的场景下执行，虽然tdp_page_fault在执行__direct_map时，还是全程加着mmu_lock但是全程在tdp_page_fault流程中，全程加着spin_lock(mmu_lock), 是不是不合理呢？为此 xiaoguangrong 在[6]中提出了fast path，可以让因wp触发的 ept violation可以在不加mmu_lock的情况下fix。patch 很大，我们简单分析(而且很多还不太明白, 不得不说，什么代码只要带着shadow pgtable, 就非常…..)KVM: MMU: fast page faulttdp_page_fault =&gt; fast_page_fault =&gt; some if xxx =&gt; goto not fast =&gt; fast_pf_fix_direct_spte =&gt; get_gfn =&gt; if (cmpxchg64(sptep, spte, spte | PT_WRITABLE_MASK) == spte) =&gt; mark_page_dirty(vcpu-&gt;kvm, gfn);这里有一些判断条件，只有判断满足一定条件，才会走fast path.条件包含哪些呢?我们来看下commit messagecommit messageIf the the present bit of page fault error code is set, it indicatesthe shadow page is populated on all levels, it means what we do is onlymodify the access bit which can be done out of mmu-lock&gt; 如果页面page fault error code的 \"present\" 位被设置，表示影子页表在所有级&gt; 别上都已填充，这意味着我们所做的只是修改access位，而这可以在不持有 mmu-lock &gt; 的情况下完成。The tricks in this patch is avoiding the race between fast page faultpath and write-protect path, write-protect path is a read-check-modifypath:&gt; tricks: 技巧&gt;&gt; 这个补丁中的技巧是避免快速页面故障路径和写保护路径之间的竞争。&gt; write-protect路径是一个read-check-modify 的路径：read spte, check W bit, then clear W bit. What we do is populating aidentification in spte, if write-protect meets it, it modify the spteeven if the spte is readonly. See the comment in the code to get moreinformation&gt; 读取 SPTE，检查 W 位，然后清除 W 位。我们所做的是在 SPTE 中填充一个标识符，&gt; 如果写保护遇到它，即使 SPTE 是只读的，它也会修改 SPTE。更多信息可以查看代码&gt; 中的注释。* Advantage- it is really fast it fixes page fault out of mmu-lock, and uses a very light way to avoid the race with other pathes. Also, it fixes page fault in the front of gfn_to_pfn, it means no host page table walking. 它在不持有 MMU 锁的情况下修复页错误，并使用非常轻量的方法来避免与其他路径的竞争。 此外，它在 gfn_to_pfn 之前修复页错误，这意味着不需要遍历主机页表。- we can get lots of page fault with PFEC.P = 1 in KVM: - in the case of ept/npt 　after shadow page become stable (all gfn is mapped in shadow page table, 　it is a short stage since only one shadow page table is used and only a 　few of page is needed), almost all page fault is caused by write-protect 　(frame-buffer under Xwindow, migration), the other small part is caused 　by page merge/COW under KSM/THP. &gt; 在影子页表变得稳定之后（所有的 gfn 都已经映射到影子页表中，这个阶段很短， &gt; 因为只使用一个影子页表且只需要少量的页），几乎所有的页错误都是由于写保护引 &gt; 起的（例如在 X 窗口下的帧缓冲区或迁移时）。剩下的一小部分页错误是由于在 &gt; KSM（内存合并）或 THP（透明大页）下的页合并/写时复制（COW）导致的。 We do not hope it can fix the page fault caused by the read-only host page of KSM, since after COW, all the spte pointing to the gfn will be unmapped. &gt; 我们并不期望它能解决由 KSM 的只读主机页引起的页错误，因为在写时复制 &gt; （COW）之后，所有指向该 gfn 的影子页表条目（spte）都会被取消映射。- in the case of soft mmu - many spurious page fault due to tlb lazily flushed &gt; 由于 TLB（翻译后备缓冲区）延迟刷新，导致出现许多虚假的页错误。(out-of-sync) - lots of write-protect page fault (dirty bit track for guest pte, shadow page table write-protected, frame-buffer under Xwindow, migration, ...) &gt; 许多写保护页错误（用于跟踪客户机 PTE 的脏位、影子页表被写保护、X 窗口下的 &gt; 帧缓冲区、迁移等）。这里提到的，如果P flag is set，那比较容易simple fixed，page fault，之前已经做过对 fast mmio path 的处理: EFFC.P=1 &amp;&amp; EFFC.RSV=1而作者之后要fix的，就是除了上面的其他情况对于EPT而言，主要有两个: 因migration/Xwindow frame-buffer 导致的WP KSM第二种作者不想simple fix, gfn相关的spte 在cow之后，可能会被umap影子页表比较复杂，主要有: out-of-sync 很多的WP page fault track guest page dirty shadow pgtable WP migration Xwindow frame buffer * ImplementationWe can freely walk the page between walk_shadow_page_lockless_begin andwalk_shadow_page_lockless_end, it can ensure all the shadow page is valid.在 walk_shadow_page_lockless_begin 和 walk_shadow_page_lockless_end 之间，我们可以自由地遍历页表，这可以确保所有的影子页都是有效的。In the most case, cmpxchg is fair enough to change the access bit of spte,but the write-protect path on softmmu/nested mmu is a especial case: it isa read-check-modify path: read spte, check W bit, then clear W bit. In orderto avoid marking spte writable after/during page write-protect, we do thetrick like below:在大多数情况下，使用 cmpxchg（比较并交换）来修改影子页表条目（spte）的访问位已经足够。然而，在softmmu/nested MMU 上的写保护路径是一个特殊情况：这是一个read-check-modify路径：read spte，check W bit，然后clear W bit。为了避免在页写保护之后或期间错误地将 spte 标记为可写，我们采取以下技巧： fast page fault path: lock RCU set identification in the spte smp_mb() if (!rmap.PTE_LIST_WRITE_PROTECT) cmpxchg + w - vcpu-id unlock RCU write protect path: lock mmu-lock set rmap.PTE_LIST_WRITE_PROTECT smp_mb() if (spte.w || spte has identification) clear w bit and identification unlock mmu-lockSetting identification in the spte is used to notify page-protect path tomodify the spte, then we can see the change in the cmpxchg.&gt; 在影子页表条目（spte）中设置标识用于通知写保护路径以修改 spte，&gt; 这样我们就可以在 cmpxchg 操作中看到变化。Setting identification is also a trick: it only set the last bit of sptethat does not change the mapping and lose cpu status bits.&gt; 设置标识也是一种技巧：它仅设置 spte 的最后一位，这不会改变映射，&gt; 也不会丢失 CPU 状态位。 NOTE patch理解起来较困难，需要进一步学习, 我们下面简单列举下代码 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!遗留问题!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! tdp_page_fault =&gt; fast_page_fault =&gt; fast_pf_fix_direct_spte =&gt; if (cmpxchg64(sptep, spte, spte | PT_WRITABLE_MASK) == spte) =&gt; mark_page_dirtysplit retrieval and clearing of dirty logThere are two problems with KVM_GET_DIRTY_LOG. First, and less important,it can take kvm-&gt;mmu_lock for an extended period of time. Second, its usercan actually see many false positives in some cases. The latter is dueto a benign race like this: 1. KVM_GET_DIRTY_LOG returns a set of dirty pages and write protects them. 2. The guest modifies the pages, causing them to be marked ditry. 3. Userspace actually copies the pages. 4. KVM_GET_DIRTY_LOG returns those pages as dirty again, even though they were not written to since (3).KVM_GET_DIRTY_LOG 存在两个问题。首先（虽然不太重要），它可能会长时间持有 kvm-&gt;mmu_lock。其次，在某些情况下，用户实际上可能会看到许多误报。后者是由于以下良性竞争条件导致的： 1. KVM_GET_DIRTY_LOG 返回一组脏页并对其进行写保护。 2. guest 修改了这些页面，使其被标记为脏。 3. 用户空间实际复制了这些页面。 4. KVM_GET_DIRTY_LOG 再次将这些页面返回为脏，即使自步骤（3）之后它们并 未被写入。This is especially a problem for large guests, where the time between(1) and (3) can be substantial. This patch introduces a newcapability which, when enabled, makes KVM_GET_DIRTY_LOG notwrite-protect the pages it returns. Instead, userspace has toexplicitly clear the dirty log bits just before using the contentof the page. The new KVM_CLEAR_DIRTY_LOG ioctl can operate on a64-page granularity rather than requiring to sync a full memslot.This way the mmu_lock is taken for small amounts of time, andonly a small amount of time will pass between write protectionof pages and the sending of their content.&gt; 这在大型客户机中尤其是个问题，因为步骤（1）和（3）之间的时间可能很长。&gt; 这个补丁引入了一种新功能，当启用时，使 KVM_GET_DIRTY_LOG 不再写保护它&gt; 返回的页面。相反，用户空间必须在使用页面内容之前显式清除脏日志位。新&gt; 的 KVM_CLEAR_DIRTY_LOG ioctl 可以以 64 页的粒度操作，而不需要同步整个&gt; 内存槽。这样，mmu_lock 只会被短暂持有，并且在页面写保护和发送其内容之&gt; 间只会经过很短的时间。This is entirely implemented in generic code, but only users ofkvm_get_dirty_log_protect get the support (that is x86_64, ARM and MIPS).&gt; 这一功能完全在通用代码中实现，但只有使用 kvm_get_dirty_log_protect &gt; 的用户（即 x86_64、ARM 和 MIPS）才能获得支持。主要改动有几个方面: 增加KVM_CAP_MANUAL_DIRTY_LOG_PROTECT modify kvm_get_dirty_log_protect add kvm_clear_dirty_log_protect首先我们看下CAP部分, 在Documentation/virtual/kvm/api.txt新增解释:7.18 KVM_CAP_MANUAL_DIRTY_LOG_PROTECTArchitectures: allParameters: args[0] whether feature should be enabled or notWith this capability enabled, KVM_GET_DIRTY_LOG will not automaticallyclear and write-protect all pages that are returned as dirty.Rather, userspace will have to do this operation separately usingKVM_CLEAR_DIRTY_LOG.At the cost of a slightly more complicated operation, this provides betterscalability and responsiveness for two reasons. First,KVM_CLEAR_DIRTY_LOG ioctl can operate on a 64-page granularity ratherthan requiring to sync a full memslot; this ensures that KVM does nottake spinlocks for an extended period of time. Second, in some cases alarge amount of time can pass between a call to KVM_GET_DIRTY_LOG anduserspace actually using the data in the page. Pages can be modifiedduring this time, which is inefficint for both the guest and userspace:the guest will incur a higher penalty due to write protection faults,while userspace can see false reports of dirty pages. Manual reprotectionhelps reducing this time, improving guest performance and reducing thenumber of dirty log false positives.参数：args[0] 指示是否应启用该功能。启用此功能后，KVM_GET_DIRTY_LOG 将不会自动清除并写保护所有返回为脏的页面。相反，用户空间需要使用 KVM_CLEAR_DIRTY_LOG 单独执行此操作。尽管操作稍微复杂了一些，但这提供了更好的可扩展性和响应性，原因有两个。首先，KVM_CLEAR_DIRTY_LOG ioctl 可以以 64 页的粒度操作，而不需要同步整个memslot；这确保了 KVM 不会长时间持有自旋锁。其次，在调用 KVM_GET_DIRTY_LOG 和用户空间实际使用页面中的数据之间，可能会经过很长时间。在此期间，页面可能被修改，这对客户机和用户空间来说都是低效的：客户机将因为写保护错误而遭受更高的惩罚，而用户空间可能会看到脏页的错误报告。手动重新保护有助于缩短这段时间，提高客户机性能并减少脏日志的误报数量。我们来用图看下为什么要，做get /clear 接口分离: 图示展开 此时迁移两个页，看起来工作的不错: 第二轮get_dirty_log bitmap, 恰好能表示source target两端页dirty状态 下面的情况，由于脏页似乎比较集中，在migration 线程还未send page2, page2 又被更改了，此时bitmap已经置位。随后，migration线程send page2，此时source dest page2，相同。 但是dirty_bitmap中，有page2 bit置位，意味着还需要再次无意义传递一次page2 作者想，能不能将get log和clear log, 分开. 由guest决定什么时候，将bitmap clear掉，避免发送重复的页。如下图: 等guest将page send出去之后，再clear掉 dirty_bitmap, 能尽量延迟bitmap的clear. 岂不是美滋滋。 但是这样似乎有些问题, 中间如果发生了WP，则会丢失该dirty page. 这样来看，clear_dirty_log，还是应该在save page之前，只不过要让窗口越小越好 代码部分不再展开:kvm_vm_ioctl_get_dirty_log, 主要是新增了manual_dirty_log_protect处理分支，如下:kvm_get_dirty_log_protect =&gt; if manual_dirty_log_protect is true =&gt; get dirty_bitmap =&gt; else =&gt; get dirty_bitmap and clean =&gt; copy_to_user(bitmap)新增了一个判断分支，该处理分支，和原来逻辑不同的是，没有clear bitmapkvm_clear_dirty_log_protect:新增参数数据结构:struct kvm_clear_dirty_log { __u32 slot; __u32 num_pages; __u64 first_page; union { void __user *dirty_bitmap; /* one bit per page */ __u64 padding; };}; dirty_bitmap: for each bit that is set in the input bitmap, the corresponding page is marked “clean” in KVM’s dirty bitmap. first_page: dirty_bitmap[0]:0 ‘s page, must multiple 64 num_pages: mast multiple 64, or first_page + num_pages = dirty_bitmap[end]该数据结构, 用来向kvm传递, 可以传递一个区间到KVM, 让kvm来clear dirtybit。clear主要流程如下:kvm_vm_ioctl_clear_dirty_log =&gt; mutex_lock(&amp;kvm-&gt;slots_lock); =&gt; kvm_clear_dirty_log_protect(kvm, log, &amp;flush); =&gt; for_each_long in log-&gt;dirty_bitmap[] as mask =&gt; p = memslot-&gt;dirty_bitmap[] # 与非操作, p = ~mask &amp; p , =&gt; mask &amp;= atomic_long_fetch_andnot(mask, p) =&gt; mutex_unlock(&amp;kvm-&gt;slots_lock);##相关commit/link KVM: VMX: Enable EPT feature for KVM 1439442c7b257b47a83aea4daed8fbf4a32cdff9 Sheng Yang(Mon Apr 28 12:24:45 2008) KVM: use SRCU for dirty log b050b015abbef8225826eecb6f6b4d4a6dea7b79 Marcelo Tosatti(Wed Dec 23 14:35:22 2009) [1]. convert slotslock to SRCU srcu: Implement call_srcu() Peter Zijlstra(Jan 30 2012) https://lkml.org/lkml/2012/1/31/211 KVM: Switch to srcu-less get_dirty_log() 60c34612b70711fb14a8dcbc6a79509902450d2e Takuya Yoshikawa(Sat Mar 3 14:21:48 2012 +0900) mail dirty_log_perf test mail KVM: MMU: fast page fault v1 v7 Xiao Guangrong(Thu, 29 Mar 2012) kvm: split retrieval and clearing of dirty log Paolo Bonzini(28 Nov 2018) 2a31b9db153530df4aa02dac8c32837bf5f47019 https://patchwork.kernel.org/project/kvm/cover/1543405379-21910-1-git-send-email-pbonzini@redhat.com/ KVM: x86: enable dirty log gradually in small chunks Jay Zhou(Thu, 27 Feb 2020) https://lore.kernel.org/all/20200227013227.1401-1-jianjay.zhou@huawei.com/#r 附录virt/kvm/locking.rstNOTE在kernel doc virt/kvm/locking.rst中也有提到:Fast page fault:Fast page fault is the fast path which fixes the guest page fault out ofthe mmu-lock on x86. Currently, the page fault can be fast in one of thefollowing two cases:1. Access Tracking: The SPTE is not present, but it is marked for access tracking. That means we need to restore the saved R/X bits. This is described in more detail later below.2. Write-Protection: The SPTE is present and the fault is caused by write-protect. That means we just need to change the W bit of the spte.What we use to avoid all the races is the Host-writable bit and MMU-writable biton the spte:- Host-writable means the gfn is writable in the host kernel page tables and in its KVM memslot.- MMU-writable means the gfn is writable in the guest's mmu and it is not write-protected by shadow page write-protection.On fast page fault path, we will use cmpxchg to atomically set the spte Wbit if spte.HOST_WRITEABLE = 1 and spte.WRITE_PROTECT = 1, to restore the savedR/X bits if for an access-traced spte, or both. This is safe because wheneverchanging these bits can be detected by cmpxchg.目前upstream 的x86 fast path fault, 可以在不拿mmu-lock的情况下，完成某些 guest page fault 的fix。主要有两种类型 Access Tracking; SPTE 不是我们仅需要 restore saved R/X bit.(对比xiaoguangrong patch新增) Write-Protection: 我们仅需要spte change W bit增加了两个spte上的flag:Host-writeable: 表示host kernel page table 是writeable的。MMU-writeable: 表示 gfn 是writeable的，并且 shadow page 并没有用写保护保护起来在fast PF path 中， 使用 cmpxchg 来atomically set W bit if spte &amp; (HOST_WRITEABLE | WRITE_PROTECT) == 1" }, { "title": "auto-converge", "url": "/posts/auto-converge/", "categories": "live_migration, autoconverge", "tags": "autoconverge", "date": "2024-11-12 23:20:00 +0800", "snippet": "#简介在大型机器，并且系统负载高时热迁移，工作负载往往比热迁移速度更快，从而导致live migration无法 converge.这个往往受限于bandwidth, 虽然现在网卡的带宽越来越高。但是虚拟机的核心数量以及cpu主频，内存带宽也在逐渐增大。Chegu Vinod 在 2013 年提出了 auto-converge, （但是的场景即使使用了10GigNICs也仍然NOT conv...", "content": "#简介在大型机器，并且系统负载高时热迁移，工作负载往往比热迁移速度更快，从而导致live migration无法 converge.这个往往受限于bandwidth, 虽然现在网卡的带宽越来越高。但是虚拟机的核心数量以及cpu主频，内存带宽也在逐渐增大。Chegu Vinod 在 2013 年提出了 auto-converge, （但是的场景即使使用了10GigNICs也仍然NOT converge). 所以，他提出了该方案，来限制脏页产生速度，也就是简单粗暴的限制vcpu的”频率”, (让vcpu线程sleep).我们来看下该patch 的 commit message:Busy enterprise workloads hosted on large sized VM's tend to dirtymemory faster than the transfer rate achieved via live guest migration.Despite some good recent improvements (&amp; using dedicated 10Gig NICsbetween hosts) the live migration does NOT converge.在大型虚拟机上运行的繁忙企业级工作负载通常会比实时迁移的传输速率更快地修改内存数据。尽管近期在迁移性能上已有一些改进（包括在主机之间使用专用的10Gb网卡），实时迁移仍然无法收敛。If a user chooses to force convergence of their migration via a newmigration capability \"auto-converge\" then this change will auto-detectlack of convergence scenario and trigger a slow down of the workloadby explicitly disallowing the VCPUs from spending much time in the VMcontext.如果用户选择通过一个新的迁移功能“auto-converge”来强制迁移收敛，那么该功能将自动检测到未收敛的情况，并通过显式限制虚拟 CPU 在 VM 上下文中的执行时间来减缓工作负载。The migration thread tries to catchup and this eventually leadsto convergence in some \"deterministic\" amount of time. Yes it doesimpact the performance of all the VCPUs but in my observation thatlasts only for a short duration of time. i.e. end up enteringstage 3 (downtime phase) soon after that. No external trigger isrequired.迁移线程会努力追赶同步速度，这最终会在“确定的”时间内实现收敛。虽然这确实会影响所有虚拟 CPU 的性能，但根据我的观察，这种影响只会持续较短的时间，即在此之后很快便进入第3阶段（停机阶段）。整个过程不需要额外的触发。Thanks to Juan and Paolo for their useful suggestions.我们来分析下具体代码:first patch 代码分析intro async_run_on_cpu引入该函数的主要目的，是为了让vcpu thread sleep 完成之前，migrationthread 不必wait（打得就是频率差，否则就没有意义了)原commit message:Introduce an asynchronous version of run_on_cpu() i.e. the callerdoesn't have to block till the call back routine finishes executionon the target vcpu.代码如下:void async_run_on_cpu(CPUState *cpu, void (*func)(void *data), void *data){ struct qemu_work_item *wi; if (qemu_cpu_is_self(cpu)) { func(data); return; } wi = g_malloc0(sizeof(struct qemu_work_item)); wi-&gt;func = func; wi-&gt;data = data; wi-&gt;free = true; if (cpu-&gt;queued_work_first == NULL) { cpu-&gt;queued_work_first = wi; } else { cpu-&gt;queued_work_last-&gt;next = wi; } cpu-&gt;queued_work_last = wi; wi-&gt;next = NULL; wi-&gt;done = false; qemu_cpu_kick(cpu);}intro auto-converge param &amp;&amp; capdiff --git a/migration.c b/migration.cindex 058f9e6..d0759c1 100644--- a/migration.c+++ b/migration.c@@ -473,6 +473,15 @@ void qmp_migrate_set_downtime(double value, Error **errp) max_downtime = (uint64_t)value; } +bool migrate_auto_converge(void)+{+ MigrationState *s;++ s = migrate_get_current();++ return s-&gt;enabled_capabilities[MIGRATION_CAPABILITY_AUTO_CONVERGE];+}+ int migrate_use_xbzrle(void) { MigrationState *s;diff --git a/qapi-schema.json b/qapi-schema.jsonindex a80ee40..c019fec 100644--- a/qapi-schema.json+++ b/qapi-schema.json@@ -605,10 +605,13 @@ # This feature allows us to minimize migration traffic for certain work # loads, by sending compressed difference of the pages #+# @auto-converge: If enabled, QEMU will automatically throttle down the guest+# to speed up convergence of RAM migration. (since 1.6)+# # Since: 1.2 ## { 'enum': 'MigrationCapability',- 'data': ['xbzrle'] }+ 'data': ['xbzrle', 'auto-converge'] }calc if dirty throttle or not@@ -378,8 +381,14 @@ static void migration_bitmap_sync(void)+ static int64_t bytes_xfer_prev; ...+ int64_t bytes_xfer_now;++ if (!bytes_xfer_prev) {+ bytes_xfer_prev = ram_bytes_transferred();+ }@@ -404,6 +413,23 @@ static void migration_bitmap_sync(void) /* more than 1 second = 1000 millisecons */ if (end_time &gt; start_time + 1000) {+ if (migrate_auto_converge()) {+ /* The following detection logic can be refined later. For now:+ Check to see if the dirtied bytes is 50% more than the approx.+ amount of bytes that just got transferred since the last time we+ were in this routine. If that happens &gt;N times (for now N==4)+ we turn on the throttle down logic */ /* 后续可以进一步优化以下检测逻辑。当前逻辑如下： 检查脏页字节数是否比上次进入该函数以来传输的字节数大50%。 如果这种情况发生超过 N 次（目前 N==4），则开启降速逻辑 */ //==(1)==+ bytes_xfer_now = ram_bytes_transferred();+ if (s-&gt;dirty_pages_rate &amp;&amp;+ (num_dirty_pages_period * TARGET_PAGE_SIZE &gt;+ (bytes_xfer_now - bytes_xfer_prev)/2) &amp;&amp; //==(2)==+ (dirty_rate_high_cnt++ &gt; 4)) {+ trace_migration_throttle();+ mig_throttle_on = true;+ dirty_rate_high_cnt = 0;+ }+ bytes_xfer_prev = bytes_xfer_now;+ } else { //==(3)==+ mig_throttle_on = false;+ }在migration_bitmap_sync后，还是按照之前的逻辑，本次和上次差距1s，则认为migration线程有压力。然后做了一个简单的算法:先简单介绍下其中变量含义: bytes_xfer_prev: sync之前, 要传输的memory 数量 bytes_xfer_now: sync 之后, 要传输的memory 数量 num_dirty_pages_period: 本次和上次相比，dirty page的变化量 dirty_rate_high_cnt: 进入dirty rate high if分支的次数 mig_throttle_on: 表示开启mig_throttle_on所以, 该算法比较简单，触发条件有两个 (dirty_page_now - dirty_page_last) / (byte_xfer_now) - (byte_xfer_prev) &gt; 0.5则认为需要限制脏页产生速率。 该公式分子部分，相当于脏页产生速率，而分母部分相当于 migration thread页面传输速率。值越大，说明，脏页产生速率相对越快。 达到1中的条件5次。 达到条件2后, 将dirty_rate_high_cnt 设置为0。所以下次如果再想触发mig_throttle, 还需要再次达到条件2. 一旦开启了auto_converge, 就无法将 mig_throttle_on 设置为false notify vcpu down ratio@@ -628,6 +656,7 @@ static int ram_save_iterate(QEMUFile *f, void *opaque) } total_sent += bytes_sent; acct_info.iterations++;+ check_guest_throttling(); /* we want to check in the 1st loop, just in case it was the 1st time and we had to sync the dirty bitmap. qemu_get_clock_ns() is a bit expensive, so we only check each some+static void check_guest_throttling(void)+{+ static int64_t t0;+ int64_t t1;++ if (!mig_throttle_on) {+ return;+ }++ if (!t0) {+ t0 = qemu_get_clock_ns(rt_clock);+ return;+ }++ t1 = qemu_get_clock_ns(rt_clock);++ /* If it has been more than 40 ms since the last time the guest+ * was throttled then do it again.+ */ //==(1)==+ if (40 &lt; (t1-t0)/1000000) {+ mig_throttle_guest_down();+ t0 = t1;+ }+}在在每一轮的ram_save_iterate中，查看是否需要notify vcpu，但是这里notify 也有一个频率为40ms, 我们下面看看是如何notify vcpu，以及vcpu那边如何down ratio.+/* Stub function that's gets run on the vcpu when its brought out of the+ VM to run inside qemu via async_run_on_cpu()*/+static void mig_sleep_cpu(void *opq)+{+ qemu_mutex_unlock_iothread(); //==(1)==+ g_usleep(30*1000);+ qemu_mutex_lock_iothread();+}++/* To reduce the dirty rate explicitly disallow the VCPUs from spending+ much time in the VM. The migration thread will try to catchup.+ Workload will experience a performance drop.+*/+static void mig_throttle_cpu_down(CPUState *cpu, void *data)+{+ async_run_on_cpu(cpu, mig_sleep_cpu, NULL);+}++static void mig_throttle_guest_down(void)+{+ qemu_mutex_lock_iothread();+ qemu_for_each_cpu(mig_throttle_cpu_down, NULL);+ qemu_mutex_unlock_iothread();+}从(1) 中可以看到, 每次sleep 30ms, 上面说了40ms kick 一次vcpu, 那么cpu sleep time和runtime的占比大概是3:1 ======还存在的问题======这样做似乎太简单粗暴了，是固定频率限制，在某些情况下可能不管用dynamic cpu throttleJason J. Herne 发现了之前的auto-converge不太好用，因为他是让vcpu下降到一个固定频率，但是在这个固定的频率下, 迁移的效率还是不够，导致迁移迟迟不能完成。所以，Jason J. Herne发送了一个mail，并且他想做一个真正”auto”的方式，动态的调整throttle的力度. 向大家征求意见:mail 地址 – 2015-03-12其中Dr. David Alan Gilbert从工程角度, 针对Jason J. Herne提出的一些问题给出了建议, 例如针对这些不太容易迁移的虚拟机，可以使用: xbzrle post-copy inc max-downtime等等来加速虚拟机迁移。并且对Jason J. Herne的想法给予肯定。Jason J. Herne 编写了该patch。（我们只看下v7 版本)首先看下 commit message:This patch set provides a new method for throttling a vcpu and makes use ofsaid method to dynamically increase cpu throttling during an autoconvergemigration until the migration completes. This method ensures that allmigrations will eventually converge.&gt; 这个补丁集提供了一种新的限制虚拟 CPU 的方法，并利用该方法在自动收敛迁移过程&gt; 中动态增加 CPU 限速，直到迁移完成。该方法确保所有迁移最终都能收敛。The method used here for throttling vcpus is likely not the best. However, Ibelieve that it is preferable to what is used for autoconverge today.&gt; 这里用于限制虚拟 CPU 的方法可能不是最好的。然而，我认为它比目前用于自动收敛&gt; 的方式更为可取。This work is related to the following discussion:https://lists.gnu.org/archive/html/qemu-devel/2015-03/msg00287.html该系列patch, 主要分为如下部分: 重新定义涉及throttle 相关接口 active/stop – 开启关闭 throttle set throttle_percentage – 动态调整 throttle 力度 periodic tick – 可以更精准的周期性触发 periodic tick, 不再依赖migration thread notify sleep interface – 可以根据 动态调整的 throttle 的值，来sleep. 增加两个参数 x-cpu-throttle-initial: 初始值 x-cpu-throttle-increment: 如果本次限制之后，再次达到了dirty radio 的限制，则一次增加 throttle的值 migration_bitmap_sync 处理逻辑(core) dynamic inc 增加 thrrole ratio 查询 info migrate query-migrate 我们这里，只列举下和动态调整throttle的相关改动+/* Reduce amount of guest cpu execution to hopefully slow down memory writes.+ * If guest dirty memory rate is reduced below the rate at which we can+ * transfer pages to the destination then we should be able to complete+ * migration. Some workloads dirty memory way too fast and will not effectively+ * converge, even with auto-converge.+ */+static void mig_throttle_guest_down(void)+{+ MigrationState *s = migrate_get_current(); //初始值+ uint64_t pct_initial =+ s-&gt;parameters[MIGRATION_PARAMETER_X_CPU_THROTTLE_INITIAL]; //每次增加的值+ uint64_t pct_icrement =+ s-&gt;parameters[MIGRATION_PARAMETER_X_CPU_THROTTLE_INCREMENT];+ //如果第一次执行，则设置为初始值+ /* We have not started throttling yet. Let's start it. */+ if (!cpu_throttle_active()) {+ cpu_throttle_set(pct_initial);+ } else { //说明是第2+次，触发了，则在原来值的基础上加上inc的值+ /* Throttling already on, just increase the rate */+ cpu_throttle_set(cpu_throttle_get_percentage() + pct_icrement);+ }+}可以看到，上面的patch，通过cpu_throttle_set()传入了current_pct + pct_icrement, 这个值代表的是，睡眠时间所占的比重。我们来看下为达到这样的时间比例，QEMU如何实现:首先引入了一个时间片的值，该值为10ms+#define CPU_THROTTLE_TIMESLICE_NS 10000000该值表示vcpu运行的时间片固定值, 也就是为了避免因为alarm频繁的vm-exit. 将该值固定下来，然后让睡眠时间根据该值和pct再计算定义了一个timer, 使用QEMU_CLOCK_VIRTUAL_RT, expired function为cpu_throttle_timer_tick void cpu_ticks_init(void) { seqlock_init(&amp;timers_state.vm_clock_seqlock, NULL); vmstate_register(NULL, 0, &amp;vmstate_timers, &amp;timers_state);+ throttle_timer = timer_new_ns(QEMU_CLOCK_VIRTUAL_RT,+ cpu_throttle_timer_tick, NULL); }在set 新的throttle时, 会在当前时间上，增加CPU_THROTTLE_TIMESLICE_NS设置timer，也就是使其，在下一个时间片上触发timer.+void cpu_throttle_set(int new_throttle_pct)+{+ /* Ensure throttle percentage is within valid range */+ new_throttle_pct = MIN(new_throttle_pct, CPU_THROTTLE_PCT_MAX);+ new_throttle_pct = MAX(new_throttle_pct, CPU_THROTTLE_PCT_MIN);++ atomic_set(&amp;throttle_percentage, new_throttle_pct);++ timer_mod(throttle_timer, qemu_clock_get_ns(QEMU_CLOCK_VIRTUAL_RT) ++ CPU_THROTTLE_TIMESLICE_NS);+}timer 触发时，使用CPU_THROTTLE_TIMESLICE_NS/(1-pct), 我们来看下该公式:\\[\\begin{align}timer\\_val \\\\=&amp; 总的时间片 \\\\=&amp; \\frac{vcpu时间片}{vcpu时间比例} \\\\=&amp; \\frac{CPU\\_THROTTLE\\_TIMESLICE\\_NS}{1-pct} \\\\\\end{align}\\]所以，这里传入的应该是总的时间片.我们来看下timer expired callbak+static void cpu_throttle_timer_tick(void *opaque)+{+ CPUState *cpu;+ double pct;++ /* Stop the timer if needed */+ if (!cpu_throttle_get_percentage()) {+ return;+ }+ CPU_FOREACH(cpu) {+ if (!atomic_xchg(&amp;cpu-&gt;throttle_thread_scheduled, 1)) { //通知各个vcpu睡眠+ async_run_on_cpu(cpu, cpu_throttle_thread, cpu);+ }+ }+throttle-trigger-threshold+ pct = (double)cpu_throttle_get_percentage()/100; //这里是再次触发timer的时间, 上面说到`CPU_THROTTLE_TIMESLICE_NS / (1-pct)` //是总的时间片，所以包含了vcpu time slice和sleep time slice+ timer_mod(throttle_timer, qemu_clock_get_ns(QEMU_CLOCK_VIRTUAL_RT) ++ + CPU_THROTTLE_TIMESLICE_NS / (1-pct));+}那notify callbak一定是执行sleep了，我们关注下sleep时间:+static void cpu_throttle_thread(void *opaque)+{+ CPUState *cpu = opaque;+ double pct;+ double throttle_ratio;+ long sleeptime_ns;++ if (!cpu_throttle_get_percentage()) {+ return;+ }++ pct = (double)cpu_throttle_get_percentage()/100;+ throttle_ratio = pct / (1 - pct);+ sleeptime_ns = (long)(throttle_ratio * CPU_THROTTLE_TIMESLICE_NS);++ qemu_mutex_unlock_iothread();+ atomic_set(&amp;cpu-&gt;throttle_thread_scheduled, 0);+ g_usleep(sleeptime_ns / 1000); /* Convert ns to us for usleep call */+ qemu_mutex_lock_iothread();+}睡眠时间为:\\[\\begin{align}sleep_ns = &amp; {总的时间片}* (pct) \\\\=&amp; \\frac{CPU\\_THROTTLE\\_TIMESLICE\\_NS}{1-pct} * {pct} \\\\=&amp; \\frac{pct}{1-pct} * {CPU\\_THROTTLE\\_TIMESLICE\\_NS} \\\\=&amp; {throttle\\_ratio} * {CPU\\_THROTTLE\\_TIMESLICE\\_NS}\\end{align}\\] ======还存在的问题======上面的patch虽然能动态的调整迁移过程中的throttle, 并且可以灵活的配置throttle的初始值和增量, 进入guestdown的条件，是不变的。还是按照最初的dirty page inc / xfer inc &gt; 50%, 也就是说达到这个条件后，肯定会影响虚拟机性能。zhukeqian 觉得不妥，想让用户来配置该值, 来确定这个百分比configure throttle-trigger-threshold首先来看下commit message:Currently, if the bytes_dirty_period is more than the 50% ofbytes_xfer_period, we start or increase throttling.&gt; 当前，如果 bytes_dirty_period 超过 bytes_xfer_period 的 50%，我&gt; 们将开始或增加限速。If we make this percentage higher, then we can tolerate higherdirty rate during migration, which means less impact on guest.The side effect of higher percentage is longer migration time.We can make this parameter configurable to switch between mig-ration time first or guest performance first.&gt; 如果我们提高这个百分比，就可以在迁移过程中容忍更高的脏页速率，&gt; 这意味着对虚拟机的影响会更小。提高百分比的副作用是迁移时间会变&gt; 长。我们可以将这个参数设置为可配置，以便在迁移时间优先和虚拟机&gt; 性能优先之间进行切换。The default value is 50 and valid range is 1 to 100.&gt; 默认值为 50，有效范围是 1 到 100。目前该配置的默认值是50%，保持和原来一样的值，但是可以调整为[1,100]只看下判断是否throttle的代码改动:+static void migration_trigger_throttle(RAMState *rs)+{+ MigrationState *s = migrate_get_current(); //throttle_trigger_threshold新增的调整参数+ uint64_t threshold = s-&gt;parameters.throttle_trigger_threshold;++ uint64_t bytes_xfer_period = ram_counters.transferred - rs-&gt;bytes_xfer_prev;+ uint64_t bytes_dirty_period = rs-&gt;num_dirty_pages_period * TARGET_PAGE_SIZE; //按照比例计算bytes_dirty_threshold + uint64_t bytes_dirty_threshold = bytes_xfer_period * threshold / 100;++ /* During block migration the auto-converge logic incorrectly detects+ * that ram migration makes no progress. Avoid this by disabling the+ * throttling logic during the bulk phase of block migration. */+ if (migrate_auto_converge() &amp;&amp; !blk_mig_bulk_active()) {+ /* The following detection logic can be refined later. For now:+ Check to see if the ratio between dirtied bytes and the approx.+ amount of bytes that just got transferred since the last time+ we were in this routine reaches the threshold. If that happens+ twice, start or increase throttling. */+ //使用按比例计算后的 bytes_dirty_threshold 值来比较+ if ((bytes_dirty_period &gt; bytes_dirty_threshold) &amp;&amp;+ (++rs-&gt;dirty_rate_high_cnt &gt;= 2)) {+ trace_migration_throttle();+ rs-&gt;dirty_rate_high_cnt = 0;+ mig_throttle_guest_down();+ }+ }+} ======还存在的问题======inc throttle 的目前的算法，是在初始值上累加，每次增长的睡眠值在后期的很明显。我们来看下公式:\\[\\begin{align}sleep_ns = &amp; {总的时间片}* (pct) \\\\=&amp; \\frac{CPU\\_THROTTLE\\_TIMESLICE\\_NS}{1-pct} * {pct} \\\\=&amp; \\frac{pct}{1-pct} * {CPU\\_THROTTLE\\_TIMESLICE\\_NS} \\\\\\end{align}\\]我们来看下\\[\\begin{align}y = \\frac{x}{1-x}\\end{align}\\]的图像(过于基础 - - )是一个指数级增长。该值* CPU_THROTTLE_TIMESLICE_NS(10ms)， 不过, 由于x-cpu-throttle-increment是整型, 所以其最大可以达到99 / 1 * 10ms = 990ms = 0.99s但是不可否认的是，后期的睡眠时间增长非常快。于是Keqian Zhu增加了cpu-throttle-tailslow 参数，让其在后期可以增长的相对平缓。但是延长迁移时间cpu-throttle-tailslowcommit messageAt the tail stage of throttling, the Guest is very sensitive toCPU percentage while the @cpu-throttle-increment is excessiveusually at tail stage.&gt; 在限流的尾阶段，虚拟机对 CPU 使用率非常敏感，而此时 &gt; @cpu-throttle-increment 通常过大。If this parameter is true, we will compute the ideal CPU percentageused by the Guest, which may exactly make the dirty rate match thedirty rate threshold. Then we will choose a smaller throttle incrementbetween the one specified by @cpu-throttle-increment and the onegenerated by ideal CPU percentage.&gt; 如果该参数为真，我们将计算虚拟机理想的 CPU 使用百分比，使其脏页率&gt; 正好匹配脏页率阈值。然后，我们将在 @cpu-throttle-increment 和根据&gt; 理想 CPU 使用率生成的增量之间选择一个较小的限流增量。Therefore, it is compatible to traditional throttling, meanwhilethe throttle increment won't be excessive at tail stage. This maymake migration time longer, and is disabled by default.&gt; 因此，这种方法与传统限流兼容，同时在尾阶段限流增量不会过大。这样可能会延&gt; 长迁移时间，默认情况下是禁用的。来看下如何计算得出一个新的增长量的相关代码:+static void mig_throttle_guest_down(uint64_t bytes_dirty_period,+ uint64_t bytes_dirty_threshold) { MigrationState *s = migrate_get_current(); uint64_t pct_initial = s-&gt;parameters.cpu_throttle_initial;- uint64_t pct_icrement = s-&gt;parameters.cpu_throttle_increment;+ uint64_t pct_increment = s-&gt;parameters.cpu_throttle_increment;+ bool pct_tailslow = s-&gt;parameters.cpu_throttle_tailslow; int pct_max = s-&gt;parameters.max_cpu_throttle;+ uint64_t throttle_now = cpu_throttle_get_percentage();+ uint64_t cpu_now, cpu_ideal, throttle_inc;+ /* We have not started throttling yet. Let's start it. */ if (!cpu_throttle_active()) { cpu_throttle_set(pct_initial); } else { /* Throttling already on, just increase the rate */- cpu_throttle_set(MIN(cpu_throttle_get_percentage() + pct_icrement,- pct_max));+ if (!pct_tailslow) {+ throttle_inc = pct_increment;+ } else {+ /* Compute the ideal CPU percentage used by Guest, which may+ * make the dirty rate match the dirty rate threshold. */+ cpu_now = 100 - throttle_now; //下面解释为什么这样可以得出 cpu_ideal+ cpu_ideal = cpu_now * (bytes_dirty_threshold * 1.0 /+ bytes_dirty_period); //计算cpu运行时间增量。这里和 pct_increment 取最小值+ throttle_inc = MIN(cpu_now - cpu_ideal, pct_increment);+ }+ cpu_throttle_set(MIN(throttle_now + throttle_inc, pct_max)); } }我们还是先解释下参数: bytes_dirty_period: 该period dirty byte数量. (当前值) bytes_dirty_threshold: 该period dirty threshold (目标阈值)我们期望的是，经过调整vcpu ratio, 可以让byte_dirty_period &lt;= bytes_dirty_threshold我们假设，byte_dirty_period和cpu_run_time成正比所以可得:$$\\begin{align}byte_dirty_period &amp;= A * byte_dirty_threshold \\byte_dirty_period &amp;= \\frac{cpu_ideal}{cpu_now} * byte_dirty_threshold \\cpu_ideal &amp;= \\frac{byte_dirty_period}{byte_dirty_threshold} * cpu_now\\end{align}$$(11) 解释:由于byte_dirty_period较大， 我们需要在byte_dirty_threshold乘一个因子，让其达到byte_dirty_period, 而前面我们假设认为byte_dirty_period 和cpu_run_time 成正比, 所以我们可以将 cpu_ideal / cpu_now 作为乘因子。由于上面是取 cpu_now - cpu_ideal 和 pci_increment的最小值，所以该流程仅在后期起作用。commit &amp;&amp; mail list 3c02270db980007424d797506301826310ce2db4 [PATCH v8 0/3] Throttle-down guest to help with live migration convergence [PATCH v8 3/3] Throttle-down guest to help with live migration convergence Dynamic cpu throttling for auto-converge 070afca258f973c704dcadf2769aa1ca921209a1 https://lore.kernel.org/all/1441732357-11861-1-git-send-email-jjherne@linux.vnet.ibm.com/ migration/throttle: Add throttle-trig-thres migration parameter dc14a470763c96fd9d360e1028ce38e8c3613a77 https://patchwork.kernel.org/project/qemu-devel/patch/20200316042935.28306-1-zhukeqian1@huawei.com/ migration/throttle: Add cpu-throttle-tailslow migration parameter cbbf818224faf5ede75c876e4900c9f8e6b6c0db https://lore.kernel.org/all/20200413101508.54793-1-zhukeqian1@huawei.com/ " }, { "title": "[Translate]: Migration auto-converge problem", "url": "/posts/Migration-auto-converge-problem/", "categories": "live_migration, auto-converge", "tags": "autoconverge", "date": "2024-11-12 23:20:00 +0800", "snippet": "From: \"Jason J. Herne\" &lt;jjherne@linux.vnet.ibm.com&gt;To: \"qemu-devel@nongnu.org qemu-devel\" &lt;qemu-devel@nongnu.org&gt;,\tChristian Borntraeger &lt;borntraeger@de.ibm.com&gt;Subject: [Qemu-dev...", "content": "From: \"Jason J. Herne\" &lt;jjherne@linux.vnet.ibm.com&gt;To: \"qemu-devel@nongnu.org qemu-devel\" &lt;qemu-devel@nongnu.org&gt;,\tChristian Borntraeger &lt;borntraeger@de.ibm.com&gt;Subject: [Qemu-devel] Migration auto-converge problemDate: Mon, 02 Mar 2015 16:04:54 -0500We have a test case that dirties memory very very quickly. When we run thistest case in a guest and attempt a migration, that migration never convergeseven when done with auto-converge on. 我们有一个测试用例会非常快速地修改内存。当我们在虚拟机中运行此测试并尝试迁移时，即使开启了auto-converge 功能，迁移仍然无法收敛。The auto converge behavior of Qemu functions differently purpose than I hadexpected. In my mind, I expected auto converge to continuously apply adaptivethrottling of the cpu utilization of a busy guest if Qemu detects that progressis not being made quickly enough in the guest memory transfer. The idea is thata guest dirtying pages too quickly will be adaptively slowed down by thethrottling until migration is able to transfer pages fast enough to completethe migration within the max downtime. Qemu’s current auto converge does notappear to do this in practice. QEMU 的自动收敛行为与我预期的有所不同。我的设想是，当 QEMU 检测到虚拟机内存传输进展不足时，auto-converge 会持续 自适应 地限制虚拟机的 CPU 利用率。这样，对于那些脏页速度过快的虚拟机，自适应的限速将逐步降低其速度，直到迁移速度足够快，能在最大停机时间内完成迁移。然而，QEMU 当前的自动收敛机制在实际操作中似乎并未实现这一点。A quick look at the source code shows the following: 查看源码后，逻辑如下: Autoconverge keeps a counter. This counter is only incremented if, for acompleted memory pass, the guest is dirtying pages at a rate of 50% (or more)of our transfer rate. The counter only increments at most once per pass through memory. The counter must reach 4 before any throttling is done. (a minimum of 4memory passes have to occur) - Once the counter reaches 4, it is immediatelyreset to 0, and then throttling action is taken. - Throttling occurs by doingan async sleep on each guest cpu for 30ms, exactly one time. 自动收敛维持一个计数器。仅当一个内存传输周期完成时，虚拟机脏页速率达到传输速率的 50% 或以上，计数器才会增加。 每个内存传输周期内计数器最多增加一次。 计数器需达到 4 才会触发限速（即至少经历 4 个内存传输周期）– 一旦计数器达到 4，它会立即重置为 0，然后执行限速操作。 Now consider the scenario auto-converge is meant to solve (I think): A guesttouching lots of memory very quickly. Each pass through memory is going to besending a lot of pages, and thus, taking a decent amount of time to complete.If, for every four passes, we are only sleeping the guest for 30ms, our guestis still going to be able dirty pages faster than we can transfer them. We willnever catch up because the sleep time relative to guest execution time is veryvery small. 现在考虑 auto-converge 所要解决的场景（我认为）：虚拟机快速修改大量内存。在这种情况下，每次内存传输周期都会涉及大量页传输，因此需要相当长的时间才能完成。如果每四个周期我们仅将虚拟机暂停 30 毫秒，那么虚拟机仍然可以比传输速度更快地修改页。我们永远无法赶上，因为相对于虚拟机的执行时间，暂停时间非常短。Auto converge, as it is implemented today, does not address the problem Iexpect it solve. However, after rapid prototyping a new version of autoconverge that performs adaptive modeling I’ve learned something. The workloadI’m attempting to migrate is actually a pathological case. It is an excellentexample of why throttling cpu is not always a good method of limiting memoryaccess. In this test case we are able to touch over 600 MB of pages in 50 ms ofcontinuous execution. In this case, even if I throttle the guest to 5% (50msruntime, 950ms sleep) we still cannot even come close to catching up even witha fairly speedy network link (which not every user will have). 当前 auto-converge 的实现并未解决我预期的问题。然而，在快速原型实现了一个执行自适应建模的新版本后，我了解到了一些情况。实际上，我尝试迁移的工作负载是一个“病理”案例，它很好地说明了限速 CPU 并非限制内存访问的理想方法。在这个测试用例中，虚拟机在 50 毫秒连续运行时间内可以修改超过 600 MB 的页。即便将虚拟机限速至 5%（50 毫秒运行，950 毫秒暂停），我们仍无法赶上数据传输，即便使用了较高速度的网络连接（并不是所有用户都有这样的条件）。Given the above, I believe that some workloads touch memory too fast and we’llnever be able to live migrate them with auto-converge. On the lower end thereare workloads that have a very small/stagnant working set size which will belive migratable without the need for auto-converge. Lastly, we have “thenebulous middle”. These are workloads that would benefit from auto-convergebecause they touch pages too fast for migration to be able to deal with them,AND (important conditional here), throttling will(may?) actually reduce theirrate of page modifications. I would like to try and define this “middle” set ofworkloads. 基于上述情况，我认为有些工作负载的内存修改速度过快，使用 auto-converge 无法实现实时迁移。在低端情况中，工作负载的工作集很小且变化不大，无需 auto-converge 就可以迁移成功。最后是所谓的“模糊中间层”。这类工作负载能够受益于 auto-converge，因为它们的内存修改速度超出了迁移处理能力，并且（这是个关键条件）限速可能（？）会降低其页修改速率。我希望能够定义这一“中间层”工作负载的特征。A question with no obvious answer: How much throttling is acceptable? If I haveto throttle a guest 90% and he ends up failing 75% of whatever transactions heis attempting to process then we have quite likely defeated the entire purposeof “live” migration. Perhaps it would be better in this case to just stop theguest and do a non-live migration. Maybe by reverting to non-live we actuallysave time and thus more transactions would have completed. This one may takesome experimenting to be able to get a good idea for what makes the most sense.Maybe even have max throttling be be user configurable. 一个没有明确答案的问题：究竟多少限速是可以接受的？如果我需要对虚拟机限速 90%，而它最终有 75% 的事务处理失败，那我们很可能已经完全违背了“实时”迁移的初衷。在这种情况下，也许直接停止虚拟机并进行非实时迁移会更好。或许通过回退到非实时迁移，我们实际上节省了时间，从而完成了更多的事务。这一点可能需要一些实验来更好地了解最合理的策略。或许最大限速应让用户自行配置。With all this said, I still wonder exactly how big this “nebulous middle”really is. If, in practice, that “middle” only accounts for 1% of the workloadsout there then is it really worth spending time fixing it? Keep in mind this isa two pronged test: 在这一切的前提下，我仍然想知道，这个“模糊中间层”究竟有多大。如果在实际操作中，这个“中间层”只占所有工作负载的 1%，那么是否真的值得花费时间去优化呢？请记住，这是一个双重(two pronged)测试： Guest cannot migrate because it changes memory too fast Cpu throttling slows guest’s memory writes down enough such that he can nowmigrate 虚拟机无法迁移，因为它修改内存的速度太快。 通过限速虚拟机的 CPU，降低了虚拟机的内存写入速度，从而使得它现在可以迁移。 I’m interested in any thoughts anyone has. Thanks!other commentJohn SnowThis is just a passing thought since I have not invested deeply in the live migration convergence mechanisms myself, but: passing thought: 随意的想法，随便的想法 这只是一个随便的想法，因为我自己没有深入研究实时迁移收敛机制，但是：Is it possible to apply a progressively more brutish throttle to a guest if we detect we are not making (or indeed /losing/) progress? 如果我们检测到没有取得进展（甚至是在“失去”进展），是否可以对虚拟机应用逐渐加剧的强力限速？We could start with no throttle and see how far we get, then progressively apply a tighter grip on the VM until we make satisfactory progress, then continue on until we hit our “Just pause it and ship the rest” threshold. 我们可以从不限制开始，看看能进行多远，然后逐步对虚拟机施加更紧的限制，直到取得满意的进展，然后继续进行，直到达到“就暂停它然后传输剩下的部分”阈值。That way we allow ourselves the ability to throttle very naughty guests very aggressively (To the point of effectively even paused) without disturbing the niceness of our largely idle guests. In this way, even very high throttle caps should be acceptable. 通过这种方式，我们允许在不打扰大多数空闲虚拟机的情况下，对那些“顽皮”的虚拟机施加非常强烈的限速（甚至可能暂停）。这样，即便是非常高的限速上限也应该是可以接受的。This will allow live migration to “fail gracefully” for cases that are modifying memory or disk just too absurdly fast back to essentially a paused migration. 这将允许实时迁移在那些修改内存或磁盘速度过快的情况下“优雅地失败”，实际上变成暂停状态的迁移。I’ll leave it to the migration wizards to explain why I am foolhardy.–js 我将把它留给迁移专家来解释为什么我这么做太愚蠢。相关链接原文链接" }, { "title": "live migration", "url": "/posts/live-migration-workflow/", "categories": "live_migration", "tags": "live_migration", "date": "2024-11-12 23:20:00 +0800", "snippet": "热迁移简述热迁移(live migration) 可以在虚拟机正在RUNNING时，对用户透明的从source host 迁移到dest host. 涉及迁移对象种类 热迁移的流程会大概包含几个对象: cpu 内存 设备 主要工作 而热迁移主要工作是将这几个对象的信息，从原端 copy到目的端，并且做好sync...", "content": "热迁移简述热迁移(live migration) 可以在虚拟机正在RUNNING时，对用户透明的从source host 迁移到dest host. 涉及迁移对象种类 热迁移的流程会大概包含几个对象: cpu 内存 设备 主要工作 而热迁移主要工作是将这几个对象的信息，从原端 copy到目的端，并且做好sync工作。 由于不停机vm，vcpu还会更改一些对象状态。例如: 内存，可能在迁移完一个page后，该page由于vcpu还在跑, 有可能又有更改。这时，qemu还需要track到该page，并完成对该page的再一次的迁移。 如何做到避免在热迁移过程中影响vcpu 迁移线程和vcpu线程是不同线程, 所以热迁移时，qemu进程会新增一个进程。 如何评价一个热迁移流程的质量 downtime: 热迁移过程中，虚拟机暂停的时间 migration total time: 迁移总时间 vm performance during migration: 迁移过程中虚拟机运行效率 对象分类 对于热迁移的对象来说，主要分为两类 对象传输数据量大(典型内存) 对象传输数据量小(典型cpu apic) 为什么要这样分呢? 假设, 在某个环境下, 虚拟机内存为2G , 而网络传输2G的数据需要60s.而CPU apic的传输数据仅为4096, 传输时间 0.0001s。这两个对象都会在vm running时频繁改变，但是如果要将内存迁移完全放到虚拟机暂停之后，在传输。虚拟机内的服务可能没有办法接受，但是对于CPU而言由于数据量小，vm可能能接受该停机时间。 热迁移还有一些限制条件, e.g.: 对存储有一定的限制: 要使用共享存储，例如nbd,nfs 两端的CPU类型要一致 两端虚拟化相关的software要一致，例如KVM, QEMU, ROM等等. 两端vm的machine-type, cpuid要一致。我们接下来，结合代码流程分析细节.热迁移主要流程分析迁移对象注册上面提到过，迁移过程可能涉及一些对象。qemu定义了 SaveStateEntry数据结构来描述每一个迁移对象:typedef struct SaveStateEntry { QTAILQ_ENTRY(SaveStateEntry) entry; char idstr[256]; uint32_t instance_id; int alias_id; int version_id; /* version id read from the stream */ int load_version_id; int section_id; /* section id read from the stream */ int load_section_id; const SaveVMHandlers *ops; const VMStateDescription *vmsd; void *opaque; CompatEntry *compat; int is_ram;} SaveStateEntry; entry: 用户链接每个迁移对象 idstr: 唯一标识该对象 instance_id: 表示设备实例编号 …id: 先ignore ops, vmsd: 下面详细介绍 opaque: 模块注册时，提供给热迁移过程中用到的结构体 is_ram: is ram or not ?上面提到过，对象主要分为两类, 一种是热迁移过程中需要一直sync的。另一种是可以在虚拟机暂停时，一次传输完成的。第一种会准备一个SaveVMHandlers, 存放到SaveStateEntry中的ops成员中。在热迁移的几个阶段来调用。第二种会准备一个VMStateDescription,存放在SaveStateEntry中的vmsd,该函数也会有一些回调。(!!!)两类的注册流程如下, 以内存和apic为例ram_mig_init register_savevm_live { ops = savevm_ram_handlers, opaque = ram_state }apic_common_realize vmstate_register_with_alias_id { vmsd = vmstate_apic_common, opaque = APICCommonState }ram_mig_init -- SaveStateEntry(mem) ---+ \\apic_common_realize --SaveStateEntry(apic) -----+---- link to savevm_state.handlers第一类相关的，我们在下面称为T_ram, 而第二类相关的，我们称为T_apic迁移线程上面提到过，为了避免对vcpu的性能产生影响，qemu创建了一个单独的migration thread来做热迁移工作。我们来看下相关堆栈:在qemu monitor 中输入migrate 命令后:hmp_migrate =&gt; qmp_migrate =&gt; if (channels) addr = channels-&gt;value-&gt;addr //获取到dest channel addr //仅分析tcp =&gt; socket_start_outgoing_migration =&gt; qio_channel_socket_connect_async =&gt; socket_outgoing_migration =&gt; migration_channel_connect =&gt; migrate_fd_connect //创建迁移线程 =&gt; qemu_thread_create -- migration_thread迁移线程migration_thread中调用函数流程如下:migration_thread # NOTE # # 下面的save的意思，就是迁移，将source端的数据copy并存储 # 到目的端 # # T_ram 和一些公共流程，我们用1,2,3标出 # T_apic 我们用1(t_apic),2(t_apic)...标出 # =&gt; qemu_savevm_state_header # # T_ram: 1.完成迁移前的准备工作 =&gt; qemu_savevm_state_setup =&gt; for_each(savevm_state.handlers) =&gt; if (vmsd-&gt;early_setup) vmstate_save() continue =&gt; se-&gt;ops-&gt;save_setup() # 2. 热迁移主流程，在里面会进行持续循环，直到状态满足要求 =&gt; foreach(s-&gt;state == ms_ACTIVE || ms_POSTCOPY_ACTIVE) =&gt; migration_iteration_run(简单描述可能执行到的函数) # 3. 计算本轮还要 copy的数量粗略的 =&gt; qemu_savevm_state_pending_estimate =&gt; for_each(savevm_state.handlers) se-&gt;ops-&gt;state_pending_estimate() # 4. 将pending_size &lt; s-&gt;threshold_size时，需要 # 精细的获取下还要copy的数量 =&gt; if (pending_size &lt; s-&gt;threshold_size) { =&gt; qemu_savevm_state_pending_exact() =&gt; for_each(savevm_state.handlers) se-&gt;ops-&gt;state_pending_exact() } =&gt; # 7. 如果真的达到了s-&gt;threshold_size, 则认为可以暂停虚拟机了 # 然后将剩下的信息一次性copy完 migration_completion() =&gt; migration_completion_precopy =&gt; migration_stop_vm =&gt; vm_stop_force_state =&gt; vm_stop =&gt; do_vm_stop =&gt; pause_all_vcpus =&gt; vm_state_notify =&gt; bdrv_drain_all =&gt; bdrv_flush_all =&gt; qemu_savevm_state_complete_precopy # 7.1 将剩余的全部save完 =&gt; qemu_savevm_state_complete_precopy_iterable =&gt; foreach(savevm_state.handlers) =&gt; se-&gt;ops-&gt;save_live_complete_precopy() # 7.2(t_apic) # 在该流程中，我们将T_apic类型的对象全部迁移完，注意 # 此时，vcpu已经全部pause了。 =&gt; qemu_savevm_state_complete_precopy_non_iterable =&gt; foreach(savevm_state.handlers) vmstate_save() vmstate_save_state_with_err =&gt; vmstate_save_state_v =&gt; vmsd-&gt;pre_save() =&gt; !!进行vmsd递归!! OR field-&gt;info-&gt;put() =&gt; vmsd-&gt;post_save() # (t_apic)对每一个subsection做savestate =&gt; vmstate_subsection_save =&gt; foreach(subsection) =&gt; vmstate_save_state_with_err =&gt; OR: migration_completion_postcopy # 5. 进行实际的数据save =&gt; qemu_savevm_state_iterate() =&gt; for_each(savevm_state.handlers) se-&gt;ops-&gt;save_live_iterate() # 6. 会根据带宽, 用户允许的downtime来更新 热迁移过程中的一些条件和限制信息， # e.g., threshold_size, pages_per_second =&gt; urgent = migration_rate_limit(); # END. 8. 热迁移结束， cleanup资源 =&gt; migration_iteration_finish =&gt; switch s-&gt;state ... do something =&gt; migration_bh_schedule(migrate_fd_cleanup_bh,...) =&gt; migrate_fd_cleanup该流程比较复杂，我们按照下面的条目进行展开: qemu 热迁移传输 ram::save_setupqemu 热迁移传输qemu使用MigrationState表示当前热迁移的状态, 其中struct MigrationState { ... QEMUFile *to_dst_file; ... JSONWriter *vmdesc; ...}; to_dst_file: src和dst通信文件fd, src write，source read vmdesc: qemu发送数据都是json格式, 将所要发送的json信息，存储到vmdesc.在migration_thread() 首先调用qemu_savevm_state_header()函数, 将迁移数据的头信息发送出去:void qemu_savevm_state_header(QEMUFile *f){ MigrationState *s = migrate_get_current(); //新创建一个writer s-&gt;vmdesc = json_writer_new(false); trace_savevm_state_header(); //发送 MAGIC, VERSON qemu_put_be32(f, QEMU_VM_FILE_MAGIC); qemu_put_be32(f, QEMU_VM_FILE_VERSION); //如果需要发送configuration, 则会讲`vmstate_configuration` //相关数据发送 if (s-&gt;send_configuration) { qemu_put_byte(f, QEMU_VM_CONFIGURATION); json_writer_start_object(s-&gt;vmdesc, NULL); json_writer_start_object(s-&gt;vmdesc, \"configuration\"); vmstate_save_state(f, &amp;vmstate_configuration, &amp;savevm_state, s-&gt;vmdesc); json_writer_end_object(s-&gt;vmdesc); }}但是对于某些数据，其没有字段这样的信息（没有vmsd), 这时，就没有必要用json传输。我们下面会看到.ram:: save_setupram_save_setup# save ram总大小=&gt; ram_init_all =&gt; ram_init_bitmaps =&gt; ram_list_init_bitmaps =&gt; foreach RAMBlock # 新申请一个bmap, 并且bitmap_set全部设置为1, # 表示所有页都是脏的，需要全部copy到目的端 =&gt; block-&gt;bmap = bitmap_new() =&gt; bitmap_set(block-&gt;bmap, 0, pages) =&gt; block-&gt;clear_bmap() =&gt; memory_global_dirty_log_start =&gt; set global_dirty_tracking bit GLOBAL_DIRTY_MIGRATION =&gt; memory_region_transaction_commit =&gt; flatview_reset() =&gt; flatview_init() =&gt; foreach(as) =&gt; physmr = memory_region_get_flatview_root(as-&gt;root); =&gt; generate_memory_topology(physmr); =&gt; render_memory_region() ## 根据新的拓扑，更新flatview，而在 ## 该流程中，实际上只是FlatRange的 ## dirty_log_mask需要更改 =&gt; fr.dirty_log_mask = memory_region_get_dirty_log_mask(mr); =&gt; if (global_dirty_tracking &amp;&amp; (qemu_ram_is_migratable(rb) ||memory_region_is_iommu(mr)) =&gt; return mr-&gt;dirty_log_mask | (1 &lt; &lt; DIRTY_MEMORY_MIGRATION) =&gt; address_space_set_flatview ## new view `dirty_log_mask` has =&gt; address_space_update_topology_pass ## 如果两个flatview完全一样 =&gt; compare oldview and newview every ranges[] =&gt; if (frold &amp;&amp; frnew &amp;&amp; flatrange_equal(frold, frnew)) ## 需要看下是否是dirty_log_mask改变 ## 如果是新增 dirty_log_mask =&gt; if (frnew-&gt;dirty_log_mask &amp; ~frold-&gt;dirty_log_mask) =&gt; call all memorylisteners log_start() =&gt; kvm_log_start ## 如果是减少 dirty_log_mask =&gt; if (frold-&gt;dirty_log_mask &amp; ~frnew-&gt;dirty_log_mask) =&gt; call all memorylisteners log_stop() =&gt; kvm_log_stop=&gt; qemu_put_be64(f, ram_bytes_total_with_ignored() | RAM_SAVE_FLAG_MEM_SIZE);# 遍历每一个memblock=&gt; foreach(block) =&gt; qemu_put_byte(f, strlen(block-&gt;idstr)); qemu_put_buffer(f, (uint8_t *)block-&gt;idstr, strlen(block-&gt;idstr)); # 当前使用了的mem大小 qemu_put_be64(f, block-&gt;used_length); =&gt; 根据不同内存类型，以及迁移方式进行不同的save =&gt; if # postcopy 并且block-&gt;page_size 当前block-&gt;page_size 和 max_hg_page_size # 不相同, 需要save page_size（为什么postcopy原因未知） migrate_postcopy_vm() &amp;&amp; block-&gt;page_size != max_hg_page_size) qemu_put_be64(f, block-&gt;page_size); migrate_ignore_shared() # ignore shared 不copy memory， 所以仅把首地址传递过去就可以了 qemu_put_be64(f, block-&gt;mr-&gt;addr); migrate_mapped_ram() mapped_ram_setup_ramblock() { }=&gt; rdma_registration_start(f, RAM_CONTROL_SETUP);=&gt; rdma_registration_stop(f, RAM_CONTROL_SETUP);# 根据是否开启了multifd, 选择 save ram 的 方法=&gt; if migrate_multifd =&gt; multifd_ram_save_setup(); =&gt; migration_ops-&gt;ram_save_target_page = ram_save_target_page_multifd;=&gt; NO migrate_multifd =&gt; migration_ops-&gt;ram_save_target_page = ram_save_target_page_legacy;=&gt; multifd_ram_flush_and_sync()# FLAG_EOS 表示本次写入结束=&gt; qemu_put_be64(f, RAM_SAVE_FLAG_EOS);=&gt; qemu_fflush(f) 将信息flush, 也就是发送到目的端总结下，该流程一共有几件事: 调用 log_start 通知各个memorylistener 要记录dirty log 将一些基本信息发送到dist 端 做一些multifd, 以及rdma相关初始化kvm_log_startkvm_log_start流程比较简单, 主要有: 申请dirty_bitmap 更新KVMSlots flags, 重新提交 memslots-&gt;kvm流程如下:kvm_log_start =&gt; kvm_section_update_flags =&gt; get slot by mr_section =&gt; kvm_slot_update_flags =&gt; KVMSlot-&gt;flags = kvm_mem_flags() # 更新KVMSlots flags =&gt; memory_region_get_dirty_log_mask =&gt; return flags |= KVM_MEM_LOG_DIRTY_PAGES =&gt; kvm_slot_init_dirty_bitmap =&gt; mem-&gt;dirty_bitmap = g_malloc() # 申请dirty_bitmap =&gt; mem-&gt;dirty_bmap_size = xxx; =&gt; kvm_set_user_memory_region =&gt; kvm_vm_ioctl(,KVM_SET_USER_MEMORY_REGION,); # 重新提交给KVM内存信息send将一些内存的基本信息息，例如: 内存总大小， RAMBlock相关信息发送到dst端，并且做一些multifd, 以及rdma 相关流程的初始化我们下面看下，具体的RAMBlock setup的流程:static void mapped_ram_setup_ramblock(QEMUFile *file, RAMBlock *block){ g_autofree MappedRamHeader *header = NULL; size_t header_size, bitmap_size; long num_pages; //===(1)=== header = g_new0(MappedRamHeader, 1); header_size = sizeof(MappedRamHeader); //===(2)=== num_pages = block-&gt;used_length &gt;&gt; TARGET_PAGE_BITS; bitmap_size = BITS_TO_LONGS(num_pages) * sizeof(unsigned long); /* * Save the file offsets of where the bitmap and the pages should * go as they are written at the end of migration and during the * iterative phase, respectively. */ block-&gt;bitmap_offset = qemu_get_offset(file) + header_size; block-&gt;pages_offset = ROUND_UP(block-&gt;bitmap_offset + bitmap_size, MAPPED_RAM_FILE_OFFSET_ALIGNMENT); //==(2.1)== header-&gt;version = cpu_to_be32(MAPPED_RAM_HDR_VERSION); header-&gt;page_size = cpu_to_be64(TARGET_PAGE_SIZE); header-&gt;bitmap_offset = cpu_to_be64(block-&gt;bitmap_offset); header-&gt;pages_offset = cpu_to_be64(block-&gt;pages_offset); qemu_put_buffer(file, (uint8_t *) header, header_size); //===(3)=== /* prepare offset for next ramblock */ qemu_set_offset(file, block-&gt;pages_offset + block-&gt;used_length, SEEK_SET);} 创建一个MappedRamHeader其中包含一些基本信息，例如 version: version page_size: 当前RAMBlock的 page_size bitmap_offset: 记录当前block的bitmap_offset在file中的偏移 pages_offset: 传出page 的地址 每个RAMBlock都有一个自己的bitmap(mem, bitmap每一个bit记录着，该index的page是否是dirty的. 此处先算出有多少个page，然后在算出bitmap的大小。 设置设置offset, 为下一个RAMBlock的首地址。我们用图来解释下:page[] 数组中的空白部分是空洞。这部分传输不占用传输时的带宽。ram::ram_state_pending_estimate该函数，只是粗略估计当前还剩余的要copy的dirty page。 估计值偏小static void ram_state_pending_estimate(void *opaque, uint64_t *must_precopy, uint64_t *can_postcopy){ RAMState **temp = opaque; RAMState *rs = *temp; //===(1)=== uint64_t remaining_size = rs-&gt;migration_dirty_pages * TARGET_PAGE_SIZE; //===(2)=== if (migrate_postcopy_ram()) { /* We can do postcopy, and all the data is postcopiable */ *can_postcopy += remaining_size; } else { *must_precopy += remaining_size; }} 根据当前的migration_dirty_page计算还剩余数据需要传输 根据postcopy/precopy 来选择，加到哪个出参中。ram::ram_state_pending_exact该函数，用来精确计算remain save的dirtypage 数量, 达到精确的方法是，sync下KVM传递下来的dirty bitmap, 见(1)static void ram_state_pending_exact(void *opaque, uint64_t *must_precopy, uint64_t *can_postcopy){ RAMState **temp = opaque; RAMState *rs = *temp; uint64_t remaining_size; if (!migration_in_postcopy()) { bql_lock(); WITH_RCU_READ_LOCK_GUARD() { //==(1)== migration_bitmap_sync_precopy(false); } bql_unlock(); } remaining_size = rs-&gt;migration_dirty_pages * TARGET_PAGE_SIZE; if (migrate_postcopy_ram()) { /* We can do postcopy, and all the data is postcopiable */ *can_postcopy += remaining_size; } else { *must_precopy += remaining_size; }}来看下migration_bitmap_sync_precopy整体逻辑:migration_bitmap_sync_precopy=&gt; precopy_notify(PRECOPY_NOTIFY_BEFORE_BITMAP_SYNC, &amp;local_err)=&gt; migration_bitmap_sync # ==(2.1)== # 此处是第一轮iter时，rs-&gt;time_last_bitmap_sync才会为0 =&gt; if !rs-&gt;time_last_bitmap_sync =&gt; rs-&gt;time_last_bitmap_sync = qemu_clock_get_ms(QEMU_CLOCK_REALTIME); =&gt; memory_global_dirty_log_sync =&gt; memory_region_sync_dirty_bitmap # ==(1)== # 通知各个memroy listener =&gt; foreach(memory_listeners) { =&gt; if listener-&gt;log_sync =&gt; foreach(flatview) =&gt; listener-&gt;log_sync() =&gt; else if listener-&gt;log_sync_global =&gt; foreach(flatview) =&gt; listener-&gt;log_sync_global() } =&gt; foreach(RAMBlock) =&gt; ramblock_sync_dirty_bitmap(rs, block) # ==(2.2)== =&gt; end_time = qemu_clock_get_ms(QEMU_CLOCK_REALTIME); # ==(2.3)== =&gt; if (end_time &gt; rs-&gt;time_last_bitmap_sync + 1000) { migration_trigger_throttle(rs); # ==(3)== migration_update_rates(rs, end_time); rs-&gt;time_last_bitmap_sync = end_time; }=&gt; precopy_notify(PRECOPY_NOTIFY_AFTER_BITMAP_SYNC, &amp;local_err) memory_global_dirty_log_sync会通知各个memory listener, 告诉他们要去做log sync。我们会在后面的章节, 介绍和kvm相关的log_sync函数, 这里我们只需要知道, log_sync的作用就是将内核统计的dirty page 的相关信息，同步到qemu侧. 该部分和auto-coverage 热迁移优化相关，在脏页频率比较高的情况下，限制脏页产生速率从而达到收敛的状态converage. 具体做法是， 自动降低vcpu的CPU使用率，来降低该vcpu产生脏页的速度 这里的条件也是, 本轮和上一轮时间差距1s的情况下，认为本轮发送的dirty page过于多。 和xbzrle(XOR-Based zero Run-length Encoding 一个压缩算法)相关, 指在带宽不足的情况下, 将内存进行压缩传输，从而提升压缩效率 上面两种迁移优化的策略, 我们会在后面的章节中介绍 " }, { "title": "Mce", "url": "/posts/mce/", "categories": "", "tags": "", "date": "2024-10-21 00:00:00 +0800", "snippet": "16.1 MACHINE-CHECK ARCHITECTUREThe Pentium 4, Intel Xeon, Intel Atom, and P6 family processors implement amachine-check architecture that provides a mechanism for detecting andreporting hardware (m...", "content": "16.1 MACHINE-CHECK ARCHITECTUREThe Pentium 4, Intel Xeon, Intel Atom, and P6 family processors implement amachine-check architecture that provides a mechanism for detecting andreporting hardware (machine) errors, such as: system bus errors, ECC errors,parity errors, cache errors, and TLB errors. It consists of a set ofmodel-specific registers (MSRs) that are used to set up machine checking andadditional banks of MSRs used for recording errors that are detected. Pentium 4、Intel Xeon、Intel Atom 和 P6 系列处理器实现了一种机器检查架构（Machine-Check Architecture，MCA），该架构提供了一种检测和报告硬件（机器）错误的机制，例如：系统总线错误、ECC（纠错码）错误、奇偶校验错误、缓存错误以及 TLB（转换后备缓冲区）错误。它由一组特定型号的寄存器（MSRs）组成，这些寄存器用于设置机器检查功能，以及用于记录检测到的错误的额外 MSR 组。The processor signals the detection of an uncorrected machine-check error bygenerating a machine-check excep- tion (#MC), which is an abort classexception. The implementation of the machine-check architecture does notordinarily permit the processor to be restarted reliably after generating amachine-check exception. However, the machine-check-exception handler cancollect information about the machine-check error from the machine-check MSRs. 处理器通过生成机器检查异常（#MC）来信号通知检测到无法纠正的机器检查错误，这是一个中止类异常。机器检查架构的实现通常不允许处理器在产生机器检查异常后可靠地重新启动。然而，机器检查异常处理程序可以从机器检查的MSR 中收集有关该错误的信息。Starting with 45 nm Intel 64 processor on which CPUID reportsDisplayFamily_DisplayModel as 06H_1AH; see the CPUID instruction in Chapter 3,“Instruction Set Reference, A-L,” in the Intel® 64 and IA-32 ArchitecturesSoftware Developer’s Manual, Volume 2A. The processor can report information oncorrected machine-check errors and deliver a programmable interrupt forsoftware to respond to MC errors, referred to as corrected machine-check errorinterrupt (CMCI). See Section 16.5 for details. 从 45nm 的 Intel 64 处理器开始，如果 CPUID 报告的 DisplayFamily_DisplayModel 为 06H_1AH，处理器能够报告已纠正的机器检查错误，并通过可编程中断将这些错误通知软件，以响应机器检查错误（MC 错误）。这种中断被称为已纠正的机器检查错误中断（CMCI）。有关详细信息，请参见《Intel® 64 和 IA-32 架构软件开发者手册》第 2A 卷第 3 章中的CPUID 指令部分，以及第 16.5 节的相关说明。Intel 64 processors supporting machine-check architecture and CMCI may alsosupport an additional enhance- ment, namely, support for software recovery fromcertain uncorrected recoverable machine check errors. See Section 16.6 fordetails. 支持机器检查架构（MCA）和 CMCI（已纠正的机器检查错误中断）的 Intel 64 处理器可能还支持一种额外的增强功能，即从某些未纠正但可恢复的机器检查错误中进行软件恢复。有关详细信息，请参见第 16.6 节。16.2 COMPATIBILITY WITH PENTIUM PROCESSORThe Pentium 4, Intel Xeon, Intel Atom, and P6 family processors support andextend the machine-check exception mechanism introduced in the Pentiumprocessor. The Pentium processor reports the following machine-check errors: Pentium 4、Intel Xeon、Intel Atom 和 P6 系列处理器支持并扩展了在 Pentium 处理器中引入的机器检查异常机制。Pentium 处理器报告以下机器检查错误： Data parity errors during read cycles. 读取周期中的数据奇偶校验错误。 Unsuccessful completion of a bus cycle. 总线周期的未成功完成。 The above errors are reported using the P5_MC_TYPE and P5_MC_ADDR MSRs(implementation specific for the Pentium processor). Use the RDMSR instructionto read these MSRs. See Chapter 2, “Model-Specific Registers (MSRs)‚” in theIntel® 64 and IA-32 Architectures Software Developer’s Manual, Volume 4, forthe addresses. 上述错误使用 P5_MC_TYPE 和 P5_MC_ADDR MSR（特定于 Pentium 处理器的实现）进行报告。可以使用 RDMSR 指令读取这些 MSR。The machine-check error reporting mechanism that Pentium processors use issimilar to that used in Pentium 4, Intel Xeon, Intel Atom, and P6 familyprocessors. When an error is detected, it is recorded in P5_MC_TYPE andP5_MC_ADDR; the processor then generates a machine-check exception (#MC). Pentium 处理器使用的机器检查错误报告机制与 Pentium 4、Intel Xeon、IntelAtom 和 P6 系列处理器使用的机制类似。当检测到错误时，它会记录在 P5_MC_TYPE 和 P5_MC_ADDR 中；然后处理器会生成 machine-check exception (#MC）。See Section 16.3.3, “Mapping of the Pentium Processor Machine-Check Errors tothe Machine-Check Architecture,” and Section 16.10.2, “Pentium ProcessorMachine-Check Exception Handling,” for information on compatibility betweenmachine-check code written to run on the Pentium processors and code written torun on P6 family processors. 有关与 Pentium 处理器上运行的机器检查代码和在 P6 系列处理器上运行的代码兼容性的信息，请参见第 16.3.3 节“Pentium 处理器机器检查错误映射到机器检查架构”和第 16.10.2 节“Pentium 处理器机器检查异常处理”。16.3 MACHINE-CHECK MSRSMachine check MSRs in the Pentium 4, Intel Atom, Intel Xeon, and P6 familyprocessors consist of a set of global control and status registers and severalerror-reporting register banks. See Figure 16-1. Pentium 4、Intel Atom、Intel Xeon 和 P6 系列处理器中的机器检查 MSR（模型特定寄存器）由一组全局控制和状态寄存器以及多个错误报告寄存器组组成。请参见图 16-1。Each error-reporting bank is associated with a specific hardware unit (or groupof hardware units) in the processor. Use RDMSR and WRMSR to read and to writethese registers 每个错误报告寄存器组与处理器中的特定硬件单元（或硬件单元组）相关联。可以使用 RDMSR 和 WRMSR 指令来读取和写入这些寄存器。16.3.1 Machine-Check Global Control MSRsThe machine-check global control MSRs include the IA32_MCG_CAP,IA32_MCG_STATUS, and optionally IA32_MC- G_CTL and IA32_MCG_EXT_CTL. SeeChapter 2, “Model-Specific Registers (MSRs),” in the Intel® 64 and IA-32Architectures Software Developer’s Manual, Volume 4, for the addresses of theseregisters.16.3.1.1 IA32_MCG_CAP MSRThe IA32_MCG_CAP MSR is a read-only register that provides information aboutthe machine-check architecture of the processor. Figure 16-2 shows the layoutof the register. IA32_MCG_CAP MSR 是一个只读寄存器，提供有关处理器机器检查架构的信息。图 16-2 显示了该寄存器的布局。" }, { "title": "live migration", "url": "/posts/qemu-mem/", "categories": "qemu, mm", "tags": "qemu-mm", "date": "2024-10-16 10:00:00 +0800", "snippet": "struct内存初始化流程初始化MemoryRegionmain=&gt; qemu_init =&gt; qemu_create_machine =&gt; qmp_x_exit_preconfig =&gt; qemu_init_board =&gt; machine_run_board_init =&gt; create_de...", "content": "struct内存初始化流程初始化MemoryRegionmain=&gt; qemu_init =&gt; qemu_create_machine =&gt; qmp_x_exit_preconfig =&gt; qemu_init_board =&gt; machine_run_board_init =&gt; create_default_memdev(current_machine, mem_path, errp) =&gt; user_creatable_complete =&gt; host_memory_backend_memory_complete =&gt; ram_backend_memory_alloc =&gt; memory_region_init_ram_flags_nomigrate =&gt; pc_init_v6_2 =&gt; pc_init1qemu_create_machine ... =&gt; cpu_exec_init_all =&gt; io_mem_init =&gt; memory_map_init =&gt; system_memory { memory_region_init(system_memory, NULL, \"system\", UINT64_MAX); address_space_init(&amp;address_space_memory, system_memory, \"memory\"); } =&gt; system_io { memory_region_init_io(system_io, NULL, &amp;unassigned_io_ops, NULL, \"io\", 65536); address_space_init(&amp;address_space_io, system_io, \"I/O\"); }pc_init1=&gt; memory_region_init(pci_memory, NULL, \"pci\", UINT64_MAX); #if pcmc-&gt;pci_enabled=&gt; rom_memory = pci_memory=&gt; pc_memory_init(pcms, system_memory, rom_memory, hole64_size); =&gt; ram-below-4g { memory_region_init_alias(ram_below_4g, NULL, \"ram-below-4g\", machine-&gt;ram, 0, x86ms-&gt;below_4g_mem_size); memory_region_add_subregion(system_memory, 0, ram_below_4g); e820_add_entry(0, x86ms-&gt;below_4g_mem_size, E820_RAM); } =&gt; ram-above-4g #if pcmc-&gt;ram_above_4g { memory_region_init_alias(ram_above_4g, NULL, \"ram-above-4g\", machine-&gt;ram, x86ms-&gt;below_4g_mem_size, x86ms-&gt;above_4g_mem_size); memory_region_add_subregion(system_memory, x86ms-&gt;above_4g_mem_start, ram_above_4g); e820_add_entry(x86ms-&gt;above_4g_mem_start, x86ms-&gt;above_4g_mem_size, E820_RAM); } =&gt; firmware { pc_system_firmware_init(pcms, rom_memory); memory_region_init_ram(option_rom_mr, NULL, \"pc.rom\", PC_ROM_SIZE, &amp;error_fatal); memory_region_add_subregion_overlap(rom_memory, PC_ROM_MIN_VGA,option_rom_mr, 1); }虚拟机ram分配流程无论是分配rom，还是ram，都会通过memory_region_init_ram* 相关接口来分配虚拟机内存。memory_region_init_ram_flags_nomigrate=&gt; memory_region_init(mr, owner, name, size) # ==(1)===&gt; init member { mr-&gt;ram = true mr-&gt;terminate = true mr-&gt;destructor = memory_region_destructor_ram; mr-&gt;ram_block = qemu_ram_alloc(size, ram_flags, mr, &amp;err) =&gt; qemu_ram_alloc_internal() =&gt; 根据host page size以及TARGET_PAGE_SIZE最大值，确定align =&gt; 根据align向上取整 size, max_size =&gt; new_block = g_malloc0() # 相当于添加一块内存条 =&gt; init new_block member { new_block-&gt;mr new_block-&gt;resized new_block-&gt;used_length = size new_block-&gt;max_length = max_size; new_block-&gt;fd = -1 new_block-&gt;guest_memfd = -1 } =&gt; ram_block_add(new_block, &amp;local_err) # 相当于把内存条，添加到系统中 } memory_region_init() 参数: MemoryRegion *mr: 要初始化的mr的名称 Object *owner: 表示上一级的mr char *name: “pc.ram”, “pc.bios”,”pc.rom” uint64 size: mr 的 size qemu_ram_alloc_internal 参数 size: 当前的内存条大小 max_size: 该内存条最大的大小 resized: resize的callbak host: 表示虚拟机物理内存对应的QEMU进程地址空间的虚拟内存 ram_flags: mr: 该RAMBlock所属的MemoryRegion ram_block_addram_block_add=&gt; old_ram_size = last_ram_page() # 注意，这里使用的是max_length # 遍历所有RAMBlock, 找到最大的`block-&gt;offset + # block-&gt;max_length`=&gt; new_block-&gt;offset = find_ram_offset(new_block-&gt;max_length) # ==(1)== { 这个代码比较晦涩，我们总结下这个流程所干的事情。该函数最终是要返回一个offset， 该代码最终要从各个现有的`RAMBlock` gap 中找到一个最小的mingap, 然后将 返回mingap的begin。 }=&gt; if (!new_block-&gt;host) # 表示还未映射虚拟内存 { new_block-&gt;host = qemu_anon_ram_alloc( new_block-&gt;max_length, &amp;new_block-&gt;mr-&gt;align, shared, noreserve); =&gt; qemu_ram_mmap =&gt; mmap_activate =&gt; mmap memory_try_enable_merging(new_block-&gt;host, new_block-&gt;max_length); =&gt; qemu_madvise(addr, len, QEMU_MADV_MERGEABLE); # 设置该内存可以使用KSM合并 =&gt; madvise }=&gt; guest memfd 处理 # 这个先越过=&gt; new_ram_size=MAX(old_ram_size, new_block-&gt;offset + max_length)=&gt; if new_ram_size &gt; old_ram_size # ==(2)== dirty_memory_extend(old_ram_size, new_ram_size); # 如果新的ram size比old ram size大 # 则extend dirty memory, 这个和脏页bitmap有关=&gt; insert new_block to ram_list.blocks=&gt; cpu_physical_memory_set_dirty_range=&gt; madvise(...) 我们展开下这个函数代码: find_ram_offset代码展开 static ram_addr_t find_ram_offset(ram_addr_t size){ RAMBlock *block, *next_block; ram_addr_t offset = RAM_ADDR_MAX, mingap = RAM_ADDR_MAX; ... //第一层循环，遍历每个block RAMBLOCK_FOREACH(block) { ram_addr_t candidate, next = RAM_ADDR_MAX; /* Align blocks to start on a 'long' in the bitmap * which makes the bitmap sync'ing take the fast path. */ //获取当前block(L1)的end candidate = block-&gt;offset + block-&gt;max_length; candidate = ROUND_UP(candidate, BITS_PER_LONG &lt;&lt; TARGET_PAGE_BITS); /* Search for the closest following block * and find the gap. */ //第二层循环遍历block(L2), 查找队列中L2.begin - L1.end //值最小的, 其实也就类似与相邻的 RAMBLOCK_FOREACH(next_block) { if (next_block-&gt;offset &gt;= candidate) { next = MIN(next, next_block-&gt;offset); } } /* If it fits remember our place and remember the size * of gap, but keep going so that we might find a smaller * gap to fill so avoiding fragmentation. */ //需要满足，该gap的大小一定要 &gt; size(new_block-&gt;max_length) //同时去每一轮L1 循环中最小的值 if (next - candidate &gt;= size &amp;&amp; next - candidate &lt; mingap) { offset = candidate; mingap = next - candidate; } trace_find_ram_offset_loop(size, candidate, offset, next, mingap); } ... return offset} 脏页相关处理，我们放在....看(未完成) 小结本节描述的是AddressSpace的创建，以及添加MemoryRegion，以及初始化RAMBlock的流程, 通过上述流程，构建出了虚拟机的物理内存布局，QEMU这边还需要做的是，将这些地址空间, GPA space(以及QEMU VA space)传递给KVM。内存布局的提交MemoryListene上一节其实描述的是内存布局的改变过程。对于内存布局改变，可能需要通知到KVM, 来改变kvm mmu。其实内存布局的改变，不只是影响到KVM内存虚拟化模块，可能还有其他模块,例如vfio 等等，所以QEMU 抽象了一个MemoryListener机制, 在每次内存布局发生变化时，可以通知到相关MemoryListener, 而 KVM Memory Listener所干的事情， 就是通知KVM，改变GPA-&gt;HPA的映射关系.数据结构:struct MemoryListener { void (*begin)(MemoryListener *listener); void (*commit)(MemoryListener *listener); void (*region_add)(MemoryListener *listener, MemoryRegionSection *section); void (*region_del)(MemoryListener *listener, MemoryRegionSection *section); void (*region_nop)(MemoryListener *listener, MemoryRegionSection *section); ... log 相关 ... ... eventfd相关 ... unsigned priority; const char *name; AddressSpace *address_space; QTAILQ_ENTRY(MemoryListener) link; QTAILQ_ENTRY(MemoryListener) link_as;} begin: 在执行内存变更之前所需执行的函数 commit: 在执行内存变更 region_xxx: 添加, 删除region log_xxx: 脏页机制, 开启同步 priority: 优先级, 优先级低 的在 add 时会被 先调用 , del 时会被 后 调用 address_space: address_space of this listener link: 将各个MemoryListener链接起来 link_as: as表示 AddressSpace , 所以将相同AddressSpace的 Listener链接起来MemoryListener REGSITER我们以 kvm MemoryListener 为例kvm_init =&gt; kvm_memory_listener_register(s, &amp;s-&gt;memory_listener, &amp;address_space_memory, 0, \"kvm-memory\"); =&gt; memory_listener_register(&amp;kvm_io_listener, &amp;address_space_io);有两个空间的注册，一个是address_space_memory, 还有一个是address_space_io,我们以address_space_memory为例。kvm_memory_listener_register # 内存空间=&gt; assign listeners member: callbak and others { region_add = kvm_region_add region_del = kvm_region_del commit = kvm_region_commit priority = MEMORY_LISTENER_PRIORITY_ACCEL; # 10 very high ... log_xxx } =&gt;memory_listener_register()memory_listener_register具体代码void memory_listener_register(MemoryListener *listener, AddressSpace *as)=&gt; listeners-&gt;address_space = as=&gt; 链接到memory_listeners 全局list中, 按照priority 从小到大排列=&gt; 将listeners 链接到 as-&gt;listeners 链表中, 同样按照从小到大排列 =&gt; listener_add_address_space(listener, as)这里注册了一个listener, 所以需要执行对该as 的 所有region 进行region_add() update transaction由于上面的register流程，触发了一个add transaction(listener_add_address_space),将会走一个完整的transaction commmit的流程，主要分为三个动作。 begin add commit具体流程如下:listener_add_address_space=&gt; listener-&gt;begin()=&gt; if(global_dirty_tracking) listener-&gt;log_global_start()=&gt; 获取当前 flatview { address_space_get_flatview() =&gt; while(!flatview_ref(view)) view = address_space_to_flatview() =&gt; qatomic_rcu_read(&amp;as-&gt;current_map); # current_map表示当前的flatview, 并且如果有人replace as-&gt;current_map, # 则as-&gt;current_map 返回false, # flatview_ref # =&gt; qatomic_fetch_inc_nonzero() &gt; 0 atomic_fetch_inc return old value }=&gt; 遍历之前的view, 添加每一个 MemoryRegionSection listener-&gt;log_stop() listener-&gt;region_add() }=&gt; listener-&gt;commit()该流程先获取了当前的flatview(flatview可以认为是对该address space，平坦内存的视角), 因为该listener是刚创建的, 所以需要对flatview 中的各个region都需要做add动作。做完add动作后，再调用commit() callbak。我们之后章节会以kvm MemoryListener为例来看下涉及的各个callbakMemoryListener Commit上面流程是从MemoryListener 注册的流程中触发了一次add transaction commit, 而除此之外，其他流程也可能对该MemoryRegion 进行update，从而触发MemoryListener: 之后，补充vfio增加新的AddressSpace，从而触发listener 的流程这个过程通过memory_region_transaction_commit 实现memory_region_transaction_commit=&gt; depth处理=&gt; pending处理=&gt; flatview_reset()=&gt; MEMORY_LISTENER_CALL_GLOBAL(begin, Forward); { =&gt; foreach entry listener: listener-&gt;begin() }=&gt; foreach entry address space { =&gt; address_space_set_flatview() =&gt; address_space_update_ioeventfds() }=&gt; MEMORY_LISTENER_CALL_GLOBAL(commit, Forward) { =&gt; foreach entry listener: listener-&gt;commit() }先调用begin进行一些初始化工作，在遍历各个address space， 来更新flatview，同时，可能涉及 会调用memory listener的region_add(),region_del(), 再遍历各个memory listener 完成 commit回调flatview通过在qemu monitor中执行 info mtree, 可以发现用各个memory region展示的address space, 是一个树状结构，并且算上alias，应该是一个无环图.但是我们在进行MemoryListener 的commit 操作时，需要的是一个平坦视角的region 集合。而qemu的flatview 就时完成上述工作.先看相关数据结构:AddressSpace:struct AddressSpace { ... /* Accessed via RCU. */ struct FlatView *current_map; ...}; current_map: 表示该AddressSpace对应的FlatviewFlatView:struct FlatView { struct rcu_head rcu; unsigned ref; FlatRange *ranges; unsigned nr; unsigned nr_allocated; struct AddressSpaceDispatch *dispatch; MemoryRegion *root;}; FlatRange *ranges: 这里表示, 该平坦模型的每个区域 nr: FlagRange 的个数 nr_allocated: 表示已经分配的FlagRange的个数 AddressSpaceDispatch: 之后分析 MemoryRegion *root: address space 的 root mrFlatRange:struct FlatRange { MemoryRegion *mr; hwaddr offset_in_region; AddrRange addr; uint8_t dirty_log_mask; bool romd_mode; bool readonly; bool nonvolatile;}; addr.start: 该range的起点 addr.size romd_mode: readonly: 示内存范围是否为只读 romd_mode：表示该范围是否处于ROM数据模式 nonvolatile: 表示该range是否易失FlatView 和 FlatRange 数组关系: FlagView+-------+----+|ranges |+------------+ FlatRange arr|nr +-------+----++------------+ |r1 +----+|nr_allocated| +----+ |+------------+ |r2 +----+---+ +----+ | | |r3 +----+---+----+ +----+ | | | |... | | | | +----+ | | | | | | +--------++--++--+-+-+-------------------+ vm pa addrspace | |r1 |r2 |r3 | | +--------+-+-+---+---+-------------------+ | | r1.addr.start具体函数流程在generate_memory_topology:generate_memory_topology =&gt; flatview_new() # 创建一个新的view # 会递归调用，相当于把MemoryRegion # 展开，分成若干个FlagRange, 添加 # 到FlatView中 =&gt; render_memory_region(view, mr, int128_zero(), addrrange_make(int128_zero(), int128_2_64()), false, false, false); =&gt; flatview_simplify() # 将FlatView中的 FlagRange 能合并的进行合并 =&gt; address_space_dispatch_new() # 创建一个新的dispatch =&gt; foreach view-&gt;ranges { =&gt; flatview_add_to_dispatch() # 将各个FlagRange添加到dispatch中 } =&gt; address_space_dispatch_compact() =&gt; g_hash_table_replace(flat_views, mr, view);我们直接展开上面提到的一些函数:render_memory_region看一下render_memory_region参数:static void render_memory_region(FlatView *view, MemoryRegion *mr, Int128 base, AddrRange clip, bool readonly, bool nonvolatile, bool unmergeable); view: FlatView, FlatView一定是站在 AddressSpace 层级下的, 而并非某个 memory region mr: 需要展开的MemoryRegion base: 表示要展开的 MemoryRegion的的begin处在物理地址的offset clip: 表示物理地址的一个区间 readonly, nonvolatile, unmergeable : MemoryRegion attr我们展开看render_memory_region的代码, 该代码比较复杂，我们用一个, 例子来展示:目前flatview中有两个range(橙色), 然后对mr调用render_memory_region,mr中有一个子mr，为mr1, mr1-&gt;addr 不为0render_memory_region – part 1static void render_memory_region(FlatView *view, MemoryRegion *mr, Int128 base, AddrRange clip, bool readonly, bool nonvolatile, bool unmergeable){ MemoryRegion *subregion; unsigned i; hwaddr offset_in_region; Int128 remain; Int128 now; FlatRange fr; AddrRange tmp; if (!mr-&gt;enabled) { return; } //先加上base==(3.1)== int128_addto(&amp;base, int128_make64(mr-&gt;addr)); readonly |= mr-&gt;readonly; nonvolatile |= mr-&gt;nonvolatile; unmergeable |= mr-&gt;unmergeable; //跟据 base和size定义AddrRange，作为本次要操作的 //MemoryRegion的地址范围 tmp = addrrange_make(base, mr-&gt;size); // 查看这两者是否有重叠, 如果没有重叠，说明 // 该MemoryRegion中没有clip中想要的地址范围， // 无需更新FlatView // ==(1)== /* */ if (!addrrange_intersects(tmp, clip)) { return; } //取交集 // //==(2)== clip = addrrange_intersection(tmp, clip); //如果mr是alias, 该指针指向被alias的MemoryRegion. addr表示其在自己 //视角内的物理地址 而alias_offset, 则表示该截取的被引用MemoryRegion //的一个偏移(基于addr的一个偏移，所以这里先减addr，再减alias_offset) // //==(3)== if (mr-&gt;alias) { int128_subfrom(&amp;base, int128_make64(mr-&gt;alias-&gt;addr)); int128_subfrom(&amp;base, int128_make64(mr-&gt;alias_offset)); render_memory_region(view, mr-&gt;alias, base, clip, readonly, nonvolatile, unmergeable); return; } //subregion处理 /* Render subregions in priority order. */ QTAILQ_FOREACH(subregion, &amp;mr-&gt;subregions, subregions_link) { render_memory_region(view, subregion, base, clip, readonly, nonvolatile, unmergeable); } ...} 部分代码解释 addrrange_intersects代码 static bool addrrange_intersects(AddrRange r1, AddrRange r2){ return addrrange_contains(r1, r2.start) || addrrange_contains(r2, r1.start);} 判断逻辑也很简单，就是看r1中是否有r2的start，或者r2中是否有r1 的start addrrange_intersection代码 static AddrRange addrrange_intersection(AddrRange r1, AddrRange r2){ Int128 start = int128_max(r1.start, r2.start); Int128 end = int128_min(addrrange_end(r1), addrrange_end(r2)); return addrrange_make(start, int128_sub(end, start));} 取两者最大的start，和最小的end，构成一个AddrRange, 即为交集 举个例子: memory-region: pci 0000000000000000-ffffffffffffffff (prio -1, i/o): pci 00000000000c0000-00000000000dffff (prio 1, rom): pc.rom 00000000000e0000-00000000000fffff (prio 1, rom): alias isa-bios @pc.bios 0000000000020000-000000000003ffff memory-region: pc.bios 00000000fffc0000-00000000ffffffff (prio 0, rom): pc.bios 在该函数开始(3.1)给出了计算FlatView下 base的方法，就是用传过来的 base_flat=base' + mr-&gt;addr 而该base和实际查找的range又有一个offset base_region_flag = offset + base_flat 所以综合计算可得，传入的base'，应为 base'=base_flag - mr-&gt;addr = base_region_flag - mr - offset 所以，我们来看, 如果要在pci memory 中查找，e0000该如何传递呢？(这时isa-bios mr-&gt;addr = 0xe000) render_memory_regionbase' = 0xe0000 - 0x20000 - 0xfffc0000=&gt; render_memroy_region --&gt; 递归 { base = base' + mr-&gt;addr = 0xe0000 - 0x2000 - 0xfffc0000 + 0xfffc0000 = 0xe0000 } 经过上面代码，递归mr1render_memory_region – part2static void render_memory_region() { ... if (!mr-&gt;terminates) { return; } //找到clip在该region的offset offset_in_region = int128_get64(int128_sub(clip.start, base)); base = clip.start; remain = clip.size; fr.mr = mr; fr.dirty_log_mask = memory_region_get_dirty_log_mask(mr); fr.romd_mode = mr-&gt;romd_mode; fr.readonly = readonly; fr.nonvolatile = nonvolatile; fr.unmergeable = unmergeable; /* Render the region itself into any gaps left by the current view. */ for (i = 0; i &lt; view-&gt;nr &amp;&amp; int128_nz(remain); ++i) { //数组从小到大排列，这里就是找一个base &lt; range.end 的第一个range if (int128_ge(base, addrrange_end(view-&gt;ranges[i].addr))) { continue; } //查看[base, base + remain]是否和当前region有重叠, 如果有重叠, i //则先把重叠部分抠出来==(1)== //当然，不一定有重叠，也可能有个gap能彻底容纳该区域 if (int128_lt(base, view-&gt;ranges[i].addr.start)) { //找到重叠的左边界(相当于range end, base 相当于range begin)\t\t\t\t\t\t//当然，不一定有重叠 now = int128_min(remain, int128_sub(view-&gt;ranges[i].addr.start, base)); //fr在该region中的offset fr.offset_in_region = offset_in_region; //fr在FlatView中的地址 fr.addr = addrrange_make(base, now); //插入到数组合适位置 flatview_insert(view, i, &amp;fr); ++i; //移动base， offset_in_region, 减少remain int128_addto(&amp;base, now); offset_in_region += int128_get64(now); int128_subfrom(&amp;remain, now); } //这个时候需要跳过重叠部分 now = int128_sub(int128_min(int128_add(base, remain), addrrange_end(view-&gt;ranges[i].addr)), base); int128_addto(&amp;base, now); offset_in_region += int128_get64(now); int128_subfrom(&amp;remain, now); } //重叠部分跳过后，有两种情况，一种是还有range需要处理，另一种是没有了==(2)=== if (int128_nz(remain)) { //==(3)===, 处理剩余region(重叠右侧) fr.offset_in_region = offset_in_region; fr.addr = addrrange_make(base, remain); flatview_insert(view, i, &amp;fr); } //返回上一级递归流程 ===(4)===} 如下图 经过上面流程，新生成了一个range, 并插入到数组合适的位置。同时改变base，和remain, 所以还有remain大小的空间需要继续处理. 当然上面也提到过，不一定需要再继续处理，如下图所示: 还需要处理的情况(本文场景) 不需要处理的情况 我们还需要处理重叠区域右侧的range, 如下图: 返回上一级递归流程中，我们会对mr, 和已经更新过的FlatView的各个range在做处理如下所示 内存分派 AddressSpaceDispatch内存分派的需求时，给定一个AddressSpace和一个address， 能够快速找到MemoryRegionSection,从而找到对应的MemoryRegion. 相关数据结构如下:FlatView中dispatch成员存储着其AddressSpace 的 dispatch信息struct FlatView { struct AddressSpaceDispatch *dispatch;};struct AddressSpaceDispatch 相关数据结构:struct AddressSpaceDispatch { MemoryRegionSection *mru_section; /* This is a multi-level map on the physical address space. * The bottom level has pointers to MemoryRegionSections. */ PhysPageEntry phys_map; PhysPageMap map;}; mru_section: Most Recently Used phys_map: 类似于寻址过程中的CR3, 指向第一级页表项(MemoryRegionSection) map: 相当于表示整套页表PhysPageEntry 和PhysPageMap定义如下:struct PhysPageEntry { /* How many bits skip to next level (in units of L2_SIZE). 0 for a leaf. */ uint32_t skip : 6; /* index into phys_sections (!skip) or phys_map_nodes (skip) */ uint32_t ptr : 26;};typedef PhysPageEntry Node[P_L2_SIZE];typedef struct PhysPageMap { struct rcu_head rcu; unsigned sections_nb; unsigned sections_nb_alloc; unsigned nodes_nb; unsigned nodes_nb_alloc; Node *nodes; MemoryRegionSection *sections;} PhysPageMap;PhysPageEntry: skip: 就类似于PMD_SHIFT ptr: 类似于在页表中的offset(index)PhysPageMap: Nodes: 中间节点，类似于页表项 sections: 指向所有的 MemoryRegionSections, 类似于寻址过程中的物理页面(也就是寻址的终点) section_nb: 表示sections成员指向的数组的有效个数（实际的数组大小) section_nb_alloc: sections 指向的数组, 总共分配的个数MemoryRegionSection和 FlatView成员有点重合struct MemoryRegionSection { Int128 size; MemoryRegion *mr; FlatView *fv; hwaddr offset_within_region; hwaddr offset_within_address_space; bool readonly; bool nonvolatile; bool unmergeable;}; offset_within_address_space: 是不是指 FlagRange.addr.startinit在generate_memory_topology 函数中, 会在render_memory_region调用结束之后，创建一些 dummy section:AddressSpaceDispatch *address_space_dispatch_new(FlatView *fv){ //创建新的 AddressSpaceDispatch AddressSpaceDispatch *d = g_new0(AddressSpaceDispatch, 1); uint16_t n; //创建一个 dummy 的 MemoryRegionSection n = dummy_section(&amp;d-&gt;map, fv, &amp;io_mem_unassigned); assert(n == PHYS_SECTION_UNASSIGNED); d-&gt;phys_map = (PhysPageEntry) { .ptr = PHYS_MAP_NODE_NIL, .skip = 1 }; return d;}static uint16_t dummy_section(PhysPageMap *map, FlatView *fv, MemoryRegion *mr){ assert(fv); //offset都是0， size是最大 MemoryRegionSection section = { .fv = fv, .mr = mr, .offset_within_address_space = 0, .offset_within_region = 0, .size = int128_2_64(), }; //加入到map中 return phys_section_add(map, &amp;section);}static uint16_t phys_section_add(PhysPageMap *map, MemoryRegionSection *section){ /* The physical section number is ORed with a page-aligned * pointer to produce the iotlb entries. Thus it should * never overflow into the page-aligned value. */ assert(map-&gt;sections_nb &lt; TARGET_PAGE_SIZE); //如果使用的大小等于数组大小，需要扩展数组大小 if (map-&gt;sections_nb == map-&gt;sections_nb_alloc) { map-&gt;sections_nb_alloc = MAX(map-&gt;sections_nb_alloc * 2, 16); map-&gt;sections = g_renew(MemoryRegionSection, map-&gt;sections, map-&gt;sections_nb_alloc); } //放到数组中 map-&gt;sections[map-&gt;sections_nb] = *section; memory_region_ref(section-&gt;mr); return map-&gt;sections_nb++;}该流程主要是new了一个 AddressSpaceDispatch , 然后创建一个dummy的MemoryRegionSection， 并将其更新到dispatch-&gt;map 数组中。add/* * The range in *section* may look like this: * * |s|PPPPPPP|s| * * where s stands for subpage and P for page. */void flatview_add_to_dispatch(FlatView *fv, MemoryRegionSection *section){ MemoryRegionSection remain = *section; Int128 page_size = int128_make64(TARGET_PAGE_SIZE); /* register first subpage */ if (remain.offset_within_address_space &amp; ~TARGET_PAGE_MASK) { uint64_t left = TARGET_PAGE_ALIGN(remain.offset_within_address_space) - remain.offset_within_address_space; MemoryRegionSection now = remain; now.size = int128_min(int128_make64(left), now.size); register_subpage(fv, &amp;now); if (int128_eq(remain.size, now.size)) { return; } remain.size = int128_sub(remain.size, now.size); remain.offset_within_address_space += int128_get64(now.size); remain.offset_within_region += int128_get64(now.size); } /* register whole pages */ if (int128_ge(remain.size, page_size)) { MemoryRegionSection now = remain; now.size = int128_and(now.size, int128_neg(page_size)); register_multipage(fv, &amp;now); if (int128_eq(remain.size, now.size)) { return; } remain.size = int128_sub(remain.size, now.size); remain.offset_within_address_space += int128_get64(now.size); remain.offset_within_region += int128_get64(now.size); } /* register last subpage */ register_subpage(fv, &amp;remain);} kvm MemoryListener ops: begin, add(del), commit kvm_region_add =&gt; update = g_new0(KVMMemoryUpdate, 1);=&gt; update-&gt;section = *section;=&gt; QSIMPLEQ_INSERT_TAIL(&amp;kml-&gt;transaction_add, update, next); 大概流程是，创建一个KVMMemoryUpdate, 并且初始化其section成员，将其链接到kml(KVMMemoryListener)-&gt;transaction_add链表上 kvm_region_del =&gt; update = g_new0(KVMMemoryUpdate, 1);=&gt; update-&gt;section = *section;=&gt; QSIMPLEQ_INSERT_TAIL(&amp;kml-&gt;transaction_del, update, next); 前面两个操作类似，不过是将其链接到 kml-&gt;transaction_del 链表上 kvm_region_commit 该流程主要将 as update transaction 进行commit对于region_add()/region_del()来说主要是将 kml-&gt;transaction_del(add)链表上未commit的region更新 =&gt; 首先判断del的range和add的range是否overlaps NOTE: 这里算法默认认为两个链表是从小到大排序好的，但是kvm_region_add() 并没有这个保证 { if (range_overlaps_range(&amp;r1, &amp;r2)) need_inhibit = true; break; } 如果overlap了， 则赋值 need_inhibit, 并如下调用accel_ioctl_inhibit_begin()=&gt; kvm_slot_lock()=&gt; if (need_inhibit) accel_ioctl_inhibit_begin() # 作用未知=&gt; 遍历kml-&gt;transaction_del list，对每一个KVMMemoryUpdate 调用kvm_set_phys_mem(,,false)=&gt; 遍历kml-&gt;transaction_del list, 对每一个KVMMemoryUpdate 调用kvm_set_phys_mem(,,true)=&gt; if (need_inhibit) accel_ioctl_inhibit_end() # 作用未知 可以看到该流程比较简单，就是通过add/del callbak，将所需要添加删除的region, 添加到相应链表上，在commit 中集中通知kvm增删。 然后通知KVM流程, 该流程主要是构建kvm_vm_ioctl(s, KVM_SET_USER_MEMORY_REGION, kvm_userspace_memory_region2 *mem);所需要的相关参数，实际上就是kvm_userspace_memory_region(对于 memfd的mr来说，是struct kvm_userspace_memory_region2struct kvm_userspace_memory_region { __u32 slot; __u32 flags; __u64 guest_phys_addr; __u64 memory_size; /* bytes */ __u64 userspace_addr; /* start of the userspace allocated memory */}; slot: slot index flags: 该slot的属性，例如KVM_MEM_READONLY, 定义只读 guest_phys_addr: GPA memory_size: slot region size userspace_addr: QEMU VAkvm_set_phys_mem代码, 来看下各个成员的构建kvm_set_phys_mem =&gt; 对其 # 构建kvm_userspace_memory_region member # userspace_addr: ram # memory_size: size # guest_phys_addr # MemoryRegionSection的作用是映射某个mr中的中的 # 一部分到AddressSpace中的平坦视角, 之后的章节， # 我们会详细介绍，这里只需要知道, 可以通过 # MemoryRegionSection获取到 AddressSapce 中平坦 # 视角的一个addr(offset), 以及MemoryRegion中对应的 # 一个addr，以及其size =&gt; memory_size, guest_phys_addr { size = kvm_align_section(section, &amp;start_addr); { =&gt; 将section-&gt;offset_within_address_space 进行host page size 向上取整, 获取到一个值 作为 guest_phys_addr =&gt; 因为向上取整了，所以size将会变小，这里重新 计算一个减去取整的区间，然后在做page size mask，重新得到一个size。作为memory_size } } =&gt; userspace_addr { # 由于上面 kvm_align_section 的操作，会对guest_phys_addr # 进行向上取整。所以这里获取ram时，也得做一个相应的delta # mr_offset = section-&gt;offset_within_region + delta # = section-&gt;offset_within_region + (current - record_in_section) # = section-&gt;offset_within_region + start_addr - # section-&gt;offset_within_address_space mr_offset = section-&gt;offset_within_region + start_addr - section-&gt;offset_within_address_space; # 通过mr得到相应的RAMBlock, 然后，获取到qemu va ram = memory_region_get_ram_ptr(mr) + mr_offset; { =&gt; 通过alias映射找到最终的mr =&gt; qemu_map_ram_ptr(mr-&gt;ram_block, offset); =&gt; ramblock_ptr(block, addr); =&gt; block-&gt;host + offset } } # 该流程区分 add / del 操作 =&gt; if del: =&gt; find KVMSlot =&gt; kvm_slot_get_dirty_log # get dirty log =&gt; kvm_vm_ioctl(s, KVM_GET_DIRTY_LOG, &amp;d) =&gt; sync dirty page =&gt; kvm_set_user_memory_region(,,false) =&gt; mem.memory_size = 0 # 如果memory_size为0, kvm 认为要删除slot =&gt; kvm_vm_ioctl(s, KVM_SET_USER_MEMORY_REGION, &amp;mem); =&gt; if add: =&gt; kvm_slot_init_dirty_bitmap =&gt; kvm_set_user_memory_region(,, true); =&gt; mem.memory_size = slot-&gt;memory_size =&gt; kvm_vm_ioctl(s, KVM_SET_USER_MEMORY_REGION, &amp;mem);" }, { "title": "acs", "url": "/posts/acs/", "categories": "pcie, acs", "tags": "pcie, acs", "date": "2024-10-13 11:00:00 +0800", "snippet": "ACS defines a set of control points within a PCI Express topology to determinewhether a TLP is to be routed normally, blocked, or redirected. ACS isapplicable to RCs, Switches, and Multi-Function D...", "content": "ACS defines a set of control points within a PCI Express topology to determinewhether a TLP is to be routed normally, blocked, or redirected. ACS isapplicable to RCs, Switches, and Multi-Function Devices. 120 For ACSrequirements, single-Function devices that are SR-IOV capable must be handledas if they were Multi-Function Devices, since they essentially behave asMulti-Function Devices after their Virtual Functions (VFs) are enabled. ACS 在 PCI Express 拓扑中定义了一组控制点，以确定 TLP 是正常路由、阻止还是重定向。ACS 适用于 RC、交换机和多功能设备。对于 ACS 要求，具有 SR-IOV 功能的单功能设备必须像多功能设备一样处理，因为在启用虚拟功能 (VF) 后，它们本质上表现为多功能设备。Implementation of ACS in RCiEPs is permitted but not required. It is explicitlypermitted that, within a single Root Complex, some RCiEPs implement ACS andsome do not. It is strongly recommended that Root Complex implementationsensure that all accesses originating from RCiEPs (PFs and VFs) without ACScapability are first subjected to processing by the Translation Agent (TA) inthe Root Complex before further decoding and processing. The details of suchRoot Complex handling are outside the scope of this specification. RCiEPs: Root Complex Integrated Endpoint 在 RCiEPs 中实施 ACS 是允许的，但不是必需的。明确允许在单个根复合体中，有些 RCiEPs 实现 ACS，而有些则不实现。强烈建议根复合体实现确保所有来自没有 ACS 功能的 RCiEPs（PFs 和 VFs）的访问首先经过Root Complex 中的翻译代理（TA）的处理，然后再进行进一步的解码和处理。此类根复合体处理的详细信息超出了本规范的范围。ACS provides the following types of access control: ACS Source Validation ACS Translation Blocking ACS P2P Request Redirect ACS P2P Completion Redirect ACS Upstream Forwarding ACS P2P Egress Control ACS Direct Translated P2P ACS I/O Request Blocking ACS DSP Memory Target Access ACS USP Memory Target Access ACS Unclaimed Request RedirectThe specific requirements for each of these are discussed in the followingsection.ACS hardware functionality is disabled by default, and is enabled only byACS-aware software. With the exception of ACS Source Validation, ACS accesscontrols are not applicable to Multicast TLPs (see § Section 6.14 ), and haveno effect on them. ACS 硬件功能默认禁用，只能通过支持 ACS 的软件启用。除 ACS Source Validation，ACS access control 不适用于 Multicast TLP（参见第 6.14 节），并且对其没有影响。6.12.1 ACS Component Capability RequirementsACS functionality is reported and managed via ACS Extended Capabilitystructures. PCI Express components are permitted to implement ACS ExtendedCapability structures in some, none, or all of their applicable Functions. Theextent of what is implemented is communicated through capability bits in eachACS Extended Capability structure. A given Function with an ACS ExtendedCapability structure may be required or forbidden to implement certaincapabilities, depending upon the specific type of the Function and whether itis part of a Multi-Function Device. extent: 程度,幅度,范围 ACS 功能通过 ACS 扩展功能结构进行报告和管理。PCI Express 组件可以在部分、全部或不部分适用功能中实现 ACS 扩展功能结构。实现的范围通过每个 ACS 扩展功能结构中的功能位进行传达。具有 ACS 扩展功能结构的给定功能可能需要或禁止实现某些功能，具体取决于功能的具体类型以及它是否是多功能设备的一部分。ACS is never applicable to a PCI Express to PCI Bridge Function or a RootComplex Event Collector Function, and such Functions must never implement anACS Extended Capability structure. 不适用 PCIE to PCI bridge Function / Root Complex Event Collector Function.6.12.1.1 ACS Downstream PortsThis section applies to Root Ports and Switch Downstream Ports that implementan ACS Extended Capability structure. This section applies to Downstream PortFunctions both for single-Function devices and Multi-Function Devices. 本节适用于实现 ACS 扩展功能结构的Root Port 和Swith Downstream port。本节适用于单功能设备和多功能设备的下游端口功能。 ACS Source Validation: must be implemented. When enabled, the Downstream Port tests the Bus Number from the Requester IDof each Upstream Request received by the Port to determine if it isassociated with the Secondary side of the virtual bridge associated with theDownstream Port, by either or both of: 下游端口会检查每个上游请求的请求者 ID 中的总线编号，以确定它是否与与下游端口相关的虚拟桥的 Secondary side 相关联，可以通过以下一种或两种方式进行判断： Determining that the Requester ID falls within the Bus Number “aperture” ofthe Port - the inclusive range specified by the Secondary Bus Numberregister and the Subordinate Bus Number register. aperture: 光圈falls within: 落在 确定请求者 ID 是否落在端口的总线编号“孔径”内 - 由Subordinate总线编号寄存器和 Secondary 总线编号寄存器指定的包含范围。 If FPB is implemented and enabled, determining that the Requester ID isassociated with the bridge’s Secondary Side by the application of the FPBRouting ID mechanism. 如果实现并启用了 FPB，则通过应用 FPB 路由 ID 机制确定请求者 ID 与桥的次级侧相关联。 If the Bus Number from the Requester ID of the Request is not within thisaperture, this is a reported error (ACS Violation) associated with theReceiving Port (see § Section 6.12.5 .) 如果请求中请求者 ID 的总线号不在此范围内，则这是与接收端口相关的已报告错误（ACS 违规），详见第 6.12.5 节。 Completions are never affected by ACS Source Validation. IMPLEMENTATION NOTE:UPSTREAM MESSAGES AND ACS SOURCE VALIDATION Functions are permitted to transmit Upstream Messages before they have beenassigned a Bus Number. Such messages will have a Requester ID with a Bus Numberof 00h. If the Downstream Port has ACS Source Validation enabled, theseMessages (see § Table F-1, § Section 2.2.8.2 , and § Section 6.22.1 ) willlikely be detected as an ACS Violation error. 在未分配总线号之前，Function 允许发送Upstream消息。此类消息将具有一个总线号为 00h的请求者 ID。如果下行端口启用了 ACS 源验证，则这些消息（参见表 F-1、第 2.2.8.2 节和第6.22.1 节）可能会被检测为 ACS 违规错误。 ACS Translation Blocking: must be implemented. When enabled, the Downstream Port checks the Address Type (AT) field of eachUpstream Memory Request received by the Port. If the AT field is not thedefault value, this is a reported error (ACS Violation) associated with theReceiving Port (see § Section 6.12.5 ). This error must take precedence overACS Upstream Forwarding and any applicable ACS P2P control mechanisms.Completions are never affected by ACS Translation Blocking. 启用时，下行端口会检查其接收到的每个上行内存请求的地址类型（AT）字段。如果 AT 字段不是默认值，则这是与接收端口相关的已报告错误（ACS 违规，参见第6.12.5 节）。此错误必须优先于 ACS 上行转发和任何适用的 ACS P2P 控制机制。Completions 不会受到 ACS 翻译阻止的影响。 ACS P2P Request Redirect: must be implemented by Root Ports that supportpeer-to-peer traffic with other Root Ports; 121 must be implemented by SwitchDownstream Ports. ACS P2P 请求重定向：支持与其他根端口进行点对点通信的根端口必须实现该功能；必须由 Swith 下游端口实现。 ACS P2P Request Redirect is subject to interaction with the ACS P2P EgressControl and ACS Direct Translated P2P mechanisms (if implemented). Refer to §Section 6.12.3 for more information. ACS P2P 请求重定向受 ACS P2P 出口控制和 ACS 直接翻译 P2P 机制（如果已实现）的影响。有关更多信息，请参阅第 6.12.3 节。 When ACS P2P Request Redirect is enabled in a Switch Downstream Port,peer-to-peer Requests must be redirected Upstream towards the RC. 当在交换机下游端口启用 ACS P2P 请求重定向时，点对点请求必须向上游重定向到根复合体（RC）。 When ACS P2P Request Redirect is enabled in a Root Port, peer-to-peerRequests must be sent to Redirected Request Validation logic within the RCthat determines whether the Request is “reflected” back Downstream towardsits original target, or blocked as an ACS Violation error. The algorithms andspecific controls for making this determination are not architected by thisspecification. 当在根端口启用 ACS P2P 请求重定向时，点对点请求必须发送到根复合体（RC）中的Redirected Request logic，该逻辑确定请求是“反射”回下游以返回其原始目标，还是作为 ACS 违规错误被阻止。本规范并未设计用于作出此决定的算法和具体控制。 Downstream Ports never redirect Requests that are traveling Downstream. travel: 旅行 下游端口绝不重定向向下游发送的请求。 Completions are never affected by ACS P2P Request Redirect. ACS P2P Completion Redirect: must be implemented by Root Ports that implementACS P2P Request Redirect; must be implemented by Switch Downstream Ports. The intent of ACS P2P Completion Redirect is to avoid ordering ruleviolations between Completions and Requests when Requests are redirected.Refer to § Section 6.12.6 for more information. ACS P2P Completion Redirect does not interact with ACS controls that governRequests. When ACS P2P Completion Redirect is enabled in a Switch Downstream Port,peer-to-peer Completions 122 that do not have the Relaxed Ordering Attributebit set (1b) must be redirected Upstream towards the RC. Otherwise,peer-to-peer Completions must be routed normally. When ACS P2P Completion Redirect is enabled in a Root Port, peer-to-peerCompletions that do not have the Relaxed Ordering bit set must be handledsuch that they do not pass Requests that are sent to Redirected RequestValidation logic within the RC. Such Completions must eventually be sentDownstream towards their original peer-to-peer targets, without incurringadditional ACS access control checks. Downstream Ports never redirect Completions that are traveling Downstream. Requests are never affected by ACS P2P Completion Redirect. ACS Upstream Forwarding: must be implemented by Root Ports if the RC supportsRedirected Request Validation; must be implemented by Switch DownstreamPorts. When ACS Upstream Forwarding is enabled in a Switch Downstream Port, and itsIngress Port receives an Upstream Request or Completion TLP targeting thePort’s own Egress Port, the Port must instead forward the TLP Upstreamtowards the RC. When ACS Upstream Forwarding is enabled in a Root Port, and its Ingress Portreceives an Upstream Request or Completion TLP that targets the Port’s ownEgress Port, the Port must handle the TLP as follows. For a Request, the RootPort must handle it the same as a Request that the Port “redirects” with theACS P2P Request Redirect mechanism. For a Completion, the Root Port musthandle it the same as a Completion that the Port “redirects” with the ACS P2PCompletion Redirect mechanism. When ACS Upstream Forwarding is not enabled on a Downstream Port, and itsIngress Port receives an Upstream Request or Completion TLP that targets thePort’s own Egress Port, the handling of the TLP is undefined. ACS P2P Egress Control: implementation is optional. ACS P2P Egress Control is subject to interaction with the ACS P2P RequestRedirect and ACS Direct Translated P2P mechanisms (if implemented). Refer to§ Section 6.12.3 for more information. A Switch that supports ACS P2P Egress Control can be selectively configuredto block peer-to-peer Requests between its Downstream Ports. Software canconfigure the Switch to allow none or only a subset of its Downstream Portsto send peer-to-peer Requests to other Downstream Ports. This is configuredon a per Downstream Port basis. An RC that supports ACS P2P Egress Control can be selectively configured toblock peer-to-peer Requests between its Root Ports. Software can configurethe RC to allow none or only a subset of the Hierarchy Domains to sendpeer-to-peer Requests to other Hierarchy Domains. This is configured on a perRoot Port basis. With ACS P2P Egress Control in Downstream Ports, controls in the Ingress Port(“sending” Port) determine if the peer-to-peer Request is blocked, and if so,the Ingress Port handles the ACS Violation error per § Section 6.12.5 . Completions are never affected by ACS P2P Egress Control. ACS Direct Translated P2P: must be implemented by Root Ports that supportAddress Translation Services (ATS) and also support peer-to-peer traffic withother Root Ports; 123 must be implemented by Switch Downstream Ports. When ACS Direct Translated P2P is enabled in a Downstream Port, peer-to-peerMemory Requests whose Address Type (AT) field indicates a Translated addressmust be routed normally (“directly”) to the peer Egress Port, regardless ofACS P2P Request Redirect and ACS P2P Egress Control settings. All otherpeer-to-peer Requests must still be subject to ACS P2P Request Redirect andACS P2P Egress Control settings. Completions are never affected by ACS Direct Translated P2P. ACS I/O Request Blocking: must be implemented by Root Ports and SwitchDownstream Ports that support ACS Enhanced Capability. When enabled, the Port must handle an Upstream I/O Request received by thePort’s Ingress as an ACS Violation. ACS DSP Memory Target Access: must be implemented by Root Ports and SwitchDownstream Ports that support ACS Enhanced Capability and that haveapplicable Memory BAR Space to protect. ACS DSP Memory Target Access determines how an Upstream Request received bythe Downstream Port’s Ingress and targeting any Memory BAR Space 124associated with an applicable Downstream Port is handled. The Request can beblocked, redirected, or allowed to proceed directly to its target. In aSwitch, all Downstream Ports are applicable, including the one on which theRequest was received. In a Root Complex, the set of applicable Root Ports isimplementation specific, but always includes the one on which the Request wasreceived. ACS USP Memory Target Access: must be implemented by Switch Downstream Portsthat support ACS Enhanced Capability and that have applicable Memory BARSpace in the Switch Upstream Port to protect; is not applicable to RootPorts. ACS USP Memory Target Access determines how an Upstream Request received bythe Switch Downstream Port’s Ingress and targeting any Memory BAR Space 125associated with the Switch’s Upstream Port is handled. The Request can beblocked, redirected, or allowed to proceed directly to its target. If any Functions other than the Switch Upstream Port are associated with theUpstream Port, this field has no effect on accesses to their Memory BAR Space126 . Such access is controlled by the ACS Extended Capability (if present)in the Switch Upstream Port. ACS Unclaimed Request Redirect: must be implemented by Switch DownstreamPorts that support ACS Enhanced Capability; is not applicable to Root Ports. When enabled, incoming Requests received by the Switch Downstream Port’sIngress and targeting Memory Space within the memory window of a SwitchUpstream Port that is not within a memory window or Memory BAR Target of anyDownstream Port within the Switch are redirected Upstream out of the Switch. When not enabled, such Requests are handled by the Switch Downstream Port asan Unsupported Request (UR). 6.12.1.2 ACS Functions in SR-IOV Capable and Multi-Function DevicesThis section applies to Multi-Function Device ACS Functions, with the exceptionof Downstream Port Functions, which are covered in the preceding section. ForACS requirements, single-Function devices that are SR-IOV capable must behandled as if they were Multi-Function Devices. ACS Source Validation: must not be implemented. ACS Translation Blocking: must not be implemented. ACS P2P Request Redirect: must be implemented by Functions that supportpeer-to-peer traffic with other Functions. This includes SR-IOV VirtualFunctions (VFs). ACS P2P Request Redirect is subject to interaction with the ACS P2P EgressControl and ACS Direct Translated P2P mechanisms (if implemented). Refer to §Section 6.12.3 for more information. When ACS P2P Request Redirect is enabled in a Multi-Function Device that isnot an RCiEP, peer-to-peer Requests (between Functions of the device) must beredirected Upstream towards the RC. It is permitted but not required to implement ACS P2P Request Redirect in anRCiEP. When ACS P2P Request Redirect is enabled in an RCiEP, peer-to-peerRequests, defined as all Requests that do not target system memory, must besent to implementation specific logic within the Root Complex that determineswhether the Request is directed towards its original target, or blocked as anACS Violation error. The algorithms and specific controls for making thisdetermination are not architected by this specification. Completions are never affected by ACS P2P Request Redirect. ACS P2P Completion Redirect: must be implemented by Functions that implementACS P2P Request Redirect. The intent of ACS P2P Completion Redirect is toavoid ordering rule violations between Completions and Requests when Requestsare redirected. Refer to § Section 6.12.6 for more information. ACS P2P Completion Redirect does not interact with ACS controls that governRequests. When ACS P2P Completion Redirect is enabled in a Multi-Function Device thatis not an RCiEP, peer-to-peer Completions that do not have the RelaxedOrdering bit set must be redirected Upstream towards the RC. Otherwise,peer-to-peer Completions must be routed normally. Requests are never affected by ACS P2P Completion Redirect. ACS Upstream Forwarding: must not be implemented. ACS P2P Egress Control: implementation is optional; is based on FunctionNumbers or Function Group Numbers; controls peer-to-peer Requests between thedifferent Functions within the multi-Function or SR-IOV capable device. ACS P2P Egress Control is subject to interaction with the ACS P2P RequestRedirect and ACS Direct Translated P2P mechanisms (if implemented). Refer to§ Section 6.12.3 for more information. Each Function within a Multi-Function Device that supports ACS P2P EgressControl can be selectively enabled to block peer-to-peer communication withother Functions or Function Groups 127 within the device. This is configuredon a per Function basis. With ACS P2P Egress Control in multi-Function or SR-IOV capable devices,controls in the “sending” Function determine if the Request is blocked, andif so, the “sending” Function handles the ACS Violation error per § Section6.12.5 . When ACS Function Groups are enabled in an ARI Device (ACS Function GroupsEnable is Set), ACS P2P Egress Controls are enforced on a per Function Groupbasis instead of a per Function basis. See § Section 6.13 . Completions are never affected by ACS P2P Egress Control. ACS Direct Translated P2P: must be implemented if the Multi-Function DeviceFunction supports Address Translation Services (ATS) and also peer-to-peertraffic with other Functions. When ACS Direct Translated P2P is enabled in a Multi-Function Device,peer-to-peer Memory Requests whose Address Type (AT) field indicates aTranslated address must be routed normally (“directly”) to the peer Function,regardless of ACS P2P Request Redirect and ACS P2P Egress Control settings.All other peer-to-peer Requests must still be subject to ACS P2P RequestRedirect and ACS P2P Egress Control settings. Completions are never affected by ACS Direct Translated P2P. 6.12.1.3 Functions in Single-Function DevicesThis section applies to single-Function device Functions, with the exception ofDownstream Port Functions and SR-IOV capable Functions, which are covered in apreceding section. For ACS requirements, single-Function devices that areSR-IOV capable must be handled as if they were Multi-Function Devices.No ACS capabilities are applicable, and the Function must not implement an ACSExtended Capability structure." }, { "title": "qemu qom", "url": "/posts/qom/", "categories": "qemu, qom", "tags": "qemo_qmo", "date": "2024-10-11 19:27:00 +0800", "snippet": "本篇文章，我们主要以edu device 为例，来看一下 qom的框架. edu device 的代码在 hw/misc/edu.c简介QOM 全称 QEMU Object Model, 是QEMU 使用面向对象的方式来进行抽象设计。面向对象包括封装，继承与多态。而qom就是根据自己自身的需求，设计的一套面向对象的框架。面向对象的几个概念: 封装: 将数据和操作封装在对象中，隐藏内部细节...", "content": "本篇文章，我们主要以edu device 为例，来看一下 qom的框架. edu device 的代码在 hw/misc/edu.c简介QOM 全称 QEMU Object Model, 是QEMU 使用面向对象的方式来进行抽象设计。面向对象包括封装，继承与多态。而qom就是根据自己自身的需求，设计的一套面向对象的框架。面向对象的几个概念: 封装: 将数据和操作封装在对象中，隐藏内部细节 继承: 子类可以继承父类的属性和方法，重用代码 多态: 对象可以根据具体类型表现不同行为，通过相同接口调用不同实现，提高灵活性和可扩展性。我们会在下面的流程中讲解到qom是如何针对上面三种面向对象的特性进行设计的。 本文主要参考的书籍是 &lt;&lt;QEMU-KVM 源码解析与应用&gt;&gt; 李强, 该书集写的很详细，本文主要是根据书中的思路做了一些笔记，以及调试过程。QOM的class, instance, interface, type, object我们知道C++有class和Object, class是面向对象的 type, 可以使用具体的class来创建实例化的object。而QOM 则有点区别, 增加了一层type，需要使用type来实例化object, 而type又包含可以包含class，instance和interface，我们来解释下. class 只是针对某一类object抽象出具有共同特征的封装集合。个人认为主要是包含了一些方法和该类型的”const public”部分。”const public” 如何理解呢，例如对于edu设备类型，其vendor_id, device_id 都是一样的，所以就可以把这些成员定义到class中, class一旦 initialize, 就无需在修改。使用不同的object 实例可以引用一个 class. (当然 方法我们也可以这样认为). instance 而instance不同，instance 是每个object的 private结构，每个object可以根据自己的信息自定义该部分，例如对于edu device来说, 其io_region, irq_state都属于instance. interface interface是qom一个比较难琢磨的部分，以下是我个人理解。个人认为interface部分，实际上体现了QOM中对于多态的面向对象的实现。可以实现用父类class，来调用子类的function. 当然，子类也可以区别与父类，定义自己的interface。区别于class中的function，class中的function，必须定位到其具体的class层，例如PCIDeviceClass,才可以找到其中的realize()方法 type type其实才是QOM中的类似于C++的class，其综合了上面三种数据结构。可以用type来进行object相关操作，例如new, destroy. object 具体的对象实例. 大家可以想一下，C++做到这些事情是比较简单的，直接按照语法定义好class，编译器会帮忙做这些事情。而QOM 不同，需要自己实现。所以, 我们需要关注下， 这些type 是如何注册，并且初始化好的，然后我们在来介绍下，如何用type来实例化object, 如下图所示:+--------------+ +------ type_init|type的注册 +----+------ register_module_init+--------------+ +------ type_register+--------------+|type的初始化 +----------- type_initialize+--------------++--------------+ +------- object_new|object的初始化+----+------- object_initialize+--------------+ +------- object_init_with_type类型注册typedef enum { MODULE_INIT_MIGRATION, MODULE_INIT_BLOCK, MODULE_INIT_OPTS, MODULE_INIT_QOM, MODULE_INIT_TRACE, MODULE_INIT_XEN_BACKEND, MODULE_INIT_LIBQOS, MODULE_INIT_FUZZ_TARGET, MODULE_INIT_MAX} module_init_type;类型注册相关函数, 早于main执行Breakpoint 4, do_qemu_init_pci_edu_register_types () at ../hw/misc/edu.c:442442 type_init(pci_edu_register_types)(gdb) bt#0 do_qemu_init_pci_edu_register_types () at ../hw/misc/edu.c:442#1 0x00007ffff67f3cc4 in __libc_start_main_impl () at /lib64/libc.so.6#2 0x0000555555884885 in _start ()init 流程_start =&gt; __libc_start_main_imp =&gt; foreach constructor: { do_qemu_init_pci_edu_register_types =&gt; register_module_init =&gt; alloc ModuleEntry =&gt; init it { e-&gt;init e-&gt;type } =&gt; link to init_type_list[e-&gt;type] list }main =&gt; qemu_init =&gt; qemu_init_subsystems =&gt; module_call_init(MODULE_INIT_QOM) =&gt; foreach_list init_type_list[MODULE_INIT_QOM] { e-&gt;init(): pci_edu_register_types }pci_edu_register_typespci_edu_register_types=&gt; define : TypeInfo edu_info=&gt; type_register_static =&gt; type_register =&gt; type_register_internal =&gt; type_new :return TypeImpl* ti =&gt; { alloc TypeInfo ti ti-&gt;name = \"edu\" ti-&gt;parent = \"pci-device\" ... } =&gt; type_table_add类型初始化type_initializetype_initialize =&gt; alloc_class { init: ti-&gt;class_size: ## if not define, get parent 's class size init: ti-&gt;instance_size: ti-&gt;class = g_malloc0(ti-&gt;class_size) alloc class } =&gt; init_class { 如果是祖先interface type, 则需要判断一些东西， 例如instance_size必须是0 abstract 必须是1 等等 =&gt; init_parent_type and copy_class_from_parent { =&gt; type_initialize(parent) : ## 递归 =&gt; memcpy(ti-&gt;class, parent-&gt;class, parent-&gt;class_size) ## copy parent class =&gt; init_interface ## 这块逻辑有点怪，我们在后面贴下具体代码 } =&gt; init properties: g_hash_table_new_full(g_str_hash, g_str_equal, NULL, object_property_free); =&gt; ti-&gt;class-&gt;type = ti ## 设置该class的type =&gt; while (parent = parent-&gt;parent) parent-&gt;class_base_init() ## 递归循环 =&gt; ti-&gt;class_init()}该部分的逻辑是初始化TypeImpl, 其实主要的是初始化ti-&gt;class 和 interface , 在初始化本type的class时，首先要将class-&gt;parent 初始化，以及其 class 的interface初始化.我们展开下和interface相关的代码, 在看type_initialize相关代码之前，我们先看下type_initialize_interfacestatic void type_initialize_interface(TypeImpl *ti, TypeImpl *interface_type, TypeImpl *parent_type){ InterfaceClass *new_iface; TypeInfo info = { }; TypeImpl *iface_impl; info.parent = parent_type-&gt;name; info.name = g_strdup_printf(\"%s::%s\", ti-&gt;name, interface_type-&gt;name); info.abstract = true; iface_impl = type_new(&amp;info); iface_impl-&gt;parent_type = parent_type; type_initialize(iface_impl); g_free((char *)info.name); new_iface = (InterfaceClass *)iface_impl-&gt;class; new_iface-&gt;concrete_class = ti-&gt;class; new_iface-&gt;interface_type = interface_type; ti-&gt;class-&gt;interfaces = g_slist_append(ti-&gt;class-&gt;interfaces, new_iface);}该函数的作用是, get and init该interface的 TypeImpl, 需要做的动作大概是: init a TypeInfo TypeInfo.name = “ti-&gt;name::interface_type-&gt;name” Type.parent = parent_type-&gt;name eg interface_type-&gt;name = vmstate-if ti-&gt;name = cpu interface_type-&gt;name = device::vmstate-if TypeInfo.name = cpu::vmsate-if TypeInfo.parent = device::vmsate-if 这样来看，是不是就有面向对象编程中，interface的作用了，如果底层有对interface的实现，则给他覆盖，而且能够实现对父类的覆盖 alloc TypeImpl : type_new(TypeInfo) type_initialize(iface_impl) init InterfaceClass concrete_clas = ti-&gt;class interface_type = interface_type 将新分配的 interface 加入到 ti-&gt;class-&gt;interfacesstatic void type_initialize(TypeImpl *ti){\t\t...\t\tparent = type_get_parent(ti);\t\tif (parent) {\t\t\t\t//===(1)=== for (e = parent-&gt;class-&gt;interfaces; e; e = e-&gt;next) { InterfaceClass *iface = e-&gt;data; ObjectClass *klass = OBJECT_CLASS(iface); type_initialize_interface(ti, iface-&gt;interface_type, klass-&gt;type); } for (i = 0; i &lt; ti-&gt;num_interfaces; i++) { TypeImpl *t = type_get_by_name(ti-&gt;interfaces[i].typename); if (!t) { error_report(\"missing interface '%s' for object '%s'\", ti-&gt;interfaces[i].typename, parent-&gt;name); abort(); }\t\t\t\t\t\t//===(2)=== for (e = ti-&gt;class-&gt;interfaces; e; e = e-&gt;next) { TypeImpl *target_type = OBJECT_CLASS(e-&gt;data)-&gt;type; if (type_is_ancestor(target_type, t)) { break; } }\t\t\t\t\t\t//===(2.1)=== if (e) { continue; } //===(3)=== type_initialize_interface(ti, t, t); } }\t\t...} 调用查看parent-&gt;class-&gt;interfaces, 然后，根据parent interface，来init 当前ti-&gt;class-&gt;interface 查看当前的ti-&gt;interfaces, 然后和ti-&gt;class-&gt;interfaces中的每个进行对比，判断是否在（1）过程中添加过，如果添加过，则不需要再添加（2.1) 执行到这里，说明ti中定义了新的interface，在父类中没有定义过，所以这里parent_type:arg3传递的仍然是tedu class initstatic void edu_class_init(ObjectClass *class, void *data){ DeviceClass *dc = DEVICE_CLASS(class); PCIDeviceClass *k = PCI_DEVICE_CLASS(class); k-&gt;realize = pci_edu_realize; k-&gt;exit = pci_edu_uninit; k-&gt;vendor_id = PCI_VENDOR_ID_QEMU; k-&gt;device_id = 0x11e8; k-&gt;revision = 0x10; k-&gt;class_id = PCI_CLASS_OTHERS; set_bit(DEVICE_CATEGORY_MISC, dc-&gt;categories);}因为edu 这里 初始化 其父类，PCIDeviceClass 和 父类的父类DeviceClass, 的部分成员。并将k-&gt;realize赋值为pci_edu_realize下面展示DEVICE_CLASS宏, 我们看下如果通过当前的ObjectClass，找到相应的父类 ObjectClass: 实际上是对所有类的抽象，其ObjectClass-&gt;type 指向的是当前类的类型, 我们将此处的class认定为当前class（edu class), device class是其父类 DEVICE_CLASS 展开, 以edu为例 DEVICE_CLASS : this is a static function, declare here:OBJECT_DECLARE_TYPE(DeviceState, DeviceClass, DEVICE)OBJECT_DECLARE_TYPE(InstanceType:DeviceState, ClassType:DeviceClass, MODULE_OBJ_NAME:DEVICE) =&gt; DECLARE_OBJ_CHECKERS(InstanceType:DeviceState, ClassType:DeviceClass, MODULE_OBJ_NAME:DEVICE, TYPE_##MODLE_OBJ_NAME:TYPE_DEVICE) =&gt; DECLARE_CLASS_CHECKERS(ClassType:DeviceClass, OBJ_NAME:DEVICE, TYPENAME:TYPE_DEVICE) =&gt; { static inline DeviceClass * DEVICE_GET_CLASS(const void *obj) { return OBJECT_GET_CLASS(DeviceClass, obj, TYPE_DEVICE); } static inline DeviceClass * DEVICE_CLASS(const void *klass) { return OBJECT_CLASS_CHECK(DeviceClass, klass, TYPE_DEVICE); } } #define OBJECT_CLASS_CHECK(class_type, class, name) \\ ((class_type *)object_class_dynamic_cast_assert(OBJECT_CLASS(class), (name), \\ __FILE__, __LINE__, __func__)) (DeviceClass *)object_class_dynamic_cast_assert(Objectclass *class, \"device\", ...) 可以看到，如果调用DEVICE_CLASS(class), 先把class强转未ObjectClass, 然后调用object_class_dynamic_cast_assert, 然后最终返回DeviceClass *, 我们来看下具体调用: object_class_dynamic_cast_assert =&gt; object_class_dynamic_cast =&gt; type = class-&gt;type\t\t\t\t\t\t\t\t\t\t## 获取该类的 TypeImpl =&gt; target_type = type_get_by_name() \t\t## 首先根据typename获取TypeImpl =&gt; 查看type所在的class是否有interfaces，如果有，查看target_type是否是interface类型 =&gt; Y: 遍历class-&gt;interfaces 链表，看看哪个interface是target_type 的子类型(type_is_ancestor)，如果是则说明找到了, 可以强转 =&gt; N: 说明target_type不是interface =&gt; 查看target_type 是不是该type的父类型(type_is_ancestor), 如果是，则说明找到了，可以强转 =&gt; 如果发现可以强转，返回 object, 如果发现不能强转，则返回NULL 所以DEVICE_CLASS作用是， 判断传入的class查看是否是继承的DeviceClass， 如果是则进行强转(DeviceClass *) class, 如果不是， 则返回NULLobject 初始化现在注册好了类型，如果我们在qemu命令行执行-device edu, qemu则会使用前面定义好的类型来创建一个object以edu为例，在下面的流程中，会执行对该edu device的初始化:main qemu_init qmp_x_exit_preconfig qemu_create_cli_devices qemu_opts_foreach device_init_func简单看下device_init_func流程device_init_func(QDict *opts, ...) # opts 和字典相关，=&gt; driver = qdict_get_try_str(opts, \"driver\") # 通过opts查询 driver, 返回\"edu\"=&gt; qdev_get_device_class(&amp;driver, errp) # 通过\"edu\"找到其class， 转换为DeviceClass=&gt; path = qdict_get_try_str(opts, \"bus\"); # 找到bus=&gt; dev = qdev_new(driver); =&gt; ObjectClass *oc = object_class_by_name(edu) # 找到edu 的class =&gt; return DEVICE(object_new(name)); # 调用object_new , 然后强转为DeviceState实例=&gt; dev-&gt;opts = qdict_clone_shallow(opts);=&gt; object_set_properties_from_keyval()=&gt; qdev_realize(DEVICE(dev), bus, errp) # 对该device实例化 =&gt; qdev_set_parent_bus(dev, bus, errp); # 设置parent_bus =&gt; object_property_set_bool(OBJECT(dev), \"realized\", true, errp); { 这里先不展开, 在device_class_init, 会通过 object_class_property_add_bool(class, \"realized\", device_get_realized, device_set_realized) 注册好\"realized\" 的get callbak和set callbak } =&gt; device_set_realized # 代码比较多, 只看一个地方 =&gt; dc-&gt;realize() # 这里回调用到pci_edu_realize我们主要看下object_newobject new 流程：object_new=&gt; ti = type_get_by_name(typename) # 通过typename，找到TypeImpl=&gt; object_new_with_type(Type type:ti) =&gt; type_initialize(type) # 如果该type没有初始化过，在这里初始化 =&gt; alloc object instance { size = type-&gt;instance_size; align = type-&gt;instance_align; 这里会根据align_size的大小，选择是否对其分配，下面我们展开代码: obj = g_malloc() / qemu_memalign\t\t\t\t obj_free = g_free / qemu_vfree } =&gt; object_initialize_with_type(obj, size, type) =&gt; memset(obj, 0, type-&gt;instance_size); =&gt; obj-&gt;class = type-&gt;class; =&gt; object_ref(obj) # org refcount =&gt; object_class_property_init_all(obj) #和properties相关 =&gt; obj-&gt;properties = g_hash_table_new_full() =&gt; object_init_with_type(obj, type); =&gt; 递归对obj以及parent 都调用 ti-&gt;instance_init() =&gt; object_post_init_with_type() =&gt; 递归， 对其 obj 以及parent调用 ti-&gt;instance_post_init() 这里需要注意的是, object_init_with_type(), 调用ti-&gt;instance_init()时，需要先对父类进行init，再对子类， 而object_post_init_with_type()则相反。看下pci_edu_realizepci_edu_realize =&gt; pci_config_set_interrupt_pin(pci_conf, 1) # 设置interrupt_pin =&gt; msi_init(pdev, 0, 1, true, false, errp) # 设置msi =&gt; timer_init_ms(&amp;edu-&gt;dma_timer, ...) # dma timer =&gt; memory_region_init_io(&amp;edu-&gt;mmio, &amp;edu_mmio_ops, edu, \"edi-mmio\", 1* MiB) # register mmio =&gt; pci_register_bar(pdev, 0, PCI_BASE_ADDRESS_SPACE_MEMORY, &amp;edu-&gt;mmio) # register bar经过上面流程，edu 实例已经初始化完成。qom propertiesQOM为了便于管理对象，为每个class定义了propertiesstruct ObjectClass{ ... GHashTable *properties; ...}每个Property object定义如下:struct ObjectProperty{ char *name; char *type; char *description; ObjectPropertyAccessor *get; ObjectPropertyAccessor *set; ObjectPropertyResolve *resolve; ObjectPropertyRelease *release; ObjectPropertyInit *init; void *opaque; QObject *defval;} name: property name，key value type: property type, 类型名，例如bool, string, link init, get, set, resolve, release 则是注册的回调 opaque: 指向一个具体的类型(type中指定的), 其内定义了更具体的回调我们来看下几个具体类型:typedef struct BoolProperty{ bool (*get)(Object *, Error **); void (*set)(Object *, bool, Error **);} BoolProperty;typedef struct StringProperty{ char *(*get)(Object *, Error **); void (*set)(Object *, const char *, Error **);} StringProperty;以执行下面语句为例， 我们看下:device_class_init...=&gt; object_class_property_add_bool(class, \"realized\", device_get_realized, device_set_realized);=&gt; object_class_property_add_bool(class, \"hotpluggable\", device_get_hotpluggable, NULL);...我们来画下edu class 的图:+--------------+ |ObjectClass | +--------------+ |DeviceClass ||其他成员 |---------- PCIDeviceClass, 也就是edu class parent, 但是edu class并没有增加其他member+--------------+ |PCIDeviceClass| |其他成员 | +--------------+ ObjectClass+--------------------+ +---------------+|ObjectClass | +------+ |+--------------------+ | +---------------+|type:Type(TypeImpl*)+----+ +--------------------+ |interfaces:GSList | +--------------------+|properties |+--------------------+图: DeviceClass-&gt;properity图示 +-----------+ |ObjectClass| +-----------+ | | +-----------+ |properity +---+------+------------- +-----------+ | | | | | | +-----------+ | | | | | +-------+ | |name +--------- \"realized\" | +-------+ | |type +--------- \"bool\" | +-------+ | |set +--------- properity_set_bool | +-------+ | |get +--------- properity_get_bool | +-------+ | |opaque +-------+ +-------------+ | +-------+ +--+BoolProperty | | +-------------+ | |get +---- device_get_realized | +-------------+ | |set +---- device_set_realized +--------+ +-------------+ | +---+---+ |name +--------- \"hotpluggable\" +-------+ |type +--------- \"bool\" +-------+ |set +--------- properity_set_bool +-------+ |get +--------- properity_get_bool +-------+ |opaque +-----+ +------------+ +-------+ +----+BoolProperty| +------------+ |get +--- device_set_hotplugable +------------+ |set +--- NULL +------------+ init该成员是一个hash表，在type_initialize()初始化:type_initialize=&gt; ti-&gt;class-&gt;properties = g_hash_table_new_full(g_str_hash, g_str_equal, NULL, object_property_free);g_hash_table_new_full的参数依次为: hash_func: get key hash key_equal_func: compare key key_destroy_func: free key value_destroy_func: free valueadd上面列出来一些，这次，我们列举全:device_class_init =&gt; object_class_property_add_bool(class, \"realized\", device_get_realized, device_set_realized); =&gt; object_class_property_add_bool(class, \"hotpluggable\", device_get_hotpluggable, NULL); =&gt; object_class_property_add_bool(class, \"hotplugged\", device_get_hotplugged, NULL); =&gt; object_class_property_add_link(class, \"parent_bus\", TYPE_BUS, offsetof(DeviceState, parent_bus), NULL, 0);以简单的bool类型为例:object_class_property_add_bool =&gt; BoolProperty *prop = g_malloc =&gt; prop-&gt;get = device_get_realized =&gt; prop-&gt;set = device_set_realized =&gt; object_class_property_add(class, \"realized\", \"bool\", property_get_bool, property_set_bool, NULL, void * opaque: prop ) =&gt; prop = g_malloc0(ObjectProperty) =&gt; init { prop-&gt;name prop-&gt;type prop-&gt;get, set, release, opaque } =&gt; g_hash_table_insert(class-&gt;properties, prop-&gt;name, prop)findObjectProperty *object_class_property_find(ObjectClass *klass, const char *name){ ObjectClass *parent_klass; parent_klass = object_class_get_parent(klass); if (parent_klass) { ObjectProperty *prop = object_class_property_find(parent_klass, name); if (prop) { return prop; } } return g_hash_table_lookup(klass-&gt;properties, name);} NOTE: 自己理解 这里搜索prop时，有顺序要求么，为什么还要先递归search parent class 是因为在进行type_realized时，并没有继承property, 理论上也不允许父类和子类有相同name 的 property, 所以，这里会search parent class，并且search顺序无所谓setobject_property_set_bool =&gt; object_property_set =&gt; ObjectProperty *prop = object_property_find_err =&gt; prop-&gt;set() # property_set_bool =&gt; BoolProperty *prop = opaque =&gt; prop-&gt;set() # device_set_realized流程比较简单，先find, 再setothersgcc constructor attribute test编写程序测试:#include &lt;stdio.h&gt;void __attribute__ ((constructor)) before_main(){ printf(\"before main\\n\");}int main(){ printf(\"main exec \\n\");}执行程序获取输出:➜ constructor_test ./mainbefore mainmain execgdb 调试下:#0 before_main () at main.c:5#1 0x00007ffff7df5cc4 in __libc_start_main_impl () from /lib64/libc.so.6#2 0x0000000000401065 in _start ()发现在main前, 执行_start时，会执行constructor之前的函数参考文献[1]. «QEMU-KVM 源码解析与应用»" }, { "title": "qemu hot_upgrade org patch", "url": "/posts/qemu-hot-upgrade-upstream-org/", "categories": "qemu, hot_upgrade", "tags": "qemu_hot_upgrade", "date": "2024-10-11 19:27:00 +0800", "snippet": "patch linkhttps://patchew.org/QEMU/1658851843-236870-1-git-send-email-steven.sistare@oracle.com/commit messageThis version of the live update patch series integrates live update into thelive migrat...", "content": "patch linkhttps://patchew.org/QEMU/1658851843-236870-1-git-send-email-steven.sistare@oracle.com/commit messageThis version of the live update patch series integrates live update into thelive migration framework. The new interfaces are: integrates: 集成，融合 mode (migration parameter) cpr-exec-args (migration parameter) file (migration URI) migrate-mode-enable (command-line argument) only-cpr-capable (command-line argument) Provide the cpr-exec and cpr-reboot migration modes for live update. Thesesave and restore VM state, with minimal guest pause time, so that qemu may beupdated to a new version in between. The caller sets the mode parameterbefore invoking the migrate or migrate-incoming commands. 提供 cpr-exec 和 cpr-reboot 迁移模式用于实时更新。这些模式可以保存和恢复虚拟机的状态，并将客户机的暂停时间降至最低，从而允许在此期间更新 QEMU 到新版本。调用者在调用 migrate 或 migrate-incoming command 之前设置mode 参数。In cpr-reboot mode, the migrate command saves state to a file, allowingone to quit qemu, reboot to an updated kernel, start an updated version ofqemu, and resume via the migrate-incoming command. The caller must specifya migration URI that writes to and reads from a file. Unlike normal mode,the use of certain local storage options does not block the migration, butthe caller must not modify guest block devices between the quit and restart.The guest RAM memory-backend must be shared, and the @x-ignore-sharedmigration capability must be set, to avoid saving it to the file. Guest RAMmust be non-volatile across reboot, which can be achieved by backing it witha dax device, or /dev/shm PKRAM as proposed inhttps://lore.kernel.org/lkml/1617140178-8773-1-git-send-email-anthony.yznaga@oracle.combut this is not enforced. The restarted qemu arguments must match those usedto initially start qemu, plus the -incoming option. 在 cpr-reboot 模式下，migrate 命令将状态保存到文件中，从而允许用户退出 QEMU，重启到更新后的内核，启动更新后的 QEMU 版本，并通过 migrate-incoming 命令恢复。调用者必须指定一个迁移 URI，该 URI 用于读写文件。与正常模式不同，某些本地存储选项的使用不会阻止迁移，但调用者在退出和重启之间不得修改客户机块设备。客户机的 RAM 内存后端必须是共享的，并且必须设置 @x-ignore-shared 迁移能力，以避免将其保存到文件中。客户机的 RAM 必须在重启后保持非易失性，这可以通过使用 DAX 设备或提议的 /dev/shm PKRAM 来实现，具体请参见 该链接， https://…… 但这并不是强制性的。重启后的 QEMU 参数必须与最初启动 QEMU 时使用的参数匹配，并加上 -incoming 选项。The reboot mode supports vfio devices if the caller first suspends the guest,such as by issuing guest-suspend-ram to the qemu guest agent. The guestdrivers’ suspend methods flush outstanding requests and re-initialize thedevices, and thus there is no device state to save and restore. Afterissuing migrate-incoming, the caller must issue a system_wakeup command toresume. reboot 模式支持 VFIO 设备，前提是调用者首先暂停客户机，例如通过向 QEMU 客户机代理发出 guest-suspend-ram 命令。客户机驱动程序的暂停方法会刷新未完成的请求并重新初始化设备，因此无需保存和恢复设备状态。在发出 migrate-incoming之后，调用者必须发出 system_wakeup 命令以恢复。In cpr-exec mode, the migrate command saves state to a file and directlyexec’s a new version of qemu on the same host, replacing the original processwhile retaining its PID. The caller must specify a migration URI that writesto and reads from a file, and resumes execution via the migrate-incomingcommand. Arguments for the new qemu process are taken from the cpr-exec-argsmigration parameter, and must include the -incoming option. 在 cpr-exec 模式下，migrate 命令将状态保存到文件中，并在同一主机上直接执行新版本的 QEMU，替换原始进程，同时保留其 PID。调用者必须指定一个迁移 URI，该URI 用于读写文件，并通过 migrate-incoming 命令恢复执行。新 QEMU进程的参数来自 cpr-exec-args 迁移参数，并且必须包含 -incoming 选项。Guest RAM must be backed by a memory backend with share=on, but cannot bememory-backend-ram. The memory is re-mmap’d in the updated process, so guestram is efficiently preserved in place, albeit with new virtual addresses.In addition, the ‘-migrate-mode-enable cpr-exec’ option is required. Thiscauses secondary guest ram blocks (those not specified on the command line)to be allocated by mmap’ing a memfd. The memfds are kept open across exec,their values are saved in special cpr state which is retrieved after exec,and they are re-mmap’d. Since guest RAM is not copied, and storage blocksare not migrated, the caller must disable all capabilities related to pageand block copy. The implementation ignores all related parameters. 客户机 RAM 必须由一个共享内存后端支持（share=on），但不能是 memory-backend-ram。内存在更新后的进程中会重新映射，因此客户机 RAM 在原地有效保存，尽管会使用新的虚拟地址。此外，必须使用 -migrate-mode-enable cpr-exec 选项。这将导致secondary guest RAM blocks（未在命令行中指定的那些）通过映射一个 memfd 来分配。memfd 在执行期间保持打开，其值保存在特殊的 CPR 状态中，该状态在执行后被检索，并重新映射。由于客户机 RAM 不会被复制，存储块也不会迁移，因此调用者必须禁用所有与页面和块复制相关的功能。实现将忽略所有相关参数。The exec mode supports vfio devices by preserving the vfio container, group,device, and event descriptors across the qemu re-exec, and by updating DMAmapping virtual addresses using VFIO_DMA_UNMAP_FLAG_VADDR andVFIO_DMA_MAP_FLAG_VADDR as defined in https://lore.kernel.org/kvm/1611939252-7240-1-git-send-email-steven.sistare@oracle.comand integrated in Linux kernel 5.12. 执行模式支持 VFIO 设备，通过在 QEMU 再次执行时保留 VFIO 容器、组、设备和事件描述符，并使用 VFIO_DMA_UNMAP_FLAG_VADDR 和 VFIO_DMA_MAP_FLAG_VADDR 更新 DMA 映射的虚拟地址。这些标志的定义可以在以下位置找到： https://…… 并且合入到 Linux kernel 5.12Here is an example of updating qemu from v7.0.50 to v7.0.51 using exec mode.The software update is performed while the guest is running to minimizedowntime. 以下是使用执行模式将 QEMU 从 v7.0.50 更新到 v7.0.51 的示例。软件更新是在客户机运行时进行的，以最小化停机时间。 window 1 | window 2 |# qemu-system-$arch ... | -migrate-mode-enable cpr-exec |QEMU 7.0.50 monitor - type 'help' ... |(qemu) info status |VM status: running | | # yum update qemu(qemu) migrate_set_parameter mode cpr-exec |(qemu) migrate_set_parameter cpr-exec-args | qemu-system-$arch ... -incoming defer |(qemu) migrate -d file:/tmp/qemu.sav |QEMU 7.0.51 monitor - type 'help' ... |(qemu) info status |VM status: paused (inmigrate) |(qemu) migrate_incoming file:/tmp/qemu.sav |(qemu) info status |VM status: running | Here is an example of updating the host kernel using reboot mode.window 1 | window 2 |# qemu-system-$arch ... mem-path=/dev/dax0.0 | -migrate-mode-enable cpr-reboot |QEMU 7.0.50 monitor - type 'help' ... |(qemu) info status |VM status: running | | # yum update kernel-uek(qemu) migrate_set_parameter mode cpr-reboot |(qemu) migrate -d file:/tmp/qemu.sav |(qemu) quit | |# systemctl kexec |kexec_core: Starting new kernel |... | |# qemu-system-$arch mem-path=/dev/dax0.0 ... | -incoming defer |QEMU 7.0.51 monitor - type 'help' ... |(qemu) info status |VM status: paused (inmigrate) |(qemu) migrate_incoming file:/tmp/qemu.sav |(qemu) info status |VM status: running |" }, { "title": "qemu hot update my test", "url": "/posts/hot-upgrade-my-test/", "categories": "qemu, hot_upgrade", "tags": "qemu_hot_upgrade", "date": "2024-10-11 19:27:00 +0800", "snippet": "cpr-reboot test (upstream已经合入, 直接用上游测试)测试过程如下: 测试过程 # org process./qemu-system-x86_64 -machine type=q35 -object memory-backend-memfd,size=2G,id=ram0 -m 2G -monitor stdio~/qemu-hotupdate&gt; sh ru...", "content": "cpr-reboot test (upstream已经合入, 直接用上游测试)测试过程如下: 测试过程 # org process./qemu-system-x86_64 -machine type=q35 -object memory-backend-memfd,size=2G,id=ram0 -m 2G -monitor stdio~/qemu-hotupdate&gt; sh run.shQEMU 8.2.50 monitor - type 'help' for more information(qemu) info statusVM status: running(qemu) migrate_set_parameter mode cpr-reboot(qemu) migrate_set_capability x-ignore-shared on(qemu) migrate -d file:vm.state(qemu) info statusVM status: paused (postmigrate)(qemu) (qemu) quitunknown command: '(qemu)'(qemu) quit# incoming~/qemu-hotupdate&gt; ./qemu-system-x86_64 -machine type=q35 -m 2G -incoming defer -monitor stdioQEMU 8.2.50 monitor - type 'help' for more information(qemu) info statusVM status: paused (inmigrate)(qemu) migrate_set_parameter mode cpr-reboot(qemu) migrate_set_capability x-ignore-shared on(qemu) migrate_incoming file:vm.state(qemu) info statusVM status: running(qemu)~/qemu-hotupdate&gt; ls -lh |grep state-rw------- 1 wang wang 5.9M Oct 11 22:00 vm.state ## 文件中保存的信息很少 cpr-exec编译问题 Werror 相关问题 /home/wang/workspace/qemu/openEuler/qemu/include/qapi/qmp/qobject.h:49:17:error: ‘subqdict’ may be used uninitialized [-Werror=maybe-uninitialized] 问题原因: 代码没写好 解决方法: 增加--extra-cflags=\"--disable-werror\" 未定义符号bpf_program__set_socket_filter /home/wang/workspace/qemu/openEuler/qemu/build/../ebpf/ebpf_rss.c:52: undefinedreference to `bpf_program__set_socket_filter' 问题原因: libbpf和当前qemu版本不兼容 解决方法: 直接注释 loongarch 相关代码编译不通过 ../hw/loongarch/larch_3a.c:344:8: error: variable ‘ls3a5k_cpucfgs’ has initializer but incomplete type 344 | struct kvm_cpucfg ls3a5k_cpucfgs = { | ^~~~~~~~~~../hw/loongarch/larch_3a.c:345:6: error: ‘struct kvm_cpucfg’ has no member named ‘cpucfg’ 345 | .cpucfg[LOONGARCH_CPUCFG0] = CPUCFG0_3A5000_PRID, | ^~~~~~ 问题原因: 不知道 解决方法: 越过loongarch 编译，增加 --targe-list-exclude=loongarch64-softmmu Note 为了加快编译速度，可以只编译x86_64 --target-list=x86_64-softmmu 这里问题忘记记录了, 之后测试在记录 解决方法: 增加 --enable-splice 综合下来，执行configure的参数配置如下:./configure --extra-cflags=\"-g -Wno-maybe-uninitialized \\ -Wno-dangling-pointer -Wno-enum-int-mismatch\" --enable-spice \\ --disable-werror --target-list-exclude=loongarch64-softmmufedora 38 缺少依赖:ninja-buildglib2-develpixman-develspice-server-devel简单测试执行下面命令启动虚拟机:./qemu-system-x86_64 -machine type=q35 \\ -object memory-backend-memfd,size=2G,id=ram0,share=on \\ -m 2G -monitor stdio \\ -migrate-mode-enable cpr-exec \\ -numa node,memdev=ram0 \"在monitor中依次执行下面命令，做cpr-exec migratemigrate_set_parameter mode cpr-execmigrate_set_parameter cpr-exec-args ./qemu-system-x86_64 -machine type=q35 -object memory-backend-memfd,size=2G,id=ram0,share=on -m 2G -monitor tcp::12345,server,nowait -migrate-mode-enable cpr-exec -numa node,memdev=ram0 -incoming defermigrate file:/tmp/a.txt测试发现 monitor中有如下报错:(qemu) migrate file:/tmp/a.txtconfigure accelerator pc-q35-6.2 startQEMU 6.2.0 monitor - type 'help' for more information(qemu) machine init startqemu-system-x86_64: Device needs media, but drive is emptyqemu-system-x86_64: write ptoc eventnotifier failed查看qemu相关进程~/qemu-hotupdate&gt; ps -o ppid,pid,comm -x |grep qemu 212311 212312 qemu-system-x86 212312 212488 qemu-system-x86 &lt;defunct&gt;这表示子进程变成僵尸进程。我们来调试下问题原因。调试这里有一行报错qemu-system-x86_64: Device needs media, but drive is empty查看代码: 1 hw/ide/core.c|2548 col 31| error_setg(errp, \"Device needs media, but drive is empty\"); 3 hw/scsi/scsi-disk.c|2387 col 27| error_setg(errp, \"Device needs media, but drive is empty\"); 5 hw/block/virtio-blk.c|1173 col 27| error_setg(errp, \"Device needs media, but drive is empty\");目前在3个地方都打上断点, 断点断在:(gdb) f 0#0 ide_init_drive (s=s@entry=0x55eaaf05c030, blk=0x55eaae695c40, kind=kind@entry=IDE_HD, version=0x0, serial=0x0, model=0x0, wwn=0, cylinders=2, heads=16, secs=63, chs_trans=1, errp=0x7fffb4999a70) at ../hw/ide/core.c:25472547 if (!blk_is_inserted(s-&gt;blk)) {查看设备:(gdb) p s-&gt;blk-&gt;name$4 = 0x55eaae695350 \"ide0-hd0\"查看qtreebus: main-system-bus dev: q35-pcihost, id \"\" bus: pcie.0 dev: ich9-ahci, id \"\" bus: ide.2 type IDE dev: ide-cd, id \"\" drive = \"ide2-cd0\" bus: ide.1 type IDE bus: ide.0 type IDE继续执行:(gdb) cContinuing.[Thread 0x7f78456186c0 (LWP 217603) exited][Thread 0x7f7845ffb6c0 (LWP 217602) exited][Thread 0x7f7846ffd6c0 (LWP 217600) exited][Thread 0x7f78477fe6c0 (LWP 217599) exited][Thread 0x7f7847fff6c0 (LWP 217598) exited][Thread 0x7f784cd1e6c0 (LWP 217597) exited][Thread 0x7f784cd21d40 (LWP 217596) exited][Thread 0x7f78467fc6c0 (LWP 217601) exited][New process 217596][Inferior 2 (process 217596) exited with code 01]继续调试ide_init_drive(gdb) nide_init_drive (s=s@entry=0x555f8349a4a0, blk=0x555f82ad53a0, kind=kind@entry=IDE_HD, version=0x0, serial=0x0, model=0x0, wwn=0, cylinders=2, heads=16, secs=63, chs_trans=1, errp=0x7fffae243680) at ../hw/ide/core.c:25482548 error_setg(errp, \"Device needs media, but drive is empty\");(gdb) p s-&gt;blk-&gt;root$9 = (BdrvChild *) 0x0结合源端调试使用下面命令进行调试:cgdb ./build/qemu-system-x86_64 \\ -ex 'set args -m 2G \\ -machine type=q35' \\ -ex 'b ide_init_drive'发现仅有一次断住, 打印如下:(gdb) p s-&gt;blk-&gt;name$1 = 0x555556b177b0 \"ide2-cd0\"(gdb) p s-&gt;blk-&gt;dev-&gt;parent_bus-&gt;name$2 = 0x55555754d7f0 \"ide.2\"(gdb) p s-&gt;blk-&gt;dev-&gt;parent_bus-&gt;parent-&gt;parent_bus-&gt;name$3 = 0x555556bce3c0 \"pcie.0\"而调试目的端时, 我们在qemu进程启动后，使用gdb attach, 同样在ide_init_driver,设置断点.发现第一次断到了这里（和上面也提到的一样)(gdb) p s-&gt;blk-&gt;name$1 = 0x5614bc254ee0 \"ide0-hd0\"(gdb) p s-&gt;blk-&gt;dev-&gt;parent_bus-&gt;name$2 = 0x5614bcc7ec20 \"ide.0\"(gdb) p s-&gt;blk-&gt;dev-&gt;parent_bus-&gt;parent-&gt;parent_bus-&gt;name$3 = 0x5614bc2711f0 \"pcie.0\"(gdb) p kind$4 = IDE_HD此时我们如果不修改kind值，则会在:ide_init_drive =&gt; if kind != IDE_CD: blk_is_inserted return true if return true error_setg(\"Device needs media, but drive is empty\") return -1错误退出，这里，我们暂时修改下kind, 修改为IDE_CD(gdb) set kind=1(gdb) p kind$6 = IDE_CD修改完之后，执行c(gdb) p s-&gt;blk-&gt;name$2 = 0x557c8942f4e0 \"ide2-cd0\"(gdb) p s-&gt;blk-&gt;dev-&gt;parent_bus-&gt;name$3 = 0x557c89dab350 \"ide.2\"(gdb) p s-&gt;blk-&gt;dev-&gt;parent_bus-&gt;parent-&gt;parent_bus-&gt;name$4 = 0x557c89421580 \"pcie.0\"继续执行cqemu-monitor打印(qemu) machine init startadd rom file: vga.romadd rom file: e1000e.romdevice init startreset all devicesqemu enter main_loop似乎这步成功了, 我们看下qemu进程状态:~/qemu-hotupdate&gt; ps -o ppid,pid,comm,state -x |grep qemu279886 279887 qemu-system-x86 S279887 280102 qemu-system-x86 S父子进程都正常：进入source monitor, 执行下面命令:(qemu) info statusVM status: paused (inmigrate)进入dest monitor 执行下面命令:(qemu) info statusinfo statusVM status: paused (inmigrate)(qemu) migrate_incoming file:/tmp/a.txtmigrate_incoming file:/tmp/a.txt(qemu) info statusinfo statusVM status: runningsource端monitor退出，qemu子进程 变为daemon~/qemu-hotupdate&gt; ps -o ppid,pid,comm,state -x |grep qemu 1 280102 qemu-system-x86 S迁移成功我们在看下qtreebus: main-system-bus dev: q35-pcihost, id \"\" bus: pcie.0 dev: ich9-ahci, id \"\" bus: ide.1 type IDE bus: ide.2 type IDE dev: ide-cd, id \"\" drive = \"ide2-cd0\" bus: ide.0 type IDE dev: ide-hd, id \"\" drive = \"ide0-hd0\"发现dst端，居然多了一个bus: ide.0，上居然多添加了一个ide0-hd0 device 为了方便后续调试 我们将代码暂时做下面的微调: @@ -2525,6 +2527,7 @@ int ide_init_drive(IDEState *s, BlockBackend *blk, IDEDriveKind kind, uint64_t nb_sectors; s-&gt;blk = blk;+ kind=1; s-&gt;drive_kind = kind; !!!!! 2024-10-14在另外一台机器上测试不再复现!!!!子进程异常退出，产生僵尸进程上面的测试虽然在2024-10-14在另一台机器上无法复现，但是暴露了另一个问题，子进程异常退出后，似乎父进程没有回收到子进程资源，从而导致子进程变为僵尸进程。该僵尸进程会在父进程迁移成功后，随着父进程退出，僵尸进程消失。但是, 从需求上来说，还是应该尽量避免僵尸进程产生，此问题是一个BUG().模拟测试我们修改代码, 模拟下子进程异常退出:@@ -47,6 +47,8 @@ int main(int argc, char **argv) int main(int argc, char **argv, char **envp) {+ printf(\"NOLY FOR test, exit!!!\\n\");+ return -1; qemu_init(argc, argv, envp); qemu_log(\"qemu enter main_loop\\n\"); qemu_main_loop();我们让其在main函数开始，退出子进程.执行cpr-exec相关命令(qemu) migrate file:/tmp/file.txtNOLY FOR test, exit!!!qemu-system-x86_64: write ptoc eventnotifier failed查看相关进程:fedora :: ~/qemu_test/build % ps aux |grep qemuwang 492077 5.0 0.1 3474956 43948 pts/12 Sl+ 16:09 0:04 ./qemu-system-x86_64 -machine type=q35 -object memory-backend-memfd,size=2G,id=ram0,share=on -m 2G -monitor stdio -migrate-mode-enable cpr-exec -numa node,memdev=ram0 -nographic --serial telnet:localhost:6666,server,nowaitwang 496565 0.1 0.0 0 0 pts/12 Z+ 16:10 0:00 [qemu-system-x86] &lt;defunct&gt;问题原因在下面流程中:qmp_migrate =&gt; if (strstart(uri, \"file:\", &amp;p)) { ... cpr_exec(); if (fork &gt; 0) { ... 父进程动作: ... } else { execvp(); } }父进程在fork后，没有获取子进程pid，也就没有wait()子进程. 我们需要添加该流程,增加如下 PATCH patch代码折叠 diff --git a/include/sysemu/sysemu.h b/include/sysemu/sysemu.hindex 3e58f9cbaf..5fa7b01551 100644--- a/include/sysemu/sysemu.h+++ b/include/sysemu/sysemu.h@@ -18,6 +18,7 @@ extern GStrv exec_argv; extern int eventnotifier_ptoc[2]; extern int eventnotifier_ctop[2]; extern RunState vm_run_state;+extern pid_t cpr_exec_child; void qemu_add_exit_notifier(Notifier *notify); void qemu_remove_exit_notifier(Notifier *notify);diff --git a/migration/migration.c b/migration/migration.cindex d878d512b9..22b8641676 100644--- a/migration/migration.c+++ b/migration/migration.c@@ -2567,6 +2567,7 @@ void qmp_migrate(const char *uri, bool has_blk, bool blk, Error *local_err = NULL; MigrationState *s = migrate_get_current(); const char *p = NULL;+ pid_t child_pid; if (!migrate_prepare(s, has_blk &amp;&amp; blk, has_inc &amp;&amp; inc, has_resume &amp;&amp; resume, errp)) {@@ -2603,9 +2604,11 @@ void qmp_migrate(const char *uri, bool has_blk, bool blk, cpr_preserve_fds(); cpr_exec(); vm_run_state = runstate_get();- if (fork() &gt; 0) {+ child_pid=fork();+ if (child_pid &gt; 0) { close(eventnotifier_ptoc[0]); close(eventnotifier_ctop[1]);+ cpr_exec_child = child_pid; while (read(eventnotifier_ctop[0], &amp;data, sizeof(data)) &lt; 0) { if (errno == EINTR) continue;diff --git a/softmmu/globals.c b/softmmu/globals.cindex 4f940bd7f3..29118f7313 100644--- a/softmmu/globals.c+++ b/softmmu/globals.c@@ -75,3 +75,4 @@ bool cpr_exec_migrating = false; int eventnotifier_ptoc[2] = {-1, -1}; int eventnotifier_ctop[2] = {-1, -1}; RunState vm_run_state;+pid_t cpr_exec_child = -1;diff --git a/softmmu/runstate.c b/softmmu/runstate.cindex 4496ed564b..f365894a47 100644--- a/softmmu/runstate.c+++ b/softmmu/runstate.c@@ -749,6 +749,8 @@ static bool main_loop_should_exit(void) { RunState r; ShutdownCause request;+ int status;+ pid_t child_pid; if (qemu_debug_requested()) { vm_stop(RUN_STATE_DEBUG);@@ -768,6 +770,8 @@ static bool main_loop_should_exit(void) } while (ret &lt; 0 &amp;&amp; errno == EINTR); if (ret &lt;= 0) { error_report(\"write ptoc eventnotifier failed\");+ child_pid = waitpid(cpr_exec_child, &amp;status, WNOHANG);+ error_report(\"child process %d exit, status %d\\n\", child_pid, status); MigrationState *s = migrate_get_current(); bdrv_invalidate_cache_all(&amp;err); exec_argv = NULL; 简单测试:执行测试命令:(qemu) migrate file:/tmp/file.txtNOLY FOR test, exit!!!qemu-system-x86_64: write ptoc eventnotifier failedqemu-system-x86_64: child process 568774 exit, status 65280 # 回收到子进程(qemu)查看是否有僵尸进程:fedora :: ~/qemu_test/build % ps aux |grep qemu |grep -v grepwang 568716 4.1 0.1 3474956 45052 pts/3 Sl+ 16:33 0:04 ./qemu-system-x86_64 -machine type=q35 -object memory-backend-memfd,size=2G,id=ram0,share=on -m 2G -monitor stdio -migrate-mode-enable cpr-exec -numa node,memdev=ram0 -nographic --serial telnet:localhost:6666,server,nowait可以发现该patch，规避了这个问题.参考链接[cpr-exec][]" }, { "title": "sys_mincore", "url": "/posts/mincore/", "categories": "mm, sys_mincore", "tags": "sys_mincore", "date": "2024-10-11 16:57:00 +0800", "snippet": "简介sys_mincore 是 Linux 内核中实现 mincore 系统调用的函数，位于内核源码中。它主要用于检查用户态传递的内存地址范围中哪些页面驻留在物理内存中（即是否被分页到内存中）。 这里我们需要注意 驻留在物理内存和是否建立映射关系是两回事，例如，我们mmap了一段内存到文件中，该文件在内存中有全部offset的pagecache, 但是此时进程未访问这些虚拟地址空间，此时...", "content": "简介sys_mincore 是 Linux 内核中实现 mincore 系统调用的函数，位于内核源码中。它主要用于检查用户态传递的内存地址范围中哪些页面驻留在物理内存中（即是否被分页到内存中）。 这里我们需要注意 驻留在物理内存和是否建立映射关系是两回事，例如，我们mmap了一段内存到文件中，该文件在内存中有全部offset的pagecache, 但是此时进程未访问这些虚拟地址空间，此时,进程还未建立虚拟地址到物理地址的映射。这种情况下，mincore也会认为这部分地址所在的page驻留在物理内存中。原理mincore的原理很简单，就是去walk_page_range(), 然后, 在walk_page_range的hook中，去判断，这段空间中的page是否驻留在内存中。主要区分以下三种类型page: swapcache pagecache anonymous代码流程大概如下:SYS_mincore: =&gt; 验证参数[vec, vec+len) 是否`access_ok()` =&gt; find_vma() &lt;- 上面range =&gt; walk_page_range -- mincore_walk_ops =&gt; walk_page_test ## 先不看 =&gt; __walk_page_range =&gt; ops-&gt;pre_vma =&gt; walk_pgd_range (省略一些流程) =&gt; walk_p4d_range =&gt; walk_pud_range =&gt; walk_pmd_range =&gt; walk_pte_range =&gt; ops-&gt;ops_vma由于walk_xxx_range()代码相似度高，我们对其中两个流程做分析: intermediate pgtable walk - walk_pmd_range 为啥要看这个呢，因为回调中只定义了pmd_entry, see follow last pgtable walk - walk_pte_range其中回调如下:static const struct mm_walk_ops mincore_walk_ops = { .pmd_entry = mincore_pte_range, .pte_hole = mincore_unmapped_range, .hugetlb_entry = mincore_hugetlb,};walk_pmd_rangestatic int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end, struct mm_walk *walk){ ... do {again: if (pmd_none(*pmd) || (!walk-&gt;vma &amp;&amp; !walk-&gt;no_vma)) { if (ops-&gt;pte_hole) //=======(1)==== err = ops-&gt;pte_hole(addr, next, depth, walk); if (err) break; continue; } ... //=======(2)==== if (ops-&gt;pmd_entry) err = ops-&gt;pmd_entry(pmd, addr, next, walk); ... //=======(3)==== err = walk_pte_range(pmd, addr, next, walk); if (err) break; } while (pmd++, addr = next, addr != end); return err;} 这个表示整个pmd 是none, 没有建立映射, 此时为中间页表，大概有两种情况: anonymous page: 整个range，没有驻留的page pagecache: 需要根据该range在pagecache中的index范围，去查看是否有存在的pagecache 代码如下: 代码折叠 static int __mincore_unmapped_range(unsigned long addr, unsigned long end, struct vm_area_struct *vma, unsigned char *vec){ unsigned long nr = (end - addr) &gt;&gt; PAGE_SHIFT; int i; //如果是file，则调用`mincore_page()` if (vma-&gt;vm_file) { pgoff_t pgoff; //找到在vma中的偏移 pgoff = linear_page_index(vma, addr); for (i = 0; i &lt; nr; i++, pgoff++) //根据address_space， 查找 vec[i] = mincore_page(vma-&gt;vm_file-&gt;f_mapping, pgoff); //如果是anonymous page则直接全赋值0 } else { for (i = 0; i &lt; nr; i++) vec[i] = 0; } return nr;} static int mincore_unmapped_range(unsigned long addr, unsigned long end, __always_unused int depth, struct mm_walk *walk){ walk-&gt;private += __mincore_unmapped_range(addr, end, walk-&gt;vma, walk-&gt;private); return 0;} 我们在后面再展开mincore_page()内容 如果pmd不为none，则说明映射的有pte，则调用pmd_entry回调, 遍历pte table, 查看每个pte所指向的空间，是否驻留page 代码如下: 代码折叠 static int mincore_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end, struct mm_walk *walk){ ... for (; addr != end; ptep++, addr += PAGE_SIZE) { if (pte_none(pte)) //===(1)=== __mincore_unmapped_range(addr, addr + PAGE_SIZE, vma, vec); //===(2)=== else if (pte_present(pte)) *vec = 1; else { /* pte is a swap entry */ //===(3)=== swp_entry_t entry = pte_to_swp_entry(pte); if (non_swap_entry(entry)) { /* * migration or hwpoison entries are always * uptodate */ *vec = 1; } else {#ifdef CONFIG_SWAP *vec = mincore_page(swap_address_space(entry), swp_offset(entry));#else WARN_ON(1); *vec = 1;#endif } } vec++; } ...} 进一步判断pagecache 说明已经映射 进一步判断swapcache, 这里会从pte中获取swp entry，然后根据entry获取到所在的swap_addres_space以及在其中的offset, 这里不再展开 继续深层遍历mincore_page该部分代码比较简单， 不展开代码mincore_page: =&gt; page = find_get_page() =&gt; present = PageUptodate(page); =&gt; return presentfind_get_page()的作用是，利用address_space以及其offset，获取到相应的page，而PG_updtodate 则表示这个page中的内容是合法的，例如如果磁盘I/O error发生时，虽然为其建立了相应的pagecache， 但是其中的内容和磁盘上是不一致的,此时这种pagecache不被统计在内PG_uptodate tells whether the page's contents is valid. When a readcompletes, the page becomes uptodate, unless a disk I/O error happened." }, { "title": "tick", "url": "/posts/tick/", "categories": "schedule, tick", "tags": "tick", "date": "2024-10-10 10:10:00 +0800", "snippet": "timer 源 timer 描述 频率 PIT 最古老的pc时钟设备. intel 8253/8254 PLT有三个16bit counter 1.193182MHz HPET High Precision Event Timer, 设计初衷为了替代 PIT 和...", "content": "timer 源 timer 描述 频率 PIT 最古老的pc时钟设备. intel 8253/8254 PLT有三个16bit counter 1.193182MHz HPET High Precision Event Timer, 设计初衷为了替代 PIT 和 RTC 最低时钟频率为10MHz APIC timer local apic timer 古老机型频率不稳定, 所以启动初会通过PIT/Hpet 校准 Local APIC 频率 CMOS RTC 电池供电，可以产生 Update interrupt, alarm interrupt 2Hz~8192Hz TSC 根据CPU的时钟振荡器产生的周期计数 古老机型频率稳定 ACPI timer Power management timer (PM timer) 3.57945Mhz PIT时钟频率3倍 timer在系统中的作用timer在系统中的角色很重要，其主要作用是让程序在当前配置期望的时间，做想做的事情。而timer早期linux时钟系统的设计在Linux早期，timer的作用就是产生周期性的tick。在tick中，会去做一些周期性的事务。但是有些事务全局性的，在smp中，只有一个cpu去做就行。另一类事务，是和每个cpu相关的, 需要每个cpu都去做, 我们来简单列举下. timekeeping – update wall time –global low-precision clock – rolling timer wheel – percpu sched related – update rq clock – percpu …其实, 除了update wall time，其他的处理的事项几乎都是percpu的。固定周期往往会带来一个抉择问题 – 选择什么精度合适每个cpu都需要timer，这样不就是需要很多么？是这样的，但是以x86为例，在 smp 引入时，同时引入了 apic timer, 每个cpu的apic都可以单独配置, 所以不存在timer不够的情况。参考链接commit 88ad0bf6890505cbd9ca1dbb79944a27b5c8697dAuthor: Ingo Molnar &lt;mingo@elte.hu&gt;Date: Fri Feb 16 01:28:16 2007 -0800 [PATCH] Add SysRq-Q to print timer_list debug info...commit b463fc60730bea6cdd73fec6edc6ec4658d47d37Author: Andrew Morton &lt;akpm@osdl.org&gt;Date: Fri Feb 16 01:27:23 2007 -0800 [PATCH] vmi-versus-hrtimers main timercommit 73dea47faeb96d54a984b9d7f4de564816966354Author: Andi Kleen &lt;ak@suse.de&gt;Date: Fri Feb 3 21:50:50 2006 +0100 [PATCH] x86_64: Allow to run main time keeping from the local APIC interrupt commit d25bf7e5fe73b5b6d2246ab0be08ae35d718456bAuthor: Venkatesh Pallipadi &lt;venkatesh.pallipadi@intel.com&gt;Date: Wed Jan 11 22:44:24 2006 +0100 [PATCH] x86_64: Handle missing local APIC timer interrupts on C3 state Whenever we see that a CPU is capable of C3 (during ACPI cstate init), we disable local APIC timer and switch to using a broadcast from external timer interrupt (IRQ 0).timer 源参考" }, { "title": "pgcacher", "url": "/posts/pgcacher/", "categories": "mm, pgcacher", "tags": "pgcacher", "date": "2024-10-10 10:10:00 +0800", "snippet": "简介pgcacher可以用来获取当前系统中pagecache相关信息。pgclear打印的信息格式大致如下:# sudo pgcacher -pid=933 -worker=5 -limit 2+---------------------------------+----------------+-------------+----------------+-------------+--...", "content": "简介pgcacher可以用来获取当前系统中pagecache相关信息。pgclear打印的信息格式大致如下:# sudo pgcacher -pid=933 -worker=5 -limit 2+---------------------------------+----------------+-------------+----------------+-------------+---------+| Name | Size │ Pages │ Cached Size │ Cached Pages│ Percent │|---------------------------------+----------------+-------------+----------------+-------------+---------|| /usr/lib64/libc.so.6 | 2.131M | 546 | 2.131M | 546 | 100.000 || /usr/lib64/libsystemd.so.0.36.0 | 939.391K | 235 | 939.391K | 235 | 100.000 ||---------------------------------+----------------+-------------+----------------+-------------+---------|│ Sum │ 3.048M │ 781 │ 3.048M │ 781 │ 100.000 │+---------------------------------+----------------+-------------+----------------+-------------+---------+ name: 文件名称 size: 文件大小 page cached size: pagecache 占用的大小 cached pages: pagecache 占用的 page 数量 percent: cached size / sizepgcacher大致的原理:从系统中的/proc/{pid}/maps和/proc/{pid}/fd获取进程所使用的文件路径, 然后将文件mmap到当前进程，通过syscall.mincore获取该文件block在内存中有pagecache。 这里面只画了mmap一条线，没有画FD的，不知道是作者漏画了，还是别有深意下面是syscall.mincore的原理图，我们再另一篇文章中展示常用命令 查看某个进程 pgcache 占用情况 wang@fedora ~ sudo pgcacher -pid=1 -worker 5 -limit 32024/10/10 13:03:02 skipping \"/sys/fs/cgroup\": file is a directory+-------------------------------------------------------+----------------+-------------+----------------+-------------+---------+| Name | Size │ Pages │ Cached Size │ Cached Pages│ Percent │|-------------------------------------------------------+----------------+-------------+----------------+-------------+---------|| /usr/lib64/libcrypto.so.3.0.9 | 4.237M | 1085 | 4.237M | 1085 | 100.000 || /usr/lib64/systemd/libsystemd-shared-253.12-1.fc38.so | 3.651M | 935 | 3.651M | 935 | 100.000 || /usr/lib64/libc.so.6 | 2.131M | 546 | 2.131M | 546 | 100.000 ||-------------------------------------------------------+----------------+-------------+----------------+-------------+---------|│ Sum │ 10.019M │ 2566 │ 10.019M │ 2566 │ 100.000 │+-------------------------------------------------------+----------------+-------------+----------------+-------------+---------+ pid: 指定进程号 worker: 指定多线程数量，加快处理速度 limit: 限制打印条目 查看某些文件的pgcache占用 wang@fedora ~/workspace/vm/centos-qcow2 pgcacher cgdb.sh test.sh dange.xml+-----------+----------------+-------------+----------------+-------------+---------+| Name | Size │ Pages │ Cached Size │ Cached Pages│ Percent │|-----------+----------------+-------------+----------------+-------------+---------|| test.sh | 4.166K | 2 | 4.166K | 2 | 100.000 || cgdb.sh | 1.471K | 1 | 1.471K | 1 | 100.000 || dange.xml | 6.869K | 2 | 0B | 0 | 0.000 ||-----------+----------------+-------------+----------------+-------------+---------|│ Sum │ 12.506K │ 5 │ 5.637K │ 3 │ 60.000 │+-----------+----------------+-------------+----------------+-------------+---------+ 本次测试，只cat了test.sh, cgdb.sh 没有cat dange.xml 查看某个目录的文件pgcache占用（递归） wang@fedora ~ pgcacher --depth 4 a+-----------+----------------+-------------+----------------+-------------+---------+| Name | Size │ Pages │ Cached Size │ Cached Pages│ Percent │|-----------+----------------+-------------+----------------+-------------+---------|| a/b/a.md | 16B | 1 | 16B | 1 | 100.000 || a/b/a.txt | 69B | 1 | 0B | 0 | 0.000 ||-----------+----------------+-------------+----------------+-------------+---------|│ Sum │ 85B │ 2 │ 16B │ 1 │ 50.000 │+-----------+----------------+-------------+----------------+-------------+---------+ 查看系统中所有进程占用的 pgcache 情况 wang@fedora ~ pgcacher -top -limit 3;+--------------------------------+----------------+-------------+----------------+-------------+---------+| Name | Size │ Pages │ Cached Size │ Cached Pages│ Percent │|--------------------------------+----------------+-------------+----------------+-------------+---------|| /usr/lib/locale/locale-archive | 213.972M | 54777 | 213.972M | 54777 | 100.000 || /usr/lib64/firefox/libxul.so | 125.989M | 32254 | 125.989M | 32254 | 100.000 || /usr/lib64/libLLVM-16.so | 117.789M | 30154 | 117.789M | 30154 | 100.000 ||--------------------------------+----------------+-------------+----------------+-------------+---------|│ Sum │ 457.750M │ 117185 │ 457.750M │ 117185 │ 100.000 │+--------------------------------+----------------+-------------+----------------+-------------+---------+ 该命令只能查询当前系统中，进程打开的文件，以及mmap的文件占用。并不是系统中所有文件的pgcache占用统计 -top: scan the open files of all processes, show the top few files that occupy the most memory space in the page cache, default: false pgcacher 执行流程pgcacher流程主要分为以下几个步骤 汇总所有要统计的文件 对这些文件进行mmap, 然后调用mincore, 获取该文件在内存中的有哪些block申请了pgcache 统计打印我们先看下如何汇总所有要统计的文件汇总所有统计的文件主要分为种方式: 从进程角度出发, 统计某个或者某些进程使用的文件, 而从进程角度出发主要又分两种方式: -pid : 统计某个进程 -top: 统计所有进程 从文件系统角度出发, 统计某些文件，或者某个目录下的所有文件从进程角度出发该方式主要分为两种, -pid获取某一个进程。-top获取所有进程，我们分别来看下: -pid main =&gt; pg.appendProcessFiles(pid) =&gt; pg.getProcessFiles(pid) =&gt; pg.getProcessFdFiles(pid) =&gt; loop /proc/${pid}/fd # 获取所有打开fd的 文件路径 =&gt; pg.getProcessMaps(pid) # 获取mmap 的文件路径 =&gt; loop /proc/${pid}/maps =&gt; AREADY GET THIS PROCESS ALL FILEs =&gt; pg.getPageCacheStats() # 获取 files pagecache 情况 =&gt; pg.output() # 以一定格式输出 -top main =&gt; pg.handleTop() =&gt; psutils.Processes() =&gt; processes() # return all process =&gt; loop all process =&gt; pg.getProcessFiles(process.Pid()) =&gt; pg.getPageCacheStats() =&gt; pg.output() 两者流程差不多，-top就是先获取所有进程，然后收集每个进程使用的文件从文件角度出发从文件角度出发，这个比较简单，从文件系统tree，或者参数中的文件列表就可以只要所需要统计的文件的路径，流程不再展开统计文件的pgcache占用情况该流程主要在pg.getPageCacheStats()流程pg.getPageCacheStats() =&gt; pcstats.GetPcStatus() =&gt; GetFileMincore() =&gt; unix.Mmap() # 先mmap文件，目的是为了下面调用sys_mincore =&gt; unix.Syscall(unix.SYS_MINCORE, ...) # 调用mincore # 根据mincore的返回，获取各种 =&gt; Get Cached =&gt; Get Pages =&gt; Get Uncached =&gt; Calculate Percent该流程也比较简单，对文件依次，进行sys_mmap(), 然后对mmap()的地址空间做sys_mincore(), 查看这些地址所在的page, 是否存在其pgcache 虽然此时 VA-&gt;PA 没有建立起映射， 但是sys_mincore流程，仍然可以通过address_space-&gt;i_pages获取到该地址空间是否有pgcache，我们在另外一篇文章中，看下sys_mincore()原理pgcacher遗漏 pgcache – blockdev目前，pgcacher 发现对blockdev 有忽略, 例如执行下面命令:简单测试 从文件角度查看 # sudo ./pgcacher /dev/nvme0n1 +--------------+----------------+-------------+----------------+-------------+---------+ | Name | Size │ Pages │ Cached Size │ Cached Pages│ Percent │ |--------------+----------------+-------------+----------------+-------------+---------| | /dev/nvme0n1 | 0B | 0 | 0B | 0 | 0.000 | |--------------+----------------+-------------+----------------+-------------+---------| │ Sum │ 0B │ 0 │ 0B │ 0 │ NaN │ +--------------+----------------+-------------+----------------+-------------+---------+ 从进程调度 # sudo xxd /dev/nvme0n1|less# sudo ~/workspace/tools/mm/pgcacher_org/pgcacher -pid $(ps aux |grep xxd |grep -v -E 'grep|sudo' |awk '{print $2}')+---------------------------------+----------------+-------------+----------------+-------------+---------+| Name | Size │ Pages │ Cached Size │ Cached Pages│ Percent │|---------------------------------+----------------+-------------+----------------+-------------+---------|| /usr/lib64/libc.so.6 | 2.131M | 546 | 2.131M | 546 | 100.000 || /usr/lib64/ld-linux-x86-64.so.2 | 829.688K | 208 | 829.688K | 208 | 100.000 || /usr/bin/xxd | 23.758K | 6 | 23.758K | 6 | 100.000 ||---------------------------------+----------------+-------------+----------------+-------------+---------|│ Sum │ 2.964M │ 760 │ 2.964M │ 760 │ 100.000 │+---------------------------------+----------------+-------------+----------------+-------------+---------+ 相关代码从进程角度出发, 在获取进程所使用的文件列表时(getProcessFiles()), 忽略了/dev下的所有文件.func (pg *pgcacher) getProcessFdFiles(pid int) []string { ... readlink := func(file fs.DirEntry) { ... if strings.HasPrefix(target, \"/dev\") { // ignore devices return } ... } ...} 个人建议，可以增加选择统计下blockdevfile.从文件角度出发，似乎也有问题:func GetPcStatus(fname string, filter func(f *os.File) error) (PcStatus, error) { ... f, err := os.Open(fname) ... finfo, err := f.Stat() ... pcs.Size = finfo.Size() ... mincore, err := GetFileMincore(f, finfo.Size()) ...}通过os.Stat()并不能统计到block_dev文件大小，导致finfo.Size()输出为0, 个人建议, 对于block_dev, 可以使用下面的接口获取文件大小 unix.Syscall(unix.SYS_IOCTL, , BLKGETSIZE64,,) 个人修改个人针对上面的问题进行了修改，目前的方案还需要进一步改进:目前支持statblockdev参数，添加该参数，可以对blockdev设备进行统计。但是个人认为该方法不太好，最好总能打印出blockdev， 但是选择性统计，blockdev设备的pgcache情况.(因为这个操作比较耗时)git 链接 [3]参考链接[1]. [GITHUB PROTECT]: pgcacher[2]. [GITHUB ISSUE]: pgcache ignore block_dev file[3]. [MY GITHUB PROTECT]: pgcacher " }, { "title": "Keyboard Controller Style (KCS) Interface", "url": "/posts/kcs/", "categories": "ipmi_spec", "tags": "ipmi", "date": "2024-05-06 15:40:00 +0800", "snippet": "abstractThis section describes the Keyboard Controller Style (KCS) Interface. The KCSinterface is one of the supported BMC to SMS interfaces. The KCS interface isspecified solely for SMS messages. ...", "content": "abstractThis section describes the Keyboard Controller Style (KCS) Interface. The KCSinterface is one of the supported BMC to SMS interfaces. The KCS interface isspecified solely for SMS messages. SMM messages between the BMC and an SMIHandler will typically require a separate interface, though the KCS interfaceis designed so that system software can detect if a transaction wasinterrupted. Any BMC-to-SMI Handler communication via the KCS interface isimplementation specific and is not covered by this specification. solely: 单独地; 本节介绍键盘控制器样式 (KCS) 接口。 KCS 接口是受支持的 BMC 到 SMS 接口之一。 KCS 接口专为 SMS 消息而指定。 BMC 和 SMI 处理程序之间的 SMM 消息通常需要单独的接口，尽管 KCS 接口的设计使得系统软件可以检测事务是否被中断。 任何通过 KCS 接口的 BMC 到 SMI 处理程序通信都是特定于实现的，并且不包含在本规范中。The KCS Interface is designed to support polled operation. Implementations canoptionally provide an interrupt driven from the OBF flag, but this must notprevent driver software from the using the interface in a polled manner. Thisallows software to default to polled operation. It also allows software to usethe KCS interface in a polled mode until it determines the type of interruptsupport. Methods for assigning and enabling such an interrupt are outside thescope of this specification. KCS 接口旨在支持轮询操作。 实现可以选择提供由 OBF flag 驱动的中断，但这不能阻止驱动程序软件以轮询方式使用接口。 这允许软件默认进行轮询操作。 它还允许软件以轮询模式使用 KCS 接口，直到确定中断支持的类型。 分配和启用此类中断的方法超出了本规范的范围.9.1 KCS Interface/BMC LUNsLUN 00b is typically used for all messages to the BMC through the KCSInterface. LUN 10b is reserved for Receive Message Queue use and should not beused for sending commands to the BMC. Note that messages encapsulated in a SendMessage command can use any LUN in the encapsulated portion. encapsulated [ɪnˈkæpsjuleɪtɪd] : v. 压缩;概括;简述; adj. 密封的;封装的 LUN 00b 通常用于通过 KCS 接口发送至 BMC 的所有消息。 LUN 10b 保留供Receive Message Queue 使用，不应用于向 BMC 发送命令。 请注意，Send Message command 中封装的消息可以使用封装部分中的任何 LUN。9.2 KCS Interface-BMC Request Message FormatRequest Messages are sent to the BMC from system software using a writetransfer through the KCS Interface. The message bytes are organized accordingto the following format specification: Request Messages 通过 KCS 接口使用写传输从系统软件发送到 BMC。 消息字节按照以下格式规范组织：Where: LUN Logical Unit Number. This is a sub-address that allows messages to be routedto different ‘logical units’ that reside behind the same physical interface.The LUN field occupies the least significant two bits of the first messagebyte. occupy [ˈɑːkjupaɪ]: 占据; 使用，占用 这是一个子地址，允许消息路由到驻留在同一物理接口后面的不同“逻辑单元”。 LUN 字段占据第一个消息字节的最低有效两位。 NetFn Network Function code. This provides the first level of functional routingfor messages received by the BMC via the KCS Interface. The NetFn fieldoccupies the most significant six bits of the first message byte. Even NetFnvalues are used for requests to the BMC, and odd NetFn values are returned inresponses from the BMC. even: 偶数odd /ɑːd/: 奇数 Network Function Code。 这为 BMC 通过 KCS 接口接收的消息提供第一级功能路由。 NetFn 字段占据第一个消息字节的最高有效六位。 偶数 NetFn 值用于向 BMC 发出请求，奇数 NetFn 值在 BMC 的响应中返回。 Cmd Command code. This message byte specifies the operation that is to beexecuted under the specified Network Function. Command code. 该消息字节指定要在指定Network Function 下执行的操作。 Data Zero or more bytes of data, as required by the given command. The generalconvention is to pass data LS-byte first, but check the individual commandspecifications to be sure. convention [kənˈvenʃn]: 协定,协议;约束,约定;惯例 根据给定命令的需要，零个或多个字节的数据。 一般约定是首先传递数据 LS-byte ，但请检查各个命令规范来确定。 9.3 BMC-KCS Interface Response Message FormatResponse Messages are read transfers from the BMC to system software via theKCS Interface. Note that the BMC only returns responses via the KCS Interfacewhen Data needs to be returned. The message bytes are organized according tothe following format specification: Response Messages 是通过 KCS 接口从 BMC 到系统软件的读取传输。 请注意，BMC 仅在需要返回数据时才通过 KCS 接口返回响应。 消息字节按照以下格式规范组织：Where: LUN Logical Unit Number. This is a return of the LUN that was passed in theRequest Message. NetFn Network Function. This is a return of the NetFn code that was passed in theRequest Message. Except that an odd NetFn value is returned. 注意其返回的NetFn 是偶数. 而 Request Message 中是奇数. Cmd Command. This is a return of the Cmd code that was passed in the RequestMessage. Completion Code The Completion Code indicates whether the request completed successfully ornot. Completion Code 指示请求是否成功完成。 Data Zero or more bytes of data. The BMC always returns a response toacknowledge the request, regardless of whether data is returned or not. 零个或多个字节的数据。 无论是否返回数据，BMC 都会返回响应来确认请求。 9.4 Logging Events from System Software via KCS InterfaceThe KCS Interface can be used for sending Event Messages from system softwareto the BMC Event Receiver. The following figures show the format for KCSInterface Event Request and corresponding Event Response messages. Note thatonly Event Request Messages to the BMC via the KCS Interface have a Software IDfield. This is so the Software ID can be saved in the logged event. KCS 接口可用于将Event Message 从系统软件发送到 BMC Event Receiver。 下图显示了KCS接口Event Request 和相应 Event Response 消息的格式。 请注意，只有通过 KCS 接口发送至 BMC 的事件请求消息才具有软件 ID 字段。 这样软件 ID 就可以保存在记录的事件中。 designates /ˈdezɪɡneɪts/ 指定；命名；选定，指派，委任9.5 KCS Interface RegistersThe KCS Interface defines a set of I/O mapped communication registers. The bitdefinitions, and operation of these registers follows that used in the Intel8742 Universal Peripheral Interface microcontroller. The term ‘KeyboardController Style’ reflects the fact that the 8742 interface is used as thesystem keyboard controller interface in PC architecture computer systems. KCS 接口定义了一组 I/O 映射通信寄存器。 这些寄存器的位定义和操作遵循 Intel 8742 Universal Peripheral 微控制器中使用的定义和操作。 术语“Keyboard ControllerStyle” 反映了 8742 接口在 PC 架构计算机系统中用作系统键盘控制器接口.The specification of the KCS Interface registers is given solely with respectto the ‘system software side’ view of the interface in system I/O space. Thefunctional behavior of the management controller to support the KCS Interfaceregisters is specified, but the physical implementation of the interface andthe organization of the interface from the management controller side isimplementation dependent and is beyond the scope of this specification. KCS 接口寄存器的规范仅针对系统 I/O 空间中接口的“系统软件端”视图给出。 管理控制器支持 KCS 接口寄存器的功能行为已被指定，但管理控制器侧接口的物理实现和接口的组织取决于实现，并且超出了本规范的范围。On the system side, the registers are mapped to system I/O space and consistsof four byte-wide registers. 在系统方面，寄存器映射到系统I/O空间并由四个byte-wide的寄存器组成。 Status Register - provides flags and status bits for use in variousdefined operations. 提供了在各种定义的操作中使用的标志和状态位。 Command Register - provides port into which ‘Write Control Codes’ may bewritten. 提供可以写入“Write Control Codes”的端口。 Data_In - provides a port into which data bytes and ‘Read Control Codes’may be written. 提供一个可以写入数据字节和“Read Control Codes”的端口。 Data_Out - provides a port from which data bytes may be read. 提供可以读取数据字节的端口。 The default system base address for an I/O mapped KCS SMS Interface is CA2h.Refer to Appendix C1 - Locating IPMI System Interfaces via SM BIOS Tables forinformation on using SM BIOS tables for describing optional interrupt usage,memory mapped registers, 32-bit and 16-byte aligned registers, and alternativeKCS interface addresses. Software can assume the KCS interface registers areI/O mapped and byte aligned at the default address unless other information isprovided. Appendix [əˈpendɪks]: 附录 I/O 映射的 KCS SMS 接口的默认系统基地址是 CA2h。 请参阅附录 C1 - 通过 SM BIOS表定位 IPMI 系统接口，了解有关使用 SM BIOS 表描述可选中断使用、内存映射寄存器、32 位和 16 字节对齐寄存器以及备用 KCS 接口地址的信息。 软件可以假定 KCS 接口寄存器已进行 I/O 映射并在默认地址处进行字节对齐，除非提供其他信息9.6 KCS Interface Control CodesControl codes are used for ‘framing’ message data transferred across the KCSInterface. Control Codes are used to: frame: n. 框架 v. 表达,指定,拟定 Control Code 用于“构建”通过 KCS 接口传输的消息数据。 控制代码用于： Identify the first and last bytes of a packet. Identify when an error/abort has occurred. Request additional data bytes.9.7 Status RegisterSystem software always initiates a transfer. If the BMC has a message for SMS,it can request attention by setting the SMS_ATN bit in the status register.System software then detects the flag and initiates the transfer. initiates [ɪˈnɪʃieɪts] 系统软件总是发起传输。 如果 BMC 有 SMS 消息，则可以通过设置状态寄存器中的 SMS_ATN 位来请求关注。 然后系统软件检测该标志并启动传输。Other bits in the status register are used to arbitrate access to the commandand data registers between the BMC and system software and to indicate the“state” (write, read, error, or idle) of the current transaction. The followingtables summarize the functions of the Status Register bits. arbitrate [ˈɑːrbɪtreɪt]: v. 仲裁,中断 状态寄存器中的其他位用于仲裁 BMC 和系统软件之间对命令和数据寄存器的访问，并指示当前事务的“状态”（写入、读取、错误或空闲）。 下表总结了状态寄存器位的功能。 C/D#: Specifies whether the last write was to the Command register or the Data_Inregister (1=command, 0=data). Set by hardware to indicate whether last writefrom the system software side was to Command or Data_In register. 指定最后一次写入是写入命令寄存器还是 Data_In 寄存器（1=命令，0=数据）。 由硬件设置以指示系统软件端的最后一次写入是对 Command 还是 Data_In 寄存器。 IBF Automatically set to 1 when either the associated Command or Data_In registerhas been written by system-side software. 当系统端软件写入关联的 Command 或 Data_In 寄存器时，自动设置为 1。 OBF Set to 1 when the associated Data_Out register has been written by the BMC. 当系统端软件写入关联的 Command 或 Data_In 寄存器时，自动设置为 1。 Bits 7:6 are state bits that provide information that is used to ensure thatthe BMC and system software remain in sync with one another. Following are thepossible states and their meaning: 位 7:6 是状态位，提供用于确保 BMC 和系统软件保持彼此同步的信息。 以下是可能的状态及其含义： IDLE_STATE. Interface is idle. System software should not be expecting nor sending any data. 接口空闲。 系统软件不应期待也不发送任何数据。 READ_STATE. BMC is transferring a packet to system software. System software should be in the “Read Message” state. BMC 正在向系统软件传输数据包。 系统软件应处于“Read Message”状态。 WRITE_STATE. BMC is receiving a packet from system software. System software should bewriting a command to the BMC. BMC正在接收来自系统软件的数据包。 系统软件应该向 BMC 写入命令。 ERROR_STATE. BMC has detected a protocol violation at the interface level, or the transfer has been aborted. System software can either use the “Get_Status’ control code to request the nature of the error, or it can just retry the command. nature : n. 自然; 性质 BMC 检测到接口级别的协议违规，或者传输已中止。 系统软件可以使用“Get_Status” controlcode 来请求错误的性质，也可以仅重试该命令。 9.7.1 SMS_ATN Flag UsageThe SMS_ATN flag is used to indicate that the BMC requires attention fromsystem software. This could either be because a message was received into theReceive Message Queue and ready for delivery to system software, the EventMessage Buffer is full (if the Event Message Buffer is configured to generatean interrupt to system management software), a watchdog pre-timeout occurred,or because of an OEM event. Flags in the BMC identify which conditions arecausing the SMS_ATN flag to be set. All conditions must be cleared (i.e. allmessages must be flushed) in order for the SMS_ATN bit to be cleared. SMS_ATN 标志用于指示 BMC 需要系统软件的关注。 这可能是因为消息已接收到接收消息队列并准备好传送到系统软件、事件消息缓冲区已满（如果事件消息缓冲区配置为向系统管理软件生成中断）、看门狗预置 发生超时，或由于 OEM 事件。 BMC 中的标志标识哪些条件导致 SMS_ATN 标志被设置。 必须清除所有条件（即必须刷新所有消息）才能清除 SMS_ATN 位。The SMS_ATN bit is also used when the KCS interface is interrupt driven, orwhen OEM events or watchdog pre-timeouts generate a system interrupt. Refer tosections 9.12, KCS Communication and Non-communication Interrupts, 9.13,Physical Interrupt Line Sharing, and 9.14, Additional Specifications for theKCS interface for additional information on the use and requirements for theSMS_ATN bit. 当 KCS 接口是中断驱动时，或者当 OEM 事件或看门狗预超时生成系统中断时，也会使用 SMS_ATN 位。 有关 SMS_ATN 位的使用和要求的附加信息，请参阅第 9.12 节“KCS 通信和非通信中断”、第 9.13 节“物理中断线路共享”和 9.14 节“KCS 接口的附加规范”。9.8 Command RegisterThe Command register must only be written from the system side when the IBFflag is clear. Only WRITE_START, WRITE_END, or GET_STATUS/ABORT Control Codesare written to the command register. 仅当 IBF 标志清零时，才必须从系统侧写入命令寄存器。 仅 WRITE_START、WRITE_END 或 GET_STATUS/ABORT Control Codes 会写入Control Register。9.9 Data RegistersPackets to and from the BMC are passed through the data registers. These bytescontain all the fields of a message, such as the Network Function code, CommandByte, and any additional data required for the Request or Response message. 进出 BMC 的数据包通过Data Register 传递。 这些字节包含消息的所有字段，例如NetFn、Command Byte 以及Request 或 Response 消息所需的任何附加数据。The Data_In register must only be written from the system side when the IBFflag is clear. The OBF flag must be set (1) before the Data_Out register can beread (see status register). 仅当 IBF 标志清零时，才必须从系统侧写入 Data_In 寄存器。 在读取 Data_Out 寄存器之前，必须将 OBF 标志设置为 (1)（请参阅状态寄存器）。9.10 KCS Control CodesThe following table details the usage of ‘Control Codes’ by the KCS interface.9.11 Performing KCS Interface Message TransfersSystem Management Software that uses the KCS Interface will typically berunning under a multi-tasking operating system. This means transfers with theBMC may be interrupted by higher priority tasks or delayed by other SystemManagement Software processing. The SMS channel handshake is optimized to allowthe BMC to continue to perform tasks between data byte transfers with SystemManagement Software. The BMC does not time out data byte transfers on the SMSinterface. 使用 KCS 接口的系统管理软件通常在多任务操作系统下运行。 这意味着与 BMC 的传输可能会被更高优先级的任务中断或被其他系统管理软件处理延迟。SMS 通道握手经过优化，允许 BMC 通过系统管理软件在数据字节传输之间继续执行任务。BMC 不会使 SMS 接口上的数据字节传输超时。Request and Response Messages are paired together as a Write Transfer to theBMC to send the request followed by a Read Transfer from the BMC to get theresponse. Request 和 Response 消息配对在一起，作为向 BMC 的Write Transfer 以发送请求，然后从 BMC 进行Read Transfer 以获取响应。The process, as seen from the system perspective is depicted in Figure 9-6, KCSInterface SMS to BMC Write Transfer Flow Chart, and Figure 9-7, KCS InterfaceBMC to SMS Read Transfer Flow Chart, below. 从系统角度来看，该过程如图 9-6（KCS 接口 SMS 到 BMC 写入传输流程图）和图 9-7（KCS 接口 BMC 到 SMS 读取传输流程图）所示。During the write transfer each write of a Control Code to the command registerand each write of a data byte to Data_In will cause IBF to become set,triggering the BMC to read in the corresponding Control Code or data byte. 在写传输期间，每次向命令寄存器写入Control Code 以及每次向 Data_In 写入数据字节都会导致 IBF 置位，从而触发 BMC 读入相应的 Control Code 或数据字节。If the KCS interface uses an interrupt, the BMC will write a dummy value of 00hto the output data register after it has updated the status register and readthe input buffer. This generates an ‘OBF’ interrupt. The points at which thiswould occur are shown as “OBF” in Figure 9-6, KCS Interface SMS to BMC WriteTransfer Flow Chart, below. 如果KCS接口使用中断，则BMC在更新状态寄存器并读取输入缓冲区后将向输出数据寄存器写入虚拟值00h。 这会生成“OBF”中断。 发生这种情况的点在下面的图 9-6 KCS 接口 SMS 到 BMC 写入传输流程图中显示为“OBF”。During the read phase, each write of a READ Control Code to Data_In will causeIBF to become set, causing the BMC to read in the Control Code and write a databyte to Data_Out in response. If the KCS interface uses an interrupt, the writeof the data byte to Data_Out will also generate an interrupt. The point atwhich this would occur during the READ_STATE is shown as “OBF” in Figure 9-7,KCS Interface BMC to SMS Read Transfer Flow Chart, below. 在读取阶段，每次向 Data_In 写入 READ 控制代码都会导致 IBF 置位，从而导致 BMC 读取控制代码并将数据字节写入 Data_Out 作为响应。 如果KCS接口使用中断，则将数据字节写入Data_Out也会产生中断。 在 READ_STATE 期间发生这种情况的点在下面的图9-7 KCS 接口 BMC 到 SMS 读取传输流程图中显示为“OBF”。Note that software does not need to use the Get Status/Abort transaction toreturn the interface to the IDLE_STATE or handle an error condition. Theinterface should return to IDLE_STATE on successful completion of any fullcommand/response transaction with the BMC. Thus, since the interface will allowa command transfer to be started or restarted at any time when the input bufferis empty, software could elect to simply retry the command upon detecting anerror condition, or issue a ‘known good’ command in order to clear ERROR_STATE. 请注意，软件不需要使用Get Status/Abort translation 来将接口返回到 IDLE_STATE 或处理错误情况。成功完成与 BMC 的任何完整命令/响应事务后，接口应返回 IDLE_STATE。因此，由于接口将允许在输入缓冲区为空时随时启动或重新启动命令传输，因此软件可以选择在检测到错误情况时简单地重试该命令，或者发出“known good”命令，以便 清除 ERROR_STATE。9.12 KCS Communication and Non-communication InterruptsThe following lists some general requirements and clarifications to supportboth KCS communication and KCS non-communication interrupts on the sameinterrupt line using the OBF signal. A KCS communications interrupt is definedas an OBF-generated interrupt that occurs during the process of sending arequest message to the BMC and receiving the corresponding response. Thisoccurs from the start of the write (request) phase of the message (issuingWRITE_START to the command register) through to the normal conclusion of thecorresponding read (response) phase of the message. (The conclusion of thecommunications interval is normally identified by the interface going toIDLE_STATE). KCS communications interrupts are also encountered during thecourse of processing a GET_STATUS/ABORT control code. clarifications [ˌklɛrɪfɪˈkeɪʃənz]: 澄清（法）；净（纯）化；清化（理）；说（阐）明；解释conclusion [kənˈkluːʒn] 结论; 结果; 结束; 缔结; 达成; 签订; 结局; 推论; 结尾interval: /ˈɪntərvl/: 间隔；(时间上的)间隙；间歇; 下面列出了使用 OBF 信号在同一中断线上支持 KCS 通信和 KCS 非通信中断的一些一般要求和说明。 KCS通信中断被定义为OBF-generated 的中断，该中断发生在向BMC发送请求消息并接收相应响应的过程中。 这发生在从消息的写入（请求）阶段（向命令寄存器发出 WRITE_START）开始到消息的相应读取（响应）阶段正常结束之间。 （通信间隔的结束通常由接口进入 IDLE_STATE 来标识）。 在处理 GET_STATUS/ABORT 控制代码的过程中也会遇到 KCS 通信中断。A KCS non-communication interrupt is defined as an OBF-generated interrupt thatoccurs when the BMC is not in the process of transferring message data orgetting error status. This will typically be an interrupt that occurs while theinterface is in the IDLE_STATE. KCS非通信中断被定义为OBF-generated 的中断，当BMC不在传输消息数据或获取错误状态的过程中时发生。 这通常是接口处于 IDLE_STATE 时发生的中断。There are several options in the BMC that can be enabled to cause KCSnon-communication interrupts as described in the Set BMC Global Enablescommand, and Get Message Flags commands. These are the watchdog timerpre-timeout interrupt, event message buffer interrupt, receive message queueinterrupt, and the OEM interrupts. Software can detect which of the standardinterrupts are supported by attempting to enable them using the Set BMC GlobalEnables command and checking for an error completion code. BMC 中有多个选项可以启用以导致 KCS 非通信中断，如 Set BMC Global Enables Command,和Get Message Flags commands中所述。 它们是看门狗定时器预超时中断、事件消息缓冲区中断、接收消息队列中断和 OEM 中断。 软件可以通过尝试使用 Set BMC Global Enables 命令启用标准中断并检查错误完成代码来检测支持哪些标准中断。9.13 Physical Interrupt Line SharingA typical interrupt-driven implementation will assert a physical interrupt linewhen OBF is asserted. In order to allow a single interrupt line to serve forboth communication and non-communication interrupts, the physical interruptline must be automatically deasserted by the BMC whenever a communication phasebegins, even if there is a pending non-communications interrupt to be serviced.This is necessary so the interrupt line can be used for signaling communicationinterrupts . Once the communication operations have completed (return to idlephase) the controller must re-assert the interrupt line if thenon-communications interrupt is still pending. 典型的中断驱动实现将在 OBF 置位时置位物理中断线。 为了允许单个中断线同时服务于通信和非通信中断，无论何时通信阶段开始，物理中断线都必须由 BMC 自动取消置位，即使有待处理的非通信中断需要服务。 这是必要的，因此中断线可用于发出通信中断信号。一旦通信操作完成（返回空闲阶段），如果非通信中断仍待处理，控制器必须重新断言中断线。9.14 Additional Specifications for the KCS interfaceThis section lists additional specifications for the KCS interface. The BMC must generate an OBF whenever it changes the status to ERROR_STATE.This will ensure that any transition to ERROR_STATE will cause the interrupthandler to run and catch the state. 每当 BMC 将状态更改为 ERROR_STATE 时，都必须生成 OBF。 这将确保任何到 ERROR_STATE 的转换都会导致中断处理程序运行并捕获状态。 The BMC generates an OBF upon changing the status to IDLE_STATE. An IPMI 1.5implementation is allowed to share this interrupt with a pending KCSnon-communication interrupt, or it elect to always generate a separate OBFinterrupt for non-communications interrupts. BMC 在状态更改为 IDLE_STATE 时生成 OBF。 IPMI 1.5 实现允许与待处理的 KCS 非通信中断共享此中断，或者选择始终为非通信中断生成单独的 OBF 中断。 A BMC implementation that elects to always generate a separatenon-communications interrupt must wait for the OBF interrupt that signalsentering the IDLE_STATE to be cleared before it asserts an OBF interrupt forthe non-communications interrupt. elects /ɪˈlekts/: 选举; 选择; 决定 选择始终生成单独的非通信中断的 BMC 实现必须等待信号进入 IDLE_STATE 的 OBF中断被清除，然后才能为非通信中断断言 OBF 中断。 IPMI v1.5 systems are allowed to generate a single OBF that covers both thelast communications interrupt (when the BMC status goes to IDLE_STATE) and apending non-communications interrupt. I.e. it is not required to generate aseparate OBF interrupt for the non-communications interrupt if anon-communications interrupt was pending at the time the BMC status goes toIDLE_STATE. In order to support this, an IPMI v1.5 KCS interfaceimplementation must set SMS_ATN for all standard (IPMI defined)non-communication interrupt sources. IPMI v1.5 系统允许生成单个 OBF，该 OBF 涵盖最后的通信中断（当 BMC 状态进入 IDLE_STATE 时）和待处理的非通信中断。 IE。 如果 BMC 状态进入 IDLE_STATE 时非通信中断处于待处理状态，则不需要为非通信中断生成单独的 OBF 中断。 为了支持这一点，IPMI v1.5 KCS 接口实现必须为所有标准（IPMI 定义）非通信中断源设置 SMS_ATN。 For IPMI v1.5, the BMC must set the SMS_ATN flag if any of the standardmessage flags become set. This includes Receive Message Available, EventMessage Buffer Full (if the Event Message Buffer Full condition is intendedto be handled by System Management Software), and Watchdog Timer pre-timeoutflags, as listed in the Get Message Flags command. This is independent ofwhether the corresponding interrupt is enabled or not. 对于 IPMI v1.5，如果设置了任何标准消息标志，BMC 必须设置 SMS_ATN 标志。 这包括接收消息可用、事件消息缓冲区已满（如果事件消息缓冲区已满条件旨在由系统管理软件处理）和看门狗定时器预超时标志，如获取消息标志命令中列出的。 这与相应的中断是否启用无关。 The BMC must change the status to ERROR_STATE on any condition where itaborts a command transfer in progress. For example, if the BMC had an OEMcommand that allowed the KCS interface to be asynchronously reset via IPMB,the KCS interface status should be put into the ERROR_STATE and OBF set, notIDLE_STATE, in order for software to be notified of the change. However, theBMC does not change the status to the ERROR_STATE, but to the IDLE_STATE,when the BMC executes the Get Status/Abort control code from SMS I/F, even ifthe Get Status/Abort control code is used to abort a transfer. 在任何中止正在进行的命令传输的情况下，BMC 都必须将状态更改为 ERROR_STATE。 例如，如果 BMC 有一个 OEM 命令允许通过 IPMB 异步重置 KCS 接口，则应将 KCS 接口状态设置为 ERROR_STATE 和 set OBF， 而不是 IDLE_STATE ，以便向软件通知更改。 然而，当 BMC 从 SMS I/F 执行 Get Status/Abort 控制代码时，BMC 不会将状态更改为 ERROR_STATE，而是更改为 IDLE_STATE，即使 Get Status/Abort控制代码用于中止传输 。 A cross-platform driver must be able to function without handling any of theOEM bits. Therefore, enabling SMS_ATN on OEM interrupts/states must not beenabled by default, but must be explicitly enabled either by the Set BMCGlobal Enables command or by an OEM-defined command. 跨平台驱动程序必须能够在不处理任何 OEM 位的情况下运行。 因此，默认情况下不得启用 OEM 中断/状态上的 SMS_ATN，而必须通过 Set BMC Global Enables 命令或 OEM 定义的命令显式启用。 The SMS_ATN bit will remain set until all standard interrupt sources in theBMC have been cleared by the Clear Message Flags command, or by acorresponding command. For example, the Read Message command canautomatically clear the Receive Message Queue interrupt if the commandempties the queue. SMS_ATN 位将保持设置状态，直到 BMC 中的所有标准中断源已被Clear Message Flag command 或相应的命令清除。 例如，如果读取消息命令清空队列，则该命令可以自动清除接收消息队列中断。 A KCS interface implementation that allows its interrupt to be shared withother hardware must set SMS_ATN whenever it generates a KCS interrupt. Asystem will typically report whether it allows an interrupt to be shared ornot via resource usage configuration reporting structures such as those inACPI. 允许与其他硬件共享其中断的 KCS 接口实现必须在生成 KCS 中断时设置 SMS_ATN。 系统通常会通过资源使用配置报告结构（例如 ACPI 中的报告结构）来报告是否允许共享中断。 OEM non-communications interrupts should be disabled by default. They must bereturned to the disabled state whenever the controller or the system ispowered up or reset. This is necessary to allow a generic driver to be usedwith the controller. A driver or system software must be explicitly requiredto enable vendor-specific non- communications interrupt sources in order forthem to be used. OEM non-communications interrupt sources must not contributeto SMS_ATN when they are disabled. The OEM 0, 1, and 2 flags that are returned by the Get Message Flags commandmay also cause the SMS_ATN flag to be set. A platform or system software mustnot enable these interrupts/flags unless there is a corresponding driver thatcan handle them. Otherwise, a generic cross-platform driver could get into asituation where it would never be able to clear SMS_ATN. It is recommended that any OEM generated non-communications interrupts causeat least one of the OEM flags in the Get Message Flags to become set. Thiswill enable improving system efficiency by allowing a cross- platform driverto pass the value of the Get Message Flags to an OEM extension, saving theOEM extension software from having to issue an additional command todetermine whether it has an anything to process. It is recommended that an OEM that uses the OEM flags sets the SMS_ATN flagif one or more of the OEM flags (OEM 0, OEM 1, or OEM 2) becomes set,especially if those flags can be the source of a KCS non- communicationsinterrupt. The driver can use SMS_ATN as the clue to execute the Get MessageFlags command and pass the data along to an OEM extension routine. OEM non-communications interrupts may elect to either share the IDLE_STATEOBF interrupt with the non- communications interrupt OBF, or generate aseparate non-communications OBF interrupt. If the OEM non- communicationsinterrupt implementation shares the IDLE_STATE OBF interrupt, the OEM non-communications interrupt must also set SMS_ATN. OEM 非通信中断可以选择与非通信中断 OBF 共享 IDLE_STATE OBF 中断，或者生成单独的非通信 OBF 中断。 如果 OEM 非通信中断实现共享 IDLE_STATE OBF 中断，则 OEM 非通信中断还必须设置 SMS_ATN。 9.15 KCS Flow DiagramsThe following flow diagrams have been updated from corresponding diagrams inthe original IPMI v1.0, rev. 1.1 specification. This information applies to thefollowing flow diagrams: 以下流程图已根据原始 IPMI v1.0 修订版中的相应图表进行了更新。 1.1 规范。 此信息适用于以下流程图： All system software wait loops should include error timeouts. For simplicity,such timeouts are not shown explicitly in the flow diagrams. A five-secondtimeout or greater is recommended. 所有系统软件等待循环都应包括错误超时。 为简单起见，流程图中未明确显示此类超时。 建议设置五秒或更长的超时时间。 The phase values represent state information that could be kept acrossdifferent activations of an interrupt handler, and corresponding entrypoints. Based on the ‘phase’ the interrupt handler would branch to thecorresponding point when an OBF interrupt occurred. The information may alsobe useful for error reporting and handling for both polled- andinterrupt-driven drivers. Note that other state may need to be kept as well.For example, during the ‘wr_data’ phase, the handler may also need topreserve a byte counter in order to track when the last byte of the write wasto be sent. phase 值表示可以在中断处理程序的不同activations 和相应的入口点之间保存的状态信息。当 OBF 中断发生时，中断处理程序将根据“phase”分支到相应的点。 该信息对于轮询驱动驱动程序和中断驱动驱动程序的错误报告和处理也可能有用。 请注意，可能还需要保留其他状态。 例如，在“wr_data” phase ，处理程序可能还需要保留字节计数器，以便跟踪写入的最后一个字节何时发送。 The symbol of a circle with an arrow and the text ‘OBF’ inside the circlerepresents the points where the BMC would write a dummy data byte to theoutput buffer in order to create an OBF interrupt. The label above the circleindicates where an interrupt handler would branch to when the OBF interruptoccurs under in the corresponding phase. An interrupt handler would exit uponcompleting the step that occurs before where the OBF interrupt symbol points. 带箭头的圆圈符号和圆圈内的文本“OBF”代表 BMC 将dummy data byte 写入输出缓冲区以创建OBF 中断的点。圆圈上方的标签表示在相应阶段发生 OBF 中断时中断处理程序将分支到的位置。中断处理程序将在完成 OBF 中断符号指向之前发生的步骤后退出。 wr_start OBF BMC sets status to WRITE_STATE immediately after receiving any control code inthe command register unless it needs to force an ERROR_STATE. The status isset before reading the control code from the input buffer. In the unlikely event that an asynchronous interrupt occurs after clearing OBFthe interrupt handler may spin waiting for IBF=0. BMC 在接收到命令寄存器中的任何控制代码后立即将状态设置为 WRITE_STATE，除非需要强制 ERROR_STATE。 在从输入缓冲区读取Control Code 之前设置状态。 万一在清除 OBF 后发生异步中断，中断处理程序可能会旋转等待 IBF=0。 wr_data OBF BMC updates state after receiving data byte in DATA_IN, but before reading thebyte out of the input buffer. I.e. it changes state while IBF=1 BMC 在接收 DATA_IN 中的数据字节之后、从输入缓冲区读取该字节之前更新状态。 IE。 当 IBF=1 时它改变状态 before READ The BMC sets state to READ_STATE before reading data byte from data register.This ensures state change to READ_STATE occurs while IBF=1 在从data register 读取数据字节之前，BMC 将状态设置为 READ_STATE。 这确保在 IBF=1 时发生状态更改为 READ_STATE read OBF This OBF is normally caused by the BMC returning a data byte for the readoperation. After the last data byte, the BMC sets the state to IDLE_STATE whileIBF=1 and then reads the input buffer to check the control code = READ. Thestatus will be set to ERROR_STATE if the control code is not READ. The BMC thenwrites a dummy data byte to the output buffer to generate an interrupt so thedriver can see the status change. 此 OBF 通常是由 BMC 返回读操作的数据字节引起的。 在最后一个数据字节之后，当IBF=1 时，BMC 将状态设置为 IDLE_STATE，然后读取输入缓冲区以检查控制代码 = READ。 如果控制代码未读取，状态将设置为 ERROR_STATE。 然后，BMC 将虚拟数据字节写入输出缓冲区以生成中断，以便驱动程序可以看到状态更改。 Note that software must track that it has received an interrupt from‘IDLE_STATE’ while it is still in the ‘read’ phase in order to differentiate itfrom a non-communication interrupt. If the BMC needs to set the status toERROR_STATE it will do so before writing a dummy 00h byte to the output buffer. (The BMC always places a dummy byte in the output buffer whenever it sets thestatus to ERROR_STATE.) 请注意，软件必须跟踪它在仍处于“read” phase 时已从“IDLE_STATE”接收到中断，以便将其与非通信中断区分开来。 如果 BMC 需要将状态设置为 ERROR_STATE，它将在将dummy 00h byte 写入输出缓冲区之前执行此操作。 （每当 BMC 将状态设置为 ERROR_STATE 时，BMC 总是在输出缓冲区中放置一个 dummy byte。） Read dummy data byte from DATA_OUT The BMC must wait for software to read the output buffer before writing OBF togenerate a non-communications interrupt. That is, if there are any pendinginterrupts while in IDLE_STATE, but OBF is already set, it must hold off theinterrupt until it sees OBF go clear. Softw are must be careful, since missingany read of the output buffer will effectively disable interrupt generation. Itmay be a prudent safeguard for a driver to poll for OBF occassionallyw henwaiting for an interrupt from the IDLE state. BMC 必须等待软件读取输出缓冲区后才能写入 OBF 以产生非通信中断。 也就是说，如果处于 IDLE_STATE 时有任何挂起的中断，但 OBF 已设置，则必须推迟中断，直到看到OBF 清除为止。 软件必须小心，因为错过任何输出缓冲区的读取将有效地禁用中断生成。 对于驱动程序来说，在等待来自 IDLE 状态的中断时偶尔轮询 OBF 可能是一种谨慎的保护措施。 Note that for IPMI v1.5, the last OBF interrupt is allowed to be shared with apending non-communications interrupt. See text. 请注意，对于 IPMI v1.5，允许最后一个 OBF 中断与待处理的非通信中断共享。 见正文。 9.16 Write Processing SummaryThe following summarizes the main steps write transfer from system software tothe BMC: Issue a ‘WRITE_START’ control code to the Command register to start thetransaction. Write data bytes (NetFn, Command, Data) to Data_In. Issue an ‘WRITE_END’ control code then the last data byte to conclude thewrite transaction.9.17 Read Processing SummaryThe following summarizes the main steps for a read transfer from the BMC tosystem software: Read Data_Out when OBF set Issue READ command to request additional bytes If READ_STATE (after IBF = 0), repeat previous two steps.9.18 Error Processing SummaryThe following summarizes the main steps by which system software processes KCSInterface errors: Issue a ‘GET_STATUS/ABORT’ control code to the Command register. Wait forIBF=0. State should be WRITE_STATE. If OBF=1, Clear OBF by reading Data_Out register. Write 00h to data register, wait for IBF=0. State should now be READ_STATE. Wait for OBF=1. Read status from Data_Out Conclude by writing READ to data register, wait for IBF=0. State should be IDLE.9.19 Interrupting Messages in ProgressIf, during a message transfer, the system software wants to abort a message itcan do so by the following methods: 如果在消息传输过程中，系统软件想要中止消息，可以通过以下方法来完成： Place another “WRITE_START” command into the Command Register (a WRITE_STARTControl Code is always legal). The BMC then sets the state flags to“WRITE_STATE” and sets its internal flags to indicate that the stream hasbeen aborted. 将另一个“WRITE_START”命令放入命令寄存器（WRITE_START 控制代码始终合法）。 然后，BMC 将状态标志设置为“WRITE_STATE”，并设置其内部标志以指示流已被中止。 Send a “GET_STATUS/ABORT” request. This is actually the same as #1 above butis explicitly stated to indicate that this command will cause the currentpacket to be aborted. This command allows a stream to be terminated and thestate to be returned to IDLE without requiring a complete BMC request andresponse transfer. 发送“GET_STATUS/ABORT”请求。 这实际上与上面的 #1 相同，但明确指出该命令将导致当前数据包中止。 该命令允许终止流并将状态返回到 IDLE，而不需要完整的BMC 请求和响应传输。 9.20 KCS Driver Design Recommendations A generic, cross-platform driver that supports the interrupt-driven KCSinterface is not required to handle interrupts other than the interruptsignal used for IPMI message communication with the BMC. The messageinterrupt may be shared with other BMC interrupt sources, such as thewatchdog timer pre-timeout interrupt, the event message buffer fullinterrupt, and OEM interrupts. A cross-platform driver should use the Get BMC Global Enables and Set BMCGlobal Enables commands in a ‘read-modify-write’ manner to avoid modifyingthe settings of any OEM interrupts or flags. It is recommended that cross-platform driver software provide a ‘hook’ thatallows OEM extension software to do additional processing of KCSnon-communication interrupts. It is highly recommended that the driverexecute the Get Message Flags command whenever SMS_ATN remains set afternormal processing and provide the results to the OEM extension software. The driver cannot know the whether the pre-existing state of any OEMinterrupts or flags is correct. Therefore, a driver that supports OEMextensions should allow for an OEM initialization routine that can configurethe OEM flags/interrupts before KCS OBF-generated interrupts are enabled. It is recommended that cross-platform drivers or software make provision forBMC implementations that may miss generating interrupts on a command errorcondition by having a timeout that will activate the driver or software incase an expected interrupt is not received. A driver should be designed to allow for the possibility that an earlier BMCimplementation does not set the SMS_ATN flag except when there is data in theReceive Message Queue. If the driver cannot determine whether SMS_ATN issupported for all enabled standard flags or not, it should issue a GetMessage Flags command whenever it gets a KCS non-communication interrupt. A driver or system software can test for whether the Watchdog Timerpre-timeout and/or Event Message Buffer Full flags will cause SMS_ATN tobecome set. This is accomplished by disabling the associated interrupts (ifenabled) and then causing a corresponding action that sets the flag. This isstraightforward by using the watchdog timer commands in conjunction with theSet BMC Global Enables and Get Message Flags commands. For example, to test for the Event Message Buffer Full flag setting SMS_ATN,first check to see if the Event Message Buffer feature is implemented byattempting to enable the event message buffer using the Set and Get BMCGlobal Enables command. If the feature is not implemented, an errorcompletion code will be returned. Next, disable event logging and use thewatchdog timer to generate an SMS/OS ‘no action’ timeout event, then see ifthe SMS_ATN becomes set. If so, use the Get Message Flags command to verifythat the Event Message Buffer Full flag is the only one set (in case anasynchronous message came in to the Receive Message Queue during the test.)The pre-timeout interrupt can be testing in a similar manner. It is possible (though not recommended) for a BMC implementation to includeproprietary non- communication interrupt sources that do not set SMS_ATN.These sources must not be enabled by default. It is recommended that ageneric cross-platform driver have provisions for OEM extensions that getcalled whenever a non-communication interrupt occurs. It is recommended thatthe extension interface provides the last reading of the KCS flags so that anOEM extension can see the state of SMS_ATN. Software should be aware that IPMI v1.0 implementations were not required toset SMS_ATN for all non- communication interrupts. If a BMC implementationdoes not set SMS_ATN for all non-communication interrupts, it must generate aseparate OBF interrupt for non-communication interrupts. A controller thatdoes not set SMS_ATN for all non-communication interrupts is not allowed touse the same OBF interrupt to signal the both completion of communicationsand a non-communications interrupt. Regardless of whether the IDLE_STATE OBF interrupt is shared with a pendingnon-communications interrupt, software drivers must examine SMS_ATN afterclearing OBF. If SMS_ATN is asserted the driver must process thenon-communications interrupt sources. " }, { "title": "VIRTUAL MACHINE CONTROL STRUCTURES", "url": "/posts/virtual-machine-control-structures/", "categories": "intel_sdm", "tags": "virt", "date": "2024-04-26 15:00:00 +0800", "snippet": " FROM intel sdm CHAPTER 25 VIRTUAL MACHINE CONTROL STRUCTURES 25.1 OVERVIEWA logical processor uses virtual-machine control data structures (VMCSs) whileit is in VMX operation. These manage tran...", "content": " FROM intel sdm CHAPTER 25 VIRTUAL MACHINE CONTROL STRUCTURES 25.1 OVERVIEWA logical processor uses virtual-machine control data structures (VMCSs) whileit is in VMX operation. These manage transitions into and out of VMX non-rootoperation (VM entries and VM exits) as well as processor behavior in VMXnon-root operation. This structure is manipulated by the new instructionsVMCLEAR, VMPTRLD, VMREAD, and VMWRITE.A VMM can use a different VMCS for each virtual machine that it supports. For avirtual machine with multiple logical processors (virtual processors), the VMMcan use a different VMCS for each virtual processor.A logical processor associates a region in memory with each VMCS. This regionis called the VMCS region.1 Soft- ware references a specific VMCSusing the 64-bit physical address of the region (a VMCS pointer). VMCS pointersmust be aligned on a 4-KByte boundary (bits 11:0 must be zero). These pointersmust not set bits beyond the processor’s physical-address width.2,3A logical processor may maintain a number of VMCSs that are active. Theprocessor may optimize VMX operation by maintaining the state of an active VMCSin memory, on the processor, or both. At any given time, at most one of theactive VMCSs is the current VMCS. (This document frequently uses the term “theVMCS” to refer to the current VMCS.) The VMLAUNCH, VMREAD, VMRESUME, andVMWRITE instructions operate only on the current VMCS.The following items describe how a logical processor determines which VMCSs areactive and which is current: The memory operand of the VMPTRLD instruction is the address of a VMCS. Afterexecution of the instruction, that VMCS is both active and current on thelogical processor. Any other VMCS that had been active remains so, but noother VMCS is current. The VMCS link pointer field in the current VMCS (see Section 25.4.2) isitself the address of a VMCS. If VM entry is performed successfully with the1-setting of the “VMCS shadowing” VM-execution control, the VMCS referencedby the VMCS link pointer field becomes active on the logical processor. Theidentity of the current VMCS does not change. The memory operand of the VMCLEAR instruction is also the address of a VMCS.After execution of the instruction, that VMCS is neither active nor currenton the logical processor. If the VMCS had been current on the logicalprocessor, the logical processor no longer has a current VMCS. The VMPTRST instruction stores the address of the logical processor’s currentVMCS into a specified memory loca- tion (it stores the value FFFFFFFF_FFFFFFFFHif there is no current VMCS).The launch state of a VMCS determines which VM-entry instruction should be usedwith that VMCS: the VMLAUNCH instruction requires a VMCS whose launch state is“clear”; the VMRESUME instruction requires a VMCS whose launch state is“launched”. A logical processor maintains a VMCS’s launch state in thecorresponding VMCS region. The following items describe how a logical processormanages the launch state of a VMCS: If the launch state of the current VMCS is “clear”, successful execution ofthe VMLAUNCH instruction changes the launch state to “launched”. The memory operand of the VMCLEAR instruction is the address of a VMCS. Afterexecution of the instruction, the launch state of that VMCS is “clear”. There are no other ways to modify the launch state of a VMCS (it cannot bemodified using VMWRITE) and there is no direct way to discover it (it cannotbe read using VMREAD). Figure 25-1 illustrates the different states of a VMCS. It uses “X” to refer tothe VMCS and “Y” to refer to any other VMCS. Thus: “VMPTRLD X” always makes Xcurrent and active; “VMPTRLD Y” always makes X not current (because it makes Ycurrent); VMLAUNCH makes the launch state of X “launched” if X was current andits launch state was “clear”; and VMCLEAR X always makes X inactive and notcurrent and makes its launch state “clear”.The figure does not illustrate operations that do not modify the VMCS staterelative to these parameters (e.g., execution of VMPTRLD X when X is alreadycurrent). Note that VMCLEAR X makes X “inactive, not current, and clear,” evenif X’s current state is not defined (e.g., even if X has not yet beeninitialized). See Section 25.11.3.25.2 FORMAT OF THE VMCS REGIONA VMCS region comprises up to 4-KBytes.1 The format of a VMCS regionis given in Table 25-1.The first 4 bytes of the VMCS region contain the VMCS revision identifier atbits 30:0.1 Processors that maintain VMCS data in different formats (see below)use different VMCS revision identifiers. These identifiers enable soft- ware toavoid using a VMCS region formatted for one processor on a processor that usesa different format.2 Bit 31 of this 4-byte region indicates whether the VMCS isa shadow VMCS (see Section 25.10).Software should write the VMCS revision identifier to the VMCS region beforeusing that region for a VMCS. The VMCS revision identifier is never written bythe processor; VMPTRLD fails if its operand references a VMCS region whose VMCSrevision identifier differs from that used by the processor. (VMPTRLD alsofails if the shadow-VMCS indicator is 1 and the processor does not support the1-setting of the “VMCS shadowing” VM-execution control; see Section 25.6.2)Software can discover the VMCS revision identifier that a processor uses byreading the VMX capa- bility MSR IA32_VMX_BASIC (see Appendix A.1).Software should clear or set the shadow-VMCS indicator depending on whether theVMCS is to be an ordinary VMCS or a shadow VMCS (see Section 25.10). VMPTRLDfails if the shadow-VMCS indicator is set and the processor does not supportthe 1-setting of the “VMCS shadowing” VM-execution control. Software candiscover support for this setting by reading the VMX capability MSRIA32_VMX_PROCBASED_CTLS2 (see Appendix A.3.3).The next 4 bytes of the VMCS region are used for the VMX-abort indicator. Thecontents of these bits do not control processor operation in any way. A logicalprocessor writes a non-zero value into these bits if a VMX abort occurs (seeSection 28.7). Software may also write into this field.The remainder of the VMCS region is used for VMCS data (those parts of the VMCSthat control VMX non-root operation and the VMX transitions). The format ofthese data is implementation-specific. VMCS data are discussed in Section 25.3through Section 25.9. To ensure proper behavior in VMX operation, softwareshould maintain the VMCS region and related structures (enumerated in Section25.11.4) in writeback cacheable memory. Future implementations may allow orrequire a different memory type3. Software should consult the VMX capabilityMSR IA32_VMX_BASIC (see Appendix A.1)." }, { "title": "unrestricted guests", "url": "/posts/unrestricted-guests/", "categories": "intel_sdm", "tags": "virt", "date": "2024-04-25 11:21:00 +0800", "snippet": " FROM intel sdm CHAPTER 26 VMX NON-ROOT OPERATION 26.6 UNRESTRICTED GUESTS The first processors to support VMX operation require CR0.PE and CR0.PG to be 1in VMX operation (see Section 24.8). Th...", "content": " FROM intel sdm CHAPTER 26 VMX NON-ROOT OPERATION 26.6 UNRESTRICTED GUESTS The first processors to support VMX operation require CR0.PE and CR0.PG to be 1in VMX operation (see Section 24.8). This restriction implies that guestsoftware cannot be run in unpaged protected mode or in real-address mode. Laterprocessors support a VM-execution control called “unrestrictedguest”.1If this control is 1, CR0.PE and CR0.PG may be 0 in VMXnon-root operation. Such processors allow guest software to run in unpagedprotected mode or in real-address mode. The following items describe thebehavior of such software: 第一个支持 VMX operation 的处理器要求 CR0.PE 和 CR0.PG 在 VMX operation中为 1（参见第 24.8 节）。 此限制意味着guest软件不能在未分页保护模式或实地址模式下运行。更高版本的处理器支持称为“unrestricted guest”的 VM-execution control。1如果此控制字段为 1，则在 VMX non-root operation中 CR0.PE 和 CR0.PG 可能为 0。此类处理器允许guest软件在未分页保护模式或实地址模式下运行。 以下各项描述了此类软件的行为： The MOV CR0 instructions does not cause a general-protection exception simplybecause it would set either CR0.PE and CR0.PG to 0. See Section 26.3 fordetails. MOV CR0 指令不会仅仅因为将 CR0.PE 和 CR0.PG 设置为 0 而导致一般保护异常。有关详细信息，请参见第 26.3 节。 A logical processor treats the values of CR0.PE and CR0.PG in VMX non-rootoperation just as it does outside VMX operation. Thus, if CR0.PE = 0, theprocessor operates as it does normally in real-address mode (for example, ituses the 16-bit interrupt table to deliver interrupts and exceptions). IfCR0.PG = 0, the processor operates as it does normally when paging isdisabled. 逻辑处理器在 VMX non-root operation中处理 CR0.PE 和 CR0.PG 的值，就像在 VMX operation 之外一样。 因此，如果 CR0.PE = 0，处理器将像正常在实地址模式下一样运行（例如，它使用 16 位中断表来传递中断和异常）。 如果 CR0.PG = 0，则处理器在禁用分页时将正常运行。 Processor operation is modified by the fact that the processor is in VMXnon-root operation and by the settings of the VM-execution controls just asit is in protected mode or when paging is enabled. Instructions, interrupts,and exceptions that cause VM exits in protected mode or when paging isenabled also do so in real-address mode or when paging is disabled. Thefollowing examples should be noted: the fact that: ...的事实, 确切的说, 事实上, 实际上 处理器 operation 是由处理器处于VMX non-root operation 以及 VM-executioni controls 的设置来修改的(这个翻译不通)，就像它处于保护模式或启用分页时一样。在保护模式下或启用分页时导致VM退出的指令、中断和异常在实际地址模式下或禁用分页时也会这样做。应注意以下示例： 这里实际上是想表明, 在VMX non-root operation 下的行为由 VM-execution controls 控制, 和guest处于什么mode无关(protect ? paging?) If CR0.PG = 0, page faults do not occur and thus cannot cause VM exits. 如果CR0.PG=0，则不会发生page fault，因此不会导致VM exit。 If CR0.PE = 0, invalid-TSS exceptions do not occur and thus cannot cause VMexits. 如果CR0.PE=0，则不会发生invalid-TSS exception，因此不会导致VM exits。 If CR0.PE = 0, the following instructions cause invalid-opcode exceptionsand do not cause VM exits: INVEPT, INVVPID, LLDT, LTR, SLDT, STR, VMCLEAR,VMLAUNCH, VMPTRLD, VMPTRST, VMREAD, VMRESUME, VMWRITE, VMXOFF, and VMXON. 如果CR0.PE=0，则以下指令会导致 invalid-opcode 异常，并且不会导致VM exit：… If CR0.PG = 0, each linear address is passed directly to the EPT mechanismfor translation to a physical address.2 The guest memory typepassed on to the EPT mechanism is WB (writeback). 如果CR0.PG=0，则每个线性地址都直接传递给EPT机制，用于转换为物理地址。2传递到EPT机制的guest memory type 为WB（写回）。 “Unrestricted guest” is a secondary processor-based VM-execution control. Ifbit 31 of the primary processor-based VM-execution controls is 0, VMXnon-root operation functions as if the “unrestricted guest” VM-executioncontrol were 0. See Section 25.6.2. “unrestricted guest” 是 secondary processor-based VM-execution 控制字段. 如果 primary processor-based VM-execution controls 的bit 31 为1. VMX non-rootoperation 的function 就像“unrestricted guests”VM-execution control 为0一样。请看Section 25.6.2. As noted in Section 27.2.1.1, the “enable EPT” VM-execution control must be1 if the “unrestricted guest” VM-execution control is 1. 如Section 27.2.1.1 提到的, 如果 “unrestricted guest” VM-execution 控制字段为1, “enable EPT” VM-execution 控制字段也必须是1. " }, { "title": "restrictions of VMX operation", "url": "/posts/vmx-operation-restrict/", "categories": "intel_sdm", "tags": "virt", "date": "2024-04-25 11:11:00 +0800", "snippet": " FROM intel sdm CHAPTER 24 INTRODUCTION TO VIRTUAL MACHINE EXTENSIONS 24.8 RESTRICTIONS ON VMX OPERATION VMX operation places restrictions on processor operation. These are detailedbelow: VMX...", "content": " FROM intel sdm CHAPTER 24 INTRODUCTION TO VIRTUAL MACHINE EXTENSIONS 24.8 RESTRICTIONS ON VMX OPERATION VMX operation places restrictions on processor operation. These are detailedbelow: VMX operation 对处理器操作施加限制。 这些详细信息如下： In VMX operation, processors may fix certain bits in CR0 and CR4 to specificvalues and not support other values. VMXON fails if any of these bitscontains an unsupported value (see “VMXON—Enter VMX Operation” in Chapter31). Any attempt to set one of these bits to an unsupported value while inVMX operation (including VMX root operation) using any of the CLTS, LMSW, orMOV CR instructions causes a general-protection exception. VM entry or VMexit cannot set any of these bits to an unsupported value. Software shouldconsult the VMX capability MSRs IA32_VMX_CR0_FIXED0 and IA32_VMX_CR0_FIXED1to determine how bits in CR0 are fixed (see Appendix A.7). For CR4, softwareshould consult the VMX capability MSRs IA32_VMX_CR4_FIXED0 andIA32_VMX_CR4_FIXED1 (see Appendix A.8). 在VMX operation中，处理器可以将CR0和CR4中的某些位fix(固定, 相当于不能修改)为特定值并且不支持其他值。 如果这些位中的任何一个包含不支持的值，则 VMXON 失败（请参阅第 31 章中的“VMXON — Enter VMX opeartion”）。 在 VMX operation（包括 VMX root opeartion）中使用任何 CLTS、LMSW 或 MOV CR 指令将这些位之一设置为不受支持的值的任何尝试都会导致一般保护异常。 VM entry 或 VM exit无法将这些位中的任何一个设置为不受支持的值。软件应参考 VMX 功能 MSR IA32_VMX_CR0_FIXED0 和 IA32_VMX_CR0_FIXED1 以确定如何fix CR0 中的位（请参阅附录 A.7）。 对于 CR4，软件应参考 VMX 功能 MSR IA32_VMX_CR4_FIXED0 和 IA32_VMX_CR4_FIXED1（请参阅附录 A.8）。 NOTES The first processors to support VMX operation require that the followingbits be 1 in VMX operation: CR0.PE, CR0.NE, CR0.PG, and CR4.VMXE. Therestrictions on CR0.PE and CR0.PG imply that VMX operation is supportedonly in paged protected mode (including IA-32e mode). Therefore, guestsoftware cannot be run in unpaged protected mode or in real-address mode. 第一批支持 VMX opeartion的处理器要求 VMX opeartion中以下位为 1：CR0.PE、CR0.NE、CR0.PG 和 CR4.VMXE。 对 CR0.PE 和 CR0.PG 的限制意味着仅在分页保护模式（包括 IA-32e 模式）下支持 VMX operation。 因此，guest软件不能在未分页保护模式或实地址模式下运行。 Later processors support a VM-execution control called “unrestricted guest”(see Section 25.6.2). If this control is 1, CR0.PE and CR0.PG may be 0 inVMX non-root operation (even if the capability MSR IA32_VMX_CR0_FIXED0reports otherwise).1 Such processors allow guest software to run in unpagedprotected mode or in real-address mode. 更高版本的处理器支持称为“unrestricted guest”的 VM-execution control（请参阅第 25.6.2 节）。 如果此控制字段为 1，则 CR0.PE 和 CR0.PG 在 VMX non-root operation中可能为 0（即使 MSR IA32_VMX_CR0_FIXED0 功能另有报告）1。此类处理器允许guest软件在未分页保护模式或实地址下运行 模式。 VMXON fails if a logical processor is in A20M mode (see “VMXON—Enter VMXOperation” in Chapter 31). Once the processor is in VMX operation, A20Minterrupts are blocked. Thus, it is impossible to be in A20M mode in VMXoperation. 如果逻辑处理器处于 A20M 模式，VMXON 将失败（请参阅第 31 章中的“VMXON—EnterVMX opeartion”）。 一旦处理器处于 VMX opeartion 中，A20M 中断就会被阻止。 因此，在 VMX opeartion中不可能处于 A20M 模式。 The INIT signal is blocked whenever a logical processor is in VMX rootoperation. It is not blocked in VMX non-root operation. Instead, INITs causeVM exits (see Section 26.2, “Other Causes of VM Exits”). 只要逻辑处理器处于 VMX root operation中，INIT signal 就会被blocked。 在 VMX non-root opeartion中不会被block。 相反，INIT 会导致 VM exit（请参见第 26.2 节“VM exit的其他原因”）。 Intel(R) Processor Trace (Intel PT) can be used in VMX operation only ifIA32_VMX_MISC[14] is read as 1 (see Appendix A.6). On processors that supportIntel PT but which do not allow it to be used in VMX operation, execution ofVMXON clears IA32_RTIT_CTL.TraceEn (see “VMXON—Enter VMX Operation” inChapter 31); any attempt to write IA32_RTIT_CTL while in VMX operation(including VMX root operation) causes a general- protection exception. 略(和 INTEL PT 技术相关) " }, { "title": "user-timer event and interrupt", "url": "/posts/user-timer/", "categories": "intel_sdm", "tags": "virt", "date": "2024-04-25 10:30:00 +0800", "snippet": " FROM Intel® Architecture Instruction Set Extensions and Future Features doc number 319433-052 CHAPTER 13 USER-TIMER EVENTS AND INTERRUPTSabstractThis chapter describes an architectural feature...", "content": " FROM Intel® Architecture Instruction Set Extensions and Future Features doc number 319433-052 CHAPTER 13 USER-TIMER EVENTS AND INTERRUPTSabstractThis chapter describes an architectural feature called user-timer events.The feature defines a new 64-bit value called the user deadline. Software mayread and write the user deadline. When the user deadline is not zero, auser-timer event becomes pending when the logical processor’s timestamp counter(TSC) is greater than or equal to the user deadline.A pending user-timer event is processed by the processor when CPL = 3 andcertain other conditions apply. When processed, the event results in a userinterrupt with the user-timer vector. (Software may read and write theuser-timer vector). Specifically, the processor sets the bit in the UIRR (userinterrupt request register) corre- sponding to the user timer vector. Theprocessing also clears the user deadline, ensuring that there will be nosubsequent user-timer events until software writes the user deadline again.Section 13.1 discusses the enabling and enumeration of the new feature. Section13.2 presents details of the user deadline, and Section 13.3 explains how it(together with the user-timer vector) is represented in a new MSR. Section 13.4explains when and how a logical processor processes a pending user-timer event.Section 13.5 pres- ents VMX support for virtualizing the new feature.13.1 ENABLING AND ENUMERATIONProcessor support for user-timer events is enumerated by CPUID.(EAX=07H,ECX=1H):EDX.UTMR[bit 13]. If this feature flag is set, the processor supportsuser-timer events, and software can access the IA32_UINTR_TIMER MSR (seeSection 13.3).13.2 USER DEADLINEA logical processor that supports user-timer events supports a 64-bit valuecalled the user deadline. If the user deadline is non-zero, the logicalprocessor pends a user-timer event when the timestamp counter (TSC) reaches orexceeds the user deadline.Software can write the user deadline using instructions specified later in thischapter (see Section 13.3). The processor enforces the following: Writing zero to the user deadline disables user-timer events and cancels anythat were pending. As a result, no user-timer event is pending after zero iswritten to the user deadline. If software writes the user deadline with a non-zero value that is less thanthe TSC, a user-timer event will be pending upon completion of that write. If software writes the user deadline with a non-zero value that is greaterthan that of the TSC, no user-timer event will be pending after the writeuntil the TSC reaches the new user deadline. A logical processor processes a pending user-timer event under certainconditions; see Section 13.4. The logical processor clears the user deadlineafter pending a user-timer event. Races may occur if software writes a new user deadline when the value of theTSC is close to that of the original user deadline. In such a case, either ofthe following may occur: The TSC may reach the original deadline before the write to the deadline,causing a user-timer event to be pended. Either of the following may occur: If the user-timer event is processed before the write to the deadline, thelogical processor will clear the deadline before the write. The write tothe deadline may cause a second user-timer event to occur later. If the write to the deadline occurs before the user-timer event isprocessed, the original user-timer event is canceled, and any subsequentuser-timer event will be based on the new value of the deadline. When writing to the deadline, it may not be possible for software to controlwith certainty which of these two situations occurs. The write to the deadline may occur before TSC reaches the original deadline.In this case, no user-timer event will occur based on the original deadline.Any subsequent user-timer event will be based on the new value of thedeadline. Software writes to the user deadline using a new MSR described in Section 13.3.13.3 USER TIMER: ARCHITECTURAL STATEThe user-timer architecture defines a new MSR, IA32_UINTR_TIMER. This MSR canbe accessed using MSR index 1B00H.The IA32_UINTR_TIMER MSR has the following format: Bits 5:0 are the user-timer vector. Processing of a user-timer event resultsin the pending of a user interrupt with this vector (see Section 13.4). Bits 63:6 are the upper 56 bits of the user deadline (see Section 13.2). Note that no bits are reserved in the MSR and that writes to the MSR will notfault due to the value of the instruc- tion’s source operand. TheIA32_UINTR_TIMER MSR can be accessed via the following instructions: RDMSR,RDMSRLIST, URDMSR, UWRMSR, WRMSR, WRMSRLIST, and WRMSRNS.If the IA32_UINTR_TIMER MSR is written with value X, the user-timer vector getsvalue X &amp; 3FH; the user deadline gets value X &amp; ~3FH.If the user-timer vector is V (0 ≤ V ≤ 63) and the user deadline is D, a readfrom the IA32_UINTR_TIMER MSR return value (D &amp; ~3FH) | V.13.4 PENDING AND PROCESSING OF USER-TIMER EVENTSThere is a user-timer event pending whenever the user deadline (Section 13.2)is non-zero and is less than or equal to the value of the timestamp counter(TSC).If CR4.UINTR = 1, a logical processor processes a pending user-timer event atan instruction boundary at which the following conditions all hold1: IA32_EFER.LMA = CS.L = 1 (the logical processor is in 64-bit mode); CPL =3; UIF = 1; and the logical processor is not in the shutdown state or in the wait-for-SIPIstate.2When the conditions just identified hold, the logical processor processes auser-timer event. User-timer events have priority just above that ofuser-interrupt delivery. If the logical processor was in a state entered usingthe TPAUSE and UMWAIT instructions, it first wakes up from that state andbecomes active. If the logical processor was in enclave mode, it exits theenclave (via AEX) before processing the user-timer event.The following pseudocode details the processing of a user-timer event:UIRR[UserTimerVector] := 1;recognize a pending user interrupt;// may be delivered immediately after processingIA32_UINTR_TIMER := 0;// clears the deadline and the vectorProcessing of a user-timer event aborts transactional execution and results ina transition to a non-transactional execution. The transactional abort loadsEAX as it would have had it been due to an ordinary interrupt.Processing of a user-timer event cannot cause a fault or a VM exit.13.5 VMX SUPPORTThe VMX architecture supports virtualization of the instruction set and itssystem architecture. Certain extensions are needed to support virtualization ofuser-timer events. This section describes these extensions.13.5.1 VMCS ChangesOne new 64-bit VM-execution control field is defined called the virtualuser-timer control. It can be accessed with the encoding pair 2050H/2051H. SeeSection 13.5.2 for its use in VMX non-root operation. This field exists only onprocessors that enumerate CPUID.(EAX=07H, ECX=1H):EDX[13] as 1 (see Section13.1).13.5.2 Changes to VMX Non-Root OperationThis section describes changes to VMX non-root operation for user-timer events.13.5.2.1 Treatment of Accesses to the IA32_UINTR_TIMER MSRAs noted in Section 13.3, software can read and write the IA32_UINTR_TIMER MSRusing certain instructions. The operation of those instructions is changed whenthey are executed in VMX non-root operation: Any read from the IA32_UINTR_TIMER MSR (e.g., by RDMSR) returns the value ofthe virtual user-timer control. Any write to the IA32_UINTR_TIMER MSR (e.g., by WRMSR) is treated as follows: The source operand is written to the virtual user-timer control (updatingthe VMCS). Bits 5:0 of the source operand are written to the user-timer vector. If bits 63:6 of the source operand are zero, the user deadline (the valuethat actually controls when hardware generates a user time event) iscleared to 0. Section 13.2 identifies the consequences of this clearing. If bits 63:6 of the source operand are not all zero, the user deadline iscomputed as follows. The source operand (with the low 6 bits cleared) isinterpreted as a virtual user deadline. The processor converts that valueto the actual user deadline based on the current configuration of TSCoffsetting and TSC scaling.1 Following such a write, the value of the IA32_UINTR_TIMER MSR (e.g., aswould be observed following a subsequent VM exit) is such that bits 63:6contain the actual user deadline (not the virtual user deadline), whilebits 5:0 contain the user-timer vector. 13.5.2.2 Treatment of User-Timer EventsThe processor’s treatment of user-timer events is described in Section 13.4.These events occur in VMX non-root operation under the same conditionsdescribed in that section.The processing of user-timer events differs in VMX non-root operation only inthat, in addition to clearing the IA32_UINTR_TIMER MSR, the processing alsoclears the virtual user-timer control (updating the VMCS).13.5.3 Changes to VM EntriesA VM entry results in a pending user-timer event if and only if the VM entrycompletes with the user deadline non- zero and less than or equal to the(non-virtualized) TSC. The processor will process such an event only ifindicated by the conditions identified in Section 13.4." }, { "title": "virtualization cr0", "url": "/posts/virtualization-cr0/", "categories": "intel_sdm", "tags": "virt", "date": "2024-04-24 18:30:00 +0800", "snippet": "Guest/Host Masks and Read Shadows for CR0 and CR4 FROM intel sdm CHAPTER 25 VIRTUAL MACHINE CONTROL STRUCTURES 25.6 VM-EXECUTION CONTROL FIELDS 25.6.6 VM-execution control fields include gue...", "content": "Guest/Host Masks and Read Shadows for CR0 and CR4 FROM intel sdm CHAPTER 25 VIRTUAL MACHINE CONTROL STRUCTURES 25.6 VM-EXECUTION CONTROL FIELDS 25.6.6 VM-execution control fields include guest/host masks and read shadows for theCR0 and CR4 registers. These fields control executions of instructions thataccess those registers (including CLTS, LMSW, MOV CR, and SMSW). They are 64bits on processors that support Intel 64 architecture and 32 bits on processorsthat do not. VM-execution control 字段包括对于CR0 和 CR4 寄存器的 guest/host masks 和 read shadows. 这些字段控制访问这些寄存器的指令的执行（包括 CLTS、LMSW、MOV CR 和 SMSW）。 它们在支持 Intel 64 架构的处理器上为 64 位，在不支持 Intel 64 架构的处理器上为 32 位。In general, bits set to 1 in a guest/host mask correspond to bits “owned” bythe host: 一般来说，guest/host mask中设置为 1 的位对应于host“owned(拥有)”的位： Guest attempts to set them (using CLTS, LMSW, or MOV to CR) to valuesdiffering from the corresponding bits in the corresponding read shadow causeVM exits. guest尝试将它们（使用 CLTS、LMSW 或 MOV 到 CR）设置为与read shadow中的相应位不同的值，导致 VM exit Guest reads (using MOV from CR or SMSW) return values for these bits from thecorresponding read shadow. Bits cleared to 0 correspond to bits “owned” bythe guest; guest attempts to modify them succeed and guest reads returnvalues for these bits from the control register itself. correspond [ˌkɔːrəˈspɑːnd]: : 相一致, 符合;相当于;类似于 guest读取（使用 CR 或 SMSW 中的 MOV）从相应的read shadow中返回这些位的值。cleared为 0 的位对应于guest “拥有”的位；guest尝试修改它们成功，并且guest从他们自己的控制寄存器读取这些位的值. See Chapter 28 for details regarding how these fields affect VMX non-rootoperation. 有关这些字段如何影响 VMX 非 root 操作的详细信息，请参阅第 26 章。 这里intel sdm 中写错了, 应该是26章 MOV to CR0 cause VM Exits Conditionally FROM intel sdm CHAPTER 26 VMX NON-ROOT OPERATION 26.1 INSTRUCTIONS THAT CAUSE VM EXITS 26.1.3 Instructions That Cause VM Exits Conditionally MOV to CR0. The MOV to CR0 instruction causes a VM exit unless the value of its sourceoperand matches, for the position of each bit set in the CR0 guest/host mask,the corresponding bit in the CR0 read shadow. (If every bit is clear in theCR0 guest/host mask, MOV to CR0 cannot cause a VM exit.) MOV 到 CR0 指令会导致 VM 退出，除非其源操作数的值与CR0guest/host mask中设置的每个位的位置对应的 CR0 read shadow中相应位的值匹配。 （如果 CR0 guest/host mask中的每一位都被清除，则 MOV 到 CR0 不会导致 VM 退出。） E.g. CR0 guest host mask : 0 0 0 0 1 0 1 0 1 0 1 | | | | | | | |CR0 read shadow : 1 1 1 1 1 1 1 1 1 1 1compare bit : ^ ^ ^ ^source operand : x x x x 1 x 1 x 1 x 1 ---&gt; NO vm exitsource operand : x x x x x x x x x x 0 ---&gt; need vm exit 会将设置的值和 compare bit进行比较, 如果两者一样, 则不需要vm exit, 如果不一样, 则需要VM exit, 下面会说明原因 CHANGES TO “MOV from/to CR0” BEHAVIOR IN VMX NON-ROOT OPERATION FROM intel sdm CHAPTER 26 VMX NON-ROOT OPERATION 26.3 CHANGES TO INSTRUCTION BEHAVIOR IN VMX NON-ROOT OPERATION MOV from CR0. The behavior of MOV from CR0 is determined by the CR0 guest/host mask andthe CR0 read shadow. For each position corresponding to a bit clear in theCR0 guest/host mask, the destination operand is loaded with the value ofthe corresponding bit in CR0. For each position corresponding to a bit setin the CR0 guest/host mask, the destination operand is loaded with thevalue of the corresponding bit in the CR0 read shadow. Thus, if every bitis cleared in the CR0 guest/host mask, MOV from CR0 reads normally fromCR0; if every bit is set in the CR0 guest/host mask, MOV from CR0 returnsthe value of the CR0 read shadow. Depending on the contents of the CR0guest/host mask and the CR0 read shadow, bits may be set in the destinationthat would never be set when reading directly from CR0. MOV from CR0 的行为由 CR0 guest/host mask和 CR0 read shadow决定。 对于与 CR0 guest/host mask中清零位相对应的每个位置，目标操作数将加载CR0 中相应位的值。 对于与 CR0 guest/host mask中设置的位相对应的每个位置，目标操作数将加载 CR0 read shadow中相应位的值。 因此，如果 CR0 guest/host mask中的每一位都被清除，则 MOV from CR0 会正常从 CR0 读取； 如果在 CR0 guest/host mask中设置了每个位，则MOV from CR0 将返回 CR0 read shadow的值。根据 CR0 guest/host mask和 CR0 read shadow的内容，可能会在destination设置一些 直接从 CR0 读取的永远不会设置的位。 MOV to CR0 An execution of MOV to CR0 that does not cause a VM exit (see Section 26.1.3)leaves unmodified any bit in CR0 corresponding to a bit set in the CR0guest/host mask. Treatment of attempts to modify other bits in CR0 depends onthe setting of the “unrestricted guest” VM-execution control: 执行 MOV to CR0 不会导致 VM 退出（请参阅第 26.1.3 节），从而使 CR0 中与 CR0guest/host mask中设置的位相对应的任何位保持不变。 对修改 CR0 中其他位的尝试的处理取决于“unrestricted guest”VM-execution control 的设置： If the control is 0, MOV to CR0 causes a general-protection exception if itattempts to set any bit in CR0 to a value not supported in VMX operation(see Section 24.8). 如果控制为 0，则 MOV to CR0 尝试将 CR0 中的任何位设置为 VMX operation不支持的值时会导致一般保护异常（请参见第 24.8 节）。 If the control is 1, MOV to CR0 causes a general-protection exception if itattempts to set any bit in CR0 other than bit 0 (PE) or bit 31 (PG) to avalue not supported in VMX operation. It remains the case, however, thatMOV to CR0 causes a general-protection exception if it would result in CR0. 如果控制为 1，则 MOV to CR0 尝试将 CR0 中除位 0 (PE) 或位 31 (PG) 之外的任何位设置为 VMX operation 不支持的值时，会导致一般保护异常。 然而，情况仍然如此，如果 MOV to CR0 会导致 CR0，则会导致一般保护异常。 MY note读取操作:graphviz-75150b4de17bd88865df62a6317d07dcdigraph G { subgraph cluster_cr0 { cr0_bitmap [ shape=&quot;record&quot; label=&quot;&lt;0&gt;0|&lt;1&gt;0|&lt;2&gt;0|&lt;3&gt;0&quot; ] label=&quot;cr0&quot; } subgraph cluster_cr0_host_guest_mask { cr0_host_guest_mask_bitmap [ shape=&quot;record&quot; label=&quot;&lt;0&gt;0|&lt;1&gt;1|&lt;2&gt;1|&lt;3&gt;1&quot; ] label=&quot;cr0 host/guest mask&quot; } subgraph cluster_cr0_read_shadow { cr0_read_shadow_bitmap [ shape=&quot;record&quot; label=&quot;&lt;0&gt;1|&lt;1&gt;1|&lt;2&gt;1|&lt;3&gt;1&quot; ] label=&quot;cr0 read shadow bitmap&quot; } subgraph cluster_destination_value { destination_value [ shape=&quot;record&quot; label=&quot;&lt;0&gt;0|&lt;1&gt;1|&lt;2&gt;1|&lt;3&gt;1&quot; ] label=&quot;destination value&quot; } cr0_host_guest_mask_bitmap:0-&gt;cr0_bitmap:0 cr0_host_guest_mask_bitmap:1-&gt;cr0_read_shadow_bitmap:1 cr0_host_guest_mask_bitmap:2-&gt;cr0_read_shadow_bitmap:2 cr0_host_guest_mask_bitmap:3-&gt;cr0_read_shadow_bitmap:3 cr0_bitmap:0-&gt;destination_value:0 cr0_read_shadow_bitmap:1-&gt;destination_value:1 cr0_read_shadow_bitmap:2-&gt;destination_value:2 cr0_read_shadow_bitmap:3-&gt;destination_value:3}Gcluster_cr0cr0cluster_cr0_host_guest_maskcr0 host/guest maskcluster_cr0_read_shadowcr0 read shadow bitmapcluster_destination_valuedestination valuecr0_bitmap0000destination_value0111cr0_bitmap:0&#45;&gt;destination_value:0cr0_host_guest_mask_bitmap0111cr0_host_guest_mask_bitmap:0&#45;&gt;cr0_bitmap:0cr0_read_shadow_bitmap1111cr0_host_guest_mask_bitmap:1&#45;&gt;cr0_read_shadow_bitmap:1cr0_host_guest_mask_bitmap:2&#45;&gt;cr0_read_shadow_bitmap:2cr0_host_guest_mask_bitmap:3&#45;&gt;cr0_read_shadow_bitmap:3cr0_read_shadow_bitmap:1&#45;&gt;destination_value:1cr0_read_shadow_bitmap:2&#45;&gt;destination_value:2cr0_read_shadow_bitmap:3&#45;&gt;destination_value:3write 操作 write to read shadowgraphviz-e91004f4e482ae0d030678a73454dad1digraph G { subgraph cluster_cr0_host_guest_mask { cr0_host_guest_mask_bitmap [ shape=&quot;record&quot; label=&quot;&lt;0&gt;0|&lt;1&gt;0|&lt;2&gt;0|&lt;3&gt;1&quot; ] label=&quot;cr0 host/guest mask&quot; } subgraph cluster_cr0_read_shadow { cr0_read_shadow_bitmap [ shape=&quot;record&quot; label=&quot;&lt;0&gt;0|&lt;1&gt;0|&lt;2&gt;0|&lt;3&gt;1&quot; ] label=&quot;cr0 read shadow bitmap&quot; } subgraph cluster_source_value { source_value [ shape=&quot;record&quot; label=&quot;&lt;0&gt;0|&lt;1&gt;0|&lt;2&gt;0|&lt;3&gt;0&quot; ] label=&quot;source value&quot; } subgraph cluster_source_value2 { source_value2 [ shape=&quot;record&quot; label=&quot;&lt;0&gt;0|&lt;1&gt;0|&lt;2&gt;0|&lt;3&gt;1&quot; ] label=&quot;source value2&quot; } cr0_host_guest_mask_bitmap:3-&gt;source_value:3 [ label=&quot;indicate compare bit&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] cr0_host_guest_mask_bitmap:3-&gt;source_value2:3 [ label=&quot;indicate compare bit&quot; color=&quot;blue&quot; fontcolor=&quot;blue&quot; ] source_value2:3-&gt;cr0_read_shadow_bitmap:3 [ label=&quot;compare equal SKIP&quot; color=&quot;blue&quot; fontcolor=&quot;blue&quot; ] source_value:3-&gt;cr0_read_shadow_bitmap:3 [ label=&quot;compare NOT equal \\nvm EXIT, hypervisor \\nmay execute some emulations&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ]}Gcluster_cr0_host_guest_maskcr0 host/guest maskcluster_cr0_read_shadowcr0 read shadow bitmapcluster_source_valuesource valuecluster_source_value2source value2cr0_host_guest_mask_bitmap0001source_value0000cr0_host_guest_mask_bitmap:3&#45;&gt;source_value:3indicate compare bitsource_value20001cr0_host_guest_mask_bitmap:3&#45;&gt;source_value2:3indicate compare bitcr0_read_shadow_bitmap0001source_value:3&#45;&gt;cr0_read_shadow_bitmap:3compare NOT equal vm EXIT, hypervisor may execute some emulationssource_value2:3&#45;&gt;cr0_read_shadow_bitmap:3compare equal SKIP direct write to CR3graphviz-ce0fa82903fbaa57303151b692d88cc2digraph G { subgraph cluster_cr0_host_guest_mask { cr0_host_guest_mask_bitmap [ shape=&quot;record&quot; label=&quot;&lt;0&gt;0|&lt;1&gt;0|&lt;2&gt;0|&lt;3&gt;1&quot; ] label=&quot;cr0 host/guest mask&quot; } subgraph cluster_source_value { source_value [ shape=&quot;record&quot; label=&quot;&lt;0&gt;1|&lt;1&gt;0|&lt;2&gt;1|&lt;3&gt;0&quot; ] label=&quot;source value&quot; } subgraph cluster_beg_write_cr0 { cr0_beg_w [ shape=&quot;record&quot; label=&quot;&lt;0&gt;x|&lt;1&gt;x|&lt;2&gt;x|&lt;3&gt;x&quot; ] label=&quot;cr0 beg write&quot; } subgraph cluster_end_write_cr0 { cr0_end_w [ shape=&quot;record&quot; label=&quot;&lt;0&gt;1|&lt;1&gt;0|&lt;2&gt;1|&lt;3&gt;x&quot; ] label=&quot;cr0 : end write&quot; } cr0_host_guest_mask_bitmap:0-&gt;source_value:0 cr0_host_guest_mask_bitmap:1-&gt;source_value:1 cr0_host_guest_mask_bitmap:2-&gt;source_value:2 [ label=&quot;indicate write direct cr0 bit&quot; ] source_value:0-&gt;cr0_beg_w:0 source_value:1-&gt;cr0_beg_w:1 source_value:2-&gt;cr0_beg_w:2 [ label=&quot;write&quot; ] cr0_beg_w:0-&gt;cr0_end_w:0 cr0_beg_w:1-&gt;cr0_end_w:1 cr0_beg_w:2-&gt;cr0_end_w:2 [ label=&quot;write success&quot; ]}Gcluster_cr0_host_guest_maskcr0 host/guest maskcluster_source_valuesource valuecluster_beg_write_cr0cr0 beg writecluster_end_write_cr0cr0 : end writecr0_host_guest_mask_bitmap0001source_value1010cr0_host_guest_mask_bitmap:0&#45;&gt;source_value:0cr0_host_guest_mask_bitmap:1&#45;&gt;source_value:1cr0_host_guest_mask_bitmap:2&#45;&gt;source_value:2indicate write direct cr0 bitcr0_beg_wxxxxsource_value:0&#45;&gt;cr0_beg_w:0source_value:1&#45;&gt;cr0_beg_w:1source_value:2&#45;&gt;cr0_beg_w:2writecr0_end_w101xcr0_beg_w:0&#45;&gt;cr0_end_w:0cr0_beg_w:1&#45;&gt;cr0_end_w:1cr0_beg_w:2&#45;&gt;cr0_end_w:2write success 上面是 MOV from CR0的大概流程 这里需要注意的是, cr0 host/guest mask能决定的是 MOV from CR0 的这些bit 从哪个地方获取: CR0 CR0 read shadow MOV to CR0, 要不要VM exit. 如果修改了 cr0 host/guest mask中bit对应的 read shadow, 是一定要vm exit. 当然 在 VMX OPERATION中CR0还有一些限制, 不满足这些限制也会VM exit. 这里需要思考下 Q: 为什么要这样设计? A: 为的就是对CR0 的某些bit进行软件(hypervisor)上的虚拟化. Q: 怎么控制虚拟化哪些呢? A: 虚拟化 cr0 host/guest mask为1 的bit. read from read shadow write cause VM-exit other bit normal read/write to CR0 " }, { "title": "protected-mode memory management", "url": "/posts/proctected-mode/", "categories": "intel_sdm", "tags": "virt", "date": "2024-04-23 11:00:00 +0800", "snippet": " FROM intel sdm CHAPTER 3 PROTECTED-MODE MEMORY MANAGEMENTabstractThis chapter describes the Intel 64 and IA-32 architecture’s protected-modememory management facilities, including the physical m...", "content": " FROM intel sdm CHAPTER 3 PROTECTED-MODE MEMORY MANAGEMENTabstractThis chapter describes the Intel 64 and IA-32 architecture’s protected-modememory management facilities, including the physical memory requirements,segmentation mechanism, and paging mechanism. 本章介绍 Intel 64 和 IA-32 架构的保护模式内存管理 facilities ，包括物理内存要求、分段机制和分页机制。See also: Chapter 5, “Protection” (for a description of the processor’sprotection mechanism) and Chapter 21, “8086 Emulation” (for a description ofmemory addressing protection in real-address and virtual-8086 modes). 另请参见：第 5 章“保护”（有关处理器保护机制的说明）和第 21 章“8086 仿真”（有关实地址和虚拟 8086 模式下的内存寻址保护的说明）。3.1 MEMORY MANAGEMENT OVERVIEWThe memory management facilities of the IA-32 architecture are divided into twoparts: segmentation and paging. Segmentation provides a mechanism of isolatingindividual code, data, and stack modules so that multiple programs (or tasks)can run on the same processor without interfering with one another. Pagingprovides a mechanism for implementing a conventional demand-paged,virtual-memory system where sections of a program’s execution environment aremapped into physical memory as needed. Paging can also be used to provideisolation between multiple tasks. When operating in protected mode, some formof segmentation must be used. There is no mode bit to disable segmentation. Theuse of paging, however, is optional. conventional [kənˈvenʃənl] : 常规的; 传统的; 习惯的;demand [dɪˈmænd] : 需要, 要求 IA-32架构的内存管理 facilities 分为两部分：分段和分页。 分段提供了一种隔离各个代码、数据和堆栈模块的机制，以便多个程序（或任务）可以在同一处理器上运行而不会相互干扰。 分页提供了一种实现传统的按需分页虚拟内存系统的机制，其中程序执行环境的各个部分根据需要映射到物理内存中。 分页还可用于提供多个任务之间的隔离。 当在保护模式下运行时，必须使用某种形式的分段。 没有mode bit可以禁用分段。 然而，分页的使用是可选的。These two mechanisms (segmentation and paging) can be configured to supportsimple single-program (or single-task) systems, multitasking systems, ormultiple-processor systems that used shared memory. As shown in Figure 3-1,segmentation provides a mechanism for dividing the processor’s addressablememory space (called the linear address space) into smaller protected addressspaces called segments. Segments can be used to hold the code, data, and stackfor a program or to hold system data structures (such as a TSS or LDT). If morethan one program (or task) is running on a processor, each program can beassigned its own set of segments. The processor then enforces the boundariesbetween these segments and ensures that one program does not interfere with theexecution of another program by writing into the other program’s segments. Thesegmentation mechanism also allows typing of segments so that the operationsthat may be performed on a particular type of segment can be restricted. 这两种机制（分段和分页）可配置为支持简单的单程序（或单任务）系统、多任务系统或多处理器系统（使用共享内存）。 如图 3-1 所示，分段提供了一种将处理器的可寻址内存空间（称为线性地址空间）划分为更小的受保护地址空间（称为段）的机制。 段可用于保存程序的代码、数据和堆栈，或保存系统数据结构（例如 TSS或 LDT）。如果处理器上运行多个程序（或任务），则可以为每个程序分配其自己的一组段。 然后，处理器enforces(强制执行)这些段之间的边界，并确保一个程序不会通过写入另一个程序的段来干扰另一个程序的执行。 分段机制还允许对段进行类型化，以便可以限制对特定类型的段执行的操作。All the segments in a system are contained in the processor’s linear addressspace. To locate a byte in a particular segment, a logical address (also calleda far pointer) must be provided. A logical address consists of a segmentselector and an offset. The segment selector is a unique identifier for asegment. Among other things it provides an offset into a descriptor table (suchas the global descriptor table, GDT) to a data structure called a segmentdescriptor. Each segment has a segment descriptor, which specifies the size ofthe segment, the access rights and privilege level for the segment, the segmenttype, and the location of the first byte of the segment in the linear addressspace (called the base address of the segment). The offset part of the logicaladdress is added to the base address for the segment to locate a byte withinthe segment. The base address plus the offset thus forms a linear address inthe processor’s linear address space. among other things: 除此之外 系统中的所有段都包含在处理器的线性地址空间中。 要在特定段中定位字节，必须提供逻辑地址（也称为far pointer）。 逻辑地址由段选择器和偏移量组成。 段选择器是段的唯一标识符。 除此之外，它还提供描述符表（例如全局描述符表，GDT）到一个被称为段描述符的数据结构的偏移量(某个entry, 实际上就是在GDT中的偏移)。 每个段都有一个段描述符，它指定了段的大小、段的访问权限和特权级别、段类型以及该段的第一个字节在线性地址空间中的位置（称为段的基地址)。 逻辑地址的偏移部分被添加到段的基地址上来locate段内的某个byte。 因此，基地址加上偏移量就形成了处理器线性地址空间中的线性地址。If paging is not used, the linear address space of the processor is mappeddirectly into the physical address space of processor. The physical addressspace is defined as the range of addresses that the processor can generate onits address bus. 如果不使用分页，则处理器的线性地址空间直接映射到处理器的物理地址空间。 物理地址空间定义为处理器可以在其地址总线上产生的地址范围。Because multitasking computing systems commonly define a linear address spacemuch larger than it is economically feasible to contain all at once inphysical memory, some method of “virtualizing” the linear address space isneeded. This virtualization of the linear address space is handled through theprocessor’s paging mechanism. economically [ˌiːkəˈnɒmɪkli]: 经济地; 在经济学上; 节约地; 节俭地; 节省地; 实惠地feasible [ˈfiːzəbl] : 可行的, 行的通的all at once : 同时;突然;忽然;一起 由于多任务计算系统通常定义的线性地址空间比在 physical memory 上同时包含所有地址在 economically 要更feasible(经济上更合适). 因此需要某种“virtualizing”线性地址空间的方法。 线性地址空间的虚拟化是通过处理器的分页机制来处理的。Paging supports a “virtual memory” environment where a large linear addressspace is simulated with a small amount of physical memory (RAM and ROM) andsome disk storage. When using paging, each segment is divided into pages(typically 4 KBytes each in size), which are stored either in physical memoryor on the disk. The operating system or executive maintains a page directoryand a set of page tables to keep track of the pages. When a program (or task)attempts to access an address location in the linear address space, theprocessor uses the page directory and page tables to translate the linearaddress into a physical address and then performs the requested operation (reador write) on the memory location. 分页支持“虚拟内存”环境，在该环境上使用少量物理内存（RAM 和 ROM）和一些磁盘存储来模拟大型线性地址空间。使用分页时，每个段被分为一些pages（通常每个大小为 4 KB），这些页存储在物理内存或磁盘上。操作系统或执行程序维护一个页目录和一组页表来track页面。当程序（或任务）尝试访问线性地址空间中的地址位置时，处理器使用页目录和页表将线性地址转换为物理地址，然后在该内存位置上执行请求的操作（读或写)。If the page being accessed is not currently in physical memory, the processorinterrupts execution of the program (by generating a page-fault exception). Theoperating system or executive then reads the page into physical memory from thedisk and continues executing the program. 如果正在访问的页面当前不在物理内存中，则处理器会中断程序的执行（通过生成page-fault异常）。然后操作系统或执行程序将页面内容从磁盘读入物理内存并继续执行程序.When paging is implemented properly in the operating-system or executive, theswapping of pages between physical memory and the disk is transparent to thecorrect execution of a program. Even programs written for 16-bit IA-32processors can be paged (transparently) when they are run in virtual-8086 mode. 当操作系统或执行程序中正确实现分页时，物理内存和磁盘之间的页面交换对于程序的正确执行是透明的。 即使是为 16 位 IA-32 处理器编写的程序在virtual-8086 模式下运行时也可以进行分页（透明地）。3.2 USING SEGMENTSThe segmentation mechanism supported by the IA-32 architecture can be used toimplement a wide variety of system designs. These designs range from flatmodels that make only minimal use of segmentation to protect programs tomulti-segmented models that employ segmentation to create a robust operatingenvironment in which multiple programs and tasks can be executed reliably. robust [roʊˈbʌst]: 强健的; 坚固的 A-32架构支持的分段机制可用于实现多种系统设计。 这些设计的范围从仅使用最少的分段来保护程序的平面模型到使用分段来创建可以可靠地执行多个程序和任务的强健的操作环境的多分段模型。The following sections give several examples of how segmentation can beemployed in a system to improve memory management performance and reliability. 以下部分给出了几个示例，说明如何在系统中使用分段来提高内存管理性能和可靠性。3.2.1 Basic Flat ModelThe simplest memory model for a system is the basic “flat model,” in which theoperating system and application programs have access to a continuous,unsegmented address space. To the greatest extent possible, this basic flatmodel hides the segmentation mechanism of the architecture from both the systemdesigner and the application programmer. continuous [kənˈtɪnjuəs] : 连续的,持续的,不断的extent [ɪkˈstent] : 程度greatest extent possible: 最大程度的 系统最简单的内存模型是基本的“平面模型”，在该模型中，操作系统和应用程序可以访问连续的、未分段的地址空间。在最大程度上，这个基本的平面模型向系统设计者和应用程序程序员隐藏了体系结构的segmentation 机制。To implement a basic flat memory model with the IA-32 architecture, at leasttwo segment descriptors must be created, one for referencing a code segment andone for referencing a data segment (see Figure 3-2). Both of these segments,however, are mapped to the entire linear address space: that is, both segmentdescriptors have the same base address value of 0 and the same segment limit of4 GBytes. By setting the segment limit to 4 GBytes, the segmentation mechanismis kept from generating exceptions for out of limit memory references, even ifno physical memory resides at a particular address. ROM (EPROM) is generallylocated at the top of the physical address space, because the processor beginsexecution at FFFF_FFF0H. RAM (DRAM) is placed at the bottom of the addressspace because the initial base address for the DS data segment after resetinitialization is 0. keep from: 阻止,避免, 免于reside: 驻留 要使用 IA-32 架构实现基本平面内存模型，必须至少创建两个段描述符，一个用于引用代码段，另一个用于引用数据段（见图 3-2）。 然而，这两个段都映射到整个线性地址空间：也就是说，两个段描述符具有相同的基地址值 0 和相同的 4 GB 段限制。 通过将段限制设置为 4 GB，即使特定地址没有物理内存reside，分段机制也不会因超出限制的内存引用而生成异常。 ROM（EPROM）一般位于物理地址空间的顶部，因为处理器从FFFF_FFF0H开始执行。 RAM（DRAM）被放置在地址空间的底部，因为复位初始化后DS数据段的初始基地址为0。3.2.2 Protected Flat ModelThe protected flat model is similar to the basic flat model, except the segmentlimits are set to include only the range of addresses for which physical memoryactually exists (see Figure 3-3). A general-protection exception (#GP) is thengenerated on any attempt to access nonexistent memory. This model provides aminimum level of hardware protection against some kinds of program bugs. 受保护的平面模型与基本平面模型相似，只是段限制设置为仅包括物理内存实际存在的地址范围（见图3-3）。然后，在任何访问不存在的内存的尝试中都会生成一个通用保护异常（#GP）。该模型提供了针对某些程序错误的最低级别的硬件保护。More complexity can be added to this protected flat model to provide moreprotection. For example, for the paging mechanism to provide isolation betweenuser and supervisor code and data, four segments need to be defined: code anddata segments at privilege level 3 for the user, and code and data segments atprivilege level 0 for the supervisor. Usually these segments all overlay eachother and start at address 0 in the linear address space. This flatsegmentation model along with a simple paging structure can protect theoperating system from applications, and by adding a separate paging structurefor each task or process, it can also protect applications from each other.Similar designs are used by several popular multitasking operating systems. 这种受保护的平面模型可以增加更多的复杂性，以提供更多的保护。例如，对于在用户和supervisor 代码和数据之间提供隔离的分页机制，需要定义四个段：用户权限级别为3的代码和数据段，supervisor 权限级别为0的代码和资料段。通常，这些段都相互重叠，并从线性地址空间中的地址0开始。这种扁平的分段模型和简单的分页结构可以保护操作系统不受应用程序的影响，并且通过为每个任务或进程添加单独的分页结构，它还可以保护应用程序不受彼此的影响。几种流行的多任务操作系统也使用了类似的设计。 这块我持怀疑态度, 如果 offset, base 都设置成一样的, 那么该保护就不是使用 segment来保护的, 实际上还是使用的paging 但是实际上Linux kernel还真是这么做的, 还需要仔细思考下3.2.3 Multi-Segment ModelA multi-segment model (such as the one shown in Figure 3-4) uses the fullcapabilities of the segmentation mechanism to provide hardware enforcedprotection of code, data structures, and programs and tasks. Here, each program(or task) is given its own table of segment descriptors and its own segments.The segments can be completely private to their assigned programs or sharedamong programs. Access to all segments and to the execution environments ofindividual programs running on the system is controlled by hardware. 多段模型（如图 3-4 所示）使用分段机制的全部功能来提供对代码、数据结构以及程序和任务的硬件强制保护。 这里，每个程序（或任务）都有自己的段描述符表和自己的段。 这些段对于其分配的程序来说可以是完全私有的，也可以在程序之间共享。对系统上运行的各个程序的所有段和执行环境的访问均由硬件控制。Access checks can be used to protect not only against referencing an addressoutside the limit of a segment, but also against performing disallowedoperations in certain segments. For example, since code segments are desig-nated as read-only segments, hardware can be used to prevent writes into codesegments. The access rights information created for segments can also be usedto set up protection rings or levels. Protection levels can be used to protectoperating-system procedures from unauthorized access by application programs. 访问检查不仅可以用于防止引用段限制之外的地址，还可以防止在某些段中执行不允许的操作。 例如，由于代码段被指定为只读段，因此硬件可以使用其来防止写入代码段。 为段创建的访问权限信息也可用于设置protection rings 或levels。 protection levels可用于保护操作系统过程免遭应用程序未经授权的访问。3.2.4 Segmentation in IA-32e ModeIn IA-32e mode of Intel 64 architecture, the effects of segmentation depend onwhether the processor is running in compatibility mode or 64-bit mode. Incompatibility mode, segmentation functions just as it does using legacy 16-bitor 32-bit protected mode semantics. compatibility [kəmˌpætəˈbɪləti] : 兼容性 在Intel 64架构的IA-32e模式下，分段的效果取决于处理器是运行在兼容模式还是64位模式。 在兼容模式下，分段功能就像使用传统 16 位或 32 位保护模式semantics一样。In 64-bit mode, segmentation is generally (but not completely) disabled,creating a flat 64-bit linear-address space. The processor treats the segmentbase of CS, DS, ES, SS as zero, creating a linear address that is equal to theeffective address. The FS and GS segments are exceptions. These segmentregisters (which hold the segment base) can be used as additional baseregisters in linear address calculations. They facilitate addressing local dataand certain operating system data structures. 在 64 位模式下，通常（但不是完全）禁用分段，从而创建平坦的 64 位线性地址空间。处理器将 CS、DS、ES、SS 的段基数视为零，从而创建等于有效地址的线性地址。 FS 和 GS 段是例外。 这些段寄存器（保存段基址）可以用作线性地址计算中的附加基址寄存器。 它们有助于寻址本地数据和某些操作系统数据结构Note that the processor does not perform segment limit checks at runtime in64-bit mode. 请注意，处理器在64位模式下运行时不执行段限制检查。3.2.5 Paging and SegmentationPaging can be used with any of the segmentation models described in Figures3-2, 3-3, and 3-4. The processor’s paging mechanism divides the linear addressspace (into which segments are mapped) into pages (as shown in Figure 3-1).These linear-address-space pages are then mapped to pages in the physicaladdress space. The paging mechanism offers several page-level protectionfacilities that can be used with or instead of the segment-protectionfacilities. For example, it lets read-write protection be enforced on apage-by-page basis. The paging mechanism also provides two-leveluser-supervisor protection that can also be specified on a page-by-page basis. 分页可以与图 3-2、3-3 和 3-4 中描述的任何分段模型一起使用。 处理器的分页机制将线性地址空间（段被映射到其中(线性地址空间)）划分为页（如图 3-1 所示）。 然后，这些线性地址空间页被映射到物理地址空间中的页。 分页机制提供了多种页级保护设施，可以与段保护设施一起使用或代替段保护设施。 例如，它允许page-by-page页强制执行读写保护。 分页机制还提供两级 user-supervisor 保护，也可以page-by-page 指定。3.3 PHYSICAL ADDRESS SPACEIn protected mode, the IA-32 architecture provides a normal physical addressspace of 4 GBytes (232 bytes). This is the address space that the processor canaddress on its address bus. This address space is flat (unsegmented), withaddresses ranging continuously from 0 to FFFFFFFFH. This physical address spacecan be mapped to read-write memory, read-only memory, and memory mapped I/O.The memory mapping facilities described in this chapter can be used to dividethis physical memory up into segments and/or pages. 在保护模式下，IA-32 架构提供 4 GB（232 字节）的 normal 物理地址空间。 这是处理器可以在其地址总线上寻址的地址空间。 该地址空间是平坦的（未分段），地址范围从0 到 FFFFFFFFH 连续。 该物理地址空间可以映射到读写memory、只读memory和 mmeory mappedI/O(mmio)。 本章中描述的内存映射 facilities 可用于将该物理内存划分为段和/或页。Starting with the Pentium Pro processor, the IA-32 architecture also supportsan extension of the physical address space to 236 bytes (64 GBytes); with amaximum physical address of FFFFFFFFFH. This extension is invoked in either oftwo ways: 从奔腾Pro处理器开始，IA-32体系结构还支持将物理地址空间扩展到236字节（64GB）;最大物理地址为FFFFFFFFFH。此扩展可通过以下两种方式之一 invoke(引入)： Using the physical address extension (PAE) flag, located in bit 5 of controlregister CR4. 使用位于 CR4 [bit 5] 的 PAE flag Using the 36-bit page size extension (PSE-36) feature (introduced in thePentium III processors). 使用 PSE-36 feature (在奔腾 3 处理器引入) Physical address support has since been extended beyond 36 bits. See Chapter 4,“Paging” for more information about 36-bit physical addressing. 此后，物理地址支持已扩展到 36 位以上。 有关 36 位物理寻址的更多信息，请参见第 4 章“分页”。Intel® 64 Processors and Physical Address SpaceOn processors that support Intel 64 architecture (CPUID.80000001H:EDX[29] = 1),the size of the physical address range is implementation-specific and indicatedby CPUID.80000008H:EAX[bits 7-0]. 在支持 Intel 64 架构 (CPUID.80000001H:EDX[29] = 1) 的处理器上，物理地址范围的大小是 implementation-specific，并由 CPUID.80000008H:EAX[位 7-0] 指示。For the format of information returned in EAX, see “CPUID—CPU Identification”in Chapter 3 of the Intel® 64 and IA-32 Architectures Software Developer’sManual, Volume 2A. See also: Chapter 4, “Paging.” 有关 EAX 中返回信息的格式，intel sdm CPUID 指令介绍和 chapter 4 Paging 章节3.4 LOGICAL AND LINEAR ADDRESSESAt the system-architecture level in protected mode, the processor uses twostages of address translation to arrive at a physical address: logical-addresstranslation and linear address space paging. 在保护模式下的system-architecture level，处理器使用两个阶段的地址转换来获得物理地址：逻辑地址转换和线性地址空间分页。Even with the minimum use of segments, every byte in the processor’s addressspace is accessed with a logical address. A logical address consists of a16-bit segment selector and a 32-bit offset (see Figure 3-5). The segmentselector identifies the segment the byte is located in and the offset specifiesthe location of the byte in the segment relative to the base address of thesegment. 即使使用最少的段，处理器地址空间中的每个字节也可以通过逻辑地址进行访问。 逻辑地址由 16 位段选择器和 32 位偏移量组成（见图 3-5）。 段选择器标识该字节所在的段，偏移量指定字节在段中相对于段基地址的位置。The processor translates every logical address into a linear address. A linearaddress is a 32-bit address in the processor’s linear address space. Like thephysical address space, the linear address space is a flat (unsegmented),232-byte address space, with addresses ranging from 0 to FFFFFFFFH.The linear address space contains all the segments and system tables definedfor a system. 处理器将每个逻辑地址转换为线性地址。 线性地址是处理器线性地址空间中的 32 位地址。与物理地址空间一样，线性地址空间是平坦（不分段）的 2^32 字节地址空间，地址范围从0 到 FFFFFFFFH。 线性地址空间包含为系统定义的所有段和system table.To translate a logical address into a linear address, the processor does thefollowing: 为了将逻辑地址转换为线性地址，处理器执行以下操作： Uses the offset in the segment selector to locate the segment descriptor forthe segment in the GDT or LDT and reads it into the processor. (This step isneeded only when a new segment selector is loaded into a segment register.) 使用段选择器中的偏移量来定位 GDT 或 LDT 中该段的段描述符，并将其读入处理器.（只有当新的段选择器加载到段寄存器中时才需要执行此步骤。） Examines the segment descriptor to check the access rights and range of thesegment to ensure that the segment is accessible and that the offset iswithin the limits of the segment. Examines [ɪɡˈzæmɪnz]: 检查; 审查; 检查段描述符以检查该段的access rights和range，以确保该段是 accessible并且偏移量在该段的限制内。 Adds the base address of the segment from the segment descriptor to theoffset to form a linear address. 将段描述符中的段基地址与偏移量相加，形成线性地址。 If paging is not used, the processor maps the linear address directly to aphysical address (that is, the linear address goes out on the processor’saddress bus). If the linear address space is paged, a second level of addresstranslation is used to translate the linear address into a physical address. 如果不使用分页，处理器会将线性地址直接映射到物理地址（即，线性地址则输出在处理器地址总线上）。 如果线性地址空间被分页，则使用第二级地址转换来将线性地址转换为物理地址。See also: Chapter 4, “Paging.”3.4.1 Logical Address Translation in IA-32e ModeIn IA-32e mode, an Intel 64 processor uses the steps described above totranslate a logical address to a linear address. In 64-bit mode, the offset andbase address of the segment are 64-bits instead of 32 bits. The linear addressformat is also 64 bits wide and is subject to the canonical form requirement. be subject to :受支配，从属于; 有…倾向的 在 IA-32e 模式下，Intel 64 处理器使用上述步骤将逻辑地址转换为线性地址。 在64位模式下，段的偏移量和基地址都是64位而不是32位。 线性地址格式也是 64 位宽，并且符合规范形式要求。 之前调研AMD dma use-after-free的时候介绍过 Each code segment descriptor provides an L bit. This bit allows a code segmentto execute 64-bit code or legacy 32-bit code by code segment. 每个代码段描述符提供一个L位。 该位允许每个代码段指定执行 64 位代码或legacy 32 位代码。3.4.2 Segment SelectorsA segment selector is a 16-bit identifier for a segment (see Figure 3-6). Itdoes not point directly to the segment, but instead points to the segmentdescriptor that defines the segment. A segment selector contains the followingitems: 段选择器是段的 16 位标识符（见图 3-6）。 它并不直接指向段，而是指向定义该段的段描述符。 段选择器包含以下item: Index (Bits 3 through 15) Selects one of 8192 descriptors in the GDT or LDT. The processor multipliesthe index value by 8 (the number of bytes in a segment descriptor) and addsthe result to the base address of the GDT or LDT (from the GDTR or LDTRregister, respectively). 选择 GDT 或 LDT 中的 8192 个描述符之一。 处理器将索引值乘以 8（段描述符中的字节数），并将结果添加到 GDT 或 LDT 的基地址（分别来自 GDTR 或 LDTR 寄存器）。 TI (table indicator) flag (Bit 2) Specifies the descriptor table to use: clearing this flag selects the GDT;setting this flag selects the current LDT. 指定要使用的描述符表：清除该标志选择GDT； 设置此标志选择当前 LDT。 Requested Privilege Level (RPL) (Bits 0 and 1) Specifies the privilege level of the selector. The privilege level can rangefrom 0 to 3, with 0 being the most privileged level. See Section 5.5,“Privilege Levels,” for a description of the relationship of the RPL to theCPL of the executing program (or task) and the descriptor privilege level(DPL) of the descriptor the segment selector points to. 指定选择器的权限级别。 权限级别的范围为 0 到 3，其中 0 是最高权限级别。 请参阅第 5.5 节“Privilege levels”，了解 RPL 与执行程序（或任务）的 CPL 以及段选择器指向的描述符的描述符特权级别 (DPL) 之间的关系的描述。 The first entry of the GDT is not used by the processor. A segment selectorthat points to this entry of the GDT (that is, a segment selector with an indexof 0 and the TI flag set to 0) is used as a “null segment selector.” Theprocessor does not generate an exception when a segment register (other thanthe CS or SS registers) is loaded with a null selector. It does, however,generate an exception when a segment register holding a null selector is usedto access memory. A null selector can be used to initialize unused segmentregisters. Loading the CS or SS register with a null segment selector causes ageneral-protection exception (#GP) to be generated. initialize [ɪˈnɪʃəlaɪz] 处理器不使用 GDT 的第一个条目。 指向 GDT 的该条目的段选择器（即索引为 0 且 TI 标志设置为 0 的段选择器）被用作“null segment selector”。 当段寄存器（CS 或 SS 寄存器除外）加载了 null selector 时，处理器不会生成异常。 然而，当使用持有 null selector的段寄存器来访问内存时，它确实会生成异常。 null selector 可用于初始化未使用的段寄存器。 使用空段选择器加载 CS 或 SS 寄存器会导致生成一般保护异常 (#GP)。Segment selectors are visible to application programs as part of a pointervariable, but the values of selectors are usually assigned or modified by linkeditors or linking loaders, not application programs. 段选择器作为指针变量的一部分对应用程序可见，但选择器的值通常由link editors或ilinloader而不是应用程序分配或修改。3.4.3 Segment RegistersTo reduce address translation time and coding complexity, the processorprovides registers for holding up to 6 segment selectors (see Figure 3-7). Eachof these segment registers support a specific kind of memory reference (code,stack, or data). For virtually any kind of program execution to take place, atleast the code-segment (CS), data-segment (DS), and stack-segment (SS)registers must be loaded with valid segment selectors. The processor alsoprovides three additional data-segment registers (ES, FS, and GS), which can beused to make additional data segments available to the currently executingprogram (or task). 为了减少地址转换时间和编码复杂性，处理器提供了用于保存最多 6 个段选择器的寄存器（见图 3-7）。 每个段寄存器都支持特定类型的内存引用（代码、堆栈或数据）。 实际上，要执行任何类型的程序，至少必须向代码段 (CS)、数据段 (DS) 和堆栈段 (SS) 寄存器加载有效的段选择器。 该处理器还提供三个附加数据段寄存器（ES、FS 和 GS），可用于为当前正在执行的程序（或任务）提供附加数据段。For a program to access a segment, the segment selector for the segment musthave been loaded in one of the segment registers. So, although a system candefine thousands of segments, only 6 can be available for immediate use. Othersegments can be made available by loading their segment selectors into theseregisters during program execution. 对于要访问段的程序，该段的段选择器必须已加载到段寄存器之一中。 因此，尽管系统可以定义数千个段，但只有 6 个可以立即使用。 通过在程序执行期间将其段选择器加载到这些寄存器中，可以使其他段可用。Every segment register has a “visible” part and a “hidden” part. (The hiddenpart is sometimes referred to as a “descriptor cache” or a “shadow register.”)When a segment selector is loaded into the visible part of a segment register,the processor also loads the hidden part of the segment register with the baseaddress, segment limit, and access control information from the segmentdescriptor pointed to by the segment selector. The information cached in thesegment register (visible and hidden) allows the processor to translateaddresses without taking extra bus cycles to read the base address and limitfrom the segment descriptor. In systems in which multiple processors haveaccess to the same descriptor tables, it is the responsibility of software toreload the segment registers when the descriptor tables are modified. If thisis not done, an old segment descriptor cached in a segment register might beused after its memory-resident version has been modified. reside [rɪˈzaɪd] vi: 居住在; 定居于resident [ˈrezɪdənt] n: 居民; 住户; adj.(在某地)居住的 每个段寄存器都有一个“可见”部分和一个“隐藏”部分。 （隐藏部分有时称为“描述符缓存”或“影子寄存器”。）当将段选择器加载到段寄存器的可见部分时，处理器还会将段选择器的隐藏部分加载到段寄存器中。 段选择器指向的段描述符中的基地址、段限制和访问控制信息。 缓存在段寄存器（可见和隐藏）中的信息允许处理器转换地址，而无需花费额外的总线周期来从段描述符中读取基地址和限制。 在多个处理器可以访问相同描述符表的系统中，当描述符表被修改时，软件负责重新加载段寄存器。 如果不这样做，则在修改其 memory-resident version后，可能会使用缓存在段寄存器中的旧段描述符。Two kinds of load instructions are provided for loading the segment registers: 提供两种加载指令用于加载段寄存器： Direct load instructions such as the MOV, POP, LDS, LES, LSS, LGS, and LFSinstructions. These instructions explicitly reference the segment registers. 直接加载指令，例如 MOV、POP、LDS、LES、LSS、LGS 和 LFS 指令。 这些指令显式引用段寄存器。 Implied load instructions such as the far pointer versions of the CALL, JMP,and RET instructions, the SYSENTER and SYSEXIT instructions, and the IRET,INT n, INTO, INT3, and INT1 instructions. These instructions change thecontents of the CS register (and sometimes other segment registers) as anincidental part of their operation. incidental [ˌɪnsɪˈdentl] : 次要的; 附带发生的; 伴随而来的; 非有意的; 隐式加载指令，例如 CALL、JMP 和 RET 指令的far pointer version、SYSENTER 和 SYSEXIT 指令以及 IRET、INT n、INTO、INT3 和 INT1 指令。 这些指令更改CS 寄存器（有时还更改其他段寄存器）的内容，作为其操作的附带部分。 The MOV instruction can also be used to store the visible part of a segmentregister in a general-purpose register. MOV指令还可用于将段寄存器的可见部分 store 在通用寄存器中。3.4.4 Segment Loading Instructions in IA-32e ModeBecause ES, DS, and SS segment registers are not used in 64-bit mode, theirfields (base, limit, and attribute) in segment descriptor registers areignored. Some forms of segment load instructions are also invalid (for example,LDS, POP ES). Address calculations that reference the ES, DS, or SS segmentsare treated as if the segment base is zero. 由于 ES、DS 和 SS 段寄存器在 64 位模式下不使用，因此它们在段描述符寄存器中的字段（基址、限制和属性）将被忽略。 某些形式的段加载指令也是无效的（例如LDS、POP ES）。引用 ES、DS 或 SS 段的地址计算被视为段基址为零。The processor checks that all linear-address references are in canonical forminstead of performing limit checks. Mode switching does not change the contentsof the segment registers or the associated descriptor registers. Theseregisters are also not changed during 64-bit mode execution, unless explicitsegment loads are performed. 处理器检查所有线性地址引用是否采用规范形式，而不是执行limit check。 模式切换不会更改段寄存器或相关描述符寄存器的内容。 这些寄存器在 64 位模式执行期间也不会更改，除非执行显式段加载。In order to set up compatibility mode for an application, segment-loadinstructions (MOV to Sreg, POP Sreg) work normally in 64-bit mode. An entry isread from the system descriptor table (GDT or LDT) and is loaded in the hiddenportion of the segment register. The descriptor-register base, limit, andattribute fields are all loaded. However, the contents of the data and stacksegment selector and the descriptor registers are ignored. 为了为应用程序设置兼容模式，段加载指令（MOV 到 Sreg、POP Sreg）在 64 位模式下正常工作。 从系统描述符表（GDT 或 LDT）中读取一个条目，并将其加载到段寄存器的隐藏部分中。 描述符寄存器基址、限制和属性字段均已加载。 但是，数据和堆栈段选择器以及描述符寄存器的内容将被忽略。When FS and GS segment overrides are used in 64-bit mode, their respective baseaddresses are used in the linear address calculation: (FS or GS).base + index +displacement. FS.base and GS.base are then expanded to the full linear-addresssize supported by the implementation. The resulting effective addresscalculation can wrap across positive and negative addresses; the resultinglinear address must be canonical. displacement [dɪsˈpleɪsmənt] : 移位; 取代; 替代; 当在 64 位模式下使用 FS 和 GS 段 overrides(相当于指定特定的该段)时，它们各自的基地址用于线性地址计算：（FS 或 GS).base + index + displacement。 然后 FS.base 和 GS.base 扩展到implemention支持的完整线性地址大小。 由此产生的有效地址计算可以跨越 positive和 negative 地址； 生成的线性地址必须是规范的。In 64-bit mode, memory accesses using FS-segment and GS-segment overrides arenot checked for a runtime limit nor subjected to attribute-checking. Normalsegment loads (MOV to Sreg and POP Sreg) into FS and GS load a standard 32-bitbase value in the hidden portion of the segment register. The base address bitsabove the standard 32 bits are cleared to 0 to allow consistency forimplementations that use less than 64 bits. 在 64 位模式下，使用 FS 段和 GS 段覆盖的内存访问不会检查 runtime limit,也不会进行属性检查。 正常段加载（MOV 到 Sreg 和 POP Sreg）到 FS 中，GS 加载段寄存器隐藏部分中的标准 32 位基值。 标准 32 位以上的基地址位被清除为0，以保证使用少于 64 位的实现的一致性。The hidden descriptor register fields for FS.base and GS.base are physicallymapped to MSRs in order to load all address bits supported by a 64-bitimplementation. Software with CPL = 0 (privileged software) can load allsupported linear-address bits into FS.base or GS.base using WRMSR. Addresseswritten into the 64-bit FS.base and GS.base registers must be in canonicalform. A WRMSR instruction that attempts to write a non-canonical address tothose registers causes a #GP fault. FS.base 和 GS.base 的隐藏描述符寄存器字段物理映射到 MSR，以便加载 64 位实现支持的所有地址位。 CPL = 0 的软件（特权软件）可以使用 WRMSR 将所有支持的线性地址位加载到 FS.base 或 GS.base 中。 写入 64 位 FS.base 和 GS.base 寄存器的地址必须采用规范形式。 尝试将非规范地址写入这些寄存器的 WRMSR 指令会导致 #GP 错误。When in compatibility mode, FS and GS overrides operate as defined by 32-bitmode behavior regardless of the value loaded into the upper 32 linear-addressbits of the hidden descriptor register base field. Compatibility mode ignoresthe upper 32 bits when calculating an effective address. 在兼容模式下，FS 和 GS 覆盖按照 32 位模式行为定义进行操作，无论加载到隐藏描述符寄存器基字段的高 32 个线性地址位中的值如何。 兼容模式在计算有效地址时忽略高 32 位。A new 64-bit mode instruction, SWAPGS, can be used to load GS base. SWAPGSexchanges the kernel data structure pointer from the IA32_KERNEL_GS_BASE MSRwith the GS base register. The kernel can then use the GS prefix on normalmemory references to access the kernel data structures. An attempt to write anon-canonical value (using WRMSR) to the IA32_KERNEL_GS_BASE MSR causes a #GPfault. 新的 64 位模式指令 SWAPGS 可用于加载 GS 基址。 SWAPGS 将 IA32_KERNEL_GS_BASEMSR 中的内核数据结构指针与 GS 基址寄存器进行交换。 然后，内核可以在普通内存引用上使用 GS 前缀来访问内核数据结构。 尝试将非规范值（使用 WRMSR）写入 IA32_KERNEL_GS_BASE MSR 会导致 #GP 错误。3.4.5 Segment DescriptorsA segment descriptor is a data structure in a GDT or LDT that provides theprocessor with the size and location of a segment, as well as access controland status information. Segment descriptors are typically created by compilers,linkers, loaders, or the operating system or executive, but not applicationprograms. Figure 3-8 illustrates the general descriptor format for all typesof segment descriptors. 段描述符是 GDT 或 LDT 中的一种数据结构，它为处理器提供段的大小和位置，以及访问控制和状态信息。 段描述符通常由编译器、链接器、加载器或操作系统或executive创建，但不是应用程序。图 3-8 说明了所有类型的段描述符的通用描述符格式。The flags and fields in a segment descriptor are as follows: Segment limit field Specifies the size of the segment. The processor puts together the twosegment limit fields to form a 20-bit value. The processor interprets thesegment limit in one of two ways, depending on the setting of the G(granularity) flag: granularity [grænju'læriti]: 颗粒,粒度 指定段的大小。 处理器将两个段限制字段放在一起形成一个 20 位值。 处理器以两种方式之一解释段限制，具体取决于 G（粒度）标志的设置： If the granularity flag is clear, the segment size can range from 1 byte to1 MByte, in byte increments. 如果粒度标志被清除，则段大小的范围可以从 1 byte到 1 MByte，以byte为增量。 If the granularity flag is set, the segment size can range from 4 KBytes to4 GBytes, in 4-KByte increments. 如果设置了粒度标志，则段大小的范围可以从 4 KB 到 4 GB，以 4 KB 为增量。 The processor uses the segment limit in two different ways, depending onwhether the segment is an expand-up or an expand-down segment. See Section3.4.5.1, “Code- and Data-Segment Descriptor Types,” for more informationabout segment types. For expand-up segments, the offset in a logical addresscan range from 0 to the segment limit. Offsets greater than the segment limitgenerate general-protection exceptions (#GP, for all segments other than SS)or stack-fault exceptions (#SS for the SS segment). For expand-downsegments, the segment limit has the reverse function; the offset can rangefrom the segment limit plus 1 to FFFFFFFFH or FFFFH, depending on the settingof the B flag. Offsets less than or equal to the segment limit generategeneral-protection exceptions or stack-fault exceptions. Decreasing the valuein the segment limit field for an expand-down segment allocates new memoryat the bottom of the segment’s address space, rather than at the top. IA-32architecture stacks always grow downwards, making this mechanism convenientfor expandable stacks. 处理器以两种不同的方式使用段限制，具体取决于该段是expand-down段还是expand-up。 有关段类型的更多信息，请参见第 3.4.5.1 节“代码段和数据段描述符类型”。 对于expand-up段，逻辑地址中的偏移量范围可以从 0 到段限制。 大于段限制的偏移量会生成一般保护异常（#GP，对于除 SS 之外的所有段）或堆栈错误异常（#SS 对于 SS 段）。 对于expand-down段，段限制具有相反的功能； 偏移量的范围可以从段限制加 1 到 FFFFFFFFH 或 FFFFH，具体取决于B flag的设置。 小于或等于段限制的偏移量会生成一般保护异常或堆栈错误异常。 减小expand-down段的段限制字段中的值会在段地址空间的底部而不是顶部分配新内存。 IA-32架构堆栈总是向下增长，使得这种机制便于扩展堆栈。 Base address fields Defines the location of byte 0 of the segment within the 4-GByte linearaddress space. The processor puts together the three base address fields toform a single 32-bit value. Segment base addresses should be aligned to16-byte boundaries. Although 16-byte alignment is not required, thisalignment allows programs to maximize performance by aligning code and dataon 16-byte boundaries. maximize [ˈmæksɪmaɪz] :最大化;最大限度的利用;充分利用;使增大到最大限度 定义 4 GB 线性地址空间内段的字节 0 的位置。 处理器将三个基地址字段放在一起形成一个 32 位值。 段基地址应与 16 字节边界对齐。 尽管不需要 16 字节对齐，但这种对齐允许程序通过在 16 字节边界上对齐代码和数据来最大限度地提高性能。 Type field Indicates the segment or gate type and specifies the kinds of access that canbe made to the segment and the direction of growth. The interpretation ofthis field depends on whether the descriptor type flag specifies anapplication (code or data) descriptor or a system descriptor. The encoding ofthe type field is different for code, data, and system descriptors (seeFigure 5-1). See Section 3.4.5.1, “Code-and Data-Segment Descriptor Types,”for a description of how this field is used to specify code and data-segmenttypes. 指示段或门类型，并指定可以对段进行访问的类型和增长方向。 该字段的解释取决于描述符类型标志指定应用程序（代码或数据）描述符还是系统描述符。 代码、数据和系统描述符的类型字段的编码是不同的（见图 5-1）。 有关如何使用此字段来指定代码和数据段类型的说明，请参见第 3.4.5.1 节“代码段和数据段描述符类型”。 S (descriptor type) flag Specifies whether the segment descriptor is for a system segment (S flag isclear) or a code or data segment (S flag is set). 指定段描述符是用于系统段（S 标志清零）还是代码或数据段（S 标志设置）。 DPL (descriptor privilege level) field Specifies the privilege level of the segment. The privilege level can rangefrom 0 to 3, with 0 being the most privileged level. The DPL is used tocontrol access to the segment. See Section 5.5, “Privilege Levels,” for adescription of the relationship of the DPL to the CPL of the executing codesegment and the RPL of a segment selector. 指定段的权限级别。 权限级别的范围为 0 到 3，其中 0 是最高权限级别。 DPL 用于控制对段的访问。 有关 DPL 与执行代码段的 CPL 和段选择器的 RPL 之间关系的描述，请参见第 5.5 节“特权级别”。 P (segment-present) flag Indicates whether the segment is present in memory (set) or not present(clear). If this flag is clear, the processor generates a segment-not-presentexception (#NP) when a segment selector that points to the segment descriptoris loaded into a segment register. Memory management software can use thisflag to control which segments are actually loaded into physical memory at agiven time. It offers a control in addition to paging for managing virtualmemory. Figure 3-9 shows the format of a segment descriptor when thesegment-present flag is clear. When this flag is clear, the operating systemor executive is free to use the locations marked “Available” to store its owndata, such as information regarding the whereabouts of the missing segment. regard [rɪˈɡɑːrd] : 将...认为;看待;注意;关注;尊重regarding: prep. 关于；至于; v. 把...视为;看待whereabout: 下落,行踪 指示该段是否存在于内存中（设置）或不存在（clear）。 如果清除该标志，则当指向段描述符的段选择器加载到段寄存器中时，处理器会生成 segment-not-presentexception (#NP)。 内存管理软件可以使用此标志来控制在给定时间哪些段实际加载到物理内存中。 除了分页之外，它还提供了用于管理虚拟内存的控制。 图 3-9显示了当段存在标志清零时段描述符的格式。 当该标志被清除时，操作系统或执行程序可以自由地使用标记为“可用”的位置来存储其自己的数据，例如有关missing segment的下落的信息。 D/B (default operation size/default stack pointer size and/or upper bound) flag Performs different functions depending on whether the segment descriptor isan executable code segment, an expand-down data segment, or a stack segment.(This flag should always be set to 1 for 32-bit code and data segments and to0 for 16-bit code and data segments.) 根据段描述符是可执行代码段、向下扩展数据段还是堆栈段来执行不同的功能。 （对于 32 位代码和数据段，该标志应始终设置为 1；对于 16 位代码和数据段，该标志应始终设置为 0。） Executable code segment. The flag is called the D flag and it indicates the default length foreffective addresses and operands referenced by instructions in the segment.If the flag is set, 32-bit addresses and 32-bit or 8-bit operands areassumed; if it is clear, 16-bit addresses and 16-bit or 8-bit operands areassumed. 该标志称为 D 标志，它指示段中指令引用的有效地址和操作数的默认长度。 如果设置了该标志，则假定为 32 位地址和 32 位或 8 位操作数； 如果清除，则假定为 16 位地址和 16 位或 8 位操作数。 The instruction prefix 66H can be used to select an operand size other thanthe default, and the prefix 67H can be used select an address size otherthan the default. 指令前缀66H可用于选择默认值以外的操作数大小，并且前缀67H可用于选择默认值以外的地址大小。 Stack segment (data segment pointed to by the SS register). The flag is called the B (big) flag and it specifies the size of the stackpointer used for implicit stack operations (such as pushes, pops, andcalls). If the flag is set, a 32-bit stack pointer is used, which is storedin the 32-bit ESP register; if the flag is clear, a 16-bit stack pointer isused, which is stored in the 16-bit SP register. If the stack segment isset up to be an expand-down data segment (described in the next paragraph),the B flag also specifies the upper bound of the stack segment. 该标志称为 B（big）标志，它指定用于隐式堆栈操作（例如压入、弹出和调用）的堆栈指针的大小。 如果该标志被设置，则使用32位堆栈指针，该指针存储在32位ESP寄存器中； 如果标志清零，则使用 16 位堆栈指针，该指针存储在 16 位 SP 寄存器中。 如果堆栈段设置为向下扩展数据段（在下一段中描述），则 B 标志还指定堆栈段的上限。 Expand-down data segment. The flag is called the B flag and it specifies the upper bound of thesegment. If the flag is set, the upper bound is FFFFFFFFH (4 GBytes); ifthe flag is clear, the upper bound is FFFFH (64 KBytes). 该标志称为 B 标志，它指定段的上限。 如果设置了该标志，则上限为 FFFFFFFFH (4 GBytes)； 如果该标志被清除，则上限为 FFFFH (64 KB)。 G (granularity) flag Determines the scaling of the segment limit field. When the granularity flagis clear, the segment limit is interpreted in byte units; when flag is set,the segment limit is interpreted in 4-KByte units. (This flag does not affectthe granularity of the base address; it is always byte granular.) When thegranularity flag is set, the twelve least significant bits of an offset arenot tested when checking the offset against the segment limit. For example,when the granularity flag is set, a limit of 0 results in valid offsets from0 to 4095. 确定段限制字段的scaling(缩放比例)。 当粒度标志清除时，段限制以byte为单位解释；当设置标志时，段限制以 4 KB 为单位进行解释。 （此标志不会影响基地址的粒度；它始终是字节粒度的。）设置粒度标志后，在根据段限制检查偏移量时，不会测试偏移量的 12 个最低有效位。 例如，当设置粒度标志时，限制为 [0, 4095] 的有效偏移量。 L (64-bit code segment) flag In IA-32e mode, bit 21 of the second doubleword of the segment descriptorindicates whether a code segment contains native 64-bit code. A value of 1indicates instructions in this code segment are executed in 64-bit mode. Avalue of 0 indicates the instructions in this code segment are executed incompatibility mode. If L-bit is set, then D-bit must be cleared. When not inIA-32e mode or for non-code segments, bit 21 is reserved and should always beset to 0. 在 IA-32e 模式中，段描述符的第二个双字的位 21 指示代码段是否包含native 64 位代码。 值为 1 表示该代码段中的指令以 64 位模式执行。 值为 0 表示该代码段中的指令以兼容模式执行。 如果设置了 L 位，则必须清除 D 位。 当不在 IA-32e 模式下或对于非代码段时，位 21 被保留并且应始终设置为 0。 Available and reserved bits Bit 20 of the second doubleword of the segment descriptor is available foruse by system software. 段描述符的第二个双字的位 20 可供系统软件使用。 3.4.5.1 Code-and Data-Segment Descriptor TypesWhen the S (descriptor type) flag in a segment descriptor is set, thedescriptor is for either a code or a data segment. The highest order bit of thetype field (bit 11 of the second double word of the segment descriptor) thendetermines whether the descriptor is for a data segment (clear) or a codesegment (set). 当段描述符中的 S（描述符类型）标志被设置时，该描述符用于代码段或数据段。然后，类型字段的最高位（段描述符的第二个双字的位 11）确定描述符是用于数据段（清除）还是代码段（设置）。For data segments, the three low-order bits of the type field (bits 8, 9, and10) are interpreted as accessed (A), write-enable (W), and expansion-direction(E). See Table 3-1 for a description of the encoding of the bits in the typefield for code and data segments. Data segments can be read-only or read/writesegments, depending on the setting of the write-enable bit. 对于数据段，类型字段的三个低位（位 8、9 和 10）被解释为已访问 (A)、可写 (W)和扩展方向 (E)。 有关代码段和数据段类型字段中位编码的说明，请参见表 3-1。 数据段可以是只读段或读/写段，具体取决于write-enable bit的设置。Stack segments are data segments which must be read/write segments. Loading theSS register with a segment selector for a nonwritable data segment generates ageneral-protection exception (#GP). If the size of a stack segment needs to bechanged dynamically, the stack segment can be an expand-down data segment(expansion-direction flag set). Here, dynamically changing the segment limitcauses stack space to be added to the bottom of the stack. If the size of astack segment is intended to remain static, the stack segment may be either anexpand-up or expand-down type. 堆栈段是数据段，必须是读/写段。使用不可写数据段的段选择器加载 SS 寄存器会生成一般保护异常 (#GP)。 如果堆栈段的大小需要动态改变，则堆栈段可以是expand-donw的数据段（扩展方向标志设置）。 在这里，动态更改段限制会导致堆栈空间添加到堆栈底部。如果堆栈段的大小旨在保持static，则堆栈段可以是向上扩展类型或向下扩展类型。The accessed bit indicates whether the segment has been accessed since the lasttime the operating-system or executive cleared the bit. The processor sets thisbit whenever it loads a segment selector for the segment into a segmentregister, assuming that the type of memory that contains the segment descriptorsupports processor writes. The bit remains set until explicitly cleared. Thisbit can be used both for virtual memory management and for debugging. explicitly /ɪkˈsplɪsətli/: 明确的,显示的 已访问位指示自上次操作系统或执行程序清除该位以来该段是否已被访问过。 每当处理器将段的段选择器加载到段寄存器中时，假设包含段描述符的内存类型支持处理器写入，处理器就会设置该位。 该位保持设置状态，直到显示的清除为止。该位可用于虚拟内存管理和调试。For code segments, the three low-order bits of the type field are interpretedas accessed (A), read enable (R), and conforming (C). Code segments can beexecute-only or execute/read, depending on the setting of the read-enable bit.An execute/read segment might be used when constants or other static data havebeen placed with instruction code in a ROM. Here, data can be read from thecode segment either by using an instruction with a CS override prefix or byloading a segment selector for the code segment in a data-segment register (theDS, ES, FS, or GS registers). In protected mode, code segments are notwritable. 对于代码段，类型字段的三个低位被解释为已访问 (A)、read enable (R) 和conforming (C)。 代码段可以是只执行的，也可以是执行/读取的，具体取决于read-enable bit的设置。 当常量或其他静态数据与指令代码一起放置在 ROM 中时，可能会使用执行/读取段。 这里，可以通过使用带有 CS 覆盖前缀的指令或通过在数据段寄存器（DS、ES、FS 或 GS 寄存器）中加载代码段的段选择器来从代码段读取数据。在保护模式下，代码段不可写。Code segments can be either conforming or nonconforming. A transfer ofexecution into a more-privileged conforming segment allows execution tocontinue at the current privilege level. A transfer into a nonconformingsegment at a different privilege level results in a general-protectionexception (#GP), unless a call gate or task gate is used (see Section 5.8.1,“Direct Calls or Jumps to Code Segments,” for more information on conformingand nonconforming code segments). System utilities that do not access protectedfacilities and handlers for some types of exceptions (such as, divide error oroverflow) may be loaded in conforming code segments. Utilities that need to beprotected from less privileged programs and procedures should be placed innonconforming code segments. 代码段可以是conforming，也可以是 nonconforming。 将执行转移到特权更高的 conformingsegment 允许执行在当前特权级别上continue。 除非使用call-gate或task-gate，否则以不同权限级别传输到nonconforming段会导致一般保护异常 (#GP)（请参阅第 5.8.1 节“直接调用或跳转到代码段”有关conforming和nonconforming代码段的更多信息）。 不访问受保护facilities和某些类型异常（例如除法错误或溢出）处理程序的system utilities 可以加载到 conforming 代码段中。需要保护其不受特权较低的程序和procedure 影响的utilities 应放置在 nonconforming的代码段中。 ??? 难道某些异常handler所在的Trap Gate的Segment selector, 要指定为 conforming 代码段?? 看下面描述是的 NOTE Execution cannot be transferred by a call or a jump to a less-privileged(numerically higher privilege level) code segment, regardless of whether thetarget segment is a conforming or nonconforming code segment. Attempting suchan execution transfer will result in a general-protection exception. numerically [nuˈmɛrɪkli]: 数字上的 无论目标段是conforming代码段还是nonconforming代码段，都不能通过调用或跳转将执行转移到特权较低（数字上特权级别较高）的代码段。 尝试这样的执行转移将导致一般保护异常。 All data segments are nonconforming, meaning that they cannot be accessed byless privileged programs or procedures (code executing at numerically higherprivilege levels). Unlike code segments, however, data segments can be accessedby more privileged programs or procedures (code executing at numerically lowerprivilege levels) without using a special access gate. 所有数据段都是nonconforming的，这意味着它们不能被特权较低的程序或procedure（在数字上较高特权级别执行的代码）访问。 然而，与代码段不同的是，数据段可以由更高特权的程序或过程（在数字上较低特权级别执行的代码）访问，而无需使用特殊的访问门。If the segment descriptors in the GDT or an LDT are placed in ROM, theprocessor can enter an indefinite loop if software or the processor attempts toupdate (write to) the ROM-based segment descriptors. To prevent this problem,set the accessed bits for all segment descriptors placed in a ROM. Also, removeoperating-system or executive code that attempts to modify segment descriptorslocated in ROM. indefinite [ɪnˈdefɪnət]: 无限期的 如果 GDT 或 LDT 中的段描述符放置在 ROM 中，则当软件或处理器尝试更新（写入）ROM-based 的段描述符时，处理器可能会进入 indefinite loop。 为了防止出现此问题，请设置 ROM 中所有段描述符的accessed bits。另外，删除尝试修改 ROM 中的段描述符的操作系统或执行代码。3.5 SYSTEM DESCRIPTOR TYPESWhen the S (descriptor type) flag in a segment descriptor is clear, thedescriptor type is a system descriptor. The processor recognizes the followingtypes of system descriptors: 当段描述符中的S（描述符类型）标志清零时，该描述符类型是系统描述符。 处理器可识别以下类型的系统描述符： Local descriptor-table (LDT) segment descriptor. Task-state segment (TSS) descriptor. Call-gate descriptor. Interrupt-gate descriptor. Trap-gate descriptor. Task-gate descriptor.These descriptor types fall into two categories: system-segment descriptors andgate descriptors. System-segment descriptors point to system segments (LDT andTSS segments). Gate descriptors are in themselves “gates,” which hold pointersto procedure entry points in code segments (call, interrupt, and trap gates) orwhich hold segment selectors for TSS’s (task gates). in themselves: 本身, 自身 这些描述符类型分为两类：system-segment descriptor(系统段描述符)和gate descriptor(门描述符)。 系统段描述符指向系统段（LDT 和 TSS 段）。Gate descriptor 本身就是“gate”，它保存指向代码段中过程入口点的指针（调用、中断和陷阱门），或者保存 TSS（任务门）的段选择器。 TSS descriptor 只能在 GDT中. Table 3-2 shows the encoding of the type field for system-segment descriptorsand gate descriptors. Note that system descriptors in IA-32e mode are 16 bytesinstead of 8 bytes. Table 3-2 显示了系统段描述符和门描述符的类型字段的编码。 请注意，IA-32e 模式下的系统描述符是 16 字节，而不是 8 字节。See also: Section 3.5.1, “Segment Descriptor Tables,” and Section 8.2.2, “TSSDescriptor” (for more information on the system-segment descriptors); seeSection 5.8.3, “Call Gates”, Section 6.11, “IDT Descriptors”, and Section8.2.5, “Task-Gate Descriptor” (for more information on the gate descriptors). 另请参见：第 3.5.1 节“段描述符表”和第 8.2.2 节“TSS 描述符”（有关系统段描述符的更多信息）； 请参见第 5.8.3 节“调用门”、第 6.11 节“IDT 描述符”和第 8.2.5 节“任务门描述符”（有关门描述符的更多信息）。3.5.1 Segment Descriptor TablesA segment descriptor table is an array of segment descriptors (see Figure3-10). A descriptor table is variable in length and can contain up to 8192(213) 8-byte descriptors. There are two kinds of descriptor tables: 段描述符表是段描述符的数组（见图3-10）。 描述符表的长度是可变的，最多可以包含 8192 (2^13) 个 8 字节描述符。 描述符表有两种： The global descriptor table (GDT) The local descriptor tables (LDT)Each system must have one GDT defined, which may be used for all programs andtasks in the system. Optionally, one or more LDTs can be defined. For example,an LDT can be defined for each separate task being run, or some or all taskscan share the same LDT. 每个系统必须定义一个GDT，它可以用于系统中的所有程序和任务。 可选地，可以定义一个或多个LDT。 例如，可以为正在运行的每个单独的任务定义LDT，或者一些或所有任务可以共享相同的LDT。The GDT is not a segment itself; instead, it is a data structure in linearaddress space. The base linear address and limit of the GDT must be loaded intothe GDTR register (see Section 2.4, “Memory-Management Registers”). The baseaddress of the GDT should be aligned on an eight-byte boundary to yield thebest processor performance. The limit value for the GDT is expressed in bytes.As with segments, the limit value is added to the base address to get theaddress of the last valid byte. A limit value of 0 results in exactly one validbyte. Because segment descriptors are always 8 bytes long, the GDT limit shouldalways be one less than an integral multiple of eight (that is, 8N – 1). yield: v: 产生;屈服,投降;让步;放弃 n:产量;产出;利润integral: adj 完整的; n 整体integral multiple: 整数倍 GDT 本身不是段；相反，它是线性地址空间中的数据结构。 GDT 的基线性地址和限制必须加载到 GDTR 寄存器中（请参见第 2.4 节“内存管理寄存器”）。 GDT 的基地址应在8-byte边界上对齐，以产生最佳的处理器性能。 GDT 的限制值以字节表示。 与段一样，将限制值添加到基地址以获得最后一个有效字节的地址。 限制值 0 只会产生一个有效字节。 由于段描述符始终为 8 字节长，因此 GDT 限制应始终小于 8 的整数倍（即 8N – 1）. 这里大概的意思是说, limit达到的效果是 [0, limit], 实际上range 为 limit - 0 + 1 = limit + 1 所以要想指定N个 GDT descriptor大小, 需要将限制设置为 8N - 1, 该range正好为8N.The first descriptor in the GDT is not used by the processor. A segmentselector to this “null descriptor” does not generate an exception when loadedinto a data-segment register (DS, ES, FS, or GS), but it always generates ageneral-protection exception (#GP) when an attempt is made to access memoryusing the descriptor. By initializing the segment registers with this segmentselector, accidental reference to unused segment registers can be guaranteedto generate an exception. accidental [ˌæksɪˈdentl]: 意外的;偶然的guaranteed [ˌɡærənˈtiːd]J GDT 中的第一个描述符未被处理器使用。 当加载到数据段寄存器（DS、ES、FS 或 GS）时，此“null descriptor” 的段选择器不会生成异常，但当尝试使用该描述符access memory时，它总是生成一般保护异常 (#GP)。通过使用该段选择器初始化段寄存器，可以保证意外引用 unused 段寄存器会生成异常。The LDT is located in a system segment of the LDT type. The GDT must contain asegment descriptor for the LDT segment. If the system supports multiple LDTs,each must have a separate segment selector and segment descriptor in the GDT.The segment descriptor for an LDT can be located anywhere in the GDT. SeeSection 3.5, “System Descriptor Types”, for information on the LDTsegment-descriptor type. LDT位于LDT类型的系统段中。 GDT 必须包含 LDT 段的段描述符。 如果系统支持多个LDT，则每个LDT在GDT中必须有一个单独的段选择器和段描述符。 LDT 的段描述符可以位于 GDT 中的任何位置。 有关 LDT 段描述符类型的信息，请参见第 3.5 节“系统描述符类型”。An LDT is accessed with its segment selector. To eliminate address translationswhen accessing the LDT, the segment selector, base linear address, limit, andaccess rights of the LDT are stored in the LDTR register (see Section 2.4,“Memory-Management Registers”). eliminate [ɪˈlɪmɪneɪt]: 消除, 排除;消灭, 清除 LDT 通过其段选择器来访问。为了避免访问 LDT 时的地址转换，LDT 的段选择器、基线性地址、限制和访问权限存储在 LDTR 寄存器中（请参见第 2.4 节“内存管理寄存器”）。When the GDTR register is stored (using the SGDT instruction), a 48-bit“pseudo-descriptor” is stored in memory (see top diagram in Figure 3-11). Toavoid alignment check faults in user mode (privilege level 3), the pseudo-descriptor should be located at an odd word address (that is, address MOD 4 isequal to 2). This causes the processor to store an aligned word, followed by analigned doubleword. User-mode programs normally do not storepseudo-descriptors, but the possibility of generating an alignment check faultcan be avoided by aligning pseudo-descriptors in this way. The same alignmentshould be used when storing the IDTR register using the SIDT instruction.When storing the LDTR or task register (using the SLDT or STR instruction,respectively), the pseudo-descriptor should be located at a doubleword address(that is, address MOD 4 is equal to 0). odd /ɑːd/: 古怪的，奇怪的 当 GDTR 寄存器被存储时（使用 SGDT 指令），一个 48 位“伪描述符”被存储在memory中（参见图 3-11 中的顶图）。 为了避免user mode （权限级别 3）下的对齐检查错误，伪描述符应位于奇数字地址（即地址 MOD 4 等于 2）。 这导致处理器存储先对齐的word，后跟在对齐doubleword。 用户态程序通常不存储伪描述符，但通过这种方式对齐伪描述符可以避免产生对齐检查错误的可能性。 使用 SIDT 指令存储 IDTR 寄存器时应使用相同的对齐方式。 当存储 LDTR 或任务寄存器（分别使用 SLDT 或 STR 指令）时，伪描述符应位于双字地址（即地址 MOD 4 等于 0）。 这个没看懂, 不知道SGDT 对 地址对齐要求和 特全级有什么关系 3.5.2 Segment Descriptor Tables in IA-32e ModeIn IA-32e mode, a segment descriptor table can contain up to 8192(213) 8-byte descriptors. An entry in the segment descriptor tablecan be 8 bytes. System descriptors are expanded to 16 bytes (occupying thespace of two entries). 在IA-32e模式中，段描述符表可以包含多达8192（2^13）个8字节描述符。段描述符表中的一个条目可以是8个字节。系统描述符被扩展到16个字节（占用两个条目的空间）。GDTR and LDTR registers are expanded to hold 64-bit base address. Thecorresponding pseudo-descriptor is 80 bits. (see the bottom diagram in Figure3-11). GDTR 和 LDTR 寄存器扩展为保存 64 位基地址。 相应的伪描述符是80位。 （参见图 3-11 中的底部部分）。The following system descriptors expand to 16 bytes: Call gate descriptors (see Section 5.8.3.1, “IA-32e Mode Call Gates”). IDT gate descriptors (see Section 6.14.1, “64-Bit Mode IDT”). LDT and TSS descriptors (see Section 8.2.3, “TSS Descriptor in 64-bitmode”)." }, { "title": "protected-mode memory management", "url": "/posts/pml/", "categories": "intel_sdm, pml", "tags": "virt, pml", "date": "2024-04-23 11:00:00 +0800", "snippet": "29.3.6 Page-Modification LoggingWhen accessed and dirty flags for EPT are enabled, software can track writes toguest-physical addresses using a feature called page-modification logging. 当启用了 EPT 的...", "content": "29.3.6 Page-Modification LoggingWhen accessed and dirty flags for EPT are enabled, software can track writes toguest-physical addresses using a feature called page-modification logging. 当启用了 EPT 的访问和脏标志时，软件可以使用一种称为PML记录的功能来跟踪对客户物理地址的写操作。Software can enable page-modification logging by setting the “enable PML”VM-execution control (see Table 25-7 in Section 25.6.2). When this control is1, the processor adds entries to the page-modification log as described below.The page-modification log is a 4-KByte region of memory located at the physicaladdress in the PML address VM-execution control field. The page-modificationlog consists of 512 64-bit entries; the PML index VM-execution control fieldindicates the next entry to use. 软件可以通过设置“enable PML” VM-execution control（参见第 25.6.2 节的表 25-7）来启用PML 记录。当该控制位为 1 时，处理器会按照下面描述的方式将条目添加到PML 中。PML 是一个 4 KB 的内存区域，位于 PML 地址虚拟机执行控制字段中指定的物理地址。PML 由 512 个 64 位条目组成；PML 索引虚拟机执行控制字段指示下一个要使用的条目。Before allowing a guest-physical access, the processor may determine that itfirst needs to set an accessed or dirty flag for EPT (see Section 29.3.5). Whenthis happens, the processor examines the PML index. If the PML index is not inthe range 0–511, there is a page-modification log-full event and a VM exitoccurs. In this case, the accessed or dirty flag is not set, and theguest-physical access that triggered the event does not occur. 在允许客户物理访问之前，处理器可能会确定它首先需要为 EPT 设置访问或脏标志（参见第 29.3.5 节）。当发生这种情况时，处理器会检查 PML 索引。如果 PML 索引不在 0-511 的范围内，就会发生PML log full event ，并且会产生 VM exit。在这种情况下，访问或脏标志不会被设置，并且触发事件的客户物理访问也不会发生。If instead the PML index is in the range 0–511, the processor proceeds toupdate accessed or dirty flags for EPT as described in Section 29.3.5. If theprocessor updated a dirty flag for EPT (changing it from 0 to 1), it thenoperates as follows: 如果 PML 索引在 0-511 的范围内，处理器会继续按照第 29.3.5 节中描述的方式更新EPT 的访问或脏标志。如果处理器更新了 EPT 的脏标志（将其从 0 变为 1），则它会继续执行以下操作： The guest-physical address of the access is written to the page-modificationlog. Specifically, the guest- physical address is written to physicaladdress determined by adding 8 times the PML index to the PML address. Bits11:0 of the value written are always 0 (the guest-physical address writtenis thus 4-KByte aligned). 访问的 GPA 被写入PML。具体来说，GPA被写入通过将 PML address 加上 PML index 乘以 8 所确定的物理地址。写入值的第 11 到 0 位始终为 0（因此写入的GPA是 4 KB 对齐的）。 The PML index is decremented by 1 (this may cause the value to transitionfrom 0 to FFFFH). PML 索引减 1（这可能导致其值从 0 变为 FFFFH）。 Because the processor decrements the PML index with each log entry, the valuemay transition from 0 to FFFFH. At that point, no further logging will occur,as the processor will determine that the PML index is not in the range 0– 511and will generate a page-modification log-full event (see above). 由于处理器在每次日志记录时都会将 PML 索引减 1，该值可能会从 0 变为 FFFFH。此时，将不再进行日志记录，因为处理器会判断 PML 索引不在 0 到 511 的范围内，并触发page-modification log-full event。29.3.7 EPT and Memory TypingThis section specifies how a logical processor determines the memory type usefor a memory access while EPT is in use. (See Chapter 12, “Memory CacheControl‚” of the Intel® 64 and IA-32 Architectures Software Developer’s Manual,Volume 3A, for details of memory typing in the Intel 64 architecture.) Section29.3.7.1 explains how the memory type is determined for accesses to the EPTpaging structures. Section 29.3.7.2 explains how the memory type is determinedfor an access using a guest-physical address that is translated using EPT. 这一部分说明了在使用扩展页表（EPT）时，逻辑处理器如何确定内存访问所使用的内存类型。（有关 Intel 64 架构中的内存类型的详细信息，请参阅《Intel® 64 和 IA-32 架构软件开发人员手册》第 3A 卷第 12 章“Memory Cache Control”。）第 29.3.7.1 节解释了如何确定对 EPT 分页结构访问的内存类型。第 29.3.7.2 节解释了如何确定使用 EPT 翻译的 GPA 进行访问时的内存类型。29.3.7.1 Memory Type Used for Accessing EPT Paging StructuresThis section explains how the memory type is determined for accesses to the EPTpaging structures. The determi- nation is based first on the value of bit 30(cache disable—CD) in control register CR0: 这一部分解释了如何确定对 EPT 分页结构访问时的内存类型。首先，这一确定过程基于控制寄存器 CR0 中第 30 位（disable—CD）的值： If CR0.CD = 0, the memory type used for any such reference is the EPTpaging-structure memory type, which is specified in bits 2:0 of theextended-page-table pointer (EPTP), a VM-execution control field (see Section25.6.11). A value of 0 indicates the uncacheable type (UC), while a value of6 indicates the write-back type (WB). Other values are reserved. 如果 CR0.CD = 0，则用于任何此类引用的内存类型是 EPT 分页结构的内存类型，该类型在扩展页表指针（EPTP）的位 2:0 中指定，这是一个 VM-execution controlfield（参见第 25.6.11 节）。值为 0 表示不可缓存类型（UC），而值为 6 表示写回类型（WB）。其他值是保留的。 If CR0.CD = 1, the memory type used for any such reference is uncacheable (UC). 如果 CR0.CD = 1，则用于任何此类引用的内存类型是不可缓存的（UC）。 The MTRRs have no effect on the memory type used for an access to an EPT pagingstructure.29.3.7.2 Memory Type Used for Translated Guest-Physical AddressesThe effective memory type of a memory access using a guest-physical address (anaccess that is translated using EPT) is the memory type that is used to accessmemory. The effective memory type is based on the value of bit 30 (cachedisable—CD) in control register CR0; the last EPT paging-structure entry usedto translate the guest- physical address (either an EPT PDE with bit 7 set to 1or an EPT PTE); and the PAT memory type (see below): The PAT memory type depends on the value of CR0.PG: If CR0.PG = 0, the PAT memory type is WB (writeback).1 If CR0.PG = 1, the PAT memory type is the memory type selected from theIA32_PAT MSR as specified in Section 12.12.3, “Selecting a Memory Type fromthe PAT.”2 The EPT memory type is specified in bits 5:3 of the last EPT paging-structureentry: 0 = UC; 1 = WC; 4 = WT; 5 = WP; and 6 = WB. Other values are reservedand cause EPT misconfigurations (see Section 29.3.3). If CR0.CD = 0, the effective memory type depends upon the value of bit 6 ofthe last EPT paging-structure entry: If the value is 0, the effective memory type is the combination of the EPTmemory type and the PAT memory type specified in Table 12-7 in Section12.5.2.2, using the EPT memory type in place of the MTRR memory type. If the value is 1, the memory type used for the access is the EPT memorytype. The PAT memory type is ignored. If CR0.CD = 1, the effective memory type is UC.The MTRRs have no effect on the memory type used for an access to aguest-physical address.流程图展示" }, { "title": "when load control register during in switching rmode 2 pmode", "url": "/posts/enter-protect-mode/", "categories": "my_test", "tags": "protect-mode", "date": "2024-04-22 10:30:00 +0800", "snippet": "the intel sdm suggestionFrom intel sdm 10.9.1 Switching to Protected Mode, it give followingsteps:...3. Execute a MOV CR0 instruction that sets the PE flag (and optionally the PG flag) in control...", "content": "the intel sdm suggestionFrom intel sdm 10.9.1 Switching to Protected Mode, it give followingsteps:...3. Execute a MOV CR0 instruction that sets the PE flag (and optionally the PG flag) in control register CR0.4. Immediately following the MOV CR0 instruction, execute a far JMP or far CALL instruction. (This operation is typically a far jump or call to the next instruction in the instruction stream.)5. It can be found that after executing \"MOV to CR0\" (set PE), the relevant instructions of the load control register can be executed....9. After entering protected mode, the segment registers continue to hold the contents they had in real-address mode. The JMP or CALL instruction in step 4 resets the CS register. Perform one of the following operations to update the contents of the remaining segment registers. + Reload segment registers DS, SS, ES, FS, and GS. If the ES, FS, and/or GS registers are not going to be used, load them with a null selector. ...This means that after we execute the MOV to CR0 (set PE) instruction, it isbest to execute a far jump or call immediately. After completing the above steps, go to load control registerBut on my own opinion, after executing “MOV to CR0” (set PE), the CPU hasentered protected mode. At this time, executing MOV to SS, MOV to DS will alsosuccessfully load the control register from the segment descriptor.testI wrote a small program and run it in qemu, part of it is as follows:lgdt %cs:gdt_descmov $0x28, %eaxmov %eax, %esmov $1, %eaxmov %eax, %cr0mov $0x28, %eaxmov %eax, %esljmp $0x10, $.startup_protThe above code modifies the ES selector before and after the mov to CR0 (setPE) instruction. Debugging using qemu+gdb. gdb print information(gdb) ni38 mov $0x28, %eax(gdb) ni39 mov %eax, %es(gdb)41 mov $1, %eax(gdb) ni42 mov %eax, %cr0(gdb) ni44 mov $0x28, %eax(gdb) ni45 mov %eax, %es(gdb) ni46 ljmp $0x10, $.startup_prot When gdb is executed to position 39, use the qemu monitor info registerscommand to obtain the ES register value: ES =0028 00000280 0000ffff 00009300 When gdb is executed to position 46, use the qemu monitor info registerscommand to obtain the ES register value: ES =0028 00000000 ffffffff 00cf9300 DPL=0 DS [-WA] It can be found that after executing “MOV to CR0” (set PE), the relevantinstructions of the load control register can be executed successfully.So I want to ask, is it reasonable to execute the “load control register”instruction immediately after MOV to CR0 (set PE), and what is the meaning ofstep 5 in the manual?Thanks" }, { "title": "vm86", "url": "/posts/vm86/", "categories": "intel_sdm", "tags": "virt", "date": "2024-04-22 10:30:00 +0800", "snippet": " FROM intel sdm chapter 21 8086 emulationabstractIA-32 processors (beginning with the Intel386 processor) provide two ways toexecute new or legacy programs that are assembled and/or compiled to ru...", "content": " FROM intel sdm chapter 21 8086 emulationabstractIA-32 processors (beginning with the Intel386 processor) provide two ways toexecute new or legacy programs that are assembled and/or compiled to run on anIntel 8086 processor: IA-32处理器(起始于intel386处理器)提供了两种来执行new/legacy 程序,这些程序被assembled(组装?) and/or compiled(编译) 以在intel 8086处理器上运行. Real-address mode. Virtual-8086 mode.Figure 2-3 shows the relationship of these operating modes to protected modeand system management mode (SMM). Figure 2-3 展示了这些操作模式和 保护模式以及 SMM 的关系When the processor is powered up or reset, it is placed in the real-addressmode. This operating mode almost exactly duplicates the execution environmentof the Intel 8086 processor, with some extensions. Virtually any programassembled and/or compiled to run on an Intel 8086 processor will run on anIA-32 processor in this mode. 当处理器被 power up 或者 reset, 他处于 real-address mode. 这种操作模式几乎完全复制了Intel 8086处理器的执行环境，并进行了一些扩展。实际上，任何在 Intel 8086 处理器上组装和/或编译的程序都可以在此模式下在 IA-32 处理器上运行.When running in protected mode, the processor can be switched to virtual-8086mode to run 8086 programs. This mode also duplicates the execution environmentof the Intel 8086 processor, with extensions. In virtual-8086 mode, an 8086program runs as a separate protected-mode task. Legacy 8086 programs are thusable to run under an operating system (such as Microsoft Windows*) that takesadvantage of protected mode and to use protected-mode facilities, such as theprotected-mode interrupt- and exception-handling facilities. Protected-modemultitasking permits multiple virtual-8086 mode tasks (with each task running aseparate 8086 program) to be run on the processor along with othernon-virtual-8086 mode tasks. take advantage of : 利用facility [fəˈsɪləti] : 组件, 设施, 特色, 天赋, 才能 当运行在保护模式下时，处理器可以切换到virtual-8086模式来运行8086程序。该模式还复制了Intel 8086 处理器的执行环境，并进行了扩展。在virtual-8086 模式下，8086 程序作为单独的保护模式任务运行。 因此，传统 8086 程序能够在利用保护模式的操作系统（例如 Microsoft Windows*）下运行并使用保护模式facilities(组件, 设施)，例如保护模式中断和异常处理facilities。 保护模式多任务处理允许多个virtual-8086 模式task（每个任务运行一个单独的 8086 程序）与其他 non-virtual-8086 模式任务一起在处理器上运行。This section describes both the basic real-address mode execution environmentand the virtual-8086-mode execution environment, available on the IA-32processors beginning with the Intel386 processor. 本节介绍基于 real-address 模式执行环境和virtual-8086 模式执行环境，可在从 Intel386 处理器开始的 IA-32 处理器上使用。21.1 REAL-ADDRESS MODEThe IA-32 architecture’s real-address mode runs programs written for the Intel8086, Intel 8088, Intel 80186, and Intel 80188 processors, or for thereal-address mode of the Intel 286, Intel386, Intel486, Pentium, P6 family,Pentium 4, and Intel Xeon processors. IA-32 架构的实地址模式运行为 Intel 8086、Intel 8088、Intel 80186 和 Intel 80188 处理器编写的程序，或为 Intel 286、Intel386、Intel486、Pentium、P6 系列、Pentium 4 和 英特尔至强处理器, 实地址模式编写的程序.The execution environment of the processor in real-address mode is designed toduplicate the execution environment of the Intel 8086 processor. To an 8086program, a processor operating in real-address mode behaves like a high-speed8086 processor. The principal features of this architecture are defined inChapter 3, “Basic Execution Environment”, of the Intel® 64 and IA-32Architectures Software Developer’s Manual, Volume 1. real-address模式下处理器的执行环境旨在复制Intel 8086处理器的执行环境。对于 8086 程序来说，运行在实地址模式下的处理器的行为类似于高速 8086 处理器。该架构的主要功能在 itnel sdm Volume 1 Chapter 3 “Basic Execution Environment”The following is a summary of the core features of the real-address modeexecution environment as would be seen by a program written for the 8086: 以下是为 8086 编写的程序所看到的实地址模式执行环境的核心功能的摘要： The processor supports a nominal 1-MByte physical address space (see Section21.1.1, “Address Translation in Real-Address Mode”, for specific details).This address space is divided into segments, each of which can be up to 64KBytes in length. The base of a segment is specified with a 16-bit segmentselector, which is shifted left by 4 bits to form a 20-bit offset fromaddress 0 in the address space. An operand within a segment is addressed witha 16-bit offset from the base of the segment. A physical address is thusformed by adding the offset to the 20-bit segment base (see Section 21.1.1,“Address Translation in Real-Address Mode”). 处理器支持标称 1 MB 物理地址空间（有关具体细节，请参见第 21.1.1 节”实地址模式下的地址转换”）。 该地址空间分为多个段，每个段的长度最多可达 64 KB。 段的基址由 16 位段选择器指定，该段选择器左移 4 位，形成距地址空间中地址 0 的 20 位偏移量。 段内的操作数通过距段基址的 16 位偏移量进行寻址。 因此，通过将偏移量添加到 20 位段基址来形成物理地址（请参见第 21.1.1 节“实地址模式下的地址转换”）。 All operands in “native 8086 code” are 8-bit or 16-bit values. (Operand sizeoverride prefixes can be used to access 32-bit operands.) “native 8086 代码”中的所有操作数都是 8 位或 16 位值。（操作数大小覆盖前缀可用于访问 32 位操作数。） Eight 16-bit general-purpose registers are provided: AX, BX, CX, DX, SP, BP,SI, and DI. The extended 32 bit registers (EAX, EBX, ECX, EDX, ESP, EBP, ESI,and EDI) are accessible to programs that explicitly perform a size overrideoperation. 提供 8 个 16 位通用寄存器：AX、BX、CX、DX、SP、BP、SI 和 DI。 扩展的 32 位寄存器（EAX、EBX、ECX、EDX、ESP、EBP、ESI 和 EDI）可供显式执行大小覆盖操作的程序访问。 Four segment registers are provided: CS, DS, SS, and ES. (The FS and GSregisters are accessible to programs that explicitly access them.) The CSregister contains the segment selector for the code segment; the DS and ESregisters contain segment selectors for data segments; and the SS registercontains the segment selector for the stack segment. 提供了四个段寄存器：CS、DS、SS 和 ES。 （FS 和 GS 寄存器可供显式访问它们的程序访问。）CS 寄存器包含代码段的段选择器； DS和ES寄存器包含数据段的段选择器；SS 寄存器包含堆栈段的段选择器。 The 8086 16-bit instruction pointer (IP) is mapped to the lower 16-bits ofthe EIP register. Note this register is a 32-bit register and unintentionaladdress wrapping may occur. unintentional: [ˌʌnɪnˈtenʃənl] 无意的; 非故意的; 偶然的 8086 16 位指令指针（IP）映射到 EIP 寄存器的低 16 位。 请注意，该寄存器是 32 位寄存器，可能会发生 unintentional address wrapping unintentional address wrapping 在 “21.1.1 Address Translation in Real-Address Mode” 有介绍 The 16-bit FLAGS register contains status and control flags. (This registeris mapped to the 16 least significant bits of the 32-bit EFLAGS register.) 16 位标志寄存器包含状态和控制标志。（该寄存器映射到 32 位 EFLAGS 寄存器的 16 个最低有效位。） All of the Intel 8086 instructions are supported (see Section 21.1.3,“Instructions Supported in Real-Address Mode”). 支持所有 Intel 8086 指令（请参见第 21.1.3 节“instruction supported in real-address mode”）。 A single, 16-bit-wide stack is provided for handling procedure calls andinvocations of interrupt and exception handlers. This stack is contained inthe stack segment identified with the SS register. The SP (stack pointer)register contains an offset into the stack segment. The stack grows down(toward lower segment offsets) from the stack pointer. The BP (base pointer)register also contains an offset into the stack segment that can be used as apointer to a parameter list. When a CALL instruction is executed, theprocessor pushes the current instruction pointer (the 16 least-significantbits of the EIP register and, on far calls, the current value of the CSregister) onto the stack. On a return, initiated with a RET instruction, theprocessor pops the saved instruction pointer from the stack into the EIPregister (and CS register on far returns). When an implicit call to aninterrupt or exception handler is executed, the processor pushes the EIP, CS,and EFLAGS (low-order 16-bits only) registers onto the stack. On a returnfrom an interrupt or exception handler, initiated with an IRET instruction,the processor pops the saved instruction pointer and EFLAGS image from thestack into the EIP, CS, and EFLAGS registers. invocations [ˌɪnvəʊˈkeɪʃənz]: 调用; 启用; 祈祷procedure [prəˈsiːdʒə(r)] : 程序, 步骤, 手续, 手术 提供了一个 16 位宽的堆栈来处理procedure calls以及中断和异常处理程序的调用。 该堆栈包含在由 SS 寄存器标识的堆栈段中。 SP（堆栈指针）寄存器包含堆栈段的偏移量。 堆栈从堆栈指针向下增长（朝向较低的段偏移量）。 BP（基指针）寄存器还包含堆栈段的偏移量，可用作指向参数列表的指针。 当执行 CALL 指令时，处理器将当前指令指针（EIP 寄存器的 16 个最低有效位，以及far all 时 CS 寄存器的当前值）推送到堆栈上。 在使用 RET 指令启动的返回时，处理器将保存的指令指针从堆栈pop到 EIP 寄存器（以及远返回时的 CS 寄存器）。 当执行对中断或异常处理程序的隐式调用时，处理器会将 EIP、CS 和 EFLAGS（仅限低位 16 位）寄存器压入堆栈。 从使用 IRET 指令启动的中断或异常处理程序返回时，处理器将保存的指令指针和 EFLAGS image从堆栈pop到 EIP、CS 和 EFLAGS 寄存器中。 A single interrupt table, called the “interrupt vector table” or “interrupttable,” is provided for handling interrupts and exceptions (see Figure 21-2).The interrupt table (which has 4-byte entries) takes the place of theinterrupt descriptor table (IDT, with 8-byte entries) used when handlingprotected-mode interrupts and exceptions. Interrupt and exception vectornumbers provide an index to entries in the interrupt table. Each entryprovides a pointer (called a “vector”) to an interrupt- or exception-handlingprocedure. See Section 21.1.4, “Interrupt and Exception Handling”, for moredetails. It is possible for software to relocate the IDT by means of the LIDTinstruction on IA-32 processors beginning with the Intel386 processor. by means of: 通过take the place of: 取代, 代替 提供了一个称为“中断向量表”或“中断表”的中断表来处理中断和异常（见图 21-2）。 中断表（具有 4-byte 条目）取代了处理保护模式中断和异常时使用的中断描述符表（IDT，具有 8-byte条目）。 中断和异常向量号提供中断表中条目的索引。 每个条目都提供一个指向interrupt &amp;&amp; exception-handling procedure 的指针（称为“向量”）。 更多详细信息，请参见第 21.1.4 节”中断和异常处理”。 从 Intel386 处理器开始，软件可以通过 IA-32 处理器上的 LIDT 指令来重新定位 IDT。 The x87 FPU is active and available to execute x87 FPU instructions inreal-address mode. Programs written to run on the Intel 8087 and Intel 287math coprocessors can be run in real-address mode without modification. coprocessors: 辅助处理器, 协处理器 x87 FPU 处于活动状态，可在实地址模式下执行 x87 FPU 指令。 为在 Intel8087 和 Intel 287 数学协处理器上运行而编写的程序无需修改即可在实地址模式下运行。 The following extensions to the Intel 8086 execution environment are availablein the IA-32 architecture’s real-address mode. If backwards compatibility toIntel 286 and Intel 8086 processors is required, these features should not beused in new programs written to run in real-address mode. Intel 8086 执行环境的以下扩展可在 IA-32 架构的实地址模式下使用。如果需要向后兼容 Intel 286 和 Intel 8086 处理器，则不应在为在实地址模式下运行而编写的新程序中使用这些功能。 Two additional segment registers (FS and GS) are available. Many of the integer and system instructions that have been added to laterIA-32 processors can be executed in real-address mode (see Section 21.1.3,“Instructions Supported in Real-Address Mode”). integer [ˈɪntɪdʒər]: 整数 许多已添加到后续 IA-32 处理器中的整数和系统指令都可以在实地址模式下执行（请参见第 21.1.3 节“实地址模式支持的指令”）。 The 32-bit operand prefix can be used in real-address mode programs toexecute the 32-bit forms of instructions. This prefix also allowsreal-address mode programs to use the processor’s 32-bit general-purposeregisters. 32 位操作数前缀可用于实地址模式程序来执行 32 位形式的指令。 该前缀还允许实地址模式程序使用处理器的 32 位通用寄存器。 The 32-bit address prefix can be used in real-address mode programs, allowing32-bit offsets. Many of the integer and system instructions that have beenadded to later IA-32 processors can be executed in real-address mode (seeSection 21.1.3, “Instructions Supported in Real-Address Mode”). 32位地址前缀可用于实地址模式程序，允许32位偏移。 许多已添加到后续 IA-32 处理器中的整数和系统指令都可以在实地址模式下执行（请参见第 21.1.3 节“实地址模式支持的指令”）。 The following sections describe address formation, registers, availableinstructions, and interrupt and exception handling in real-address mode. Forinformation on I/O in real-address mode, see Chapter 19, “Input/Output”, of theIntel® 64 and IA-32 Architectures Software Developer’s Manual, Volume 1. a32-bit register and unintentional address wrapping may occur. 以下部分描述了实地址模式下的地址形成、寄存器、可用指令以及中断和异常处理。有关实地址模式下 I/O 的信息，请参阅intel sdm 第 1 卷第 19 章“Input/Output”。可能会发生 32 位寄存器和unintentional address wrapping。21.1.1 Address Translation in Real-Address ModeIn real-address mode, the processor does not interpret segment selectors asindexes into a descriptor table; instead, it uses them directly to form linearaddresses as the 8086 processor does. It shifts the segment selector left by 4bits to form a 20-bit base address (see Figure 21-1). The offset into a segmentis added to the base address to create a linear address that maps directly tothe physical address space. interpret [ɪnˈtɜːprət]: 解释, 说明; 把...理解为 在实地址模式下，处理器不会将段选择器解释为描述符表的索引； 相反，它像 8086 处理器一样直接使用它们来形成线性地址。 它将段选择子左移 4 位，形成 20 位基地址（见图 21-1）。 段中的偏移量被添加到基地址以创建直接映射到物理地址空间的线性地址。When using 8086-style address translation, it is possible to specify addresseslarger than 1 MByte. For example, with a segment selector value of FFFFH and anoffset of FFFFH, the linear (and physical) address would be 10FFEFH (1 megabyteplus 64 KBytes). The 8086 processor, which can form addresses only up to 20bits long, truncates the high-order bit, thereby “wrapping” this address toFFEFH. When operating in real-address mode, however, the processor does nottruncate such an address and uses it as a physical address. (Note, however,that for IA-32 processors beginning with the Intel486 processor, the A20M#signal can be used in real-address mode to mask address line A20, therebymimicking the 20-bit wrap-around behavior of the 8086 processor.) Care shouldbe take to ensure that A20M# based address wrapping is handled correctly inmultiprocessor based system. 当使用 8086 类型的地址转换时，可以指定大于 1 MB 的地址。 例如，如果段选择器值为 FFFFH，偏移量为 FFFFH，则线性（物理）地址将为 10FFEFH（1 兆字节加 64 KB）。 0xffff&lt;&lt;4 + 0xffff = 64K * 16 + 64k = 1M-byte + 64K = 0x10ffef 8086 处理器只能形成最多 20 位长的地址，它会截断高位，从而将该地址“包装”为 FFEFH。然而，当在实地址模式下运行时，处理器不会截断此类地址并将其用作物理地址。 （但请注意，对于从 Intel486 处理器开始的 IA-32 处理器，可以在实地址模式下使用 A20M# 信号来maks address line A20，从而模仿 8086 处理器的 20 位wrap-around行为。 ）应注意确保在基于多处理器的系统中正确处理基于 A20M# 的address wrappingThe IA-32 processors beginning with the Intel386 processor can generate 32-bitoffsets using an address override prefix; however, in real-address mode, thevalue of a 32-bit offset may not exceed FFFFH without causing an exception. 从Intel386处理器开始的IA-32处理器可以使用address override prefix生成32位偏移量；然而，在实地址模式下，32位偏移量的值不能超过FFFFH而不引起异常。For full compatibility with Intel 286 real-address mode, pseudo-protectionfaults (interrupt 12 or 13) occur if a 32- bit offset is generated outside therange 0 through FFFFH. 为了与 Intel 286 实地址模式完全兼容，如果在 0 到 FFFFH 范围之外生成 32 位偏移，则会发生 pseudo-protection (伪保护故障（中断 12 或 13）。21.1.2 Registers Supported in Real-Address ModeThe register set available in real-address mode includes all the registersdefined for the 8086 processor plus the new registers introduced in later IA-32processors, such as the FS and GS segment registers, the debug registers, thecontrol registers, and the floating-point unit registers. The 32-bit operandprefix allows a real-address mode program to use the 32-bit general-purposeregisters (EAX, EBX, ECX, EDX, ESP, EBP, ESI, and EDI). 实地址模式下可用的寄存器集包括为 8086 处理器定义的所有寄存器以及后来的 IA-32 处理器中引入的新寄存器，例如 FS 和 GS 段寄存器、调试寄存器、控制寄存器和浮点寄存器。 32 位操作数前缀允许实地址模式程序使用 32 位通用寄存器（EAX、EBX、ECX、EDX、ESP、EBP、ESI 和 EDI）。21.1.3 Instructions Supported in Real-Address ModeThe following instructions make up the core instruction set for the 8086processor. If backwards compatibility to the Intel 286 and Intel 8086processors is required, only these instructions should be used in a new programwritten to run in real-address mode. 以下指令构成了 8086 处理器的核心指令集。 如果需要向后兼容 Intel 286 和 Intel 8086 处理器，在编写为在实地址模式下运行的新程序中仅使用这些指令。 Move (MOV) instructions that move operands between general-purpose registers,segment registers, and between memory and general-purpose registers. The exchange (XCHG) instruction. Load segment register instructions LDS and LES. Arithmetic instructions ADD, ADC, SUB, SBB, MUL, IMUL, DIV, IDIV, INC, DEC,CMP, and NEG. Logical instructions AND, OR, XOR, and NOT. Decimal instructions DAA, DAS, AAA, AAS, AAM, and AAD. decimal [ˈdesɪml] :十进制 Stack instructions PUSH and POP (to general-purpose registers and segmentregisters). Type conversion instructions CWD, CDQ, CBW, and CWDE. conversion : 转换, 转变 Shift and rotate instructions SAL, SHL, SHR, SAR, ROL, ROR, RCL, and RCR. rotate [ˈroʊteɪt]: 旋转 TEST instruction. Control instructions JMP, Jcc, CALL, RET, LOOP, LOOPE, and LOOPNE. Interrupt instructions INT n, INTO, and IRET. EFLAGS control instructions STC, CLC, CMC, CLD, STD, LAHF, SAHF, PUSHF, andPOPF. I/O instructions IN, INS, OUT, and OUTS. Load effective address (LEA) instruction, and translate (XLATB) instruction. LOCK prefix. Repeat prefixes REP, REPE, REPZ, REPNE, and REPNZ. Processor halt (HLT) instruction. No operation (NOP) instruction.The following instructions, added to later IA-32 processors (some in the Intel286 processor and the remainder in the Intel386 processor), can be executed inreal-address mode, if backwards compatibility to the Intel 8086 processor isnot required. 如果不需要向后兼容 Intel 8086 处理器，则添加到后来的 IA-32 处理器（一些在 Intel 286 处理器中，其余在 Intel386 处理器中）的以下指令可以在实地址模式下执行。 Move (MOV) instructions that operate on the control and debug registers. Load segment register instructions LSS, LFS, and LGS. Generalized multiply instructions and multiply immediate data. Generalized [ˈdʒenrəlaɪzd] : 广义的 Shift and rotate by immediate counts. Stack instructions PUSHA, PUSHAD, POPA, POPAD, and PUSH immediate data. Move with sign extension instructions MOVSX and MOVZX. Long-displacement Jcc instructions. displacement: 移位;取代 Exchange instructions CMPXCHG, CMPXCHG8B, and XADD. String instructions MOVS, CMPS, SCAS, LODS, and STOS. Bit test and bit scan instructions BT, BTS, BTR, BTC, BSF, and BSR; thebyte-set-on condition instruction SETcc;and the byte swap (BSWAP) instruction. EFLAGS control instructions PUSHF and POPF. ENTER and LEAVE control instructions. BOUND instruction. CPU identification (CPUID) instruction. System instructions CLTS, INVD, WINVD, INVLPG, LGDT, SGDT, LIDT, SIDT, LMSW,SMSW, RDMSR, WRMSR, RDTSC, and RDPMC.Execution of any of the other IA-32 architecture instructions (not given in theprevious two lists) in real-address mode result in an invalid-opcode exception(#UD) being generated. 在实地址模式下执行任何其他 IA-32 架构指令（前两个列表中未给出）都会导致生成invaild-opcode 异常 (#UD)。 21.1.4 Interrupt and Exception HandlingWhen operating in real-address mode, software must provide interrupt andexception-handling facilities that are separate from those provided inprotected mode. Even during the early stages of processor initialization whenthe processor is still in real-address mode, elementary real-address modeinterrupt and exception-handling facilities must be provided to ensure reliableoperation of the processor, or the initialization code must ensure that nointerrupts or exceptions will occur. 当在实地址模式下运行时，软件必须提供与保护模式下提供的中断和异常处理设施分开的facilities。 即使在处理器初始化的早期阶段，当处理器仍处于实地址模式时，也必须提供基本实地址模式中断和异常处理设施，以确保处理器的可靠运行，或者必须保证初始化代码没有中断或触发异常。The IA-32 processors handle interrupts and exceptions in real-address modesimilar to the way they handle them in protected mode. When a processorreceives an interrupt or generates an exception, it uses the vector number ofthe interrupt or exception as an index into the interrupt table. (In protectedmode, the interrupt table is called the interrupt descriptor table (IDT), butin real-address mode, the table is usually called the interrupt vector table,or simply the interrupt table.) The entry in the interrupt vector tableprovides a pointer to an interrupt- or exception-handler procedure. (Thepointer consists of a segment selector for a code segment and a 16-bit offsetinto the segment.) The processor performs the following actions to make animplicit call to the selected handler: IA-32 处理器在实地址模式下处理中断和异常的方式与在保护模式下处理中断和异常的方式类似。 当处理器接收到中断或生成异常时，它使用中断或异常的向量号作为中断表的索引。（在保护模式下，中断表称为中断描述符表（IDT），但在实地址模式下，该表通常称为中断向量表，或简称为中断表。）中断向量表中的条目提供 指向中断或异常处理程序的指针。（指针由代码段的段选择器和段中的 16 位偏移量组成。）处理器执行以下操作以隐式调用所选处理程序： Pushes the current values of the CS and EIP registers onto the stack. (Onlythe 16 least-significant bits of the EIP register are pushed.) Pushes the low-order 16 bits of the EFLAGS register onto the stack. Clears the IF flag in the EFLAGS register to disable interrupts. Clears the TF, RF, and AC flags, in the EFLAGS register. Transfers program control to the location specified in the interrupt vectortable.An IRET instruction at the end of the handler procedure reverses these steps toreturn program control to the interrupted program. Exceptions do not returnerror codes in real-address mode. The interrupt vector table is an array of4-byte entries (see Figure 21-2). Each entry consists of a far pointer to ahandler procedure, made up of a segment selector and an offset. The processorscales the interrupt or exception vector by 4 to obtain an offset into theinterrupt table. Following reset, the base of the interrupt vector table islocated at physical address 0 and its limit is set to 3FFH. In the Intel 8086processor, the base address and limit of the interrupt vector table cannot bechanged. In the later IA-32 processors, the base address and limit of theinter- rupt vector table are contained in the IDTR register and can be changedusing the LIDT instruction. reverses [rɪˈvɜːsɪz] : 反转 处理程序末尾的 IRET 指令反转这些步骤，将程序控制权返回给被中断的程序。 在实地址模式下，异常不会返回错误代码。 中断向量表是一个 4 字节条目的数组（见图 21-2）。 每个条目都包含一个指向处理程序过程的远指针，该指针由段选择器和偏移量组成。 处理器将中断或异常向量 乘 4 以获得中断表中的偏移量。 复位后，中断向量表的基址位于物理地址 0，其限制设置为 3FFH。 在Intel 8086处理器中，中断向量表的基地址和限制是不能改变的。 在后来的 IA-32 处理器中，中断向量表的基址和限制包含在 IDTR 寄存器中，并且可以使用 LIDT 指令进行更改。(For backward compatibility to Intel 8086 processors, the default base addressand limit of the interrupt vector table should not be changed.) （为了向后兼容 Intel 8086 处理器，不应更改中断向量表的默认基址和限制。）Table 21-1 shows the interrupt and exception vectors that can be generated inreal-address mode and virtual-8086 mode, and in the Intel 8086 processor. SeeChapter 6, “Interrupt and Exception Handling”, for a description of theexception conditions. 表 21-1 显示了在实地址模式和虚拟 8086 模式以及 Intel 8086 处理器中可以生成的中断和异常向量。有关异常情况的描述，请参见第 6 章“interrupt and exception handling”。21.2 VIRTUAL-8086 MODEVirtual-8086 mode is actually a special type of a task that runs in protectedmode. When the operating-system or executive switches to a virtual-8086-modetask, the processor emulates an Intel 8086 processor. The execution environmentof the processor while in the 8086-emulation state is the same as is describedin Section 21.1, “Real-Address Mode” for real-address mode, including theextensions. The major difference between the two modes is that in virtual-8086mode the 8086 emulator uses some protected-mode services (such as theprotected-mode interrupt and exception-handling and paging facilities). Virtual-8086 模式实际上是一种在保护模式下运行的特殊任务类型。 当操作系统或执行程序切换到虚拟 8086 模式任务时，处理器将模拟 Intel 8086 处理器。处理器在 8086 仿真状态下的执行环境与第 21.1 节“real address mode”中描述的实地址模式相同，包括扩展。 两种模式之间的主要区别在于，在虚拟 8086 模式下，8086 仿真器使用一些保护模式服务（例如保护模式中断、异常处理和分页功能）。As in real-address mode, any new or legacy program that has been assembledand/or compiled to run on an Intel 8086 processor will run in avirtual-8086-mode task. And several 8086 programs can be run asvirtual-8086-mode tasks concurrently with normal protected-mode tasks, usingthe processor’s multitasking facilities. concurrently [kənˈkʌrəntli]: 同时 与实地址模式一样，任何已组装和/或编译以在 Intel 8086 处理器上运行的new/legacy程序都将在虚拟 8086 模式任务中运行。 使用处理器的多任务处理功能，多个 8086 程序可以作为虚拟 8086 模式任务与正常保护模式任务同时运行。 NOTE: In the real-address mode, vector 13 is the segment overrun exception. Inprotected and virtual-8086 modes, this exception covers allgeneral-protection error conditions, including traps to the virtual-8086monitor from virtual-8086 mode. 在实地址模式下，向量13是段溢出异常。 在受保护模式和虚拟 8086 模式下，此异常涵盖所有general-proctection错误情况，包括从virtual-8086 模式到virtual-8086监视器的陷阱。 21.2.1 Enabling Virtual-8086 ModeThe processor runs in virtual-8086 mode when the VM (virtual machine) flag inthe EFLAGS register is set. This flag can only be set when the processorswitches to a new protected-mode task or resumes virtual-8086 mode via an IRETinstruction. 当 EFLAGS 寄存器中的 VM（虚拟机）标志被设置时，处理器以virtual-8086 模式运行。 仅当处理器切换到新的protected-mode task 或通过 IRET 指令恢复虚拟 8086 模式时，才能设置此标志。System software cannot change the state of the VM flag directly in the EFLAGSregister (for example, by using the POPFD instruction). Instead it changes theflag in the image of the EFLAGS register stored in the TSS or on the stackfollowing a call to an interrupt- or exception-handler procedure. For example,software sets the VM flag in the EFLAGS image in the TSS when first creating avirtual-8086 task. 系统软件无法直接更改 EFLAGS 寄存器中 VM 标志的状态（例如，通过使用 POPFD 指令）。相反，它会在调用中断或异常处理程序过程后更改存储在 TSS 或堆栈中的 EFLAGS 寄存器映像中的标志。 例如，当首次创建虚拟 8086 任务时，软件会在 TSS 的 EFLAGS image中设置 VM 标志。The processor tests the VM flag under three general conditions: When loading segment registers, to determine whether to use 8086-styleaddress translation. 当加载段寄存器时，确定是否使用8086风格的地址转换。 When decoding instructions, to determine which instructions are not supportedin virtual-8086 mode and which instructions are sensitive to IOPL sensitive: 敏感 解码指令时，确定哪些指令在 virtual-8086 模式下不支持以及哪些指令对 IOPL sensitive When checking privileged instructions, on page accesses, or when performingother permission checks. (Virtual-8086 mode always executes at CPL 3.) 检查特权指令、页面访问或执行其他权限检查时。 （虚拟 8086 模式始终在 CPL 3 上执行。） 21.2.2 Structure of a Virtual-8086 TaskA virtual-8086-mode task consists of the following items: A 32-bit TSS for the task. The 8086 program. A virtual-8086 monitor. 8086 operating-system services.The TSS of the new task must be a 32-bit TSS, not a 16-bit TSS, because the16-bit TSS does not load the most-significant word of the EFLAGS register,which contains the VM flag. All TSS’s, stacks, data, and code used to handleexceptions when in virtual-8086 mode must also be 32-bit segments. 新任务的 TSS 必须是 32 位 TSS，而不是 16 位 TSS，因为 16 位 TSS 不加载 EFLAGS 寄存器的最高有效字，该寄存器包含 VM 标志。 在虚拟 8086 模式下用于处理异常的所有 TSS、堆栈、数据和代码也必须是 32 位段。 RFLAGS.VM (Bit 17) The processor enters virtual-8086 mode to run the 8086 program and returns toprotected mode to run the virtual- 8086 monitor. 处理器进入虚拟8086模式以运行8086程序，并返回到保护模式以运行虚拟8086 monitor。The virtual-8086 monitor is a 32-bit protected-mode code module that runs at aCPL of 0. The monitor consists of initialization, interrupt- andexception-handling, and I/O emulation procedures that emulate a personalcomputer or other 8086-based platform. Typically, the monitor is either part ofor closely associated with the protected-mode general-protection (#GP)exception handler, which also runs at a CPL of 0. As with any protected-modecode module, code-segment descriptors for the virtual-8086 monitor must existin the GDT or in the task’s LDT. The virtual-8086 monitor also may needdata-segment descriptors so it can examine the IDT or other parts of the 8086program in the first 1 MByte of the address space. The linear addresses above10FFEFH are available for the monitor, the operating system, and other systemsoftware. closely: 紧密的, 接近的examine [ɪɡˈzæmɪn]: 检查审查 virtual-8086 monitor是一个 32 位保护模式code module，运行于 CPL 0 。监视器由初始化、中断和异常处理以及模拟个人计算机或其他 8086-based platform 的 I/O 模拟程序组成。通常，监视器是受保护模式通用保护 (#GP) 异常处理程序的一部分或与之密切相关，该异常处理程序也在 CPL 为 0 时运行。与任何受保护模式代码模块一样， virtual-8086 监视器必须存在于 GDT 或任务的 LDT 中。 virtual-8086 监视器还可能需要数据段描述符，以便它可以检查地址空间前 1 MB 中的 IDT 或 8086 程序的其他部分。 10FFEFH 以上的线性地址可供显示器、操作系统和其他系统软件使用。The 8086 operating-system services consists of a kernel and/or operating-systemprocedures that the 8086 program makes calls to. These services can beimplemented in either of the following two ways: 8086 操作系统服务由 8086 程序调用的内核和/或操作系统过程组成。 这些服务可以通过以下两种方式实现： They can be included in the 8086 program. This approach is desirable foreither of the following reasons: desirable [dɪˈzaɪərəbl]: 可取的 它们可以包含在 8086 程序中。 由于以下任一原因，这种方法是可取的： The 8086 program code modifies the 8086 operating-system services. 8086程序代码修改8086操作系统服务。 There is not sufficient development time to merge the 8086 operating-systemservices into main operating system or executive. sufficient [səˈfɪʃnt] : 足够的 没有足够的开发时间将 8086 操作系统服务合并到主操作系统或执行程序中。 They can be implemented or emulated in the virtual-8086 monitor. This approachis desirable for any of the following reasons: 它们可以在virtual-8086 监视器中实现或模拟。 由于以下任一原因，这种方法是可取的： The 8086 operating-system procedures can be more easily coordinated amongseveral virtual-8086 tasks. coordinateda [koʊˈɔːrdɪneɪtɪd] : 使协调; 使相配合; 8086 操作系统程序可以更轻松地在多个虚拟 8086 任务之间进行协调。 Memory can be saved by not duplicating 8086 operating-system procedure codefor several virtual-8086 tasks. 通过不为多个虚拟 8086 任务复制 8086 操作系统过程代码，可以节省内存。 The 8086 operating-system procedures can be easily emulated by calls to themain operating system or executive. 通过调用主操作系统或执行程序可以轻松模拟 8086 操作系统程序。 The approach chosen for implementing the 8086 operating-system services mayresult in different virtual-8086- mode tasks using different 8086operating-system services. 选择用于实现 8086 操作系统服务的方法可能会因使用不同 8086 操作系统服务的导致virtual-8086-mode task 不同21.2.3 Paging of Virtual-8086 TasksEven though a program running in virtual-8086 mode can use only 20-bit linearaddresses, the processor converts these addresses into 32-bit linear addressesbefore mapping them to the physical address space. If paging is being used, the8086 address space for a program running in virtual-8086 mode can be paged andlocated in a set of pages in physical address space. If paging is used, it istransparent to the program running in virtual-8086 mode just as it is for anytask running on the processor. done B, before done A: done B, 然后在 done A 即使在virtual-8086 模式下运行的程序只能使用 20 位线性地址，处理器也会将这些地址转换为 32 位线性地址，然后再将它们映射到物理地址空间。 如果正在使用分页，则可以对在虚拟 8086 模式下运行的程序的 8086 地址空间进行分页并将其定位在物理地址空间中的一组页面中。 如果使用分页，则它对于在虚拟 8086 模式下运行的程序是透明的，就像对于处理器上运行的任何任务一样。Paging is not necessary for a single virtual-8086-mode task, but paging isuseful or necessary in the following situations: 对于单个虚拟 8086 模式任务来说，分页不是必需的，但在以下情况下分页是有用或必要的： When running multiple virtual-8086-mode tasks. Here, paging allows the lower1 MByte of the linear address space for each virtual-8086-mode task to bemapped to a different physical address location. 运行多个虚拟 8086 模式任务时。 这里，分页允许将每个虚拟 8086 模式任务的线性地址空间的低 1 MB 映射到不同的物理地址位置。 When emulating the 8086 address-wraparound that occurs at 1 MByte. When using8086-style address translation, it is possible to specify addresses largerthan 1 MByte. These addresses automatically wraparound in the Intel 8086processor (see Section 21.1.1, “Address Translation in Real-Address Mode”).If any 8086 programs depend on address wraparound, the same effect can beachieved in a virtual-8086-mode task by mapping the linear addresses between100000H and 110000H and linear addresses between 0 and 10000H to the samephysical addresses. 当模拟 1 MB 处发生的 8086 address-wraparound时。 当使用 8086 类型的地址转换时，可以指定大于 1 MB 的地址。 这些地址在 Intel 8086 处理器中自动 wraparound（请参见第 21.1.1 节“实地址模式下的地址转换”）。如果任何8086程序依赖于addresswraparound, 则通过将100000H和110000H之间的线性地址以及0和10000H之间的线性地址映射到相同的物理地址，可以在虚拟8086模式任务中实现相同的效果。 When sharing the 8086 operating-system services or ROM code that is common toseveral 8086 programs running as different 8086-mode tasks. 当共享作为不同 8086 模式任务运行的多个 8086 程序所共用的 8086 操作系统服务或ROM 代码时。 When redirecting or trapping references to memory-mapped I/O devices. 当重定向或捕获对 memory-mapped I/O 设备的引用时。 21.2.4 Protection within a Virtual-8086 TaskProtection is not enforced between the segments of an 8086 program. Either ofthe following techniques can be used to protect the system software running ina virtual-8086-mode task from the 8086 program: enforced [ɪnˈfɔːst]: 强迫的, 强制性的 8086 程序的段之间不强制执行保护。 可以使用以下任一技术来保护在 virtual-8086 模式任务中运行的系统软件免受 8086 程序的影响： Reserve the first 1 MByte plus 64 KBytes of each task’s linear address spacefor the 8086 program. An 8086 processor task cannot generate addressesoutside this range. 为 8086 程序保留每个任务的前 1 MB 加上 64 KB 的线性地址空间。 8086 处理器task无法生成此范围之外的地址。 Use the U/S flag of page-table entries to protect the virtual-8086 monitorand other system software in the virtual-8086 mode task space. When theprocessor is in virtual-8086 mode, the CPL is 3. Therefore, an 8086 processorprogram has only user privileges. If the pages of the virtual-8086 monitorhave supervisor privilege, they cannot be accessed by the 8086 program. 使用页表条目的 U/S 标志来保护virtual-8086 monitor和virtual-8086 模式任务空间中的其他系统软件。 当处理器处于virtual-8086模式时，CPL为3。因此，8086处理器程序仅具有用户权限。 如果虚拟 8086 监视器的页面具有管理员权限，则 8086 程序无法访问它们。 21.2.5 Entering Virtual-8086 ModeFigure 21-3 summarizes the methods of entering and leaving virtual-8086 mode.The processor switches to virtual-8086 mode in either of the followingsituations: 图21-3总结了进入和离开虚拟8086模式的方法。 在以下任一情况下，处理器会切换到虚拟 8086 模式： Task switch when the VM flag is set to 1 in the EFLAGS register image storedin the TSS for the task. Here the task switch can be initiated in either oftwo ways: 当任务的 TSS 中存储的 EFLAGS 寄存器映像中的 VM 标志设置为 1 时，进行任务切换。 这里可以通过两种方式启动任务切换： A CALL or JMP instruction. An IRET instruction, where the NT flag in the EFLAGS image is set to 1. IRET, with EFLAGS image ‘s NT flag == 1 Return from a protected-mode interrupt or exception handler when the VM flagis set to 1 in the EFLAGS register image on the stack. 当堆栈上 EFLAGS 寄存器映像中的 VM 标志设置为 1 时，从保护模式中断或异常处理程序返回。 When a task switch is used to enter virtual-8086 mode, the TSS for thevirtual-8086-mode task must be a 32-bit TSS. (If the new TSS is a 16-bit TSS,the upper word of the EFLAGS register is not in the TSS, causing the processorto clear the VM flag when it loads the EFLAGS register.) The processor updatesthe VM flag prior to loading the segment registers from their images in the newTSS. The new setting of the VM flag determines whether the processor interpretsthe contents of the segment registers as 8086-style segment selectors orprotected-mode segment selectors. When the VM flag is set, the segmentregisters are loaded from the TSS, using 8086-style address translation to formbase addresses. 当使用任务切换进入virtual-8086模式时，virtual-8086模式任务的TSS必须是32位TSS。（如果新的TSS是16位TSS，则EFLAGS寄存器的高位不在TSS中，导致处理器在加载EFLAGS寄存器时清除VM标志。）处理器在加载之前更新VM标志 该段将image注册到新的 TSS 中。 VM 标志的新设置确定处理器是否将段寄存器的内容解释为 8086 型段选择器或保护模式段选择器。 当VM标志被设置时，段寄存器从TSS加载，使用8086类型的地址转换来形成基地址。 否则, 段寄存器的值将在段选择子 indicate 的 段描述符中加载 See Section 21.3, “Interrupt and Exception Handling in Virtual-8086 Mode”, forinformation on entering virtual- 8086 mode on a return from an interrupt orexception handler. 有关从中断或异常处理程序返回时进入虚拟 8086 模式的信息，请参见第 21.3 节“虚拟 8086 模式中的中断和异常处理”。21.2.6 Leaving Virtual-8086 ModeThe processor can leave the virtual-8086 mode only through an interrupt orexception. The following are situations where an interrupt or exception willlead to the processor leaving virtual-8086 mode (see Figure 21-3): 处理器只能通过中断或异常离开虚拟 8086 模式。 以下是中断或异常将导致处理器离开虚拟 8086 模式的情况（见图 21-3）： The processor services a hardware interrupt generated to signal thesuspension of execution of the virtual-8086 application. This hardwareinterrupt may be generated by a timer or other external mechanism. Uponreceiving the hardware interrupt, the processor enters protected mode andswitches to a protected-mode (or another virtual-8086 mode) task eitherthrough a task gate in the protected-mode IDT or through a trap or interruptgate that points to a handler that initiates a task switch. A task switchfrom a virtual-8086 task to another task loads the EFLAGS register from theTSS of the new task. The value of the VM flag in the new EFLAGS determines ifthe new task executes in virtual-8086 mode or not. 处理器处理硬件中断，该中断产生用于作为暂停执行虚拟 8086 应用程序的信号。 该硬件中断可以由定时器或其他外部机制产生。 收到硬件中断后，处理器进入保护模式并且切换到一个 protected-mode task(或者 另一个 virtual-8086 mode task).方式有两种, 要么通过保护模式 IDT 中的任务门或通过指向handler的陷阱或中断门,该handler 会发起一个task switch. 从virtual-8086 任务到另一个任务的任务切换会从新任务的 TSS 加载 EFLAGS 寄存器。新 EFLAGS 中 VM 标志的值决定新任务是否以 virtual-8086 模式执行。 The processor services an exception caused by code executing the virtual-8086task or services a hardware interrupt that “belongs to” the virtual-8086task. Here, the processor enters protected mode and services the exception orhardware interrupt through the protected-mode IDT (normally through aninterrupt or trap gate) and the protected-mode exception- andinterrupt-handlers. The processor may handle the exception or interruptwithin the context of the virtual 8086 task and return to virtual-8086 modeon a return from the handler procedure. The processor may also execute a taskswitch and handle the exception or interrupt in the context of another task. 处理器为执行 virtual-8086 任务的代码引起的异常提供服务，或者为“属于”virtual-8086任务的硬件中断提供服务。 这里，处理器进入保护模式并通过保护模式IDT（通常通过中断或陷阱门）以及保护模式异常和中断处理程序来处理异常或硬件中断。 处理器可以在虚拟8086任务的上下文中处理异常或中断，并在从处理程序过程返回时返回到虚拟8086模式。 处理器还可以执行任务切换并在另一个任务的上下文中处理异常或中断。 The processor services a software interrupt generated by code executing inthe virtual-8086 task (such as a software interrupt to call a MS-DOS*operating system routine). The processor provides several methods of handlingthese software interrupts, which are discussed in detail in Section 21.3.3,“Class 3—Software Interrupt Handling in Virtual-8086 Mode”. Most of theminvolve the processor entering protected mode, often by means of ageneral-protection (#GP) exception. In protected mode, the processor can sendthe interrupt to the virtual-8086 monitor for handling and/or redirect theinterrupt back to the application program running in virtual-8086 mode taskfor handling. 处理器为 virtual-8086 任务中执行的代码生成的软件中断提供服务（例如调用 MS-DOS* 操作系统例程的软件中断）。 处理器提供了几种处理这些软件中断的方法，这些方法在第 21.3.3 节“Class 3—Software Interrupt Handling in Virtual-8086 Mode”中详细讨论。其中大多数涉及处理器进入保护模式，通常是通过通用保护（#GP）异常的方式。 在保护模式下，处理器可以将中断发送到virtual-8086监视器以进行处理和/或将中断重定向回以virtual-8086模式任务运行的应用程序以进行处理。 IA-32 processors that incorporate the virtual mode extension (enabled withthe VME flag in control register CR4) are capable of redirectingsoftware-generated interrupts back to the program’s interrupt handlerswithout leaving virtual-8086 mode. See Section 21.3.3.4, “Method 5: SoftwareInterrupt Handling”, for more information on this mechanism. incorporate /ɪnˈkɔːpəreɪt/: 合并,包含 包含虚拟模式扩展（通过控制寄存器 CR4 中的 VME 标志启用）的 IA-32 处理器能够将软件生成的中断重定向回程序的中断处理程序，而无需离开虚拟 8086 模式。 有关此机制的更多信息，请参见第 21.3.3.4 节“方法 5：softwareinterrupt handling”。 A hardware reset initiated by asserting the RESET or INIT pin is a specialkind of interrupt. When a RESET or INIT is signaled while the processor is invirtual-8086 mode, the processor leaves virtual-8086 mode and entersreal-address mode. 通过置位 RESET 或 INIT 引脚启动的硬件复位是一种特殊类型的中断。 当处理器处于虚拟 8086 模式时发出 RESET 或 INIT 信号时，处理器将离开虚拟 8086 模式并进入实地址模式。 Execution of the HLT instruction in virtual-8086 mode will cause ageneral-protection (GP#) fault, which the protected-mode handler generallysends to the virtual-8086 monitor. The virtual-8086 monitor then determinesthe correct execution sequence after verifying that it was entered as aresult of a HLT execution. 在虚拟 8086 模式下执行 HLT 指令将导致general-protection (GP#) fault，保护模式处理程序通常会将其发送到 virtual-8086 监视器。 然后，virtual-8086 监视器在验证它是作为 HLT 执行的结果输入后确定正确的执行顺序。 See Section 21.3, “Interrupt and Exception Handling in Virtual-8086 Mode”, forinformation on leaving virtual-8086 mode to handle an interrupt or exceptiongenerated in virtual-8086 mode.有关离开 virtual-8086 模式以处理 virtual-8086 模式中生成的中断或异常的信息，请参见第 21.3 节 “Virtual-8086 模式下的中断和异常处理”。21.2.7 Sensitive InstructionsWhen an IA-32 processor is running in virtual-8086 mode, the CLI, STI, PUSHF,POPF, INT n, and IRET instructions are sensitive to IOPL. The IN, INS, OUT, andOUTS instructions, which are sensitive to IOPL in protected mode, are notsensitive in virtual-8086 mode. 当 IA-32 处理器运行在virtual-8086 模式下时，CLI、STI、PUSHF、POPF、INT n 和 IRET 指令对 IOPL 敏感。 IN、INS、OUT 和 OUTS 指令在保护模式下对 IOPL 敏感，但在virtual-8086 模式下不敏感。The CPL is always 3 while running in virtual-8086 mode; if the IOPL is lessthan 3, an attempt to use the IOPL-sensitive instructions listed abovetriggers a general-protection exception (#GP). These instructions are sensitiveto IOPL to give the virtual-8086 monitor a chance to emulate the facilitiesthey affect. 在virtual-8086 模式下运行时，CPL 始终为 3； 如果 IOPL 小于 3，则尝试使用上面列出的 IOPL 敏感指令会触发一般保护异常 (#GP)。 这些指令对 IOPL 敏感，使virtual-8086 监视器有机会模拟它们影响的facilities。21.2.8 Virtual-8086 Mode I/ONULL21.3 INTERRUPT AND EXCEPTION HANDLING IN VIRTUAL-8086 MODENULL21.4 PROTECTED-MODE VIRTUAL INTERRUPTSNULL" }, { "title": "async pf -- GUP change", "url": "/posts/async-pf-gup-change/", "categories": "kvm, async_pf", "tags": "para_virt", "date": "2024-04-16 15:00:00 +0800", "snippet": "参考代码 该部分代码, mail list中和commit中内容不同, 而且mail list中也没有提到为什么当时没有全部合入, 我们先以mail list 为准: [KVM: Add host swap event notifications for PV guest][v7] 后续, 我们在另一篇文章中详细介绍: [link][2] 遗留问题 get_user_pag...", "content": "参考代码 该部分代码, mail list中和commit中内容不同, 而且mail list中也没有提到为什么当时没有全部合入, 我们先以mail list 为准: [KVM: Add host swap event notifications for PV guest][v7] 后续, 我们在另一篇文章中详细介绍: [link][2] 遗留问题 get_user_pages_noio 该部分代码来自于: [Add get_user_pages() variant that fails if major fault is required.][3]社区并没有合入该patch该patch 引入了 get_user_page()的noio 版的变体, 他只在不需要 major fault的情况下才会成功的 get page reference 以上来自该patch的commit message This patch add get_user_pages() variant that only succeeds if gettinga reference to a page doesn't require major fault. 具体改动是: 增加了新的flow flag和fault flag flow/fault flag细节 flow flag: 该flag会作为gup_flags入参传入__get_user_pages(), 可以作为一些约束e.g., FOLL_WRITE 表明要要get的pages必须是可写入的 会对比vma-&gt;vm_flags 是否是可写的. 另外, 会在 follow page期间, 影响handle_mm_fault的fault flag e.g., FOLL_WRITE-&gt;FAULT_FLAG_WRITE 新增的flag为: +#define FOLL_MINOR\t0x20\t/* do only minor page faults */ 表明要在 follow page期间, 只允许处理 minor page fault.(不能处理swapin) fault flag: 用于handle_mm_fault()入参flags, 表明本次fault的类型.该参数可以用于一些优化和约束. e.g. 如果检测到没有FAULT_FLAG_WRITE, 说明是read access, 而又是第一次建立映射, 那么可以建立和zero page的映射 do_anonymous_page(){ ... if (!(flags &amp; FAULT_FLAG_WRITE)) { entry = pte_mkspecial(pfn_pte(my_zero_pfn(address), vma-&gt;vm_page_prot)); page_table = pte_offset_map_lock(mm, pmd, address, &amp;ptl); if (!pte_none(*page_table)) goto unlock; goto setpte; } ...} 新增的flag为: +#define FAULT_FLAG_MINOR\t0x08\t/* Do only minor fault */ 是由 FOLL_MINOR转化而来, 和其作用一样. vm fault reason: 其实, 还有一些flag, 是用于表示handle_mm_fault()的原因, 例如这里我们遇到的:VM_FAULT_MAJOR, 实际上是表明, 该函数返回失败是由于该fault是 major fault. 我们下面会结合代码改动详细看下, 这些flag的应用 关于处理这些\"flags\"具体代码流程改动: \"flags\"具体代码改动 __get_user_pages-&gt;handle_mm_fault @@ -1441,10 +1441,13 @@ int __get_user_pages(struct task_struct *tsk, struct mm_struct *mm, \t\t\tcond_resched(); \t\t\twhile (!(page = follow_page(vma, start, foll_flags))) { \t\t\t\tint ret;+\t\t\t\tunsigned int fault_fl =+\t\t\t\t\t((foll_flags &amp; FOLL_WRITE) ?+\t\t\t\t\tFAULT_FLAG_WRITE : 0) |+\t\t\t\t\t((foll_flags &amp; FOLL_MINOR) ?+\t\t\t\t\tFAULT_FLAG_MINOR : 0); -\t\t\t\tret = handle_mm_fault(mm, vma, start,-\t\t\t\t\t(foll_flags &amp; FOLL_WRITE) ?-\t\t\t\t\tFAULT_FLAG_WRITE : 0);+\t\t\t\tret = handle_mm_fault(mm, vma, start, fault_fl); 可以看到, 这里会将 FOLL_WRITE-&gt;FAULT_FLAG_WRITE, FOLL_MINOR-&gt;FAULT_FLAG_MINOR do_swap_page @@ -2648,6 +2670,9 @@ static int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma, \tdelayacct_set_flag(DELAYACCT_PF_SWAPIN); \tpage = lookup_swap_cache(entry); \tif (!page) {+\t\tif (flags &amp; FAULT_FLAG_MINOR)+\t\t\treturn VM_FAULT_MAJOR | VM_FAULT_ERROR;+ \t\tgrab_swap_token(mm); /* Contend for token _before_ read-in */ \t\tpage = swapin_readahead(entry, \t\t\t\t\tGFP_HIGHUSER_MOVABLE, vma, address); 如果没有在swap cache中找到, 说明该page 被swap出去, 并且被free了,需要swapin, 这时, 如果有FAULT_FLAG_MINOR,表明只允许处理 minor fault, 而swapin, 属于 major fault, 不允许处理, 需要返回错误, 同时把错误原因:VM_FAULT_MAJOR也返回 filemap_fault 和swapcache 相对应的还有pagecache diff --git a/mm/filemap.c b/mm/filemap.cindex 3d4df44..ef28b6d 100644--- a/mm/filemap.c+++ b/mm/filemap.c@@ -1548,6 +1548,9 @@ int filemap_fault(struct vm_area_struct *vma, struct vm_fault *vmf) \t\t\tgoto no_cached_page; \t\t} \t} else {+\t\tif (vmf-&gt;flags &amp; FAULT_FLAG_MINOR)+\t\t\treturn VM_FAULT_MAJOR | VM_FAULT_ERROR; 也是同样的处理逻辑 handle_mm_fault() --return-&gt;__get_user_pages() --handle retval @@ -1452,6 +1455,8 @@ int __get_user_pages(struct task_struct *tsk, struct mm_struct *mm, \t\t\t\t\tif (ret &amp; \t\t\t\t\t (VM_FAULT_HWPOISON|VM_FAULT_SIGBUS)) \t\t\t\t\t\treturn i ? i : -EFAULT;+\t\t\t\t\telse if (ret &amp; VM_FAULT_MAJOR)+\t\t\t\t\t\treturn i ? i : -EFAULT; \t\t\t\t\tBUG(); 如果是 FAULT_MAJOR并且没有get 到 page, 直接返回错误 新增get_user_pages_noio接口 +int get_user_pages_noio(struct task_struct *tsk, struct mm_struct *mm,+\t\tunsigned long start, int nr_pages, int write, int force,+\t\tstruct page **pages, struct vm_area_struct **vmas)+{+\tint flags = FOLL_TOUCH | FOLL_MINOR;++\tif (pages)+\t\tflags |= FOLL_GET;+\tif (write)+\t\tflags |= FOLL_WRITE;+\tif (force)+\t\tflags |= FOLL_FORCE;++\treturn __get_user_pages(tsk, mm, start, nr_pages, flags, pages, vmas);+}+EXPORT_SYMBOL(get_user_pages_noio);+ 不多解释, 在该接口中将FOLL_MINOR置位. NOTE 所以该部分patch的主要作用就是增加了get_user_pages_noio(), 使其, 只处理minor fault(假如只是alloc page, 那属于minor fault), 但是不能处理major fault.(例如swapin, pagecachein) 目前个人理解是这样, 之后还需要看下page fault的相关细节 我们接下来看下, async pf 框架是如何利用上面GUP noio接口的usage of GUP noio in async pf我们先看下, 触发 async pf 的入口, 我们上面介绍到, 在 EPT violation hook 中会去start 该work, 过程如下:tdp_page_fault@@ -2609,7 +2655,11 @@ static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, mmu_seq = vcpu-&gt;kvm-&gt;mmu_notifier_seq; smp_rmb(); //==(1)==- pfn = gfn_to_pfn(vcpu-&gt;kvm, gfn);+ //==(2)==+ if (try_async_pf(vcpu, gfn, gpa, &amp;pfn))+ return 0;++ /* mmio */ if (is_error_pfn(pfn)) 将现有的gfn_to_pfn(), 替换为try_async_pf(), 之前的gfn2pfn接口, 是必须async, 也就是上面提到的使用GUP时, 可以处理 MAJOR FAULT. 而现在替换为了 try_async_pf(), 打算尝试执行 async pf(也可能不需要, 例如遇到了 MINOR FAULT. 我们接下来会详细看下该接口 tdp_page_fault()中会执行try_async_pf()该函数返回值为true, 表示已经做了async pf,所以现在还不能去 map GPA-&gt;HPA. 需要该接口直接返回. 对于HALT的处理方式, 则是让vcpublock. 我们下面会看到.old version of gfn_to_pfn在看try_async_pf之前, 我们先看下合入patch之前的 gfn_to_pfn接口.gfn_to_pfn { __gfn_to_pfn(atomic=false) { gfn_to_hva { gfn_to_hva_many gfn_to_hva_memslot } //gfn_to_hva ----- 上面 gfn_to_hva 下面 hva_to_pfn ----- hva_to_pfn(atomic=false) { if (atomic) __get_user_pages_fast() else //走这个路径 get_user_pages_fast() } //hva_to_pfn } //__gfn_to_pfn} //gfn_to_pfn关于__get_user_pages_fast和get_user_pages_fast的不同, 主要是: __get_user_pages_fast()是atomic版本(IRQ-safe), 主要是因为get_user_pages_fast需要走slow path, 这个时候需要开中断, 而__get_user_pages_fast则不需要, 所以其过程是关中断的, 也可以在关中断的情况下执行 由于上面提到的原因, get_user_pages_fast 并不保存中断状态, 所以该函数必须在开中断的情况下执行 两个接口前的代码注释, 以及大致流程 get_user_pages_fast /** * get_user_pages_fast() - pin user pages in memory * @start: starting user address * @nr_pages: number of pages from start to pin * @write: whether pages will be written to * @pages: array that receives pointers to the pages pinned. * Should be at least nr_pages long. * * Attempt to pin user pages in memory without taking mm-&gt;mmap_sem. * If not successful, it will fall back to taking the lock and * calling get_user_pages(). * * &gt; 在不拿mm-&gt;mmap_sem 锁的情况下, 尝试将user page pin 到memory中. * &gt; 如果没有成功, 它将fall back 来拿锁, 并且调用get_user_pages() * * Returns number of pages pinned. This may be fewer than the number * requested. If nr_pages is 0 or negative, returns 0. If no pages * were pinned, returns -errno. * * &gt; 返回 page 被 pinned数量. 他可能比所需的数量要少. 如果nr_pages是0, * 或者是负数, 返回0. 如果没有page被pinned, 返回 -errno */get_user_pages_fast { local_irq_disable fast_path { //仅去看有多少page present } local_irq_enable get_user_page} __get_user_pages_fast /* * Like get_user_pages_fast() except its IRQ-safe in that it won't fall * back to the regular GUP. * * 除了他的 IRQ-safe(因为他不会fall bak to regular GUP), 其他的和 * get_user_pages_fast()一样 */__get_user_pages_fast { local_irq_save() fast_path local_irq_restore()} 这里, 我们不再过多展开GUP的代码, 总之, 早期的__get_user_pages_fast不会fall back到 regular GPU(slow pathget_user_pages)我们再来看下其改动,gfn_to_pfn-&gt;get_user_pages新增gfn_to_pfn_async(), 替代现有流程中的gfn_to_pfn(), 该接口新增了async:bool*参数, 该参数是一个iparam &amp;&amp; oparam iparam: 表示只走get_user_page fast path也就是__get_user_pages_fast oparam: 表示是否需要做 async pf具体改动如下+pfn_t gfn_to_pfn_async(struct kvm *kvm, gfn_t gfn, bool *async)+{+ return __gfn_to_pfn(kvm, gfn, false, async);+}+EXPORT_SYMBOL_GPL(gfn_to_pfn_async);+ pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn) {- return __gfn_to_pfn(kvm, gfn, false); //可以走slow path+ return __gfn_to_pfn(kvm, gfn, false, NULL); } EXPORT_SYMBOL_GPL(gfn_to_pfn);/* * !!MY NOTE!! * __gfn_to_pfn { * ... * //先初始化为false * if (async) * *async = false; * ... * return hva_to_pfn(kvm, addr, atomic, async); * } */我们再来看下hva_to_pfn改动:+static pfn_t hva_to_pfn(struct kvm *kvm, unsigned long addr, bool atomic,+ bool *async) { struct page *page[1];- int npages;+ int npages = 0; pfn_t pfn;- if (atomic)+ /* we can do it either atomically or asynchronously, not both */+ BUG_ON(atomic &amp;&amp; async); //==(1)==+ if (atomic || async) npages = __get_user_pages_fast(addr, 1, 1, page);- else {+ //==(2)==+ if (unlikely(npages != 1) &amp;&amp; !atomic) { might_sleep();- npages = get_user_pages_fast(addr, 1, 1, page);+ + if (async) {+ down_read(&amp;current-&gt;mm-&gt;mmap_sem);+ npages = get_user_pages_noio(current, current-&gt;mm,+ \t\t\t addr, 1, 1, 0, page, NULL);+ up_read(&amp;current-&gt;mm-&gt;mmap_sem);+ } else+ npages = get_user_pages_fast(addr, 1, 1, page); } if (unlikely(npages != 1)) { struct vm_area_struct *vma; if (atomic) goto return_fault_page; down_read(&amp;current-&gt;mm-&gt;mmap_sem); if (is_hwpoison_address(addr)) { up_read(&amp;current-&gt;mm-&gt;mmap_sem); get_page(hwpoison_page); return page_to_pfn(hwpoison_page); } vma = find_vma(current-&gt;mm, addr); if (vma == NULL || addr &lt; vma-&gt;vm_start || !(vma-&gt;vm_flags &amp; VM_PFNMAP)) { //==(3)==+ if (async &amp;&amp; !(vma-&gt;vm_flags &amp; VM_PFNMAP) &amp;&amp;+ (vma-&gt;vm_flags &amp; VM_WRITE))+ *async = true; up_read(&amp;current-&gt;mm-&gt;mmap_sem);return_fault_page: get_page(fault_page); return page_to_pfn(fault_page); } pfn = ((addr - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT) + vma-&gt;vm_pgoff; up_read(&amp;current-&gt;mm-&gt;mmap_sem); BUG_ON(!kvm_is_mmio_pfn(pfn)); } else pfn = page_to_pfn(page[0]); return pfn;} 如果是async, 会先尝试走一次fast path, 如果成功了, 则 npages = 1 如果上面fast path 失败了, 并且还是async, 则会执行get_user_pages_noio()该函数上面也提到过, 该过程不处理 MAJOR fault. 这里说明失败了, 也就是因为遇到了MAJOR fault, 所以该fault 并没有handle,需要异步处理, 那么就将 oparam async 置为true. 这里我们先不关心这里的几个判断条件, 之后放到GUP/内存管理的章节中介绍 遗留问题 " }, { "title": "async pf -- gfn2hva cache", "url": "/posts/async-pf-gfn2hva-cache/", "categories": "kvm, async_pf", "tags": "para_virt", "date": "2024-04-16 15:00:00 +0800", "snippet": "introduce该功能仅通过name就可以得知, 是为了缓存gfn(gpa)到hva的映射. 但是这个映射关系不是一直存在么, 为什么设计看似比较复杂的机制, 我们一步步来看user memory region support我们知道,在比较早期的版本, kvm 创建memslot APIkvm_vm_ioctl KVM_SET_USER_MEMORY_REGION就已经支持了对 use...", "content": "introduce该功能仅通过name就可以得知, 是为了缓存gfn(gpa)到hva的映射. 但是这个映射关系不是一直存在么, 为什么设计看似比较复杂的机制, 我们一步步来看user memory region support我们知道,在比较早期的版本, kvm 创建memslot APIkvm_vm_ioctl KVM_SET_USER_MEMORY_REGION就已经支持了对 user memory region申请的支持. 涉及patch Patch: KVM: Support assigning userspace memory to the guest mail list: mail 这里只展示下, 用户态入参的数据结构: +/* for KVM_SET_USER_MEMORY_REGION */+struct kvm_userspace_memory_region {+ __u32 slot;+ __u32 flags;+ __u64 guest_phys_addr;+ __u64 memory_size; /* bytes */+ __u64 userspace_addr; /* start of the userspace allocated memory */+}; 可以看到最后一个参数为, userspace_addr在该接口的支持下, qemu为guest申请memory region同时, 该内存也作为qemu进程的 anon memoryspace 存在, qemu 可以通过管理匿名页的方式, 对该地址空间进行管理, kernel其他组建, 也可以通过操作这部分匿名页来操作guest memory, 例如: memory reclaim, memory migrate…所以基本流程是: guest先通过mmap申请匿名内存空间, 调用完成后, kernel已经申请好了这段内存空间的virtual base address(userspace_addr) 调用kvm_vm_ioctl -- KVM_SET_USER_MEMORY_REGION 执行完成时, kvm 已经建立起了 hva-&gt;gpa映射关系 guest访问gpa, 触发EPT violation trap kvm, kvm 调用 get_user_page() 申请page, 并建立hva-&gt;hpa 同时, 创建gpa-&gt;hpa的mmu pgtables(if guest enable EPT feature, is ept pgtable)re-set memory region所以, 既然两者在set memory region 接口中就已经确立了映射关系, 那是不是只是保存下[hva, gpa]就相当于cache了.大部分情况下是这样, 但是在下面情况下, hva-&gt;gpa的映射关系会改变 guest call kvm_vm_ioctl -- KVM_SET_USER_MEMORY_REGION, map [hva-&gt;gpa]-&gt;[hpa_1, gpa] guest call kvm_vm_ioctl -- KVM_SET_USER_MEMORY_REGION again, remap [hva-&gt;gpa]-&gt;[hpa_2, gpa]在这种情况下, 映射关系就改变了.所以, 我们需要一个机制, 在re-set memory region 发生之后, 我们再次 “access this cache”时, 需要“invalidate this cache”, 这就是该patch要做的事情.patch 细节change of struct struct kvm_memslots { \tint nmemslots;+\tu32 generation; \tstruct kvm_memory_slot memslots[KVM_MEMORY_SLOTS + \t\t\t\t\tKVM_PRIVATE_MEM_SLOTS]; }; generation: 表示当前memslots 的generation, 也就是latest.+struct gfn_to_hva_cache {+\tu32 generation;+\tgpa_t gpa;+\tunsigned long hva;+\tstruct kvm_memory_slot *memslot;+}; generation: 获取cache时, memslots的generation, 可能是old的. memslot: 当前gpa所属的memslot, 主要用于 mark_page_dirty 作者在这里有个小心思, 因为这个地方没有该成员也没有关系, 也可以执行, mark_page_dirty, 但是在执行时, 需要每次获取memslot, 所以作者想了,既然缓存, 那为什么不缓存多一些, 将memslot也缓存 interfacecache init+int kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,+\t\t\t gpa_t gpa)+{+\tstruct kvm_memslots *slots = kvm_memslots(kvm);+\tint offset = offset_in_page(gpa);+\tgfn_t gfn = gpa &gt;&gt; PAGE_SHIFT;++\tghc-&gt;gpa = gpa;+\t//==(1)==+\tghc-&gt;generation = slots-&gt;generation;+\tghc-&gt;memslot = __gfn_to_memslot(kvm, gfn);+\tghc-&gt;hva = gfn_to_hva_many(ghc-&gt;memslot, gfn, NULL);+\t//==(2)==+\tif (!kvm_is_error_hva(ghc-&gt;hva))+\t\tghc-&gt;hva += offset;+\telse+\t\treturn -EFAULT;++\treturn 0;+} 将此时slots-&gt;generation赋值给ghc-&gt;generation 错误情况暂时不看.write cache+int kvm_write_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,+\t\t\t void *data, unsigned long len)+{+\tstruct kvm_memslots *slots = kvm_memslots(kvm);+\tint r;++\t//==(1)==+\tif (slots-&gt;generation != ghc-&gt;generation)+\t\tkvm_gfn_to_hva_cache_init(kvm, ghc, ghc-&gt;gpa);+\t+\tif (kvm_is_error_hva(ghc-&gt;hva))+\t\treturn -EFAULT;++\t//==(2)==+\tr = copy_to_user((void __user *)ghc-&gt;hva, data, len);+\tif (r)+\t\treturn -EFAULT;+\t//==(3)==+\tmark_page_dirty_in_slot(kvm, ghc-&gt;memslot, ghc-&gt;gpa &gt;&gt; PAGE_SHIFT);++\treturn 0;+} 如果slots-&gt;generation 和当前cache generation(ghc-&gt;generation)不一致, 说明 该cache已经是stale的了, 需要update, 那就直接重新init cache(调用 kvm_gfn_to_hva_cache_init()) 将数据写入hva 该接口是新增的, 为mark_page_dirty()的变体. -void mark_page_dirty(struct kvm *kvm, gfn_t gfn)+void mark_page_dirty_in_slot(struct kvm *kvm, struct kvm_memory_slot *memslot,+\t\t\t gfn_t gfn) {-\tstruct kvm_memory_slot *memslot;--\tmemslot = gfn_to_memslot(kvm, gfn); \tif (memslot &amp;&amp; memslot-&gt;dirty_bitmap) { \t\tunsigned long rel_gfn = gfn - memslot-&gt;base_gfn; @@ -1284,6 +1325,14 @@ void mark_page_dirty(struct kvm *kvm, gfn_t gfn) \t} } +void mark_page_dirty(struct kvm *kvm, gfn_t gfn)+{+\tstruct kvm_memory_slot *memslot;++\tmemslot = gfn_to_memslot(kvm, gfn);+\tmark_page_dirty_in_slot(kvm, memslot, gfn);+} 该变体较mark_page_dirty()来说, 主要是增加memslot参数. 原因在介绍数据结构的时候已经说明 该功能和dirty log功能相关, guest可以通过bitmap知道那些page是dirty的,在热迁移的时候会用到, 这里不过多介绍 那slots-&gt;generation 什么时候改变的呢bump slots-&gt;generationdiff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.cindex db58a1b..45ef50c 100644--- a/virt/kvm/kvm_main.c+++ b/virt/kvm/kvm_main.cint __kvm_set_memory_region(struct kvm *kvm, struct kvm_userspace_memory_region *mem, int user_alloc){skip_lpage: //==(1)== if (!npages) { r = -ENOMEM; slots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL); if (!slots) goto out_free; memcpy(slots, kvm-&gt;memslots, sizeof(struct kvm_memslots)); if (mem-&gt;slot &gt;= slots-&gt;nmemslots) slots-&gt;nmemslots = mem-&gt;slot + 1;+\t\tslots-&gt;generation++; slots-&gt;memslots[mem-&gt;slot].flags |= KVM_MEMSLOT_INVALID; old_memslots = kvm-&gt;memslots; rcu_assign_pointer(kvm-&gt;memslots, slots); synchronize_srcu_expedited(&amp;kvm-&gt;srcu); /* From this point no new shadow pages pointing to a deleted * memslot will be created. * * validation of sp-&gt;gfn happens in: * - gfn_to_hva (kvm_read_guest, gfn_to_pfn) * - kvm_is_visible_gfn (mmu_check_roots) */ kvm_arch_flush_shadow(kvm); kfree(old_memslots); } //==(2)== r = kvm_arch_prepare_memory_region(kvm, &amp;new, old, mem, user_alloc); if (r) goto out_free; /* map the pages in iommu page table */ if (npages) { r = kvm_iommu_map_pages(kvm, &amp;new); if (r) goto out_free; } r = -ENOMEM; slots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL); if (!slots) goto out_free; memcpy(slots, kvm-&gt;memslots, sizeof(struct kvm_memslots)); if (mem-&gt;slot &gt;= slots-&gt;nmemslots) slots-&gt;nmemslots = mem-&gt;slot + 1;+ slots-&gt;generation++; 说明不是内存, 有可能是mmio, 这里变动memslots, 需要使用rcu机制,这样可以保证在无锁的情况下把这个动作完成 normal 内存因为更新到了memslots, 说明hva-&gt;gpa的关系有改变, 所以需要更新generationhistory of change avi在Re: [PATCH v2 02/12] Add PV MSR to enable asynchronous page faults delivery. 中提到, 目前这一版本patch可能在遇到 memslots 情况下, 会有问题 原文 &gt; +static int kvm_pv_enable_async_pf(struct kvm_vcpu *vcpu, u64 data)&gt; +{&gt; +\tu64 gpa = data&amp; ~0x3f;&gt; +\tint offset = offset_in_page(gpa);&gt; +\tunsigned long addr;&gt; +&gt; +\taddr = gfn_to_hva(vcpu-&gt;kvm, gpa&gt;&gt; PAGE_SHIFT);&gt; +\tif (kvm_is_error_hva(addr))&gt; +\t\treturn 1;&gt; + //只初始化一次&gt; +\tvcpu-&gt;arch.apf_data = (u32 __user*)(addr + offset);&gt; +&gt; +\t/* check if address is mapped */&gt; +\tif (get_user(offset, vcpu-&gt;arch.apf_data)) {&gt; +\t\tvcpu-&gt;arch.apf_data = NULL;&gt; +\t\treturn 1;&gt; +\t}&gt; What if the memory slot arrangement changes? This needs to be revalidated (and gfn_to_hva() called again).&gt; validate &lt;==&gt; invalidate&gt; revalidate : 重新生效, 重新验证 在[PATCH v3 07/12] Maintain memslot version number和[PATCH v3 08/12] Inject asynchronous page fault into a guest if page is swapped out.中, 作者引入了该功能, 不过该功能是嵌入到async pf 功能中, 并非独立接口 代码 diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.hindex 600baf0..3f5ebc2 100644--- a/include/linux/kvm_host.h+++ b/include/linux/kvm_host.h@@ -163,6 +163,7 @@ struct kvm { \tspinlock_t requests_lock; \tstruct mutex slots_lock; \tstruct mm_struct *mm; /* userspace tied to this vm */+\tu32 memslot_version; \tstruct kvm_memslots *memslots; \tstruct srcu_struct srcu;@@ -364,7 +364,9 @@ struct kvm_vcpu_arch { \tunsigned long singlestep_rip; \tu32 __user *apf_data;+\tu32 apf_memslot_ver; \tu64 apf_msr_val;+\tu32 async_pf_id; };+static int apf_put_user(struct kvm_vcpu *vcpu, u32 val)+{+\tif (unlikely(vcpu-&gt;arch.apf_memslot_ver !=+\t\t vcpu-&gt;kvm-&gt;memslot_version)) {+\t\tu64 gpa = vcpu-&gt;arch.apf_msr_val &amp; ~0x3f;+\t\tunsigned long addr;+\t\tint offset = offset_in_page(gpa);++\t\taddr = gfn_to_hva(vcpu-&gt;kvm, gpa &gt;&gt; PAGE_SHIFT);+\t\tvcpu-&gt;arch.apf_data = (u32 __user*)(addr + offset);+\t\tif (kvm_is_error_hva(addr)) {+\t\t\tvcpu-&gt;arch.apf_data = NULL;+\t\t\treturn -EFAULT;+\t\t}+\t}++\treturn put_user(val, vcpu-&gt;arch.apf_data);+} 可以看到, 相当于引入了两个version, 并在apf_put_user()时,比对两个version. 作者在Re: [PATCH v4 08/12] Inject asynchronous page fault into a guest if page is swapped out.回答了为什么不使用kvm_write_guest Q: why A: want to cache gfn_to_hva_translation avi 在Re: [PATCH v5 08/12] Inject asynchronous page fault into a guest if page is swapped out.建议将该功能剥离, 因为这个功能很好,其他代码也可以用. 原文 This nice cache needs to be outside apf to reduce complexity for reviewers and since it is useful for others. Would be good to have memslot-cached kvm_put_guest() and kvm_get_guest(). 作者在Re: [PATCH v5 08/12] Inject asynchronous page fault into a guest if page is swapped out.首次提供该接口. Marcelo Tosatti 在Re: [PATCH v6 04/12] Add memory slot versioning and use it to provide fast guest write interface提到两个问题: 在kvm_gfn_to_hva_cache_init中使用gfn_to_memslot 获取memslot, 可能会造成如下问题 自己的理解 thread1 thread2 guestkvm_write_guest_cached kvm_gfn_to_hva_cache_init { __kvm_set_memory_region slots = kvm_memslots(kvm) { rcu_dereference_check } ghc-&gt;generation = slots-&gt;generation; slots-&gt;generation++; ghc-&gt;memslot = gfn_to_memslot( slots, gfn) { rcu_dereference_check { //may have a gp rcu_assign_pointer( kvm-&gt;memslots, slots); } ghc-&gt;hva = gfn_to_hva_many( ghc-&gt;memslot, gfn, NULL); }copy_to_user(); kvm_arch_commit_memory_region do_munmap access apf reason, LOSS 这样可能会导致在这个函数中, 前面和后面获取的信息来自于不同的memslots, 个人认为, 不仅仅是这样, 还可能导致, thread2 因为中间释放了rcu, 导致其流程和thread1有race, 最终导致 本次copy_to_user()数据丢失 作者在下一版patch中将gfn_to_memslot修改为了__gfn_to_memslot, 该接口不会在使用rcu_dereference_check " }, { "title": "embedding markdown in HTML tag", "url": "/posts/embedding_markdown_in_html_tag/", "categories": "markdown", "tags": "markdown, html-details", "date": "2024-04-12 10:53:00 +0800", "snippet": "ISSUEWhen I try to use the markdown syntax in &lt;details&gt; HTML tags, for example,code blocks, encounter the problem of code blocks that cannot be rendered.The source code is as follows:&lt;deta...", "content": "ISSUEWhen I try to use the markdown syntax in &lt;details&gt; HTML tags, for example,code blocks, encounter the problem of code blocks that cannot be rendered.The source code is as follows:&lt;details&gt;&lt;summary&gt; aaa &lt;/summary&gt;` ` `cppint a = 1;` ` `&lt;/details&gt; ` char seems unable to be translated in code block, so I added spacecharacters between themIt will display in browser as follows:aaa```cppint a = 1;```SOLUTIONThis issue seems to occur in the kramdowm markup process, rather than in GFM: GFM allows embedding HTML inside Markdown Embedding Markdown in Jekyll HTMLAnd in the link Embedding Markdown in Jekyll HTML, a solution is provided:Use &lt;details markdown=\"1\"&gt; instead &lt;details&gt;.It will run as expected aaa int a = 1; " }, { "title": "async pf", "url": "/posts/async-pf/", "categories": "kvm, async_pf", "tags": "para_virt", "date": "2024-04-10 12:20:00 +0800", "snippet": "introduce在支持EPT的架构中, 对于GVA-&gt;HPA一般有两段映射: GVA-&gt;GPA GPA-&gt;HPA而host kernel (kvm) 需要关心的是 GPA-&gt;HPA的映射, 需要host做的事情主要有以下几个: 捕捉相关 VM-exit event (EPT violation), 得到 GPA 分配page 建立映射关系(当然这个映射关系...", "content": "introduce在支持EPT的架构中, 对于GVA-&gt;HPA一般有两段映射: GVA-&gt;GPA GPA-&gt;HPA而host kernel (kvm) 需要关心的是 GPA-&gt;HPA的映射, 需要host做的事情主要有以下几个: 捕捉相关 VM-exit event (EPT violation), 得到 GPA 分配page 建立映射关系(当然这个映射关系, 不止是GPA-&gt;HPA的mmu pgtable, 还有 HVA – GPA,在这里不展开, 总之分配好具体的page(分配HPA), 以及为其建立好 mmu pgtable, 就可以完成该事件的处理)如下图:图示graphviz-ae4ca25f7bf30b9e61f0f3b83bc12338digraph G { subgraph cluster_guest { EPT_violation [ label=&quot;EPT mapping(GPA-&gt;HPA) \\nloss, trigger EPT violation&quot; ] &quot;access a VA&quot;-&gt; &quot;trigger PF in VMX\\n non-root operation&quot;-&gt; &quot;mapping GVA-&gt;GPA\\n in GUEST #PF hook&quot;-&gt; &quot;fixup #PF, continue \\naccess this VA&quot;-&gt; EPT_violation label=&quot;guest&quot; } subgraph cluster_host { &quot;find HVA though GPA&quot;-&gt; &quot;GUP(HVA)&quot;-&gt; &quot;mapping GPA-&gt;HPA&quot; label=&quot;host&quot; } &quot;mapping GPA-&gt;HPA&quot;-&gt;&quot;access a VA&quot; [ label=&quot;fixup EPT violation,\\n VM entry&quot; ] EPT_violation-&gt;&quot;find HVA though GPA&quot; [ label=&quot;VM exit&quot; ]}Gcluster_guestguestcluster_hosthostEPT_violationEPT mapping(GPA&#45;&gt;HPA) loss, trigger EPT violationfind HVA though GPAfind HVA though GPAEPT_violation&#45;&gt;find HVA though GPAVM exitaccess a VAaccess a VAtrigger PF in VMX\\n non&#45;root operationtrigger PF in VMX non&#45;root operationaccess a VA&#45;&gt;trigger PF in VMX\\n non&#45;root operationmapping GVA&#45;&gt;GPA\\n in GUEST #PF hookmapping GVA&#45;&gt;GPA in GUEST #PF hooktrigger PF in VMX\\n non&#45;root operation&#45;&gt;mapping GVA&#45;&gt;GPA\\n in GUEST #PF hookfixup #PF, continue \\naccess this VAfixup #PF, continue access this VAmapping GVA&#45;&gt;GPA\\n in GUEST #PF hook&#45;&gt;fixup #PF, continue \\naccess this VAfixup #PF, continue \\naccess this VA&#45;&gt;EPT_violationGUP(HVA)GUP(HVA)find HVA though GPA&#45;&gt;GUP(HVA)mapping GPA&#45;&gt;HPAmapping GPA&#45;&gt;HPAGUP(HVA)&#45;&gt;mapping GPA&#45;&gt;HPAmapping GPA&#45;&gt;HPA&#45;&gt;access a VAfixup EPT violation, VM entry但是, 已经建立好映射的页面, 也是qemu进程的虚拟地址空间(匿名页), 是可以被swap out,当被swap out后, GUEST 访问该HPA对应的 GVA/GPA时, 仍然会触发 EPT violation. 这时还会再走一次 VM-exit, 而且也需要完成上面所述的三件事, 其中第二件:分配page, 需要swap in之前被swap out的page, 路径比较长, 如下:VM-exit handle_ept_violation kvm_mmu_page_fault tdp_page_fault gfn_to_pfn hva_to_pfn get_user_pages --slow pathget_user_pages会走到slow path, 由于会走swap in流程, 所以该过程执行较慢. 所以大佬们就想着能不能让其异步执行, 然后让vcpu先不complete 造成 EPT violation 的 instruction, 去干别的事情, 等page present后, 再去执行该指令. 另外将 get_user_pages 让一个 dedicated thread 去完成,这样, 对于虚拟机来说, 就相当于搞了一个额外的 硬件, 专门去处理 swap in, 解放了vcpu的算力. NOTE 大家思考下, 如果要达到该目的, 一定是让GUEST有意无意的 sche out 造成 EPT violation的进程,该上面流程总结如下:流程图graphviz-f8c81926c9687c237f8513f3a6fb3624digraph G { subgraph cluster_host { style=&quot;filled&quot; color=&quot;#693886699&quot; subgraph cluster_host_dedicated_thread { do_slow_path [ shape=&quot;note&quot; label=&quot;I&#39;m a delicated \\nthread, Like a \\nspecial hardware, \\nsharing the \\npressure of VCPU&quot; ] label=&quot;dedicated thread&quot; have_got_page_success [ label=&quot;work in done!\\n tell the guest&quot; ] do_slow_path-&gt;have_got_page_success [ label=&quot;a. get page, swap in...&quot; fontcolor=&quot;blue&quot; color=&quot;blue&quot; ] } subgraph cluster_host_kvm_vcpu_thread { ept_violation_handler [ label=&quot;ept violation handler&quot; ] dont_do_slow_path [ shape=&quot;note&quot; label=&quot;I don&#39;t want \\nhandle slow path, \\nit will speed\\nto much time&quot; ] tell_guest_sched_out [ shape=&quot;note&quot; label=&quot;work is doing,\\nneed wait\\n a a bit time,\\n let guest do\\n other things&quot; ] dont_do_slow_path -&gt;tell_guest_sched_out [ label=&quot;4.let guest \\ndo other thing&quot; fontcolor=&quot;green&quot; color=&quot;green&quot; ] ept_violation_handler-&gt; dont_do_slow_path [ label=&quot;2.find page swap out&quot; fontcolor=&quot;green&quot; color=&quot;green&quot; ] label=&quot;host kvm vcpu thread&quot; } label = &quot;host&quot; } subgraph cluster_guest { style=&quot;filled&quot; color=&quot;#77323456&quot; subgraph cluster_trigger_ept_violation_task { task1_access_a_memory [ label=&quot;acesss a memory\\n address [BEG]&quot; color=&quot;white&quot; style=&quot;filled&quot; ] label=&quot;TASK1 trigger ept vioaltion&quot; } subgraph cluster_sched_in_task2 { task2_run_a_time [ label=&quot;task2_run_a_time&quot; ] label=&quot;task2&quot; } label=&quot;guest&quot; } dont_do_slow_path-&gt;do_slow_path [ label=&quot;3. start a work \\nto do it&quot; fontcolor=&quot;green&quot; color=&quot;green&quot; ] task1_access_a_memory -&gt; ept_violation_handler [ label=&quot;1.page NOT present,\\ntrigger EPT violation&quot; fontcolor=&quot;green&quot; color=&quot;green&quot; ] have_got_page_success -&gt; task2_run_a_time [ label=&quot;b. page NOT present\\n SCHED IN&quot; fontcolor=&quot;blue&quot; color=&quot;blue&quot; ] tell_guest_sched_out -&gt; task1_access_a_memory [ label=&quot;5. page NOT present\\n SCHED OUT&quot; fontcolor=&quot;green&quot; color=&quot;green&quot; ] task2_run_a_time-&gt;task1_access_a_memory [ label=&quot;c. sched in\\n task1&quot; fontcolor=&quot;blue&quot; color=&quot;blue&quot; ] task1_access_a_memory-&gt;task2_run_a_time [ label=&quot;6.sched out\\n task1&quot; fontcolor=&quot;green&quot; color=&quot;green&quot; ]}Gcluster_hosthostcluster_host_dedicated_threaddedicated threadcluster_host_kvm_vcpu_threadhost kvm vcpu threadcluster_guestguestcluster_trigger_ept_violation_taskTASK1 trigger ept vioaltioncluster_sched_in_task2task2do_slow_pathI&#39;m a delicated thread, Like a special hardware, sharing the pressure of VCPUhave_got_page_successwork in done! tell the guestdo_slow_path&#45;&gt;have_got_page_successa. get page, swap in...task2_run_a_timetask2_run_a_timehave_got_page_success&#45;&gt;task2_run_a_timeb. page NOT present SCHED INept_violation_handlerept violation handlerdont_do_slow_pathI don&#39;t want handle slow path, it will speedto much timeept_violation_handler&#45;&gt;dont_do_slow_path2.find page swap outdont_do_slow_path&#45;&gt;do_slow_path3. start a work to do ittell_guest_sched_outwork is doing,need wait a a bit time, let guest do other thingsdont_do_slow_path&#45;&gt;tell_guest_sched_out4.let guest do other thingtask1_access_a_memoryacesss a memory address [BEG]tell_guest_sched_out&#45;&gt;task1_access_a_memory5. page NOT present SCHED OUTtask1_access_a_memory&#45;&gt;ept_violation_handler1.page NOT present,trigger EPT violationtask1_access_a_memory&#45;&gt;task2_run_a_time6.sched out task1task2_run_a_time&#45;&gt;task1_access_a_memoryc. sched in task1由上图可见, 引入async pf 的逻辑是让其能够在触发 EPT violation后, 能够让VCPU 调度到另外一个task, 从而阻塞触发 EPT violation 的进程执行. 为了达到这一目的, 做了以下改动: VCPU 线程在执行get_user_page()时, 仅执行fast path, 如果page 不是present的, 该接口直接返回, 而剩下的工作, 则交给另外一个dedicated thread 去做 KVM 会通过一些方式, 让 GUEST 执行调度, 从而避免再次执行触发EPT violation的指令. 而dedicatedthread 完成了swap in 的动作后, 会通知guest再次唤醒该之前调度出去的进程代码细节para virt interface一般的半虚拟化实现往往都有一下几个特征: use CPUID report this feature use MSR transparent less information, e.g. : a share memory address enable/disable use a share memory transparent more information而 para virt async PF 也是这样实现的. 在v1 Add shared memory hypercall to PV Linux guest版本中, 作者以hypercall的方式实现了半虚拟化, 但是avi在随后建议(link)使用MSR来替代 hypercall, 因为该方式在INIT和热迁移流程中有现成的 save/restore 接口 原文如下: Better to set this up as an MSR (with bit zero enabling, bits 1-5 features, and 64-byte alignment). This allows auto-reset on INIT and live migration using the existing MSR save/restore infrastructure.最好将其设置为MSR - bit 0: enabling - bit 1-5: features - 64-byte alignment他允许在INIT时 auto-reset, 并且可以使用现有的 MSR save/restore infrastructure 完成热迁移 接口流程图图示graphviz-5816c8c78422729ad7411897407bd311digraph G { subgraph cluster_host { host_page_not_present [ label=&quot;initiate page \\nnot present\\n APF&quot; color=&quot;red&quot; ] host_page_present [ label=&quot;initiate page \\nhave been\\n present APF&quot; color=&quot;green&quot; ] label=&quot;host&quot; } subgraph cluster_guest { pf_handler [ label=&quot;page fault handler&quot; ] guest_invoke_task [ label=&quot;invoke task&quot; ] label=&quot;guest&quot; } cpuid [ shape=&quot;record&quot; label=&quot;cpuid:\\n KVM_FEATURE_ASYNC_PF:\\n 1&quot; ] subgraph cluster_msr { msr_bit_map [ shape=&quot;record&quot; label=&quot;{ bit0\\n enable bit\\n value 1(enable)| bit 1-5\\n reserved\\n value 0| &lt;shm_gpa&gt;bit 63-6\\n 64-byte aligned GPA\\n value 0xabc }&quot; ] label=&quot;MSR_KVM_ASYNC_PF_EN&quot; } subgraph cluster_cr2 { token [ shape=&quot;record&quot; label=&quot;token: \\n unique id&quot; ] label=&quot;cr2&quot; } subgraph cluster_shm { shm [ shape=&quot;record&quot; label=&quot;APF reason&quot; ] label=&quot;share memory&quot; } cpuid-&gt;msr_bit_map [ arrowhead=&quot;none&quot; style=&quot;dashed&quot; label=&quot;indicate apf \\nfeature \\navailable,\\n so access\\n MSR_KVM_ASYNC_PF_EN \\nis valid&quot; ] msr_bit_map:shm_gpa-&gt;shm [ arrowhead=&quot;none&quot; style=&quot;dashed&quot; label=&quot;point base GPA \\nof this share\\n memory&quot; ] host_page_not_present-&gt;token [ label=&quot;1. initiate page \\nnot present \\nAPF, generate \\ntoken write \\nto CR2&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] host_page_not_present-&gt;shm [ label=&quot;2. update apf \\nreason to 1&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] host_page_not_present-&gt;pf_handler [ label=&quot;3. inject page \\nnot present \\n#APF&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] pf_handler-&gt;shm [ label=&quot;4. get reason \\nfrom shm:\\n PAGE \\nnot present\\n&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] pf_handler-&gt;token [ label=&quot;5. get token \\nfrom cr2,\\nbind sched\\n out thread\\n and token\\n&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] pf_handler-&gt;guest_invoke_task [ label=&quot;6. sched\\n out it&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] host_page_present-&gt;token [ label=&quot;a. initiate page \\n present APF, \\nwrite prev \\ntoken write \\nto CR2&quot; color=&quot;green&quot; fontcolor=&quot;green&quot; ] host_page_present-&gt;shm [ label=&quot;b. update apf \\nreason to 2&quot; color=&quot;green&quot; fontcolor=&quot;green&quot; ] host_page_present-&gt;pf_handler [ label=&quot;c. inject page \\n present \\n#APF&quot; color=&quot;green&quot; fontcolor=&quot;green&quot; ] pf_handler-&gt;shm [ label=&quot;d. get reason \\nfrom shm:\\nPAGE present &quot; color=&quot;green&quot; fontcolor=&quot;green&quot; ] pf_handler-&gt;token [ label=&quot;e. get token \\nfrom cr2,\\n find sched \\nout thread \\nby token&quot; color=&quot;green&quot; fontcolor=&quot;green&quot; ] pf_handler-&gt;guest_invoke_task [ label=&quot;f.wakeup it&quot; color=&quot;green&quot; fontcolor=&quot;green&quot; ]}Gcluster_hosthostcluster_guestguestcluster_msrMSR_KVM_ASYNC_PF_ENcluster_cr2cr2cluster_shmshare memoryhost_page_not_presentinitiate page not present APFpf_handlerpage fault handlerhost_page_not_present&#45;&gt;pf_handler3. inject page not present #APFtokentoken: unique idhost_page_not_present&#45;&gt;token1. initiate page not present APF, generate token write to CR2shmAPF reasonhost_page_not_present&#45;&gt;shm2. update apf reason to 1host_page_presentinitiate page have been present APFhost_page_present&#45;&gt;pf_handlerc. inject page present #APFhost_page_present&#45;&gt;tokena. initiate page present APF, write prev token write to CR2host_page_present&#45;&gt;shmb. update apf reason to 2guest_invoke_taskinvoke taskpf_handler&#45;&gt;guest_invoke_task6. sched out itpf_handler&#45;&gt;guest_invoke_taskf.wakeup itpf_handler&#45;&gt;token5. get token from cr2,bind sched out thread and tokenpf_handler&#45;&gt;tokene. get token from cr2, find sched out thread by tokenpf_handler&#45;&gt;shm4. get reason from shm: PAGE not presentpf_handler&#45;&gt;shmd. get reason from shm:PAGE present cpuidcpuid: KVM_FEATURE_ASYNC_PF: 1msr_bit_mapbit0 enable bit value 1(enable)bit 1&#45;5 reserved value 0bit 63&#45;6 64&#45;byte aligned GPA value 0xabccpuid&#45;&gt;msr_bit_mapindicate apf feature available, so access MSR_KVM_ASYNC_PF_EN is validmsr_bit_map:shm_gpa&#45;&gt;shmpoint base GPA of this share memory图中描述了host, guest在处理async pf时, 对寄存器/share memory 的操作从图中可以看出, 会涉及到cpuid, MSR_KVM_ASYNC_PF_EN, share memory, 由于async pf 的实现,需要注入#PF, 所以还会涉及 CR2cpuid新增半虚拟化cpuid bit: KVM_FEATURE_ASYNC_PFdiff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h+#define KVM_FEATURE_ASYNC_PF\t\t4关于该bit的文档说明diff --git a/Documentation/kvm/cpuid.txt b/Documentation/kvm/cpuid.txt+KVM_FEATURE_ASYNC_PF || 4 || async pf can be enabled by+ || || writing to msr 0x4b564d02大致意思是, 该cpuid如果时能, 表示可以通过write to MSR (0x4b564d02) 来enable async pfMSR – share memaddr &amp;&amp; enable bitdiff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h+#define MSR_KVM_ASYNC_PF_EN 0x4b564d02文档说明:diff --git a/Documentation/kvm/msr.txt b/Documentation/kvm/msr.txt+ MSR_KVM_ASYNC_PF_EN: 0x4b564d02+ data: Bits 63-6 hold 64-byte aligned physical address of a+ 64 byte memory area which must be in guest RAM and must be+ zeroed. Bits 5-1 are reserved and should be zero. Bit 0 is 1+ when asynchronous page faults are enabled on the vcpu 0 when+ disabled. &gt; Bits 63-6 保存着 64-byte 对其的 一个64 byte memory area 的物理地址, &gt; 该memory area 必须是 guest RAM, 并且必须是被赋值为0. &gt; &gt; Bit 5-1 被reserved并且应该为0. &gt; &gt; 当 在 vcpu 0 启用 async pf enable async pf(当是disable时), &gt; Bit 0 是1该段主要介绍了MSR的 bit 组成: MSR bit Bit [63, 6]: a 64-byte aligned physical address Bit [5, 1]: reserved Bit 0 : enable bit 其实文档中还介绍了. share memory format 和 CR2, 但是为了方便阅读, 我们将拆分开到各个小节shared memory structure – APF reasondiff --git a/Documentation/kvm/msr.txt b/Documentation/kvm/msr.txt ...+ First 4 byte of 64 byte memory location will be written to by+ the hypervisor at the time of asynchronous page fault (APF)+ injection to indicate type of asynchronous page fault. Value+ of 1 means that the page referred to by the page fault is not+ present. Value 2 means that the page is now available. Disabling+ interrupt inhibits APFs. Guest must not enable interrupt+ before the reason is read, or it may be overwritten by another+ APF. Since APF uses the same exception vector as regular page+ fault guest must reset the reason to 0 before it does+ something that can generate normal page fault. If during page+ fault APF reason is 0 it means that this is regular page+ fault. &gt; 在 hypervisor 触发 APF 注入时, 4 byte memory location的前4个byte将被 &gt; 写入 来指示 APF 的类型. &gt; 1: page fault 涉及到的page 是 not present的. &gt; 2: page 现在已经 available &gt; 另外Disabling interrupt 将会 inhibits APF. &gt; &gt; Guest必须不能enable interrupt 在reason 被read之前, 否则可能会被另一个 &gt; APF覆盖. 因为 APF 使用 相同的 exception vector 作为 regular page &gt; fault, 所以在做可能生成normal page fault 的事情之前, guest 必须 reset &gt; reason to 0. 如果 在 page fault 期间, APF reason 为0, 他意味着这是一个 &gt; regular page fault.shared memory 一共有64 byte, 其中前4个byte(32 bit) 用来indicate apf type. hostkvm 在注入 apf之前会将type写入该地址.APF 有两种type(APF reason): 1: page is not present 2: not present page becomes available另外, 在处理APF时, guest和host有下面约束: 如果guest处于 disable interrupt, host不能注入apf guest必须在enable interrupt 之前, 处理完当前的apf guest必须在触发 normal #PF时, 处理完当前的apf, 并且reset reason to 0CR2diff --git a/Documentation/kvm/msr.txt b/Documentation/kvm/msr.txt ...+ During delivery of type 1 APF cr2 contains a token that will+ be used to notify a guest when missing page becomes+ available. When page becomes available type 2 APF is sent with+ cr2 set to the token associated with the page. There is special+ kind of token 0xffffffff which tells vcpu that it should wake+ up all processes waiting for APFs and no individual type 2 APFs+ will be sent. &gt; 在 type1 APF delivery 期间, cr2 包含了一个token, 当missing page &gt; becomes available, 该token将会用于通知guest. &gt; &gt; 当page becomes available, type2 APF 将会把 cr2 设置为和该page相关的 &gt; token. &gt; &gt; 这里有一个特殊的类型 token 0xffffffff, 他将告诉vcpu, 需要wakeup 所有 &gt; 等待APF的process 并且不会有单独的 type 2 APF 将会再发送 + If APF is disabled while there are outstanding APFs, they will+ not be delivered. &gt; 当 outstanding APFs时, 如果APF 被disabled, 他们将不会被delivered. + Currently type 2 APF will be always delivered on the same vcpu as+ type 1 was, but guest should not rely on that. &gt; 当前 type 2 APF 将始终在与type 1 相同的vcpu上deliver, 但是guest不应该依赖它.cr2 包含了一个token, 该token 用来唯一标识, 当前正在发生的APF 的 id. 但是其有一个特殊value 0xffffffff, 该值用来告诉vcpu, 需要wakeup所有的正在等待 APF (type 2) 的 进程. 并且不会有单独的type2再发送.另外还有几点约束和限制 如果还有 outstanding APFs 时, 如果 APF 被disable了, 他们将不会被deliver guest 不应该依赖 type2 APF 和 type1 APF在相同vcpu上deliver, 虽然目前是这样实现的. 大家可以思考下, 为什么要支持wake up all这样的API 可以想象一下热迁移场景. 当进行热迁移时, 我们先suspend vcpu, 然后迁移memory, 这时, 会等所有page swapin,然后在进行迁移, 但是这时, guest已经不能再去注入异常了, 只能等dest端在注入. 此时来到dest端, 这时所有的memory都是present的. 所以直接注入wakeup all就可以唤醒所有wait task.(当然, 也可能再此期间有swapout, 无非是再触发一次async pf)GUP change关于GUP 改动的细节我们放到link中介绍.STRUCT – host总体数据结构图比较简单, 如下: struct 结构图 graphviz-7842836ccacf1278b495c08c9dc5b30cdigraph G {\tsubgraph cluster_vcpu0 {\t\tkvm_vcpu0 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;{struct kvm_vcpu||&lt;queue&gt;queue|&lt;done&gt;done}&quot;\t\t]\t\tsubgraph cluster_uncomplete_work {\t\t\twork_uncomplete_1 [\t\t\t\tshape=&quot;record&quot;\t\t\t\tlabel=&quot;{kvm_vcpu_pf||&lt;queue&gt;queue|&lt;link&gt;link}&quot;\t\t\t]\t\t\twork_uncomplete_2 [\t\t\t\tshape=&quot;record&quot;\t\t\t\tlabel=&quot;{kvm_vcpu_pf||&lt;queue&gt;queue|&lt;link&gt;link}&quot;\t\t\t]\t\t\tlabel=&quot;uncomplete work&quot;\t\t}\t\tsubgraph cluster_done_work {\t\t\twork_done_1 [\t\t\t\tshape=&quot;record&quot;\t\t\t\tlabel=&quot;{kvm_vcpu_pf||&lt;queue&gt;queue|&lt;link&gt;link}&quot;\t\t\t]\t\t\twork_done_2 [\t\t\t\tshape=&quot;record&quot;\t\t\t\tlabel=&quot;{kvm_vcpu_pf||&lt;queue&gt;queue|&lt;link&gt;link}&quot;\t\t\t]\t\t\tlabel=&quot;done work&quot;\t\t}\t\tlabel = &quot;vcpu 0&quot;\t}\tkvm_vcpu0:queue-&gt;\t\twork_done_1:queue-&gt;\t\twork_done_2:queue-&gt;\t\twork_uncomplete_1:queue-&gt;\t\twork_uncomplete_2:queue [\t\tcolor=&quot;red&quot;\t]\tkvm_vcpu0:done-&gt;\t\twork_done_1:link-&gt;\t\twork_done_2:link [\t\tcolor=&quot;blue&quot;\t]\tsubgraph cluster_vcpu1 {\t\tkvm_vcpu1 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;struct kvm_vcpu&quot;\t\t]\t\twork_5 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;kvm_vcpu_pf&quot;\t\t]\t\twork_6 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;kvm_vcpu_pf&quot;\t\t]\t\tlabel = &quot;vcpu 1&quot;\t\tkvm_vcpu1-&gt;work_5-&gt;work_6\t}\tsubgraph cluster_vcpu2 {\t\tkvm_vcpu2 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;struct kvm_vcpu&quot;\t\t]\t\twork_7 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;kvm_vcpu_pf&quot;\t\t]\t\twork_8 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;kvm_vcpu_pf&quot;\t\t]\t\tlabel = &quot;vcpu 2&quot;\t\tkvm_vcpu2-&gt;work_7-&gt;work_8\t}\tsubgraph cluster_vcpu3 {\t\tkvm_vcpu3 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;struct kvm_vcpu&quot;\t\t]\t\twork_9 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;kvm_vcpu_pf&quot;\t\t]\t\twork_10 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;kvm_vcpu_pf&quot;\t\t]\t\tlabel = &quot;vcpu 3&quot;\t\tkvm_vcpu3-&gt;work_9-&gt;work_10\t}}Gcluster_vcpu0vcpu 0cluster_uncomplete_workuncomplete workcluster_done_workdone workcluster_vcpu1vcpu 1cluster_vcpu2vcpu 2cluster_vcpu3vcpu 3kvm_vcpu0struct kvm_vcpu queuedonework_done_1kvm_vcpu_pf queuelinkkvm_vcpu0:queue&#45;&gt;work_done_1:queuekvm_vcpu0:done&#45;&gt;work_done_1:linkwork_uncomplete_1kvm_vcpu_pf queuelinkwork_uncomplete_2kvm_vcpu_pf queuelinkwork_uncomplete_1:queue&#45;&gt;work_uncomplete_2:queuework_done_2kvm_vcpu_pf queuelinkwork_done_1:queue&#45;&gt;work_done_2:queuework_done_1:link&#45;&gt;work_done_2:linkwork_done_2:queue&#45;&gt;work_uncomplete_1:queuekvm_vcpu1struct kvm_vcpuwork_5kvm_vcpu_pfkvm_vcpu1&#45;&gt;work_5work_6kvm_vcpu_pfwork_5&#45;&gt;work_6kvm_vcpu2struct kvm_vcpuwork_7kvm_vcpu_pfkvm_vcpu2&#45;&gt;work_7work_8kvm_vcpu_pfwork_7&#45;&gt;work_8kvm_vcpu3struct kvm_vcpuwork_9kvm_vcpu_pfkvm_vcpu3&#45;&gt;work_9work_10kvm_vcpu_pfwork_9&#45;&gt;work_10 每个cpu有自己链表, 串起属于该cpu的async pf work, 其中有两条链. queue: 串起所有work done: 串起所有完成的work struct kvm_async_pf该数据结构主要用来描述上面提到的dedicated threadstruct kvm_async_pf { struct work_struct work; struct list_head link; struct list_head queue; struct kvm_vcpu *vcpu; struct mm_struct *mm; gva_t gva; unsigned long addr; struct kvm_arch_async_pf arch; struct page *page; bool done;}; work: dedicated thread实例, 使用 workqueue机制 link: 在patch中, 链接点主要有一个: vcpu 的work完成队列 queue: 用于链接该vcpu上的所有 kvm_async_pf gva: 触发EPT violation, 需要get_user_page_slow的 GVA addr: hva done: indicate该work完没完成 kvm_arch_async_pf: struct kvm_arch_async_pf { u32 token; gfn_t gfn;}; token: 该成员用于唯一标识一次async PF, 由kvm_vcpu.arch.apf.id和vcpu-&gt;vcpu_id综合计算得到. 在注入#PF时, 会当作 CR2 传入GUEST, 方便guest管理每一次的async PF. 上面说提到的kvm_async_pf-&gt;link,kvm_async_pf-&gt;queue所链接的队列, 如下:CHANGE of struct kvm_vcpu@@ -104,6 +125,15 @@ struct kvm_vcpu { gpa_t mmio_phys_addr; #endif+#ifdef CONFIG_KVM_ASYNC_PF+ struct {+ u32 queued;+ struct list_head queue;+ struct list_head done;+ spinlock_t lock;+ } async_pf;+#endif queue: 链接所有kvm_async_pf(work) done: 链接以完成的kvm_async_pf(work) lock: 队列锁change of struct kvm_vcpu_archstruct kvm_vcpu_arch { ...+ struct {+ bool halted;+ gfn_t gfns[roundup_pow_of_two(ASYNC_PF_PER_VCPU)];+ struct gfn_to_hva_cache data;+ u64 msr_val;+ u32 id;+ bool send_user_only;+ } apf; ...} 该数据结构变动涉及多个patch, 这里把最终的数据结构变动列出. halted: 表示是否因为async PF halt 了vcpu gfns : 这里做了一个数组, 用于记录所有现存的async pf work 的 gfn data: 相当于HVA-&gt;HPA的cache, 这个映射关系一直存在且不变(大多数情况下, 除非执行__kvm_set_memory_region更改映射关系), 该HPA 指向上面提到的 share memory 该部分被作者做成了一个通用功能, 相当于是 memslot-cached kvm_put_guest()and kvm_get_guest(). 我们放到另一篇文章中介绍. 主要介绍这个功能引入和其实现. msr_val: 记录guest设置的msr值 id: 记录下一个async pf work的id, 和kvm_vcpu-&gt;vcpu_id一起,唯一标识一次async PF send_user_only: 表示只有trigger EPT violation in guest user space, host才能做async PFSTRUCT - GUESTguest 数据结构主要是用于管理, 因为async PF 调度出去的task.数据结构图 数据结构图 graphviz-e889c62c04290bdb7d5685fb187da7d7digraph G { sleep_head [ shape=&quot;record&quot; label=&quot;{ kvm_task_sleep_head|| [0]| &lt;key0&gt;link(key0)|| [1]| &lt;key1&gt;link(key1)|| [2]| &lt;key2&gt;link(key2) }&quot; ] subgraph cluster_cpu0 { sleep_node0 [ shape=&quot;record&quot; label=&quot;{ kvm_task_sleep_node|| &lt;link&gt;link| token=[id=0, vcpu=0]| cpu=0| mm=mm_struct of task0| halted=false }&quot; ] sleep_node1 [ shape=&quot;record&quot; label=&quot;{ kvm_task_sleep_node|| &lt;link&gt;link| token=[id=1, vcpu=0]| cpu=0| mm=mm_struct of task1| halted=false }&quot; ] run_task_vcpu0 [ label=&quot;current task: task2&quot; shape=&quot;record&quot; color=&quot;red&quot; ] label=&quot;cpu0 RUNNING&quot; } subgraph cluster_cpu1 { run_task_vcpu1 [ label=&quot;current task: task4&quot; shape=&quot;record&quot; color=&quot;red&quot; ] sleep_node3 [ shape=&quot;record&quot; label=&quot;{ kvm_task_sleep_node|| &lt;link&gt;link| token=[id=0, vcpu=1]| cpu=1| mm=mm_struct of task3| halted=false }&quot; ] sleep_node4 [ shape=&quot;record&quot; label=&quot;{ kvm_task_sleep_node|| &lt;link&gt;link| token=[id=1, vcpu=1]| cpu=1| mm=mm_struct of task4| halted=true }&quot; color=&quot;red&quot; ] label=&quot;cpu1 HALT&quot; } sleep_head:key0-&gt;sleep_node0:link [ color=&quot;blue&quot; ] sleep_head:key1-&gt;sleep_node1:link [ color=&quot;gold&quot; ] sleep_head:key2-&gt; sleep_node3:link-&gt; sleep_node4:link [ color=&quot;green&quot; ] sleep_node4-&gt;run_task_vcpu1 [ arrowhead=none style=dashed ]}Gcluster_cpu0cpu0 RUNNINGcluster_cpu1cpu1 HALTsleep_headkvm_task_sleep_head [0]link(key0) [1]link(key1) [2]link(key2)sleep_node0kvm_task_sleep_node linktoken=[id=0, vcpu=0]cpu=0mm=mm_struct of task0halted=falsesleep_head:key0&#45;&gt;sleep_node0:linksleep_node1kvm_task_sleep_node linktoken=[id=1, vcpu=0]cpu=0mm=mm_struct of task1halted=falsesleep_head:key1&#45;&gt;sleep_node1:linksleep_node3kvm_task_sleep_node linktoken=[id=0, vcpu=1]cpu=1mm=mm_struct of task3halted=falsesleep_head:key2&#45;&gt;sleep_node3:linkrun_task_vcpu0current task: task2run_task_vcpu1current task: task4sleep_node4kvm_task_sleep_node linktoken=[id=1, vcpu=1]cpu=1mm=mm_struct of task4halted=truesleep_node3:link&#45;&gt;sleep_node4:linksleep_node4&#45;&gt;run_task_vcpu1 图中一共有4个涉及async PF的task, 同时每个task关联一个kvm_task_sleep_node kvm_task_sleep_head[]-&gt;link负责将所有key相同的 sleep_node串联起来, 方便查找 每个kvm_task_sleep_node有一个唯一的 identify kvm_task_sleep_node-&gt;token cpu0 上之前触发过两次async PF, 并且涉及到的task调度走了,目前正在运行task2 cpu1 上触发过两次async PF, 当task3 触发时, 成功将task3 sched out, 当task4触发时, 由于此时guest vcpu 不能调度, 所以将该cpu halt. 目前该cpu正在task4的上下文中halt. kvm_task_sleep_headstatic struct kvm_task_sleep_head { spinlock_t lock; struct hlist_head list;} async_pf_sleepers[KVM_TASK_SLEEP_HASHSIZE];该数据结构是一个hash map, 使用token作为hash key. lock: 可以看到是每个hash key, 有一个lock. 减少race情况kvm_task_sleep_nodestruct kvm_task_sleep_node { struct hlist_node link; wait_queue_head_t wq; u32 token; int cpu; bool halted; struct mm_struct *mm;};该数据结构作为hash node, 描述每一个因为async pf 调度出去的task 这里并不一定指被调度出去的task, 可能链接着即将发生调度的task信息,我们下面会介绍到. wq: 等待队列 token: 和上面描述一样, 唯一标识一次async PF halted: 有时候kvm注入async PF时, guest在这个时间点不能做schedule, 又 为了再次避免执行该代码流, 只能halt 该cpu. 这里用于标识是否该task halt了cpuinitiate async pf-&gt;inject async pf上面提到了为了使用GUP noio接口, 将tdp_page_fault中的gfn_to_pfn改动为try_async_pf. 我们来看下该接口try_async_pfstatic bool try_async_pf(struct kvm_vcpu *vcpu, gfn_t gfn, gva_t gva, pfn_t *pfn){ bool async; //==(1)== *pfn = gfn_to_pfn_async(vcpu-&gt;kvm, gfn, &amp;async); //==(2)== if (!async) return false; /* *pfn has correct page already */ //==(3)== put_page(pfn_to_page(*pfn)); //==(4)== if (can_do_async_pf(vcpu)) { trace_kvm_try_async_get_page(async, *pfn); //==(5)== if (kvm_find_async_pf_gfn(vcpu, gfn)) { trace_kvm_async_pf_doublefault(gva, gfn); kvm_make_request(KVM_REQ_APF_HALT, vcpu); return true; //==(6)== } else if (kvm_arch_setup_async_pf(vcpu, gva, gfn)) return true; } //==(7)== *pfn = gfn_to_pfn(vcpu-&gt;kvm, gfn); return false;} 前面提到过, 在try_async_pf 中会执行到gfn_to_pfn_async(), async作为oparam 表示是否需要做async pf, 另外还有一个返回值, 该返回值表示在该过程中得到的 pfn of gfn 当然, 如果得到的async为false, 说明不需要async pf, 那肯定得到了pfn所以直接返回 false put_page 这里会判断当前vcpu的状态是否可以做async pf can_do_async_pf细节 +static bool can_do_async_pf(struct kvm_vcpu *vcpu)+{+\tif (unlikely(!irqchip_in_kernel(vcpu-&gt;kvm) ||+\t\t kvm_event_needs_reinjection(vcpu)))+\t\treturn false;++\treturn kvm_x86_ops-&gt;interrupt_allowed(vcpu);+} 我们这里详细讲解下, 这三个判断条件, irqchip_in_kernel() kvm_event_need_reinjection(): static inline bool kvm_event_needs_reinjection(struct kvm_vcpu *vcpu){ return vcpu-&gt;arch.exception.pending || vcpu-&gt;arch.interrupt.pending || vcpu-&gt;arch.nmi_injected;} 可以看到这里, 在检测到有其他pending 事件的情况下, 不允许做async pf. 自己的理解 关于pending的event, 我们需要参考__vmx_complete_interrupts, 但是这里我们不过度展开, 大概就是在 VM entry inject event 期间, 由于某些原因, 触发了VM exit, 此时, VM entry, 还没有完成, 所以这些事件并没有被inject, 需要再次VM entry时注入. 再这种情况下, 就会有这样的顺序 inject_event1-&gt; VM entry-&gt; VM exit(get uncomplete event)-&gt; get vm exit reason: EPT violation PAGE not present-&gt; (do some handler)-&gt; VM entry 那现在问题来了, 本次是该注入async PF, 还是注入 uncomplete event呢? 我个人认为是注入uncomplete event. 首先按照顺序 uncomplete event先发生.如果不注入 uncomplete event的情况下, 直接注入async pf, 给guest感觉是某些event延后了. 另外, uncomplete event是由于 EPT violation 而触发的. 所以在本次处理完EPT violation之后,正好可以注入 uncomplete event, 并且大概率不会再次触发VM exit during EVENTinject. 以上是自己的理解, 而且不确定处理 tdp_page_fault()时, 所有的event是否都来自于上一次注入失败的uncomplete event. 遗留问题 interrupt_allowed: 我们来看下intel vmx 代码 static int vmx_interrupt_allowed(struct kvm_vcpu *vcpu){ return (vmcs_readl(GUEST_RFLAGS) &amp; X86_EFLAGS_IF) &amp;&amp; !(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) &amp; (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS));} 该部分代码, 主要是检测当前interrupt windows 是否open, 这里对 这些判断条件不做过多解释, 详细见virtual interrupt 文章 还未写该文章 遗留部分 但是我们需要理解下, 为什么要关注guest 是否能接收中断呢? 毕竟async pf 注入的是#PF首先我们需要明确的是: 自己的理解 Q: async pf的目的是什么? A: 调度 Q: 该调度能发生在guest 运行的任何时机么 A: 需要满足guest意愿 所以, 综上所述, 得需要在guest认为自己可以调度的情况下, 才能做async pf这个事情. 否则,即使去启动了一个dedicated thread, 让guest调度, guest也不会去调度, 这样就没有意义了. 那好在这样的背景下, 我们分情况考虑: non-para virt: halt 在halt vcpu之后, 能够wakeup vcpu的方式有两种event interrupt async pf work complete 那在guest 不能注入中断的情况下, 只能由第二种event wakeup, 那就变成了sync的方式. 没有意义. para virt, 因为是半虚拟化方式, 相当于通知guest去主动做一次调度, 但是也得满足guest意愿.这实际上就像是和guest 协商的过程, 需要去关心guest这一刻是否能做调度. 作者在介绍MSR_KVM_ASYNC_PF_EN明确了, guest在关中断时, 不能去再次注入async PF, guest可能还处在APF handler中. 如果在此期间再次注入APF, 可能会导致 APF information 被覆盖, 例如: host guest cr2write token(a) to cr2 value: ainject APF1 trigger #PF (disable interrupt in VM-entry) do some thing...write token(b) to cr2 value: binject APF2 intend to read cr2 to get APF1 token, loss it !!! 在avi 的自问自答 中, 我们也能看到关于interrupt allow的解释. 这里说明之前, 该vcpu触发过该地址的 EPT violation , 并且已经做了async pf, 相当于再次遇到了.说明频率比较高, 那么直接halt该vcpu ????????? 下个小节中介绍 如果上述条件不满足, 则直接同步去做.kvm_setup_async_pfint kvm_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, gfn_t gfn, struct kvm_arch_async_pf *arch){ struct kvm_async_pf *work; //==(1)== if (vcpu-&gt;async_pf.queued &gt;= ASYNC_PF_PER_VCPU) return 0; /* setup delayed work */ /* * do alloc nowait since if we are going to sleep anyway we * may as well sleep faulting in page */ //==(2)== work = kmem_cache_zalloc(async_pf_cache, GFP_NOWAIT); if (!work) return 0; work-&gt;page = NULL; work-&gt;done = false; work-&gt;vcpu = vcpu; work-&gt;gva = gva; work-&gt;addr = gfn_to_hva(vcpu-&gt;kvm, gfn); work-&gt;arch = *arch; work-&gt;mm = current-&gt;mm; atomic_inc(&amp;work-&gt;mm-&gt;mm_count); kvm_get_kvm(work-&gt;vcpu-&gt;kvm); /* this can't really happen otherwise gfn_to_pfn_async would succeed */ if (unlikely(kvm_is_error_hva(work-&gt;addr))) goto retry_sync; //==(2.1)== INIT_WORK(&amp;work-&gt;work, async_pf_execute); //==(3)== if (!schedule_work(&amp;work-&gt;work)) goto retry_sync; //==(4)== list_add_tail(&amp;work-&gt;queue, &amp;vcpu-&gt;async_pf.queue); vcpu-&gt;async_pf.queued++; //==(5)== kvm_arch_async_page_not_present(vcpu, work); return 1;retry_sync: kvm_put_kvm(work-&gt;vcpu-&gt;kvm); mmdrop(work-&gt;mm); kmem_cache_free(async_pf_cache, work); return 0;} 说明per cpu async_pf(work)超过了最大限制 – ASYNC_PF_PER_VCPU 申请,work并做相关初始化, 在(2.1)中将work hook设置为async_pf_execute schedule work 将work加到 vcpu-&gt;async_pf.queue队列中 代码如下: void kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu, struct kvm_async_pf *work){ trace_kvm_async_pf_not_present(work-&gt;arch.token, work-&gt;gva); kvm_add_async_pf_gfn(vcpu, work-&gt;arch.gfn); //==(1)== if (!(vcpu-&gt;arch.apf.msr_val &amp; KVM_ASYNC_PF_ENABLED) || (vcpu-&gt;arch.apf.send_user_only &amp;&amp; kvm_x86_ops-&gt;get_cpl(vcpu) == 0)) kvm_make_request(KVM_REQ_APF_HALT, vcpu); //==(2)== else if (!apf_put_user(vcpu, KVM_PV_REASON_PAGE_NOT_PRESENT)) { vcpu-&gt;arch.fault.error_code = 0; vcpu-&gt;arch.fault.address = work-&gt;arch.token; kvm_inject_page_fault(vcpu); }} 和can_do_async_pf, 这里也有一些判断当前状态是否合适向guest注入async pf的条件, 我们放到下面介绍 如果可以注入, 则将KVM_PV_REASON_PAGE_NOT_PRESENT其写入 guest host 共享的内存中, 表示本次注入的是page not present类型的 async pf. 另外, 设置好本次注入异常的 address和 error code async pf workstatic void async_pf_execute(struct work_struct *work){ struct page *page = NULL; struct kvm_async_pf *apf = container_of(work, struct kvm_async_pf, work); struct mm_struct *mm = apf-&gt;mm; struct kvm_vcpu *vcpu = apf-&gt;vcpu; unsigned long addr = apf-&gt;addr; gva_t gva = apf-&gt;gva; might_sleep(); use_mm(mm); down_read(&amp;mm-&gt;mmap_sem); //==(1)== get_user_pages(current, mm, addr, 1, 1, 0, &amp;page, NULL); up_read(&amp;mm-&gt;mmap_sem); unuse_mm(mm); spin_lock(&amp;vcpu-&gt;async_pf.lock); //==(2)== list_add_tail(&amp;apf-&gt;link, &amp;vcpu-&gt;async_pf.done); apf-&gt;page = page; apf-&gt;done = true; spin_unlock(&amp;vcpu-&gt;async_pf.lock); /* * apf may be freed by kvm_check_async_pf_completion() after * this point */ trace_kvm_async_pf_completed(addr, page, gva); //==(3)== if (waitqueue_active(&amp;vcpu-&gt;wq)) wake_up_interruptible(&amp;vcpu-&gt;wq); mmdrop(mm); kvm_put_kvm(vcpu-&gt;kvm);} 调用get_user_pages, 该接口可以处理MAJOR fault get_user_pages() 第四个参数, 如果不为空,则会设置FOLL_GET int get_user_pages(struct task_struct *tsk, struct mm_struct *mm, unsigned long start, int nr_pages, int write, int force, struct page **pages, struct vm_area_struct **vmas){ int flags = FOLL_TOUCH; if (pages) flags |= FOLL_GET; ...} 如果设置了FOLL_GET, 则会在get_user_pages()的过程中, pin this page.也就是get_page(), 但是需要注意的是, 该接口可能会返回错误, 但是看起来此流程并没有判断该接口是否执行成功. IOW, 无论该接口是否执行成功, 都认为该work已经complete, 都需要再次wakeup GUEST blocking thread. 将该work, 链接到vcpu-&gt;async_pf.done链表中 如果vcpu在等待队列中(halt), 唤醒该vcpu接下来, 我们来看下, host是如何检测 page present事件, 并注入page present async pf的host inject PAGE PRESENT aync pf@@ -5272,6 +5288,9 @@ static int __vcpu_run(struct kvm_vcpu *vcpu) \t\t\tvcpu-&gt;run-&gt;exit_reason = KVM_EXIT_INTR; \t\t\t++vcpu-&gt;stat.request_irq_exits; \t\t}+\t\t+\t\tkvm_check_async_pf_completion(vcpu);+ \t\tif (signal_pending(current)) { \t\t\tr = -EINTR;在vm exit后, 检测是否有需要 async pf completevoid kvm_check_async_pf_completion(struct kvm_vcpu *vcpu){ struct kvm_async_pf *work; //==(1)== if (list_empty_careful(&amp;vcpu-&gt;async_pf.done) || !kvm_arch_can_inject_async_page_present(vcpu)) return; spin_lock(&amp;vcpu-&gt;async_pf.lock); work = list_first_entry(&amp;vcpu-&gt;async_pf.done, typeof(*work), link); list_del(&amp;work-&gt;link); spin_unlock(&amp;vcpu-&gt;async_pf.lock); //==(2)== if (work-&gt;page) kvm_arch_async_page_ready(vcpu, work); //==(3)== kvm_arch_async_page_present(vcpu, work); list_del(&amp;work-&gt;queue); vcpu-&gt;async_pf.queued--; if (work-&gt;page) put_page(work-&gt;page); kmem_cache_free(async_pf_cache, work);} 有两个判断条件: 判断是否有完成的work guest此时是否适合注入 page present async PF (下面章节介绍) 如果work-&gt;page为 NULL, 说明async work中, 执行get_user_pages()失败了, 那么本次就不需要在执行kvm_arch_async_page_ready(), 该函数作用是, 再次执行tdp_page_fault, 如果page is ready, 那只需要执行get_user_page fast path和__direct_map建立GPA-&gt;HPA的映射. 但是如果page is not ready(work-&gt;page)为NULL, 作者的想法是, 让其在次vm entry,wakeup guest blocking thread, 让其再次触发EPT violation, 然后再发起async pf.所以在这里没有必要在做一次kvm_arch_async_page_ready-&gt;tdp_page_fault, 那可能有同学会说, 那为什么不在HOST中, 等待get_user_pages()一定返回成功之后, 再注入 page present #PF, 实话说,我也不知道, 但这里总感觉作者不想增加复杂的代码逻辑, 需要关注下后续的patch,看看是否对这部分有优化 遗留问题 kvm_arch_async_page_present void kvm_arch_async_page_present(struct kvm_vcpu *vcpu, struct kvm_async_pf *work){ trace_kvm_async_pf_ready(work-&gt;arch.token, work-&gt;gva); //==(1)== if (is_error_page(work-&gt;page)) work-&gt;arch.token = ~0; /* broadcast wakeup */ else kvm_del_async_pf_gfn(vcpu, work-&gt;arch.gfn); //==(2)== if ((vcpu-&gt;arch.apf.msr_val &amp; KVM_ASYNC_PF_ENABLED) &amp;&amp; !apf_put_user(vcpu, KVM_PV_REASON_PAGE_READY)) { vcpu-&gt;arch.fault.error_code = 0; vcpu-&gt;arch.fault.address = work-&gt;arch.token; kvm_inject_page_fault(vcpu); }} 关于error page, 我们放在另一篇文章中讲述. 遗留问题 置入KVM_ASYNC_PF_PF_ENABLED, 准备注入 page present async #PF guest handle async PFdotraplinkage void __kprobesdo_async_page_fault(struct pt_regs *regs, unsigned long error_code){ //==(1)== switch (kvm_read_and_reset_pf_reason()) { default: //==(2)== do_page_fault(regs, error_code); break; case KVM_PV_REASON_PAGE_NOT_PRESENT: //==(3)== /* page is swapped out by the host. */ kvm_async_pf_task_wait((u32)read_cr2()); break; //==(4)== case KVM_PV_REASON_PAGE_READY: kvm_async_pf_task_wake((u32)read_cr2()); break; }}该部分代码逻辑很清晰, async PF event 是使用了原有的#PF exception vector,guest 需要在exception handler 中判断这个#PF的类型, 然后执行相应的handler 从share memory 中获取 async pf reason indicate NORMAL #PF indicate PAGE NOT PRESENT async pf indicate PAGE PRESENT async pfpage not present async pfvoid kvm_async_pf_task_wait(u32 token){ u32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS); struct kvm_task_sleep_head *b = &amp;async_pf_sleepers[key]; struct kvm_task_sleep_node n, *e; DEFINE_WAIT(wait); int cpu, idle; cpu = get_cpu(); idle = idle_cpu(cpu); put_cpu(); spin_lock(&amp;b-&gt;lock); //===(1)== e = _find_apf_task(b, token); if (e) { /* dummy entry exist -&gt; wake up was delivered ahead of PF */ hlist_del(&amp;e-&gt;link); kfree(e); spin_unlock(&amp;b-&gt;lock); return; } //===(2)== n.token = token; n.cpu = smp_processor_id(); n.mm = current-&gt;active_mm; //===(2.1)== n.halted = idle || preempt_count() &gt; 1; atomic_inc(&amp;n.mm-&gt;mm_count); init_waitqueue_head(&amp;n.wq); //===(3)== hlist_add_head(&amp;n.link, &amp;b-&gt;list); spin_unlock(&amp;b-&gt;lock); for (;;) { //===(4)== if (!n.halted) prepare_to_wait(&amp;n.wq, &amp;wait, TASK_UNINTERRUPTIBLE); if (hlist_unhashed(&amp;n.link)) break; //===(4)== if (!n.halted) { local_irq_enable(); schedule(); local_irq_disable(); } else { /* * We cannot reschedule. So halt. */ native_safe_halt(); local_irq_disable(); } } if (!n.halted) finish_wait(&amp;n.wq, &amp;wait); return;} 在kernel doc介绍MSR_KVM_ASYNC_PF_EN, 作者有提到过. 一对[type2 APF, type1 APF] 不一定会在同一个vcpu上触发, 那也就意味着两者可能并行执行(虽然现在的host kvm 没有这样做,但是guest不能依赖它), 如下: kvm vcpu1 vcpu21.inject type1 APF to VCPU1 2. inject type2 APF to VCPU2 3. handle type2 APF 4. handle type1 APF 可以看到kvm虽然是按照顺序注入的type1 APF, 和type2 APF, 但是注入到了不同的vcpu. vcpu在处理时,handle type2 APF先执行, 此时page 已经present了, 不需要再sched out, 这里会在type2 APFhandler中预先将带有该token的sleep_node放到head中, 以便type 1 APF handler 可以跳过这次的sched out(需要结合type2 APF handle – kvm_async_pf_task_wake().) 将task(current-&gt;active_mm)和token绑定, 这样当type2 APF触发时, 可以根据token找到当前block的task 需要注意的时, guest在某些情况下不能sched out, 这时, 只能halt当前cpu 我们放到另一篇文章中去介绍 遗留问题 将sleep_node链到sleep_head上 如果guest此时可以调度, 则将进程D住, sched outpage present async pfvoid kvm_async_pf_task_wake(u32 token){ u32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS); struct kvm_task_sleep_head *b = &amp;async_pf_sleepers[key]; struct kvm_task_sleep_node *n; if (token == ~0) { apf_task_wake_all(); return; }again: spin_lock(&amp;b-&gt;lock); //===(1)== n = _find_apf_task(b, token); //===(2)== if (!n) { /* * async PF was not yet handled. * Add dummy entry for the token. */ n = kmalloc(sizeof(*n), GFP_ATOMIC); if (!n) { /* * Allocation failed! Busy wait while other cpu * handles async PF. */ spin_unlock(&amp;b-&gt;lock); cpu_relax(); goto again; } n-&gt;token = token; n-&gt;cpu = smp_processor_id(); n-&gt;mm = NULL; init_waitqueue_head(&amp;n-&gt;wq); hlist_add_head(&amp;n-&gt;link, &amp;b-&gt;list); } else //===(3)== apf_task_wake_one(n); spin_unlock(&amp;b-&gt;lock); return;} 根据token, 在sleep_head中查找sleep_node 同type1 APF handler, type2 APF可能在于type1 APF不同的cpu上先执行, 此时在sleep_head中找不到和该token相关的sleep_node, 这时, 需要新创建一个sleep_node将其添加到sleep_head中, 以便type1 APF handler可以查找到,避免block该task 如果查找到了, 说明type1 APF handler已经触发, task已经block, 需要wakeup该task参考链接 MAIL list:v1 v2 v3 v4 v5 v6 v7 " } ]
