[ { "title": "vm86", "url": "/posts/vm86/", "categories": "intel_sdm", "tags": "virt", "date": "2024-04-22 10:30:00 +0800", "snippet": " FROM intel sdm chapter 21 8086 emulationabstractIA-32 processors (beginning with the Intel386 processor) provide two ways toexecute new or legacy programs that are assembled and/or compiled to ru...", "content": " FROM intel sdm chapter 21 8086 emulationabstractIA-32 processors (beginning with the Intel386 processor) provide two ways toexecute new or legacy programs that are assembled and/or compiled to run on anIntel 8086 processor: IA-32处理器(起始于intel386处理器)提供了两种来执行new/legacy 程序,这些程序被assembled(组装?) and/or compiled(编译) 以在intel 8086处理器上运行. Real-address mode. Virtual-8086 mode.Figure 2-3 shows the relationship of these operating modes to protected modeand system management mode (SMM). Figure 2-3 展示了这些操作模式和 保护模式以及 SMM 的关系When the processor is powered up or reset, it is placed in the real-addressmode. This operating mode almost exactly duplicates the execution environmentof the Intel 8086 processor, with some extensions. Virtually any programassembled and/or compiled to run on an Intel 8086 processor will run on anIA-32 processor in this mode. 当处理器被 power up 或者 reset, 他处于 real-address mode. 这种操作模式几乎完全复制了Intel 8086处理器的执行环境，并进行了一些扩展。实际上，任何在 Intel 8086 处理器上组装和/或编译的程序都可以在此模式下在 IA-32 处理器上运行.When running in protected mode, the processor can be switched to virtual-8086mode to run 8086 programs. This mode also duplicates the execution environmentof the Intel 8086 processor, with extensions. In virtual-8086 mode, an 8086program runs as a separate protected-mode task. Legacy 8086 programs are thusable to run under an operating system (such as Microsoft Windows*) that takesadvantage of protected mode and to use protected-mode facilities, such as theprotected-mode interrupt- and exception-handling facilities. Protected-modemultitasking permits multiple virtual-8086 mode tasks (with each task running aseparate 8086 program) to be run on the processor along with othernon-virtual-8086 mode tasks. take advantage of : 利用facility [fəˈsɪləti] : 组件, 设施, 特色, 天赋, 才能 当运行在保护模式下时，处理器可以切换到virtual-8086模式来运行8086程序。该模式还复制了Intel 8086 处理器的执行环境，并进行了扩展。在virtual-8086 模式下，8086 程序作为单独的保护模式任务运行。 因此，传统 8086 程序能够在利用保护模式的操作系统（例如 Microsoft Windows*）下运行并使用保护模式facilities(组件, 设施)，例如保护模式中断和异常处理facilities。 保护模式多任务处理允许多个virtual-8086 模式task（每个任务运行一个单独的 8086 程序）与其他 non-virtual-8086 模式任务一起在处理器上运行。This section describes both the basic real-address mode execution environmentand the virtual-8086-mode execution environment, available on the IA-32processors beginning with the Intel386 processor. 本节介绍基于 real-address 模式执行环境和virtual-8086 模式执行环境，可在从 Intel386 处理器开始的 IA-32 处理器上使用。21.1 REAL-ADDRESS MODEThe IA-32 architecture’s real-address mode runs programs written for the Intel8086, Intel 8088, Intel 80186, and Intel 80188 processors, or for thereal-address mode of the Intel 286, Intel386, Intel486, Pentium, P6 family,Pentium 4, and Intel Xeon processors. IA-32 架构的实地址模式运行为 Intel 8086、Intel 8088、Intel 80186 和 Intel 80188 处理器编写的程序，或为 Intel 286、Intel386、Intel486、Pentium、P6 系列、Pentium 4 和 英特尔至强处理器, 实地址模式编写的程序.The execution environment of the processor in real-address mode is designed toduplicate the execution environment of the Intel 8086 processor. To an 8086program, a processor operating in real-address mode behaves like a high-speed8086 processor. The principal features of this architecture are defined inChapter 3, “Basic Execution Environment”, of the Intel® 64 and IA-32Architectures Software Developer’s Manual, Volume 1. real-address模式下处理器的执行环境旨在复制Intel 8086处理器的执行环境。对于 8086 程序来说，运行在实地址模式下的处理器的行为类似于高速 8086 处理器。该架构的主要功能在 itnel sdm Volume 1 Chapter 3 “Basic Execution Environment”The following is a summary of the core features of the real-address modeexecution environment as would be seen by a program written for the 8086: 以下是为 8086 编写的程序所看到的实地址模式执行环境的核心功能的摘要： The processor supports a nominal 1-MByte physical address space (see Section21.1.1, “Address Translation in Real-Address Mode”, for specific details).This address space is divided into segments, each of which can be up to 64KBytes in length. The base of a segment is specified with a 16-bit segmentselector, which is shifted left by 4 bits to form a 20-bit offset fromaddress 0 in the address space. An operand within a segment is addressed witha 16-bit offset from the base of the segment. A physical address is thusformed by adding the offset to the 20-bit segment base (see Section 21.1.1,“Address Translation in Real-Address Mode”). 处理器支持标称 1 MB 物理地址空间（有关具体细节，请参见第 21.1.1 节”实地址模式下的地址转换”）。 该地址空间分为多个段，每个段的长度最多可达 64 KB。 段的基址由 16 位段选择器指定，该段选择器左移 4 位，形成距地址空间中地址 0 的 20 位偏移量。 段内的操作数通过距段基址的 16 位偏移量进行寻址。 因此，通过将偏移量添加到 20 位段基址来形成物理地址（请参见第 21.1.1 节“实地址模式下的地址转换”）。 All operands in “native 8086 code” are 8-bit or 16-bit values. (Operand sizeoverride prefixes can be used to access 32-bit operands.) “native 8086 代码”中的所有操作数都是 8 位或 16 位值。（操作数大小覆盖前缀可用于访问 32 位操作数。） Eight 16-bit general-purpose registers are provided: AX, BX, CX, DX, SP, BP,SI, and DI. The extended 32 bit registers (EAX, EBX, ECX, EDX, ESP, EBP, ESI,and EDI) are accessible to programs that explicitly perform a size overrideoperation. 提供 8 个 16 位通用寄存器：AX、BX、CX、DX、SP、BP、SI 和 DI。 扩展的 32 位寄存器（EAX、EBX、ECX、EDX、ESP、EBP、ESI 和 EDI）可供显式执行大小覆盖操作的程序访问。 Four segment registers are provided: CS, DS, SS, and ES. (The FS and GSregisters are accessible to programs that explicitly access them.) The CSregister contains the segment selector for the code segment; the DS and ESregisters contain segment selectors for data segments; and the SS registercontains the segment selector for the stack segment. 提供了四个段寄存器：CS、DS、SS 和 ES。 （FS 和 GS 寄存器可供显式访问它们的程序访问。）CS 寄存器包含代码段的段选择器； DS和ES寄存器包含数据段的段选择器；SS 寄存器包含堆栈段的段选择器。 The 8086 16-bit instruction pointer (IP) is mapped to the lower 16-bits ofthe EIP register. Note this register is a 32-bit register and unintentionaladdress wrapping may occur. unintentional: [ˌʌnɪnˈtenʃənl] 无意的; 非故意的; 偶然的 8086 16 位指令指针（IP）映射到 EIP 寄存器的低 16 位。 请注意，该寄存器是 32 位寄存器，可能会发生 unintentional address wrapping unintentional address wrapping 在 “21.1.1 Address Translation in Real-Address Mode” 有介绍 The 16-bit FLAGS register contains status and control flags. (This registeris mapped to the 16 least significant bits of the 32-bit EFLAGS register.) 16 位标志寄存器包含状态和控制标志。（该寄存器映射到 32 位 EFLAGS 寄存器的 16 个最低有效位。） All of the Intel 8086 instructions are supported (see Section 21.1.3,“Instructions Supported in Real-Address Mode”). 支持所有 Intel 8086 指令（请参见第 21.1.3 节“instruction supported in real-address mode”）。 A single, 16-bit-wide stack is provided for handling procedure calls andinvocations of interrupt and exception handlers. This stack is contained inthe stack segment identified with the SS register. The SP (stack pointer)register contains an offset into the stack segment. The stack grows down(toward lower segment offsets) from the stack pointer. The BP (base pointer)register also contains an offset into the stack segment that can be used as apointer to a parameter list. When a CALL instruction is executed, theprocessor pushes the current instruction pointer (the 16 least-significantbits of the EIP register and, on far calls, the current value of the CSregister) onto the stack. On a return, initiated with a RET instruction, theprocessor pops the saved instruction pointer from the stack into the EIPregister (and CS register on far returns). When an implicit call to aninterrupt or exception handler is executed, the processor pushes the EIP, CS,and EFLAGS (low-order 16-bits only) registers onto the stack. On a returnfrom an interrupt or exception handler, initiated with an IRET instruction,the processor pops the saved instruction pointer and EFLAGS image from thestack into the EIP, CS, and EFLAGS registers. invocations [ˌɪnvəʊˈkeɪʃənz]: 调用; 启用; 祈祷procedure [prəˈsiːdʒə(r)] : 程序, 步骤, 手续, 手术 提供了一个 16 位宽的堆栈来处理procedure calls以及中断和异常处理程序的调用。 该堆栈包含在由 SS 寄存器标识的堆栈段中。 SP（堆栈指针）寄存器包含堆栈段的偏移量。 堆栈从堆栈指针向下增长（朝向较低的段偏移量）。 BP（基指针）寄存器还包含堆栈段的偏移量，可用作指向参数列表的指针。 当执行 CALL 指令时，处理器将当前指令指针（EIP 寄存器的 16 个最低有效位，以及far all 时 CS 寄存器的当前值）推送到堆栈上。 在使用 RET 指令启动的返回时，处理器将保存的指令指针从堆栈pop到 EIP 寄存器（以及远返回时的 CS 寄存器）。 当执行对中断或异常处理程序的隐式调用时，处理器会将 EIP、CS 和 EFLAGS（仅限低位 16 位）寄存器压入堆栈。 从使用 IRET 指令启动的中断或异常处理程序返回时，处理器将保存的指令指针和 EFLAGS image从堆栈pop到 EIP、CS 和 EFLAGS 寄存器中。 A single interrupt table, called the “interrupt vector table” or “interrupttable,” is provided for handling interrupts and exceptions (see Figure 21-2).The interrupt table (which has 4-byte entries) takes the place of theinterrupt descriptor table (IDT, with 8-byte entries) used when handlingprotected-mode interrupts and exceptions. Interrupt and exception vectornumbers provide an index to entries in the interrupt table. Each entryprovides a pointer (called a “vector”) to an interrupt- or exception-handlingprocedure. See Section 21.1.4, “Interrupt and Exception Handling”, for moredetails. It is possible for software to relocate the IDT by means of the LIDTinstruction on IA-32 processors beginning with the Intel386 processor. by means of: 通过take the place of: 取代, 代替 提供了一个称为“中断向量表”或“中断表”的中断表来处理中断和异常（见图 21-2）。 中断表（具有 4-byte 条目）取代了处理保护模式中断和异常时使用的中断描述符表（IDT，具有 8-byte条目）。 中断和异常向量号提供中断表中条目的索引。 每个条目都提供一个指向interrupt &amp;&amp; exception-handling procedure 的指针（称为“向量”）。 更多详细信息，请参见第 21.1.4 节”中断和异常处理”。 从 Intel386 处理器开始，软件可以通过 IA-32 处理器上的 LIDT 指令来重新定位 IDT。 The x87 FPU is active and available to execute x87 FPU instructions inreal-address mode. Programs written to run on the Intel 8087 and Intel 287math coprocessors can be run in real-address mode without modification. coprocessors: 辅助处理器, 协处理器 x87 FPU 处于活动状态，可在实地址模式下执行 x87 FPU 指令。 为在 Intel8087 和 Intel 287 数学协处理器上运行而编写的程序无需修改即可在实地址模式下运行。 The following extensions to the Intel 8086 execution environment are availablein the IA-32 architecture’s real-address mode. If backwards compatibility toIntel 286 and Intel 8086 processors is required, these features should not beused in new programs written to run in real-address mode. Intel 8086 执行环境的以下扩展可在 IA-32 架构的实地址模式下使用。如果需要向后兼容 Intel 286 和 Intel 8086 处理器，则不应在为在实地址模式下运行而编写的新程序中使用这些功能。 Two additional segment registers (FS and GS) are available. Many of the integer and system instructions that have been added to laterIA-32 processors can be executed in real-address mode (see Section 21.1.3,“Instructions Supported in Real-Address Mode”). integer [ˈɪntɪdʒər]: 整数 许多已添加到后续 IA-32 处理器中的整数和系统指令都可以在实地址模式下执行（请参见第 21.1.3 节“实地址模式支持的指令”）。 The 32-bit operand prefix can be used in real-address mode programs toexecute the 32-bit forms of instructions. This prefix also allowsreal-address mode programs to use the processor’s 32-bit general-purposeregisters. 32 位操作数前缀可用于实地址模式程序来执行 32 位形式的指令。 该前缀还允许实地址模式程序使用处理器的 32 位通用寄存器。 The 32-bit address prefix can be used in real-address mode programs, allowing32-bit offsets. Many of the integer and system instructions that have beenadded to later IA-32 processors can be executed in real-address mode (seeSection 21.1.3, “Instructions Supported in Real-Address Mode”). 32位地址前缀可用于实地址模式程序，允许32位偏移。 许多已添加到后续 IA-32 处理器中的整数和系统指令都可以在实地址模式下执行（请参见第 21.1.3 节“实地址模式支持的指令”）。 The following sections describe address formation, registers, availableinstructions, and interrupt and exception handling in real-address mode. Forinformation on I/O in real-address mode, see Chapter 19, “Input/Output”, of theIntel® 64 and IA-32 Architectures Software Developer’s Manual, Volume 1. a32-bit register and unintentional address wrapping may occur. 以下部分描述了实地址模式下的地址形成、寄存器、可用指令以及中断和异常处理。有关实地址模式下 I/O 的信息，请参阅intel sdm 第 1 卷第 19 章“Input/Output”。可能会发生 32 位寄存器和unintentional address wrapping。21.1.1 Address Translation in Real-Address ModeIn real-address mode, the processor does not interpret segment selectors asindexes into a descriptor table; instead, it uses them directly to form linearaddresses as the 8086 processor does. It shifts the segment selector left by 4bits to form a 20-bit base address (see Figure 21-1). The offset into a segmentis added to the base address to create a linear address that maps directly tothe physical address space. interpret [ɪnˈtɜːprət]: 解释, 说明; 把...理解为 在实地址模式下，处理器不会将段选择器解释为描述符表的索引； 相反，它像 8086 处理器一样直接使用它们来形成线性地址。 它将段选择子左移 4 位，形成 20 位基地址（见图 21-1）。 段中的偏移量被添加到基地址以创建直接映射到物理地址空间的线性地址。When using 8086-style address translation, it is possible to specify addresseslarger than 1 MByte. For example, with a segment selector value of FFFFH and anoffset of FFFFH, the linear (and physical) address would be 10FFEFH (1 megabyteplus 64 KBytes). The 8086 processor, which can form addresses only up to 20bits long, truncates the high-order bit, thereby “wrapping” this address toFFEFH. When operating in real-address mode, however, the processor does nottruncate such an address and uses it as a physical address. (Note, however,that for IA-32 processors beginning with the Intel486 processor, the A20M#signal can be used in real-address mode to mask address line A20, therebymimicking the 20-bit wrap-around behavior of the 8086 processor.) Care shouldbe take to ensure that A20M# based address wrapping is handled correctly inmultiprocessor based system. 当使用 8086 类型的地址转换时，可以指定大于 1 MB 的地址。 例如，如果段选择器值为 FFFFH，偏移量为 FFFFH，则线性（物理）地址将为 10FFEFH（1 兆字节加 64 KB）。 0xffff&lt;&lt;4 + 0xffff = 64K * 16 + 64k = 1M-byte + 64K = 0x10ffef 8086 处理器只能形成最多 20 位长的地址，它会截断高位，从而将该地址“包装”为 FFEFH。然而，当在实地址模式下运行时，处理器不会截断此类地址并将其用作物理地址。 （但请注意，对于从 Intel486 处理器开始的 IA-32 处理器，可以在实地址模式下使用 A20M# 信号来maks address line A20，从而模仿 8086 处理器的 20 位wrap-around行为。 ）应注意确保在基于多处理器的系统中正确处理基于 A20M# 的address wrappingThe IA-32 processors beginning with the Intel386 processor can generate 32-bitoffsets using an address override prefix; however, in real-address mode, thevalue of a 32-bit offset may not exceed FFFFH without causing an exception. 从Intel386处理器开始的IA-32处理器可以使用address override prefix生成32位偏移量；然而，在实地址模式下，32位偏移量的值不能超过FFFFH而不引起异常。For full compatibility with Intel 286 real-address mode, pseudo-protectionfaults (interrupt 12 or 13) occur if a 32- bit offset is generated outside therange 0 through FFFFH. 为了与 Intel 286 实地址模式完全兼容，如果在 0 到 FFFFH 范围之外生成 32 位偏移，则会发生 pseudo-protection (伪保护故障（中断 12 或 13）。21.1.2 Registers Supported in Real-Address ModeThe register set available in real-address mode includes all the registersdefined for the 8086 processor plus the new registers introduced in later IA-32processors, such as the FS and GS segment registers, the debug registers, thecontrol registers, and the floating-point unit registers. The 32-bit operandprefix allows a real-address mode program to use the 32-bit general-purposeregisters (EAX, EBX, ECX, EDX, ESP, EBP, ESI, and EDI). 实地址模式下可用的寄存器集包括为 8086 处理器定义的所有寄存器以及后来的 IA-32 处理器中引入的新寄存器，例如 FS 和 GS 段寄存器、调试寄存器、控制寄存器和浮点寄存器。 32 位操作数前缀允许实地址模式程序使用 32 位通用寄存器（EAX、EBX、ECX、EDX、ESP、EBP、ESI 和 EDI）。21.1.3 Instructions Supported in Real-Address ModeThe following instructions make up the core instruction set for the 8086processor. If backwards compatibility to the Intel 286 and Intel 8086processors is required, only these instructions should be used in a new programwritten to run in real-address mode. 以下指令构成了 8086 处理器的核心指令集。 如果需要向后兼容 Intel 286 和 Intel 8086 处理器，在编写为在实地址模式下运行的新程序中仅使用这些指令。 Move (MOV) instructions that move operands between general-purpose registers,segment registers, and between memory and general-purpose registers. The exchange (XCHG) instruction. Load segment register instructions LDS and LES. Arithmetic instructions ADD, ADC, SUB, SBB, MUL, IMUL, DIV, IDIV, INC, DEC,CMP, and NEG. Logical instructions AND, OR, XOR, and NOT. Decimal instructions DAA, DAS, AAA, AAS, AAM, and AAD. decimal [ˈdesɪml] :十进制 Stack instructions PUSH and POP (to general-purpose registers and segmentregisters). Type conversion instructions CWD, CDQ, CBW, and CWDE. conversion : 转换, 转变 Shift and rotate instructions SAL, SHL, SHR, SAR, ROL, ROR, RCL, and RCR. rotate [ˈroʊteɪt]: 旋转 TEST instruction. Control instructions JMP, Jcc, CALL, RET, LOOP, LOOPE, and LOOPNE. Interrupt instructions INT n, INTO, and IRET. EFLAGS control instructions STC, CLC, CMC, CLD, STD, LAHF, SAHF, PUSHF, andPOPF. I/O instructions IN, INS, OUT, and OUTS. Load effective address (LEA) instruction, and translate (XLATB) instruction. LOCK prefix. Repeat prefixes REP, REPE, REPZ, REPNE, and REPNZ. Processor halt (HLT) instruction. No operation (NOP) instruction.The following instructions, added to later IA-32 processors (some in the Intel286 processor and the remainder in the Intel386 processor), can be executed inreal-address mode, if backwards compatibility to the Intel 8086 processor isnot required. 如果不需要向后兼容 Intel 8086 处理器，则添加到后来的 IA-32 处理器（一些在 Intel 286 处理器中，其余在 Intel386 处理器中）的以下指令可以在实地址模式下执行。 Move (MOV) instructions that operate on the control and debug registers. Load segment register instructions LSS, LFS, and LGS. Generalized multiply instructions and multiply immediate data. Generalized [ˈdʒenrəlaɪzd] : 广义的 Shift and rotate by immediate counts. Stack instructions PUSHA, PUSHAD, POPA, POPAD, and PUSH immediate data. Move with sign extension instructions MOVSX and MOVZX. Long-displacement Jcc instructions. displacement: 移位;取代 Exchange instructions CMPXCHG, CMPXCHG8B, and XADD. String instructions MOVS, CMPS, SCAS, LODS, and STOS. Bit test and bit scan instructions BT, BTS, BTR, BTC, BSF, and BSR; thebyte-set-on condition instruction SETcc;and the byte swap (BSWAP) instruction. EFLAGS control instructions PUSHF and POPF. ENTER and LEAVE control instructions. BOUND instruction. CPU identification (CPUID) instruction. System instructions CLTS, INVD, WINVD, INVLPG, LGDT, SGDT, LIDT, SIDT, LMSW,SMSW, RDMSR, WRMSR, RDTSC, and RDPMC.Execution of any of the other IA-32 architecture instructions (not given in theprevious two lists) in real-address mode result in an invalid-opcode exception(#UD) being generated. 在实地址模式下执行任何其他 IA-32 架构指令（前两个列表中未给出）都会导致生成invaild-opcode 异常 (#UD)。 21.1.4 Interrupt and Exception HandlingWhen operating in real-address mode, software must provide interrupt andexception-handling facilities that are separate from those provided inprotected mode. Even during the early stages of processor initialization whenthe processor is still in real-address mode, elementary real-address modeinterrupt and exception-handling facilities must be provided to ensure reliableoperation of the processor, or the initialization code must ensure that nointerrupts or exceptions will occur. 当在实地址模式下运行时，软件必须提供与保护模式下提供的中断和异常处理设施分开的facilities。 即使在处理器初始化的早期阶段，当处理器仍处于实地址模式时，也必须提供基本实地址模式中断和异常处理设施，以确保处理器的可靠运行，或者必须保证初始化代码没有中断或触发异常。The IA-32 processors handle interrupts and exceptions in real-address modesimilar to the way they handle them in protected mode. When a processorreceives an interrupt or generates an exception, it uses the vector number ofthe interrupt or exception as an index into the interrupt table. (In protectedmode, the interrupt table is called the interrupt descriptor table (IDT), butin real-address mode, the table is usually called the interrupt vector table,or simply the interrupt table.) The entry in the interrupt vector tableprovides a pointer to an interrupt- or exception-handler procedure. (Thepointer consists of a segment selector for a code segment and a 16-bit offsetinto the segment.) The processor performs the following actions to make animplicit call to the selected handler: IA-32 处理器在实地址模式下处理中断和异常的方式与在保护模式下处理中断和异常的方式类似。 当处理器接收到中断或生成异常时，它使用中断或异常的向量号作为中断表的索引。（在保护模式下，中断表称为中断描述符表（IDT），但在实地址模式下，该表通常称为中断向量表，或简称为中断表。）中断向量表中的条目提供 指向中断或异常处理程序的指针。（指针由代码段的段选择器和段中的 16 位偏移量组成。）处理器执行以下操作以隐式调用所选处理程序： Pushes the current values of the CS and EIP registers onto the stack. (Onlythe 16 least-significant bits of the EIP register are pushed.) Pushes the low-order 16 bits of the EFLAGS register onto the stack. Clears the IF flag in the EFLAGS register to disable interrupts. Clears the TF, RF, and AC flags, in the EFLAGS register. Transfers program control to the location specified in the interrupt vectortable.An IRET instruction at the end of the handler procedure reverses these steps toreturn program control to the interrupted program. Exceptions do not returnerror codes in real-address mode. The interrupt vector table is an array of4-byte entries (see Figure 21-2). Each entry consists of a far pointer to ahandler procedure, made up of a segment selector and an offset. The processorscales the interrupt or exception vector by 4 to obtain an offset into theinterrupt table. Following reset, the base of the interrupt vector table islocated at physical address 0 and its limit is set to 3FFH. In the Intel 8086processor, the base address and limit of the interrupt vector table cannot bechanged. In the later IA-32 processors, the base address and limit of theinter- rupt vector table are contained in the IDTR register and can be changedusing the LIDT instruction. reverses [rɪˈvɜːsɪz] : 反转 处理程序末尾的 IRET 指令反转这些步骤，将程序控制权返回给被中断的程序。 在实地址模式下，异常不会返回错误代码。 中断向量表是一个 4 字节条目的数组（见图 21-2）。 每个条目都包含一个指向处理程序过程的远指针，该指针由段选择器和偏移量组成。 处理器将中断或异常向量 乘 4 以获得中断表中的偏移量。 复位后，中断向量表的基址位于物理地址 0，其限制设置为 3FFH。 在Intel 8086处理器中，中断向量表的基地址和限制是不能改变的。 在后来的 IA-32 处理器中，中断向量表的基址和限制包含在 IDTR 寄存器中，并且可以使用 LIDT 指令进行更改。(For backward compatibility to Intel 8086 processors, the default base addressand limit of the interrupt vector table should not be changed.) （为了向后兼容 Intel 8086 处理器，不应更改中断向量表的默认基址和限制。）Table 21-1 shows the interrupt and exception vectors that can be generated inreal-address mode and virtual-8086 mode, and in the Intel 8086 processor. SeeChapter 6, “Interrupt and Exception Handling”, for a description of theexception conditions. 表 21-1 显示了在实地址模式和虚拟 8086 模式以及 Intel 8086 处理器中可以生成的中断和异常向量。有关异常情况的描述，请参见第 6 章“interrupt and exception handling”。21.2 VIRTUAL-8086 MODEVirtual-8086 mode is actually a special type of a task that runs in protectedmode. When the operating-system or executive switches to a virtual-8086-modetask, the processor emulates an Intel 8086 processor. The execution environmentof the processor while in the 8086-emulation state is the same as is describedin Section 21.1, “Real-Address Mode” for real-address mode, including theextensions. The major difference between the two modes is that in virtual-8086mode the 8086 emulator uses some protected-mode services (such as theprotected-mode interrupt and exception-handling and paging facilities). Virtual-8086 模式实际上是一种在保护模式下运行的特殊任务类型。 当操作系统或执行程序切换到虚拟 8086 模式任务时，处理器将模拟 Intel 8086 处理器。处理器在 8086 仿真状态下的执行环境与第 21.1 节“real address mode”中描述的实地址模式相同，包括扩展。 两种模式之间的主要区别在于，在虚拟 8086 模式下，8086 仿真器使用一些保护模式服务（例如保护模式中断、异常处理和分页功能）。As in real-address mode, any new or legacy program that has been assembledand/or compiled to run on an Intel 8086 processor will run in avirtual-8086-mode task. And several 8086 programs can be run asvirtual-8086-mode tasks concurrently with normal protected-mode tasks, usingthe processor’s multitasking facilities. concurrently [kənˈkʌrəntli]: 同时 与实地址模式一样，任何已组装和/或编译以在 Intel 8086 处理器上运行的new/legacy程序都将在虚拟 8086 模式任务中运行。 使用处理器的多任务处理功能，多个 8086 程序可以作为虚拟 8086 模式任务与正常保护模式任务同时运行。 NOTE: In the real-address mode, vector 13 is the segment overrun exception. Inprotected and virtual-8086 modes, this exception covers allgeneral-protection error conditions, including traps to the virtual-8086monitor from virtual-8086 mode. 在实地址模式下，向量13是段溢出异常。 在受保护模式和虚拟 8086 模式下，此异常涵盖所有general-proctection错误情况，包括从virtual-8086 模式到virtual-8086监视器的陷阱。 21.2.1 Enabling Virtual-8086 ModeThe processor runs in virtual-8086 mode when the VM (virtual machine) flag inthe EFLAGS register is set. This flag can only be set when the processorswitches to a new protected-mode task or resumes virtual-8086 mode via an IRETinstruction. 当 EFLAGS 寄存器中的 VM（虚拟机）标志被设置时，处理器以virtual-8086 模式运行。 仅当处理器切换到新的protected-mode task 或通过 IRET 指令恢复虚拟 8086 模式时，才能设置此标志。System software cannot change the state of the VM flag directly in the EFLAGSregister (for example, by using the POPFD instruction). Instead it changes theflag in the image of the EFLAGS register stored in the TSS or on the stackfollowing a call to an interrupt- or exception-handler procedure. For example,software sets the VM flag in the EFLAGS image in the TSS when first creating avirtual-8086 task. 系统软件无法直接更改 EFLAGS 寄存器中 VM 标志的状态（例如，通过使用 POPFD 指令）。相反，它会在调用中断或异常处理程序过程后更改存储在 TSS 或堆栈中的 EFLAGS 寄存器映像中的标志。 例如，当首次创建虚拟 8086 任务时，软件会在 TSS 的 EFLAGS image中设置 VM 标志。The processor tests the VM flag under three general conditions: When loading segment registers, to determine whether to use 8086-styleaddress translation. 当加载段寄存器时，确定是否使用8086风格的地址转换。 When decoding instructions, to determine which instructions are not supportedin virtual-8086 mode and which instructions are sensitive to IOPL sensitive: 敏感 解码指令时，确定哪些指令在 virtual-8086 模式下不支持以及哪些指令对 IOPL sensitive When checking privileged instructions, on page accesses, or when performingother permission checks. (Virtual-8086 mode always executes at CPL 3.) 检查特权指令、页面访问或执行其他权限检查时。 （虚拟 8086 模式始终在 CPL 3 上执行。） 21.2.2 Structure of a Virtual-8086 TaskA virtual-8086-mode task consists of the following items: A 32-bit TSS for the task. The 8086 program. A virtual-8086 monitor. 8086 operating-system services.The TSS of the new task must be a 32-bit TSS, not a 16-bit TSS, because the16-bit TSS does not load the most-significant word of the EFLAGS register,which contains the VM flag. All TSS’s, stacks, data, and code used to handleexceptions when in virtual-8086 mode must also be 32-bit segments. 新任务的 TSS 必须是 32 位 TSS，而不是 16 位 TSS，因为 16 位 TSS 不加载 EFLAGS 寄存器的最高有效字，该寄存器包含 VM 标志。 在虚拟 8086 模式下用于处理异常的所有 TSS、堆栈、数据和代码也必须是 32 位段。 RFLAGS.VM (Bit 17) The processor enters virtual-8086 mode to run the 8086 program and returns toprotected mode to run the virtual- 8086 monitor. 处理器进入虚拟8086模式以运行8086程序，并返回到保护模式以运行虚拟8086 monitor。The virtual-8086 monitor is a 32-bit protected-mode code module that runs at aCPL of 0. The monitor consists of initialization, interrupt- andexception-handling, and I/O emulation procedures that emulate a personalcomputer or other 8086-based platform. Typically, the monitor is either part ofor closely associated with the protected-mode general-protection (#GP)exception handler, which also runs at a CPL of 0. As with any protected-modecode module, code-segment descriptors for the virtual-8086 monitor must existin the GDT or in the task’s LDT. The virtual-8086 monitor also may needdata-segment descriptors so it can examine the IDT or other parts of the 8086program in the first 1 MByte of the address space. The linear addresses above10FFEFH are available for the monitor, the operating system, and other systemsoftware. closely: 紧密的, 接近的examine [ɪɡˈzæmɪn]: 检查审查 virtual-8086 monitor是一个 32 位保护模式code module，运行于 CPL 0 。监视器由初始化、中断和异常处理以及模拟个人计算机或其他 8086-based platform 的 I/O 模拟程序组成。通常，监视器是受保护模式通用保护 (#GP) 异常处理程序的一部分或与之密切相关，该异常处理程序也在 CPL 为 0 时运行。与任何受保护模式代码模块一样， virtual-8086 监视器必须存在于 GDT 或任务的 LDT 中。 virtual-8086 监视器还可能需要数据段描述符，以便它可以检查地址空间前 1 MB 中的 IDT 或 8086 程序的其他部分。 10FFEFH 以上的线性地址可供显示器、操作系统和其他系统软件使用。The 8086 operating-system services consists of a kernel and/or operating-systemprocedures that the 8086 program makes calls to. These services can beimplemented in either of the following two ways: 8086 操作系统服务由 8086 程序调用的内核和/或操作系统过程组成。 这些服务可以通过以下两种方式实现： They can be included in the 8086 program. This approach is desirable foreither of the following reasons: desirable [dɪˈzaɪərəbl]: 可取的 它们可以包含在 8086 程序中。 由于以下任一原因，这种方法是可取的： The 8086 program code modifies the 8086 operating-system services. 8086程序代码修改8086操作系统服务。 There is not sufficient development time to merge the 8086 operating-systemservices into main operating system or executive. sufficient [səˈfɪʃnt] : 足够的 没有足够的开发时间将 8086 操作系统服务合并到主操作系统或执行程序中。 They can be implemented or emulated in the virtual-8086 monitor. This approachis desirable for any of the following reasons: 它们可以在virtual-8086 监视器中实现或模拟。 由于以下任一原因，这种方法是可取的： The 8086 operating-system procedures can be more easily coordinated amongseveral virtual-8086 tasks. coordinateda [koʊˈɔːrdɪneɪtɪd] : 使协调; 使相配合; 8086 操作系统程序可以更轻松地在多个虚拟 8086 任务之间进行协调。 Memory can be saved by not duplicating 8086 operating-system procedure codefor several virtual-8086 tasks. 通过不为多个虚拟 8086 任务复制 8086 操作系统过程代码，可以节省内存。 The 8086 operating-system procedures can be easily emulated by calls to themain operating system or executive. 通过调用主操作系统或执行程序可以轻松模拟 8086 操作系统程序。 The approach chosen for implementing the 8086 operating-system services mayresult in different virtual-8086- mode tasks using different 8086operating-system services. 选择用于实现 8086 操作系统服务的方法可能会因使用不同 8086 操作系统服务的导致virtual-8086-mode task 不同21.2.3 Paging of Virtual-8086 TasksEven though a program running in virtual-8086 mode can use only 20-bit linearaddresses, the processor converts these addresses into 32-bit linear addressesbefore mapping them to the physical address space. If paging is being used, the8086 address space for a program running in virtual-8086 mode can be paged andlocated in a set of pages in physical address space. If paging is used, it istransparent to the program running in virtual-8086 mode just as it is for anytask running on the processor. done B, before done A: done B, 然后在 done A 即使在virtual-8086 模式下运行的程序只能使用 20 位线性地址，处理器也会将这些地址转换为 32 位线性地址，然后再将它们映射到物理地址空间。 如果正在使用分页，则可以对在虚拟 8086 模式下运行的程序的 8086 地址空间进行分页并将其定位在物理地址空间中的一组页面中。 如果使用分页，则它对于在虚拟 8086 模式下运行的程序是透明的，就像对于处理器上运行的任何任务一样。Paging is not necessary for a single virtual-8086-mode task, but paging isuseful or necessary in the following situations: 对于单个虚拟 8086 模式任务来说，分页不是必需的，但在以下情况下分页是有用或必要的： When running multiple virtual-8086-mode tasks. Here, paging allows the lower1 MByte of the linear address space for each virtual-8086-mode task to bemapped to a different physical address location. 运行多个虚拟 8086 模式任务时。 这里，分页允许将每个虚拟 8086 模式任务的线性地址空间的低 1 MB 映射到不同的物理地址位置。 When emulating the 8086 address-wraparound that occurs at 1 MByte. When using8086-style address translation, it is possible to specify addresses largerthan 1 MByte. These addresses automatically wraparound in the Intel 8086processor (see Section 21.1.1, “Address Translation in Real-Address Mode”).If any 8086 programs depend on address wraparound, the same effect can beachieved in a virtual-8086-mode task by mapping the linear addresses between100000H and 110000H and linear addresses between 0 and 10000H to the samephysical addresses. 当模拟 1 MB 处发生的 8086 address-wraparound时。 当使用 8086 类型的地址转换时，可以指定大于 1 MB 的地址。 这些地址在 Intel 8086 处理器中自动 wraparound（请参见第 21.1.1 节“实地址模式下的地址转换”）。如果任何8086程序依赖于addresswraparound, 则通过将100000H和110000H之间的线性地址以及0和10000H之间的线性地址映射到相同的物理地址，可以在虚拟8086模式任务中实现相同的效果。 When sharing the 8086 operating-system services or ROM code that is common toseveral 8086 programs running as different 8086-mode tasks. 当共享作为不同 8086 模式任务运行的多个 8086 程序所共用的 8086 操作系统服务或ROM 代码时。 When redirecting or trapping references to memory-mapped I/O devices. 当重定向或捕获对 memory-mapped I/O 设备的引用时。 21.2.4 Protection within a Virtual-8086 TaskProtection is not enforced between the segments of an 8086 program. Either ofthe following techniques can be used to protect the system software running ina virtual-8086-mode task from the 8086 program: enforced [ɪnˈfɔːst]: 强迫的, 强制性的 8086 程序的段之间不强制执行保护。 可以使用以下任一技术来保护在 virtual-8086 模式任务中运行的系统软件免受 8086 程序的影响： Reserve the first 1 MByte plus 64 KBytes of each task’s linear address spacefor the 8086 program. An 8086 processor task cannot generate addressesoutside this range. 为 8086 程序保留每个任务的前 1 MB 加上 64 KB 的线性地址空间。 8086 处理器task无法生成此范围之外的地址。 Use the U/S flag of page-table entries to protect the virtual-8086 monitorand other system software in the virtual-8086 mode task space. When theprocessor is in virtual-8086 mode, the CPL is 3. Therefore, an 8086 processorprogram has only user privileges. If the pages of the virtual-8086 monitorhave supervisor privilege, they cannot be accessed by the 8086 program. 使用页表条目的 U/S 标志来保护virtual-8086 monitor和virtual-8086 模式任务空间中的其他系统软件。 当处理器处于virtual-8086模式时，CPL为3。因此，8086处理器程序仅具有用户权限。 如果虚拟 8086 监视器的页面具有管理员权限，则 8086 程序无法访问它们。 21.2.5 Entering Virtual-8086 ModeFigure 21-3 summarizes the methods of entering and leaving virtual-8086 mode.The processor switches to virtual-8086 mode in either of the followingsituations: 图21-3总结了进入和离开虚拟8086模式的方法。 在以下任一情况下，处理器会切换到虚拟 8086 模式： Task switch when the VM flag is set to 1 in the EFLAGS register image storedin the TSS for the task. Here the task switch can be initiated in either oftwo ways: 当任务的 TSS 中存储的 EFLAGS 寄存器映像中的 VM 标志设置为 1 时，进行任务切换。 这里可以通过两种方式启动任务切换： A CALL or JMP instruction. An IRET instruction, where the NT flag in the EFLAGS image is set to 1. IRET, with EFLAGS image ‘s NT flag == 1 Return from a protected-mode interrupt or exception handler when the VM flagis set to 1 in the EFLAGS register image on the stack. 当堆栈上 EFLAGS 寄存器映像中的 VM 标志设置为 1 时，从保护模式中断或异常处理程序返回。 When a task switch is used to enter virtual-8086 mode, the TSS for thevirtual-8086-mode task must be a 32-bit TSS. (If the new TSS is a 16-bit TSS,the upper word of the EFLAGS register is not in the TSS, causing the processorto clear the VM flag when it loads the EFLAGS register.) The processor updatesthe VM flag prior to loading the segment registers from their images in the newTSS. The new setting of the VM flag determines whether the processor interpretsthe contents of the segment registers as 8086-style segment selectors orprotected-mode segment selectors. When the VM flag is set, the segmentregisters are loaded from the TSS, using 8086-style address translation to formbase addresses. 当使用任务切换进入virtual-8086模式时，virtual-8086模式任务的TSS必须是32位TSS。（如果新的TSS是16位TSS，则EFLAGS寄存器的高位不在TSS中，导致处理器在加载EFLAGS寄存器时清除VM标志。）处理器在加载之前更新VM标志 该段将image注册到新的 TSS 中。 VM 标志的新设置确定处理器是否将段寄存器的内容解释为 8086 型段选择器或保护模式段选择器。 当VM标志被设置时，段寄存器从TSS加载，使用8086类型的地址转换来形成基地址。 否则, 段寄存器的值将在段选择子 indicate 的 段描述符中加载 See Section 21.3, “Interrupt and Exception Handling in Virtual-8086 Mode”, forinformation on entering virtual- 8086 mode on a return from an interrupt orexception handler. 有关从中断或异常处理程序返回时进入虚拟 8086 模式的信息，请参见第 21.3 节“虚拟 8086 模式中的中断和异常处理”。21.2.6 Leaving Virtual-8086 ModeThe processor can leave the virtual-8086 mode only through an interrupt orexception. The following are situations where an interrupt or exception willlead to the processor leaving virtual-8086 mode (see Figure 21-3): 处理器只能通过中断或异常离开虚拟 8086 模式。 以下是中断或异常将导致处理器离开虚拟 8086 模式的情况（见图 21-3）： The processor services a hardware interrupt generated to signal thesuspension of execution of the virtual-8086 application. This hardwareinterrupt may be generated by a timer or other external mechanism. Uponreceiving the hardware interrupt, the processor enters protected mode andswitches to a protected-mode (or another virtual-8086 mode) task eitherthrough a task gate in the protected-mode IDT or through a trap or interruptgate that points to a handler that initiates a task switch. A task switchfrom a virtual-8086 task to another task loads the EFLAGS register from theTSS of the new task. The value of the VM flag in the new EFLAGS determines ifthe new task executes in virtual-8086 mode or not. 处理器处理硬件中断，该中断产生用于作为暂停执行虚拟 8086 应用程序的信号。 该硬件中断可以由定时器或其他外部机制产生。 收到硬件中断后，处理器进入保护模式并且切换到一个 protected-mode task(或者 另一个 virtual-8086 mode task).方式有两种, 要么通过保护模式 IDT 中的任务门或通过指向handler的陷阱或中断门,该handler 会发起一个task switch. 从virtual-8086 任务到另一个任务的任务切换会从新任务的 TSS 加载 EFLAGS 寄存器。新 EFLAGS 中 VM 标志的值决定新任务是否以 virtual-8086 模式执行。 The processor services an exception caused by code executing the virtual-8086task or services a hardware interrupt that “belongs to” the virtual-8086task. Here, the processor enters protected mode and services the exception orhardware interrupt through the protected-mode IDT (normally through aninterrupt or trap gate) and the protected-mode exception- andinterrupt-handlers. The processor may handle the exception or interruptwithin the context of the virtual 8086 task and return to virtual-8086 modeon a return from the handler procedure. The processor may also execute a taskswitch and handle the exception or interrupt in the context of another task. 处理器为执行 virtual-8086 任务的代码引起的异常提供服务，或者为“属于”virtual-8086任务的硬件中断提供服务。 这里，处理器进入保护模式并通过保护模式IDT（通常通过中断或陷阱门）以及保护模式异常和中断处理程序来处理异常或硬件中断。 处理器可以在虚拟8086任务的上下文中处理异常或中断，并在从处理程序过程返回时返回到虚拟8086模式。 处理器还可以执行任务切换并在另一个任务的上下文中处理异常或中断。 The processor services a software interrupt generated by code executing inthe virtual-8086 task (such as a software interrupt to call a MS-DOS*operating system routine). The processor provides several methods of handlingthese software interrupts, which are discussed in detail in Section 21.3.3,“Class 3—Software Interrupt Handling in Virtual-8086 Mode”. Most of theminvolve the processor entering protected mode, often by means of ageneral-protection (#GP) exception. In protected mode, the processor can sendthe interrupt to the virtual-8086 monitor for handling and/or redirect theinterrupt back to the application program running in virtual-8086 mode taskfor handling. 处理器为 virtual-8086 任务中执行的代码生成的软件中断提供服务（例如调用 MS-DOS* 操作系统例程的软件中断）。 处理器提供了几种处理这些软件中断的方法，这些方法在第 21.3.3 节“Class 3—Software Interrupt Handling in Virtual-8086 Mode”中详细讨论。其中大多数涉及处理器进入保护模式，通常是通过通用保护（#GP）异常的方式。 在保护模式下，处理器可以将中断发送到virtual-8086监视器以进行处理和/或将中断重定向回以virtual-8086模式任务运行的应用程序以进行处理。 IA-32 processors that incorporate the virtual mode extension (enabled withthe VME flag in control register CR4) are capable of redirectingsoftware-generated interrupts back to the program’s interrupt handlerswithout leaving virtual-8086 mode. See Section 21.3.3.4, “Method 5: SoftwareInterrupt Handling”, for more information on this mechanism. incorporate /ɪnˈkɔːpəreɪt/: 合并,包含 包含虚拟模式扩展（通过控制寄存器 CR4 中的 VME 标志启用）的 IA-32 处理器能够将软件生成的中断重定向回程序的中断处理程序，而无需离开虚拟 8086 模式。 有关此机制的更多信息，请参见第 21.3.3.4 节“方法 5：softwareinterrupt handling”。 A hardware reset initiated by asserting the RESET or INIT pin is a specialkind of interrupt. When a RESET or INIT is signaled while the processor is invirtual-8086 mode, the processor leaves virtual-8086 mode and entersreal-address mode. 通过置位 RESET 或 INIT 引脚启动的硬件复位是一种特殊类型的中断。 当处理器处于虚拟 8086 模式时发出 RESET 或 INIT 信号时，处理器将离开虚拟 8086 模式并进入实地址模式。 Execution of the HLT instruction in virtual-8086 mode will cause ageneral-protection (GP#) fault, which the protected-mode handler generallysends to the virtual-8086 monitor. The virtual-8086 monitor then determinesthe correct execution sequence after verifying that it was entered as aresult of a HLT execution. 在虚拟 8086 模式下执行 HLT 指令将导致general-protection (GP#) fault，保护模式处理程序通常会将其发送到 virtual-8086 监视器。 然后，virtual-8086 监视器在验证它是作为 HLT 执行的结果输入后确定正确的执行顺序。 See Section 21.3, “Interrupt and Exception Handling in Virtual-8086 Mode”, forinformation on leaving virtual-8086 mode to handle an interrupt or exceptiongenerated in virtual-8086 mode.有关离开 virtual-8086 模式以处理 virtual-8086 模式中生成的中断或异常的信息，请参见第 21.3 节 “Virtual-8086 模式下的中断和异常处理”。21.2.7 Sensitive InstructionsWhen an IA-32 processor is running in virtual-8086 mode, the CLI, STI, PUSHF,POPF, INT n, and IRET instructions are sensitive to IOPL. The IN, INS, OUT, andOUTS instructions, which are sensitive to IOPL in protected mode, are notsensitive in virtual-8086 mode. 当 IA-32 处理器运行在virtual-8086 模式下时，CLI、STI、PUSHF、POPF、INT n 和 IRET 指令对 IOPL 敏感。 IN、INS、OUT 和 OUTS 指令在保护模式下对 IOPL 敏感，但在virtual-8086 模式下不敏感。The CPL is always 3 while running in virtual-8086 mode; if the IOPL is lessthan 3, an attempt to use the IOPL-sensitive instructions listed abovetriggers a general-protection exception (#GP). These instructions are sensitiveto IOPL to give the virtual-8086 monitor a chance to emulate the facilitiesthey affect. 在virtual-8086 模式下运行时，CPL 始终为 3； 如果 IOPL 小于 3，则尝试使用上面列出的 IOPL 敏感指令会触发一般保护异常 (#GP)。 这些指令对 IOPL 敏感，使virtual-8086 监视器有机会模拟它们影响的facilities。21.2.8 Virtual-8086 Mode I/ONULL21.3 INTERRUPT AND EXCEPTION HANDLING IN VIRTUAL-8086 MODENULL21.4 PROTECTED-MODE VIRTUAL INTERRUPTSNULL" }, { "title": "async pf -- GUP change", "url": "/posts/async-pf-gup-change/", "categories": "kvm, async_pf", "tags": "para_virt", "date": "2024-04-16 15:00:00 +0800", "snippet": "参考代码 该部分代码, mail list中和commit中内容不同, 而且mail list中也没有提到为什么当时没有全部合入, 我们先以mail list 为准: [KVM: Add host swap event notifications for PV guest][v7] 后续, 我们在另一篇文章中详细介绍: [link][2] 遗留问题 get_user_pag...", "content": "参考代码 该部分代码, mail list中和commit中内容不同, 而且mail list中也没有提到为什么当时没有全部合入, 我们先以mail list 为准: [KVM: Add host swap event notifications for PV guest][v7] 后续, 我们在另一篇文章中详细介绍: [link][2] 遗留问题 get_user_pages_noio 该部分代码来自于: [Add get_user_pages() variant that fails if major fault is required.][3]社区并没有合入该patch该patch 引入了 get_user_page()的noio 版的变体, 他只在不需要 major fault的情况下才会成功的 get page reference 以上来自该patch的commit message This patch add get_user_pages() variant that only succeeds if gettinga reference to a page doesn't require major fault. 具体改动是: 增加了新的flow flag和fault flag flow/fault flag细节 flow flag: 该flag会作为gup_flags入参传入__get_user_pages(), 可以作为一些约束e.g., FOLL_WRITE 表明要要get的pages必须是可写入的 会对比vma-&gt;vm_flags 是否是可写的. 另外, 会在 follow page期间, 影响handle_mm_fault的fault flag e.g., FOLL_WRITE-&gt;FAULT_FLAG_WRITE 新增的flag为: +#define FOLL_MINOR\t0x20\t/* do only minor page faults */ 表明要在 follow page期间, 只允许处理 minor page fault.(不能处理swapin) fault flag: 用于handle_mm_fault()入参flags, 表明本次fault的类型.该参数可以用于一些优化和约束. e.g. 如果检测到没有FAULT_FLAG_WRITE, 说明是read access, 而又是第一次建立映射, 那么可以建立和zero page的映射 do_anonymous_page(){ ... if (!(flags &amp; FAULT_FLAG_WRITE)) { entry = pte_mkspecial(pfn_pte(my_zero_pfn(address), vma-&gt;vm_page_prot)); page_table = pte_offset_map_lock(mm, pmd, address, &amp;ptl); if (!pte_none(*page_table)) goto unlock; goto setpte; } ...} 新增的flag为: +#define FAULT_FLAG_MINOR\t0x08\t/* Do only minor fault */ 是由 FOLL_MINOR转化而来, 和其作用一样. vm fault reason: 其实, 还有一些flag, 是用于表示handle_mm_fault()的原因, 例如这里我们遇到的:VM_FAULT_MAJOR, 实际上是表明, 该函数返回失败是由于该fault是 major fault. 我们下面会结合代码改动详细看下, 这些flag的应用 关于处理这些\"flags\"具体代码流程改动: \"flags\"具体代码改动 __get_user_pages-&gt;handle_mm_fault @@ -1441,10 +1441,13 @@ int __get_user_pages(struct task_struct *tsk, struct mm_struct *mm, \t\t\tcond_resched(); \t\t\twhile (!(page = follow_page(vma, start, foll_flags))) { \t\t\t\tint ret;+\t\t\t\tunsigned int fault_fl =+\t\t\t\t\t((foll_flags &amp; FOLL_WRITE) ?+\t\t\t\t\tFAULT_FLAG_WRITE : 0) |+\t\t\t\t\t((foll_flags &amp; FOLL_MINOR) ?+\t\t\t\t\tFAULT_FLAG_MINOR : 0); -\t\t\t\tret = handle_mm_fault(mm, vma, start,-\t\t\t\t\t(foll_flags &amp; FOLL_WRITE) ?-\t\t\t\t\tFAULT_FLAG_WRITE : 0);+\t\t\t\tret = handle_mm_fault(mm, vma, start, fault_fl); 可以看到, 这里会将 FOLL_WRITE-&gt;FAULT_FLAG_WRITE, FOLL_MINOR-&gt;FAULT_FLAG_MINOR do_swap_page @@ -2648,6 +2670,9 @@ static int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma, \tdelayacct_set_flag(DELAYACCT_PF_SWAPIN); \tpage = lookup_swap_cache(entry); \tif (!page) {+\t\tif (flags &amp; FAULT_FLAG_MINOR)+\t\t\treturn VM_FAULT_MAJOR | VM_FAULT_ERROR;+ \t\tgrab_swap_token(mm); /* Contend for token _before_ read-in */ \t\tpage = swapin_readahead(entry, \t\t\t\t\tGFP_HIGHUSER_MOVABLE, vma, address); 如果没有在swap cache中找到, 说明该page 被swap出去, 并且被free了,需要swapin, 这时, 如果有FAULT_FLAG_MINOR,表明只允许处理 minor fault, 而swapin, 属于 major fault, 不允许处理, 需要返回错误, 同时把错误原因:VM_FAULT_MAJOR也返回 filemap_fault 和swapcache 相对应的还有pagecache diff --git a/mm/filemap.c b/mm/filemap.cindex 3d4df44..ef28b6d 100644--- a/mm/filemap.c+++ b/mm/filemap.c@@ -1548,6 +1548,9 @@ int filemap_fault(struct vm_area_struct *vma, struct vm_fault *vmf) \t\t\tgoto no_cached_page; \t\t} \t} else {+\t\tif (vmf-&gt;flags &amp; FAULT_FLAG_MINOR)+\t\t\treturn VM_FAULT_MAJOR | VM_FAULT_ERROR; 也是同样的处理逻辑 handle_mm_fault() --return-&gt;__get_user_pages() --handle retval @@ -1452,6 +1455,8 @@ int __get_user_pages(struct task_struct *tsk, struct mm_struct *mm, \t\t\t\t\tif (ret &amp; \t\t\t\t\t (VM_FAULT_HWPOISON|VM_FAULT_SIGBUS)) \t\t\t\t\t\treturn i ? i : -EFAULT;+\t\t\t\t\telse if (ret &amp; VM_FAULT_MAJOR)+\t\t\t\t\t\treturn i ? i : -EFAULT; \t\t\t\t\tBUG(); 如果是 FAULT_MAJOR并且没有get 到 page, 直接返回错误 新增get_user_pages_noio接口 +int get_user_pages_noio(struct task_struct *tsk, struct mm_struct *mm,+\t\tunsigned long start, int nr_pages, int write, int force,+\t\tstruct page **pages, struct vm_area_struct **vmas)+{+\tint flags = FOLL_TOUCH | FOLL_MINOR;++\tif (pages)+\t\tflags |= FOLL_GET;+\tif (write)+\t\tflags |= FOLL_WRITE;+\tif (force)+\t\tflags |= FOLL_FORCE;++\treturn __get_user_pages(tsk, mm, start, nr_pages, flags, pages, vmas);+}+EXPORT_SYMBOL(get_user_pages_noio);+ 不多解释, 在该接口中将FOLL_MINOR置位. NOTE 所以该部分patch的主要作用就是增加了get_user_pages_noio(), 使其, 只处理minor fault(假如只是alloc page, 那属于minor fault), 但是不能处理major fault.(例如swapin, pagecachein) 目前个人理解是这样, 之后还需要看下page fault的相关细节 我们接下来看下, async pf 框架是如何利用上面GUP noio接口的usage of GUP noio in async pf我们先看下, 触发 async pf 的入口, 我们上面介绍到, 在 EPT violation hook 中会去start 该work, 过程如下:tdp_page_fault@@ -2609,7 +2655,11 @@ static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, mmu_seq = vcpu-&gt;kvm-&gt;mmu_notifier_seq; smp_rmb(); //==(1)==- pfn = gfn_to_pfn(vcpu-&gt;kvm, gfn);+ //==(2)==+ if (try_async_pf(vcpu, gfn, gpa, &amp;pfn))+ return 0;++ /* mmio */ if (is_error_pfn(pfn)) 将现有的gfn_to_pfn(), 替换为try_async_pf(), 之前的gfn2pfn接口, 是必须async, 也就是上面提到的使用GUP时, 可以处理 MAJOR FAULT. 而现在替换为了 try_async_pf(), 打算尝试执行 async pf(也可能不需要, 例如遇到了 MINOR FAULT. 我们接下来会详细看下该接口 tdp_page_fault()中会执行try_async_pf()该函数返回值为true, 表示已经做了async pf,所以现在还不能去 map GPA-&gt;HPA. 需要该接口直接返回. 对于HALT的处理方式, 则是让vcpublock. 我们下面会看到.old version of gfn_to_pfn在看try_async_pf之前, 我们先看下合入patch之前的 gfn_to_pfn接口.gfn_to_pfn { __gfn_to_pfn(atomic=false) { gfn_to_hva { gfn_to_hva_many gfn_to_hva_memslot } //gfn_to_hva ----- 上面 gfn_to_hva 下面 hva_to_pfn ----- hva_to_pfn(atomic=false) { if (atomic) __get_user_pages_fast() else //走这个路径 get_user_pages_fast() } //hva_to_pfn } //__gfn_to_pfn} //gfn_to_pfn关于__get_user_pages_fast和get_user_pages_fast的不同, 主要是: __get_user_pages_fast()是atomic版本(IRQ-safe), 主要是因为get_user_pages_fast需要走slow path, 这个时候需要开中断, 而__get_user_pages_fast则不需要, 所以其过程是关中断的, 也可以在关中断的情况下执行 由于上面提到的原因, get_user_pages_fast 并不保存中断状态, 所以该函数必须在开中断的情况下执行 两个接口前的代码注释, 以及大致流程 get_user_pages_fast /** * get_user_pages_fast() - pin user pages in memory * @start: starting user address * @nr_pages: number of pages from start to pin * @write: whether pages will be written to * @pages: array that receives pointers to the pages pinned. * Should be at least nr_pages long. * * Attempt to pin user pages in memory without taking mm-&gt;mmap_sem. * If not successful, it will fall back to taking the lock and * calling get_user_pages(). * * &gt; 在不拿mm-&gt;mmap_sem 锁的情况下, 尝试将user page pin 到memory中. * &gt; 如果没有成功, 它将fall back 来拿锁, 并且调用get_user_pages() * * Returns number of pages pinned. This may be fewer than the number * requested. If nr_pages is 0 or negative, returns 0. If no pages * were pinned, returns -errno. * * &gt; 返回 page 被 pinned数量. 他可能比所需的数量要少. 如果nr_pages是0, * 或者是负数, 返回0. 如果没有page被pinned, 返回 -errno */get_user_pages_fast { local_irq_disable fast_path { //仅去看有多少page present } local_irq_enable get_user_page} __get_user_pages_fast /* * Like get_user_pages_fast() except its IRQ-safe in that it won't fall * back to the regular GUP. * * 除了他的 IRQ-safe(因为他不会fall bak to regular GUP), 其他的和 * get_user_pages_fast()一样 */__get_user_pages_fast { local_irq_save() fast_path local_irq_restore()} 这里, 我们不再过多展开GUP的代码, 总之, 早期的__get_user_pages_fast不会fall back到 regular GPU(slow pathget_user_pages)我们再来看下其改动,gfn_to_pfn-&gt;get_user_pages新增gfn_to_pfn_async(), 替代现有流程中的gfn_to_pfn(), 该接口新增了async:bool*参数, 该参数是一个iparam &amp;&amp; oparam iparam: 表示只走get_user_page fast path也就是__get_user_pages_fast oparam: 表示是否需要做 async pf具体改动如下+pfn_t gfn_to_pfn_async(struct kvm *kvm, gfn_t gfn, bool *async)+{+ return __gfn_to_pfn(kvm, gfn, false, async);+}+EXPORT_SYMBOL_GPL(gfn_to_pfn_async);+ pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn) {- return __gfn_to_pfn(kvm, gfn, false); //可以走slow path+ return __gfn_to_pfn(kvm, gfn, false, NULL); } EXPORT_SYMBOL_GPL(gfn_to_pfn);/* * !!MY NOTE!! * __gfn_to_pfn { * ... * //先初始化为false * if (async) * *async = false; * ... * return hva_to_pfn(kvm, addr, atomic, async); * } */我们再来看下hva_to_pfn改动:+static pfn_t hva_to_pfn(struct kvm *kvm, unsigned long addr, bool atomic,+ bool *async) { struct page *page[1];- int npages;+ int npages = 0; pfn_t pfn;- if (atomic)+ /* we can do it either atomically or asynchronously, not both */+ BUG_ON(atomic &amp;&amp; async); //==(1)==+ if (atomic || async) npages = __get_user_pages_fast(addr, 1, 1, page);- else {+ //==(2)==+ if (unlikely(npages != 1) &amp;&amp; !atomic) { might_sleep();- npages = get_user_pages_fast(addr, 1, 1, page);+ + if (async) {+ down_read(&amp;current-&gt;mm-&gt;mmap_sem);+ npages = get_user_pages_noio(current, current-&gt;mm,+ \t\t\t addr, 1, 1, 0, page, NULL);+ up_read(&amp;current-&gt;mm-&gt;mmap_sem);+ } else+ npages = get_user_pages_fast(addr, 1, 1, page); } if (unlikely(npages != 1)) { struct vm_area_struct *vma; if (atomic) goto return_fault_page; down_read(&amp;current-&gt;mm-&gt;mmap_sem); if (is_hwpoison_address(addr)) { up_read(&amp;current-&gt;mm-&gt;mmap_sem); get_page(hwpoison_page); return page_to_pfn(hwpoison_page); } vma = find_vma(current-&gt;mm, addr); if (vma == NULL || addr &lt; vma-&gt;vm_start || !(vma-&gt;vm_flags &amp; VM_PFNMAP)) { //==(3)==+ if (async &amp;&amp; !(vma-&gt;vm_flags &amp; VM_PFNMAP) &amp;&amp;+ (vma-&gt;vm_flags &amp; VM_WRITE))+ *async = true; up_read(&amp;current-&gt;mm-&gt;mmap_sem);return_fault_page: get_page(fault_page); return page_to_pfn(fault_page); } pfn = ((addr - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT) + vma-&gt;vm_pgoff; up_read(&amp;current-&gt;mm-&gt;mmap_sem); BUG_ON(!kvm_is_mmio_pfn(pfn)); } else pfn = page_to_pfn(page[0]); return pfn;} 如果是async, 会先尝试走一次fast path, 如果成功了, 则 npages = 1 如果上面fast path 失败了, 并且还是async, 则会执行get_user_pages_noio()该函数上面也提到过, 该过程不处理 MAJOR fault. 这里说明失败了, 也就是因为遇到了MAJOR fault, 所以该fault 并没有handle,需要异步处理, 那么就将 oparam async 置为true. 这里我们先不关心这里的几个判断条件, 之后放到GUP/内存管理的章节中介绍 遗留问题 " }, { "title": "async pf -- gfn2hva cache", "url": "/posts/async-pf-gfn2hva-cache/", "categories": "kvm, async_pf", "tags": "para_virt", "date": "2024-04-16 15:00:00 +0800", "snippet": "introduce该功能仅通过name就可以得知, 是为了缓存gfn(gpa)到hva的映射. 但是这个映射关系不是一直存在么, 为什么设计看似比较复杂的机制, 我们一步步来看user memory region support我们知道,在比较早期的版本, kvm 创建memslot APIkvm_vm_ioctl KVM_SET_USER_MEMORY_REGION就已经支持了对 use...", "content": "introduce该功能仅通过name就可以得知, 是为了缓存gfn(gpa)到hva的映射. 但是这个映射关系不是一直存在么, 为什么设计看似比较复杂的机制, 我们一步步来看user memory region support我们知道,在比较早期的版本, kvm 创建memslot APIkvm_vm_ioctl KVM_SET_USER_MEMORY_REGION就已经支持了对 user memory region申请的支持. 涉及patch Patch: KVM: Support assigning userspace memory to the guest mail list: mail 这里只展示下, 用户态入参的数据结构: +/* for KVM_SET_USER_MEMORY_REGION */+struct kvm_userspace_memory_region {+ __u32 slot;+ __u32 flags;+ __u64 guest_phys_addr;+ __u64 memory_size; /* bytes */+ __u64 userspace_addr; /* start of the userspace allocated memory */+}; 可以看到最后一个参数为, userspace_addr在该接口的支持下, qemu为guest申请memory region同时, 该内存也作为qemu进程的 anon memoryspace 存在, qemu 可以通过管理匿名页的方式, 对该地址空间进行管理, kernel其他组建, 也可以通过操作这部分匿名页来操作guest memory, 例如: memory reclaim, memory migrate…所以基本流程是: guest先通过mmap申请匿名内存空间, 调用完成后, kernel已经申请好了这段内存空间的virtual base address(userspace_addr) 调用kvm_vm_ioctl -- KVM_SET_USER_MEMORY_REGION 执行完成时, kvm 已经建立起了 hva-&gt;gpa映射关系 guest访问gpa, 触发EPT violation trap kvm, kvm 调用 get_user_page() 申请page, 并建立hva-&gt;hpa 同时, 创建gpa-&gt;hpa的mmu pgtables(if guest enable EPT feature, is ept pgtable)re-set memory region所以, 既然两者在set memory region 接口中就已经确立了映射关系, 那是不是只是保存下[hva, gpa]就相当于cache了.大部分情况下是这样, 但是在下面情况下, hva-&gt;gpa的映射关系会改变 guest call kvm_vm_ioctl -- KVM_SET_USER_MEMORY_REGION, map [hva-&gt;gpa]-&gt;[hpa_1, gpa] guest call kvm_vm_ioctl -- KVM_SET_USER_MEMORY_REGION again, remap [hva-&gt;gpa]-&gt;[hpa_2, gpa]在这种情况下, 映射关系就改变了.所以, 我们需要一个机制, 在re-set memory region 发生之后, 我们再次 “access this cache”时, 需要“invalidate this cache”, 这就是该patch要做的事情.patch 细节change of struct struct kvm_memslots { \tint nmemslots;+\tu32 generation; \tstruct kvm_memory_slot memslots[KVM_MEMORY_SLOTS + \t\t\t\t\tKVM_PRIVATE_MEM_SLOTS]; }; generation: 表示当前memslots 的generation, 也就是latest.+struct gfn_to_hva_cache {+\tu32 generation;+\tgpa_t gpa;+\tunsigned long hva;+\tstruct kvm_memory_slot *memslot;+}; generation: 获取cache时, memslots的generation, 可能是old的. memslot: 当前gpa所属的memslot, 主要用于 mark_page_dirty 作者在这里有个小心思, 因为这个地方没有该成员也没有关系, 也可以执行, mark_page_dirty, 但是在执行时, 需要每次获取memslot, 所以作者想了,既然缓存, 那为什么不缓存多一些, 将memslot也缓存 interfacecache init+int kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,+\t\t\t gpa_t gpa)+{+\tstruct kvm_memslots *slots = kvm_memslots(kvm);+\tint offset = offset_in_page(gpa);+\tgfn_t gfn = gpa &gt;&gt; PAGE_SHIFT;++\tghc-&gt;gpa = gpa;+\t//==(1)==+\tghc-&gt;generation = slots-&gt;generation;+\tghc-&gt;memslot = __gfn_to_memslot(kvm, gfn);+\tghc-&gt;hva = gfn_to_hva_many(ghc-&gt;memslot, gfn, NULL);+\t//==(2)==+\tif (!kvm_is_error_hva(ghc-&gt;hva))+\t\tghc-&gt;hva += offset;+\telse+\t\treturn -EFAULT;++\treturn 0;+} 将此时slots-&gt;generation赋值给ghc-&gt;generation 错误情况暂时不看.write cache+int kvm_write_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,+\t\t\t void *data, unsigned long len)+{+\tstruct kvm_memslots *slots = kvm_memslots(kvm);+\tint r;++\t//==(1)==+\tif (slots-&gt;generation != ghc-&gt;generation)+\t\tkvm_gfn_to_hva_cache_init(kvm, ghc, ghc-&gt;gpa);+\t+\tif (kvm_is_error_hva(ghc-&gt;hva))+\t\treturn -EFAULT;++\t//==(2)==+\tr = copy_to_user((void __user *)ghc-&gt;hva, data, len);+\tif (r)+\t\treturn -EFAULT;+\t//==(3)==+\tmark_page_dirty_in_slot(kvm, ghc-&gt;memslot, ghc-&gt;gpa &gt;&gt; PAGE_SHIFT);++\treturn 0;+} 如果slots-&gt;generation 和当前cache generation(ghc-&gt;generation)不一致, 说明 该cache已经是stale的了, 需要update, 那就直接重新init cache(调用 kvm_gfn_to_hva_cache_init()) 将数据写入hva 该接口是新增的, 为mark_page_dirty()的变体. -void mark_page_dirty(struct kvm *kvm, gfn_t gfn)+void mark_page_dirty_in_slot(struct kvm *kvm, struct kvm_memory_slot *memslot,+\t\t\t gfn_t gfn) {-\tstruct kvm_memory_slot *memslot;--\tmemslot = gfn_to_memslot(kvm, gfn); \tif (memslot &amp;&amp; memslot-&gt;dirty_bitmap) { \t\tunsigned long rel_gfn = gfn - memslot-&gt;base_gfn; @@ -1284,6 +1325,14 @@ void mark_page_dirty(struct kvm *kvm, gfn_t gfn) \t} } +void mark_page_dirty(struct kvm *kvm, gfn_t gfn)+{+\tstruct kvm_memory_slot *memslot;++\tmemslot = gfn_to_memslot(kvm, gfn);+\tmark_page_dirty_in_slot(kvm, memslot, gfn);+} 该变体较mark_page_dirty()来说, 主要是增加memslot参数. 原因在介绍数据结构的时候已经说明 该功能和dirty log功能相关, guest可以通过bitmap知道那些page是dirty的,在热迁移的时候会用到, 这里不过多介绍 那slots-&gt;generation 什么时候改变的呢bump slots-&gt;generationdiff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.cindex db58a1b..45ef50c 100644--- a/virt/kvm/kvm_main.c+++ b/virt/kvm/kvm_main.cint __kvm_set_memory_region(struct kvm *kvm, struct kvm_userspace_memory_region *mem, int user_alloc){skip_lpage: //==(1)== if (!npages) { r = -ENOMEM; slots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL); if (!slots) goto out_free; memcpy(slots, kvm-&gt;memslots, sizeof(struct kvm_memslots)); if (mem-&gt;slot &gt;= slots-&gt;nmemslots) slots-&gt;nmemslots = mem-&gt;slot + 1;+\t\tslots-&gt;generation++; slots-&gt;memslots[mem-&gt;slot].flags |= KVM_MEMSLOT_INVALID; old_memslots = kvm-&gt;memslots; rcu_assign_pointer(kvm-&gt;memslots, slots); synchronize_srcu_expedited(&amp;kvm-&gt;srcu); /* From this point no new shadow pages pointing to a deleted * memslot will be created. * * validation of sp-&gt;gfn happens in: * - gfn_to_hva (kvm_read_guest, gfn_to_pfn) * - kvm_is_visible_gfn (mmu_check_roots) */ kvm_arch_flush_shadow(kvm); kfree(old_memslots); } //==(2)== r = kvm_arch_prepare_memory_region(kvm, &amp;new, old, mem, user_alloc); if (r) goto out_free; /* map the pages in iommu page table */ if (npages) { r = kvm_iommu_map_pages(kvm, &amp;new); if (r) goto out_free; } r = -ENOMEM; slots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL); if (!slots) goto out_free; memcpy(slots, kvm-&gt;memslots, sizeof(struct kvm_memslots)); if (mem-&gt;slot &gt;= slots-&gt;nmemslots) slots-&gt;nmemslots = mem-&gt;slot + 1;+ slots-&gt;generation++; 说明不是内存, 有可能是mmio, 这里变动memslots, 需要使用rcu机制,这样可以保证在无锁的情况下把这个动作完成 normal 内存因为更新到了memslots, 说明hva-&gt;gpa的关系有改变, 所以需要更新generationhistory of change avi在Re: [PATCH v2 02/12] Add PV MSR to enable asynchronous page faults delivery. 中提到, 目前这一版本patch可能在遇到 memslots 情况下, 会有问题 原文 &gt; +static int kvm_pv_enable_async_pf(struct kvm_vcpu *vcpu, u64 data)&gt; +{&gt; +\tu64 gpa = data&amp; ~0x3f;&gt; +\tint offset = offset_in_page(gpa);&gt; +\tunsigned long addr;&gt; +&gt; +\taddr = gfn_to_hva(vcpu-&gt;kvm, gpa&gt;&gt; PAGE_SHIFT);&gt; +\tif (kvm_is_error_hva(addr))&gt; +\t\treturn 1;&gt; + //只初始化一次&gt; +\tvcpu-&gt;arch.apf_data = (u32 __user*)(addr + offset);&gt; +&gt; +\t/* check if address is mapped */&gt; +\tif (get_user(offset, vcpu-&gt;arch.apf_data)) {&gt; +\t\tvcpu-&gt;arch.apf_data = NULL;&gt; +\t\treturn 1;&gt; +\t}&gt; What if the memory slot arrangement changes? This needs to be revalidated (and gfn_to_hva() called again).&gt; validate &lt;==&gt; invalidate&gt; revalidate : 重新生效, 重新验证 在[PATCH v3 07/12] Maintain memslot version number和[PATCH v3 08/12] Inject asynchronous page fault into a guest if page is swapped out.中, 作者引入了该功能, 不过该功能是嵌入到async pf 功能中, 并非独立接口 代码 diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.hindex 600baf0..3f5ebc2 100644--- a/include/linux/kvm_host.h+++ b/include/linux/kvm_host.h@@ -163,6 +163,7 @@ struct kvm { \tspinlock_t requests_lock; \tstruct mutex slots_lock; \tstruct mm_struct *mm; /* userspace tied to this vm */+\tu32 memslot_version; \tstruct kvm_memslots *memslots; \tstruct srcu_struct srcu;@@ -364,7 +364,9 @@ struct kvm_vcpu_arch { \tunsigned long singlestep_rip; \tu32 __user *apf_data;+\tu32 apf_memslot_ver; \tu64 apf_msr_val;+\tu32 async_pf_id; };+static int apf_put_user(struct kvm_vcpu *vcpu, u32 val)+{+\tif (unlikely(vcpu-&gt;arch.apf_memslot_ver !=+\t\t vcpu-&gt;kvm-&gt;memslot_version)) {+\t\tu64 gpa = vcpu-&gt;arch.apf_msr_val &amp; ~0x3f;+\t\tunsigned long addr;+\t\tint offset = offset_in_page(gpa);++\t\taddr = gfn_to_hva(vcpu-&gt;kvm, gpa &gt;&gt; PAGE_SHIFT);+\t\tvcpu-&gt;arch.apf_data = (u32 __user*)(addr + offset);+\t\tif (kvm_is_error_hva(addr)) {+\t\t\tvcpu-&gt;arch.apf_data = NULL;+\t\t\treturn -EFAULT;+\t\t}+\t}++\treturn put_user(val, vcpu-&gt;arch.apf_data);+} 可以看到, 相当于引入了两个version, 并在apf_put_user()时,比对两个version. 作者在Re: [PATCH v4 08/12] Inject asynchronous page fault into a guest if page is swapped out.回答了为什么不使用kvm_write_guest Q: why A: want to cache gfn_to_hva_translation avi 在Re: [PATCH v5 08/12] Inject asynchronous page fault into a guest if page is swapped out.建议将该功能剥离, 因为这个功能很好,其他代码也可以用. 原文 This nice cache needs to be outside apf to reduce complexity for reviewers and since it is useful for others. Would be good to have memslot-cached kvm_put_guest() and kvm_get_guest(). 作者在Re: [PATCH v5 08/12] Inject asynchronous page fault into a guest if page is swapped out.首次提供该接口. Marcelo Tosatti 在Re: [PATCH v6 04/12] Add memory slot versioning and use it to provide fast guest write interface提到两个问题: 在kvm_gfn_to_hva_cache_init中使用gfn_to_memslot 获取memslot, 可能会造成如下问题 自己的理解 thread1 thread2 guestkvm_write_guest_cached kvm_gfn_to_hva_cache_init { __kvm_set_memory_region slots = kvm_memslots(kvm) { rcu_dereference_check } ghc-&gt;generation = slots-&gt;generation; slots-&gt;generation++; ghc-&gt;memslot = gfn_to_memslot( slots, gfn) { rcu_dereference_check { //may have a gp rcu_assign_pointer( kvm-&gt;memslots, slots); } ghc-&gt;hva = gfn_to_hva_many( ghc-&gt;memslot, gfn, NULL); }copy_to_user(); kvm_arch_commit_memory_region do_munmap access apf reason, LOSS 这样可能会导致在这个函数中, 前面和后面获取的信息来自于不同的memslots, 个人认为, 不仅仅是这样, 还可能导致, thread2 因为中间释放了rcu, 导致其流程和thread1有race, 最终导致 本次copy_to_user()数据丢失 作者在下一版patch中将gfn_to_memslot修改为了__gfn_to_memslot, 该接口不会在使用rcu_dereference_check " }, { "title": "embedding markdown in HTML tag", "url": "/posts/embedding_markdown_in_html_tag/", "categories": "markdown", "tags": "markdown, html-details", "date": "2024-04-12 10:53:00 +0800", "snippet": "ISSUEWhen I try to use the markdown syntax in &lt;details&gt; HTML tags, for example,code blocks, encounter the problem of code blocks that cannot be rendered.The source code is as follows:&lt;deta...", "content": "ISSUEWhen I try to use the markdown syntax in &lt;details&gt; HTML tags, for example,code blocks, encounter the problem of code blocks that cannot be rendered.The source code is as follows:&lt;details&gt;&lt;summary&gt; aaa &lt;/summary&gt;` ` `cppint a = 1;` ` `&lt;/details&gt; ` char seems unable to be translated in code block, so I added spacecharacters between themIt will display in browser as follows:aaa```cppint a = 1;```SOLUTIONThis issue seems to occur in the kramdowm markup process, rather than in GFM: GFM allows embedding HTML inside Markdown Embedding Markdown in Jekyll HTMLAnd in the link Embedding Markdown in Jekyll HTML, a solution is provided:Use &lt;details markdown=\"1\"&gt; instead &lt;details&gt;.It will run as expected aaa int a = 1; " }, { "title": "async pf", "url": "/posts/async-pf/", "categories": "kvm, async_pf", "tags": "para_virt", "date": "2024-04-10 12:20:00 +0800", "snippet": "introduce在支持EPT的架构中, 对于GVA-&gt;HPA一般有两段映射: GVA-&gt;GPA GPA-&gt;HPA而host kernel (kvm) 需要关心的是 GPA-&gt;HPA的映射, 需要host做的事情主要有以下几个: 捕捉相关 VM-exit event (EPT violation), 得到 GPA 分配page 建立映射关系(当然这个映射关系...", "content": "introduce在支持EPT的架构中, 对于GVA-&gt;HPA一般有两段映射: GVA-&gt;GPA GPA-&gt;HPA而host kernel (kvm) 需要关心的是 GPA-&gt;HPA的映射, 需要host做的事情主要有以下几个: 捕捉相关 VM-exit event (EPT violation), 得到 GPA 分配page 建立映射关系(当然这个映射关系, 不止是GPA-&gt;HPA的mmu pgtable, 还有 HVA – GPA,在这里不展开, 总之分配好具体的page(分配HPA), 以及为其建立好 mmu pgtable, 就可以完成该事件的处理)如下图:图示graphviz-ae4ca25f7bf30b9e61f0f3b83bc12338digraph G { subgraph cluster_guest { EPT_violation [ label=&quot;EPT mapping(GPA-&gt;HPA) \\nloss, trigger EPT violation&quot; ] &quot;access a VA&quot;-&gt; &quot;trigger PF in VMX\\n non-root operation&quot;-&gt; &quot;mapping GVA-&gt;GPA\\n in GUEST #PF hook&quot;-&gt; &quot;fixup #PF, continue \\naccess this VA&quot;-&gt; EPT_violation label=&quot;guest&quot; } subgraph cluster_host { &quot;find HVA though GPA&quot;-&gt; &quot;GUP(HVA)&quot;-&gt; &quot;mapping GPA-&gt;HPA&quot; label=&quot;host&quot; } &quot;mapping GPA-&gt;HPA&quot;-&gt;&quot;access a VA&quot; [ label=&quot;fixup EPT violation,\\n VM entry&quot; ] EPT_violation-&gt;&quot;find HVA though GPA&quot; [ label=&quot;VM exit&quot; ]}Gcluster_guestguestcluster_hosthostEPT_violationEPT mapping(GPA&#45;&gt;HPA) loss, trigger EPT violationfind HVA though GPAfind HVA though GPAEPT_violation&#45;&gt;find HVA though GPAVM exitaccess a VAaccess a VAtrigger PF in VMX\\n non&#45;root operationtrigger PF in VMX non&#45;root operationaccess a VA&#45;&gt;trigger PF in VMX\\n non&#45;root operationmapping GVA&#45;&gt;GPA\\n in GUEST #PF hookmapping GVA&#45;&gt;GPA in GUEST #PF hooktrigger PF in VMX\\n non&#45;root operation&#45;&gt;mapping GVA&#45;&gt;GPA\\n in GUEST #PF hookfixup #PF, continue \\naccess this VAfixup #PF, continue access this VAmapping GVA&#45;&gt;GPA\\n in GUEST #PF hook&#45;&gt;fixup #PF, continue \\naccess this VAfixup #PF, continue \\naccess this VA&#45;&gt;EPT_violationGUP(HVA)GUP(HVA)find HVA though GPA&#45;&gt;GUP(HVA)mapping GPA&#45;&gt;HPAmapping GPA&#45;&gt;HPAGUP(HVA)&#45;&gt;mapping GPA&#45;&gt;HPAmapping GPA&#45;&gt;HPA&#45;&gt;access a VAfixup EPT violation, VM entry但是, 已经建立好映射的页面, 也是qemu进程的虚拟地址空间(匿名页), 是可以被swap out,当被swap out后, GUEST 访问该HPA对应的 GVA/GPA时, 仍然会触发 EPT violation. 这时还会再走一次 VM-exit, 而且也需要完成上面所述的三件事, 其中第二件:分配page, 需要swap in之前被swap out的page, 路径比较长, 如下:VM-exit handle_ept_violation kvm_mmu_page_fault tdp_page_fault gfn_to_pfn hva_to_pfn get_user_pages --slow pathget_user_pages会走到slow path, 由于会走swap in流程, 所以该过程执行较慢. 所以大佬们就想着能不能让其异步执行, 然后让vcpu先不complete 造成 EPT violation 的 instruction, 去干别的事情, 等page present后, 再去执行该指令. 另外将 get_user_pages 让一个 dedicated thread 去完成,这样, 对于虚拟机来说, 就相当于搞了一个额外的 硬件, 专门去处理 swap in, 解放了vcpu的算力. NOTE 大家思考下, 如果要达到该目的, 一定是让GUEST有意无意的 sche out 造成 EPT violation的进程,该上面流程总结如下:流程图graphviz-f8c81926c9687c237f8513f3a6fb3624digraph G { subgraph cluster_host { style=&quot;filled&quot; color=&quot;#693886699&quot; subgraph cluster_host_dedicated_thread { do_slow_path [ shape=&quot;note&quot; label=&quot;I&#39;m a delicated \\nthread, Like a \\nspecial hardware, \\nsharing the \\npressure of VCPU&quot; ] label=&quot;dedicated thread&quot; have_got_page_success [ label=&quot;work in done!\\n tell the guest&quot; ] do_slow_path-&gt;have_got_page_success [ label=&quot;a. get page, swap in...&quot; fontcolor=&quot;blue&quot; color=&quot;blue&quot; ] } subgraph cluster_host_kvm_vcpu_thread { ept_violation_handler [ label=&quot;ept violation handler&quot; ] dont_do_slow_path [ shape=&quot;note&quot; label=&quot;I don&#39;t want \\nhandle slow path, \\nit will speed\\nto much time&quot; ] tell_guest_sched_out [ shape=&quot;note&quot; label=&quot;work is doing,\\nneed wait\\n a a bit time,\\n let guest do\\n other things&quot; ] dont_do_slow_path -&gt;tell_guest_sched_out [ label=&quot;4.let guest \\ndo other thing&quot; fontcolor=&quot;green&quot; color=&quot;green&quot; ] ept_violation_handler-&gt; dont_do_slow_path [ label=&quot;2.find page swap out&quot; fontcolor=&quot;green&quot; color=&quot;green&quot; ] label=&quot;host kvm vcpu thread&quot; } label = &quot;host&quot; } subgraph cluster_guest { style=&quot;filled&quot; color=&quot;#77323456&quot; subgraph cluster_trigger_ept_violation_task { task1_access_a_memory [ label=&quot;acesss a memory\\n address [BEG]&quot; color=&quot;white&quot; style=&quot;filled&quot; ] label=&quot;TASK1 trigger ept vioaltion&quot; } subgraph cluster_sched_in_task2 { task2_run_a_time [ label=&quot;task2_run_a_time&quot; ] label=&quot;task2&quot; } label=&quot;guest&quot; } dont_do_slow_path-&gt;do_slow_path [ label=&quot;3. start a work \\nto do it&quot; fontcolor=&quot;green&quot; color=&quot;green&quot; ] task1_access_a_memory -&gt; ept_violation_handler [ label=&quot;1.page NOT present,\\ntrigger EPT violation&quot; fontcolor=&quot;green&quot; color=&quot;green&quot; ] have_got_page_success -&gt; task2_run_a_time [ label=&quot;b. page NOT present\\n SCHED IN&quot; fontcolor=&quot;blue&quot; color=&quot;blue&quot; ] tell_guest_sched_out -&gt; task1_access_a_memory [ label=&quot;5. page NOT present\\n SCHED OUT&quot; fontcolor=&quot;green&quot; color=&quot;green&quot; ] task2_run_a_time-&gt;task1_access_a_memory [ label=&quot;c. sched in\\n task1&quot; fontcolor=&quot;blue&quot; color=&quot;blue&quot; ] task1_access_a_memory-&gt;task2_run_a_time [ label=&quot;6.sched out\\n task1&quot; fontcolor=&quot;green&quot; color=&quot;green&quot; ]}Gcluster_hosthostcluster_host_dedicated_threaddedicated threadcluster_host_kvm_vcpu_threadhost kvm vcpu threadcluster_guestguestcluster_trigger_ept_violation_taskTASK1 trigger ept vioaltioncluster_sched_in_task2task2do_slow_pathI&#39;m a delicated thread, Like a special hardware, sharing the pressure of VCPUhave_got_page_successwork in done! tell the guestdo_slow_path&#45;&gt;have_got_page_successa. get page, swap in...task2_run_a_timetask2_run_a_timehave_got_page_success&#45;&gt;task2_run_a_timeb. page NOT present SCHED INept_violation_handlerept violation handlerdont_do_slow_pathI don&#39;t want handle slow path, it will speedto much timeept_violation_handler&#45;&gt;dont_do_slow_path2.find page swap outdont_do_slow_path&#45;&gt;do_slow_path3. start a work to do ittell_guest_sched_outwork is doing,need wait a a bit time, let guest do other thingsdont_do_slow_path&#45;&gt;tell_guest_sched_out4.let guest do other thingtask1_access_a_memoryacesss a memory address [BEG]tell_guest_sched_out&#45;&gt;task1_access_a_memory5. page NOT present SCHED OUTtask1_access_a_memory&#45;&gt;ept_violation_handler1.page NOT present,trigger EPT violationtask1_access_a_memory&#45;&gt;task2_run_a_time6.sched out task1task2_run_a_time&#45;&gt;task1_access_a_memoryc. sched in task1由上图可见, 引入async pf 的逻辑是让其能够在触发 EPT violation后, 能够让VCPU 调度到另外一个task, 从而阻塞触发 EPT violation 的进程执行. 为了达到这一目的, 做了以下改动: VCPU 线程在执行get_user_page()时, 仅执行fast path, 如果page 不是present的, 该接口直接返回, 而剩下的工作, 则交给另外一个dedicated thread 去做 KVM 会通过一些方式, 让 GUEST 执行调度, 从而避免再次执行触发EPT violation的指令. 而dedicatedthread 完成了swap in 的动作后, 会通知guest再次唤醒该之前调度出去的进程代码细节para virt interface一般的半虚拟化实现往往都有一下几个特征: use CPUID report this feature use MSR transparent less information, e.g. : a share memory address enable/disable use a share memory transparent more information而 para virt async PF 也是这样实现的. 在v1 Add shared memory hypercall to PV Linux guest版本中, 作者以hypercall的方式实现了半虚拟化, 但是avi在随后建议(link)使用MSR来替代 hypercall, 因为该方式在INIT和热迁移流程中有现成的 save/restore 接口 原文如下: Better to set this up as an MSR (with bit zero enabling, bits 1-5 features, and 64-byte alignment). This allows auto-reset on INIT and live migration using the existing MSR save/restore infrastructure.最好将其设置为MSR - bit 0: enabling - bit 1-5: features - 64-byte alignment他允许在INIT时 auto-reset, 并且可以使用现有的 MSR save/restore infrastructure 完成热迁移 接口流程图图示graphviz-5816c8c78422729ad7411897407bd311digraph G { subgraph cluster_host { host_page_not_present [ label=&quot;initiate page \\nnot present\\n APF&quot; color=&quot;red&quot; ] host_page_present [ label=&quot;initiate page \\nhave been\\n present APF&quot; color=&quot;green&quot; ] label=&quot;host&quot; } subgraph cluster_guest { pf_handler [ label=&quot;page fault handler&quot; ] guest_invoke_task [ label=&quot;invoke task&quot; ] label=&quot;guest&quot; } cpuid [ shape=&quot;record&quot; label=&quot;cpuid:\\n KVM_FEATURE_ASYNC_PF:\\n 1&quot; ] subgraph cluster_msr { msr_bit_map [ shape=&quot;record&quot; label=&quot;{ bit0\\n enable bit\\n value 1(enable)| bit 1-5\\n reserved\\n value 0| &lt;shm_gpa&gt;bit 63-6\\n 64-byte aligned GPA\\n value 0xabc }&quot; ] label=&quot;MSR_KVM_ASYNC_PF_EN&quot; } subgraph cluster_cr2 { token [ shape=&quot;record&quot; label=&quot;token: \\n unique id&quot; ] label=&quot;cr2&quot; } subgraph cluster_shm { shm [ shape=&quot;record&quot; label=&quot;APF reason&quot; ] label=&quot;share memory&quot; } cpuid-&gt;msr_bit_map [ arrowhead=&quot;none&quot; style=&quot;dashed&quot; label=&quot;indicate apf \\nfeature \\navailable,\\n so access\\n MSR_KVM_ASYNC_PF_EN \\nis valid&quot; ] msr_bit_map:shm_gpa-&gt;shm [ arrowhead=&quot;none&quot; style=&quot;dashed&quot; label=&quot;point base GPA \\nof this share\\n memory&quot; ] host_page_not_present-&gt;token [ label=&quot;1. initiate page \\nnot present \\nAPF, generate \\ntoken write \\nto CR2&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] host_page_not_present-&gt;shm [ label=&quot;2. update apf \\nreason to 1&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] host_page_not_present-&gt;pf_handler [ label=&quot;3. inject page \\nnot present \\n#APF&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] pf_handler-&gt;shm [ label=&quot;4. get reason \\nfrom shm:\\n PAGE \\nnot present\\n&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] pf_handler-&gt;token [ label=&quot;5. get token \\nfrom cr2,\\nbind sched\\n out thread\\n and token\\n&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] pf_handler-&gt;guest_invoke_task [ label=&quot;6. sched\\n out it&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] host_page_present-&gt;token [ label=&quot;a. initiate page \\n present APF, \\nwrite prev \\ntoken write \\nto CR2&quot; color=&quot;green&quot; fontcolor=&quot;green&quot; ] host_page_present-&gt;shm [ label=&quot;b. update apf \\nreason to 2&quot; color=&quot;green&quot; fontcolor=&quot;green&quot; ] host_page_present-&gt;pf_handler [ label=&quot;c. inject page \\n present \\n#APF&quot; color=&quot;green&quot; fontcolor=&quot;green&quot; ] pf_handler-&gt;shm [ label=&quot;d. get reason \\nfrom shm:\\nPAGE present &quot; color=&quot;green&quot; fontcolor=&quot;green&quot; ] pf_handler-&gt;token [ label=&quot;e. get token \\nfrom cr2,\\n find sched \\nout thread \\nby token&quot; color=&quot;green&quot; fontcolor=&quot;green&quot; ] pf_handler-&gt;guest_invoke_task [ label=&quot;f.wakeup it&quot; color=&quot;green&quot; fontcolor=&quot;green&quot; ]}Gcluster_hosthostcluster_guestguestcluster_msrMSR_KVM_ASYNC_PF_ENcluster_cr2cr2cluster_shmshare memoryhost_page_not_presentinitiate page not present APFpf_handlerpage fault handlerhost_page_not_present&#45;&gt;pf_handler3. inject page not present #APFtokentoken: unique idhost_page_not_present&#45;&gt;token1. initiate page not present APF, generate token write to CR2shmAPF reasonhost_page_not_present&#45;&gt;shm2. update apf reason to 1host_page_presentinitiate page have been present APFhost_page_present&#45;&gt;pf_handlerc. inject page present #APFhost_page_present&#45;&gt;tokena. initiate page present APF, write prev token write to CR2host_page_present&#45;&gt;shmb. update apf reason to 2guest_invoke_taskinvoke taskpf_handler&#45;&gt;guest_invoke_task6. sched out itpf_handler&#45;&gt;guest_invoke_taskf.wakeup itpf_handler&#45;&gt;token5. get token from cr2,bind sched out thread and tokenpf_handler&#45;&gt;tokene. get token from cr2, find sched out thread by tokenpf_handler&#45;&gt;shm4. get reason from shm: PAGE not presentpf_handler&#45;&gt;shmd. get reason from shm:PAGE present cpuidcpuid: KVM_FEATURE_ASYNC_PF: 1msr_bit_mapbit0 enable bit value 1(enable)bit 1&#45;5 reserved value 0bit 63&#45;6 64&#45;byte aligned GPA value 0xabccpuid&#45;&gt;msr_bit_mapindicate apf feature available, so access MSR_KVM_ASYNC_PF_EN is validmsr_bit_map:shm_gpa&#45;&gt;shmpoint base GPA of this share memory图中描述了host, guest在处理async pf时, 对寄存器/share memory 的操作从图中可以看出, 会涉及到cpuid, MSR_KVM_ASYNC_PF_EN, share memory, 由于async pf 的实现,需要注入#PF, 所以还会涉及 CR2cpuid新增半虚拟化cpuid bit: KVM_FEATURE_ASYNC_PFdiff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h+#define KVM_FEATURE_ASYNC_PF\t\t4关于该bit的文档说明diff --git a/Documentation/kvm/cpuid.txt b/Documentation/kvm/cpuid.txt+KVM_FEATURE_ASYNC_PF || 4 || async pf can be enabled by+ || || writing to msr 0x4b564d02大致意思是, 该cpuid如果时能, 表示可以通过write to MSR (0x4b564d02) 来enable async pfMSR – share memaddr &amp;&amp; enable bitdiff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h+#define MSR_KVM_ASYNC_PF_EN 0x4b564d02文档说明:diff --git a/Documentation/kvm/msr.txt b/Documentation/kvm/msr.txt+ MSR_KVM_ASYNC_PF_EN: 0x4b564d02+ data: Bits 63-6 hold 64-byte aligned physical address of a+ 64 byte memory area which must be in guest RAM and must be+ zeroed. Bits 5-1 are reserved and should be zero. Bit 0 is 1+ when asynchronous page faults are enabled on the vcpu 0 when+ disabled. &gt; Bits 63-6 保存着 64-byte 对其的 一个64 byte memory area 的物理地址, &gt; 该memory area 必须是 guest RAM, 并且必须是被赋值为0. &gt; &gt; Bit 5-1 被reserved并且应该为0. &gt; &gt; 当 在 vcpu 0 启用 async pf enable async pf(当是disable时), &gt; Bit 0 是1该段主要介绍了MSR的 bit 组成: MSR bit Bit [63, 6]: a 64-byte aligned physical address Bit [5, 1]: reserved Bit 0 : enable bit 其实文档中还介绍了. share memory format 和 CR2, 但是为了方便阅读, 我们将拆分开到各个小节shared memory structure – APF reasondiff --git a/Documentation/kvm/msr.txt b/Documentation/kvm/msr.txt ...+ First 4 byte of 64 byte memory location will be written to by+ the hypervisor at the time of asynchronous page fault (APF)+ injection to indicate type of asynchronous page fault. Value+ of 1 means that the page referred to by the page fault is not+ present. Value 2 means that the page is now available. Disabling+ interrupt inhibits APFs. Guest must not enable interrupt+ before the reason is read, or it may be overwritten by another+ APF. Since APF uses the same exception vector as regular page+ fault guest must reset the reason to 0 before it does+ something that can generate normal page fault. If during page+ fault APF reason is 0 it means that this is regular page+ fault. &gt; 在 hypervisor 触发 APF 注入时, 4 byte memory location的前4个byte将被 &gt; 写入 来指示 APF 的类型. &gt; 1: page fault 涉及到的page 是 not present的. &gt; 2: page 现在已经 available &gt; 另外Disabling interrupt 将会 inhibits APF. &gt; &gt; Guest必须不能enable interrupt 在reason 被read之前, 否则可能会被另一个 &gt; APF覆盖. 因为 APF 使用 相同的 exception vector 作为 regular page &gt; fault, 所以在做可能生成normal page fault 的事情之前, guest 必须 reset &gt; reason to 0. 如果 在 page fault 期间, APF reason 为0, 他意味着这是一个 &gt; regular page fault.shared memory 一共有64 byte, 其中前4个byte(32 bit) 用来indicate apf type. hostkvm 在注入 apf之前会将type写入该地址.APF 有两种type(APF reason): 1: page is not present 2: not present page becomes available另外, 在处理APF时, guest和host有下面约束: 如果guest处于 disable interrupt, host不能注入apf guest必须在enable interrupt 之前, 处理完当前的apf guest必须在触发 normal #PF时, 处理完当前的apf, 并且reset reason to 0CR2diff --git a/Documentation/kvm/msr.txt b/Documentation/kvm/msr.txt ...+ During delivery of type 1 APF cr2 contains a token that will+ be used to notify a guest when missing page becomes+ available. When page becomes available type 2 APF is sent with+ cr2 set to the token associated with the page. There is special+ kind of token 0xffffffff which tells vcpu that it should wake+ up all processes waiting for APFs and no individual type 2 APFs+ will be sent. &gt; 在 type1 APF delivery 期间, cr2 包含了一个token, 当missing page &gt; becomes available, 该token将会用于通知guest. &gt; &gt; 当page becomes available, type2 APF 将会把 cr2 设置为和该page相关的 &gt; token. &gt; &gt; 这里有一个特殊的类型 token 0xffffffff, 他将告诉vcpu, 需要wakeup 所有 &gt; 等待APF的process 并且不会有单独的 type 2 APF 将会再发送 + If APF is disabled while there are outstanding APFs, they will+ not be delivered. &gt; 当 outstanding APFs时, 如果APF 被disabled, 他们将不会被delivered. + Currently type 2 APF will be always delivered on the same vcpu as+ type 1 was, but guest should not rely on that. &gt; 当前 type 2 APF 将始终在与type 1 相同的vcpu上deliver, 但是guest不应该依赖它.cr2 包含了一个token, 该token 用来唯一标识, 当前正在发生的APF 的 id. 但是其有一个特殊value 0xffffffff, 该值用来告诉vcpu, 需要wakeup所有的正在等待 APF (type 2) 的 进程. 并且不会有单独的type2再发送.另外还有几点约束和限制 如果还有 outstanding APFs 时, 如果 APF 被disable了, 他们将不会被deliver guest 不应该依赖 type2 APF 和 type1 APF在相同vcpu上deliver, 虽然目前是这样实现的. 大家可以思考下, 为什么要支持wake up all这样的API 可以想象一下热迁移场景. 当进行热迁移时, 我们先suspend vcpu, 然后迁移memory, 这时, 会等所有page swapin,然后在进行迁移, 但是这时, guest已经不能再去注入异常了, 只能等dest端在注入. 此时来到dest端, 这时所有的memory都是present的. 所以直接注入wakeup all就可以唤醒所有wait task.(当然, 也可能再此期间有swapout, 无非是再触发一次async pf)GUP change关于GUP 改动的细节我们放到link中介绍.STRUCT – host总体数据结构图比较简单, 如下: struct 结构图 graphviz-7842836ccacf1278b495c08c9dc5b30cdigraph G {\tsubgraph cluster_vcpu0 {\t\tkvm_vcpu0 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;{struct kvm_vcpu||&lt;queue&gt;queue|&lt;done&gt;done}&quot;\t\t]\t\tsubgraph cluster_uncomplete_work {\t\t\twork_uncomplete_1 [\t\t\t\tshape=&quot;record&quot;\t\t\t\tlabel=&quot;{kvm_vcpu_pf||&lt;queue&gt;queue|&lt;link&gt;link}&quot;\t\t\t]\t\t\twork_uncomplete_2 [\t\t\t\tshape=&quot;record&quot;\t\t\t\tlabel=&quot;{kvm_vcpu_pf||&lt;queue&gt;queue|&lt;link&gt;link}&quot;\t\t\t]\t\t\tlabel=&quot;uncomplete work&quot;\t\t}\t\tsubgraph cluster_done_work {\t\t\twork_done_1 [\t\t\t\tshape=&quot;record&quot;\t\t\t\tlabel=&quot;{kvm_vcpu_pf||&lt;queue&gt;queue|&lt;link&gt;link}&quot;\t\t\t]\t\t\twork_done_2 [\t\t\t\tshape=&quot;record&quot;\t\t\t\tlabel=&quot;{kvm_vcpu_pf||&lt;queue&gt;queue|&lt;link&gt;link}&quot;\t\t\t]\t\t\tlabel=&quot;done work&quot;\t\t}\t\tlabel = &quot;vcpu 0&quot;\t}\tkvm_vcpu0:queue-&gt;\t\twork_done_1:queue-&gt;\t\twork_done_2:queue-&gt;\t\twork_uncomplete_1:queue-&gt;\t\twork_uncomplete_2:queue [\t\tcolor=&quot;red&quot;\t]\tkvm_vcpu0:done-&gt;\t\twork_done_1:link-&gt;\t\twork_done_2:link [\t\tcolor=&quot;blue&quot;\t]\tsubgraph cluster_vcpu1 {\t\tkvm_vcpu1 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;struct kvm_vcpu&quot;\t\t]\t\twork_5 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;kvm_vcpu_pf&quot;\t\t]\t\twork_6 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;kvm_vcpu_pf&quot;\t\t]\t\tlabel = &quot;vcpu 1&quot;\t\tkvm_vcpu1-&gt;work_5-&gt;work_6\t}\tsubgraph cluster_vcpu2 {\t\tkvm_vcpu2 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;struct kvm_vcpu&quot;\t\t]\t\twork_7 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;kvm_vcpu_pf&quot;\t\t]\t\twork_8 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;kvm_vcpu_pf&quot;\t\t]\t\tlabel = &quot;vcpu 2&quot;\t\tkvm_vcpu2-&gt;work_7-&gt;work_8\t}\tsubgraph cluster_vcpu3 {\t\tkvm_vcpu3 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;struct kvm_vcpu&quot;\t\t]\t\twork_9 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;kvm_vcpu_pf&quot;\t\t]\t\twork_10 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;kvm_vcpu_pf&quot;\t\t]\t\tlabel = &quot;vcpu 3&quot;\t\tkvm_vcpu3-&gt;work_9-&gt;work_10\t}}Gcluster_vcpu0vcpu 0cluster_uncomplete_workuncomplete workcluster_done_workdone workcluster_vcpu1vcpu 1cluster_vcpu2vcpu 2cluster_vcpu3vcpu 3kvm_vcpu0struct kvm_vcpu queuedonework_done_1kvm_vcpu_pf queuelinkkvm_vcpu0:queue&#45;&gt;work_done_1:queuekvm_vcpu0:done&#45;&gt;work_done_1:linkwork_uncomplete_1kvm_vcpu_pf queuelinkwork_uncomplete_2kvm_vcpu_pf queuelinkwork_uncomplete_1:queue&#45;&gt;work_uncomplete_2:queuework_done_2kvm_vcpu_pf queuelinkwork_done_1:queue&#45;&gt;work_done_2:queuework_done_1:link&#45;&gt;work_done_2:linkwork_done_2:queue&#45;&gt;work_uncomplete_1:queuekvm_vcpu1struct kvm_vcpuwork_5kvm_vcpu_pfkvm_vcpu1&#45;&gt;work_5work_6kvm_vcpu_pfwork_5&#45;&gt;work_6kvm_vcpu2struct kvm_vcpuwork_7kvm_vcpu_pfkvm_vcpu2&#45;&gt;work_7work_8kvm_vcpu_pfwork_7&#45;&gt;work_8kvm_vcpu3struct kvm_vcpuwork_9kvm_vcpu_pfkvm_vcpu3&#45;&gt;work_9work_10kvm_vcpu_pfwork_9&#45;&gt;work_10 每个cpu有自己链表, 串起属于该cpu的async pf work, 其中有两条链. queue: 串起所有work done: 串起所有完成的work struct kvm_async_pf该数据结构主要用来描述上面提到的dedicated threadstruct kvm_async_pf { struct work_struct work; struct list_head link; struct list_head queue; struct kvm_vcpu *vcpu; struct mm_struct *mm; gva_t gva; unsigned long addr; struct kvm_arch_async_pf arch; struct page *page; bool done;}; work: dedicated thread实例, 使用 workqueue机制 link: 在patch中, 链接点主要有一个: vcpu 的work完成队列 queue: 用于链接该vcpu上的所有 kvm_async_pf gva: 触发EPT violation, 需要get_user_page_slow的 GVA addr: hva done: indicate该work完没完成 kvm_arch_async_pf: struct kvm_arch_async_pf { u32 token; gfn_t gfn;}; token: 该成员用于唯一标识一次async PF, 由kvm_vcpu.arch.apf.id和vcpu-&gt;vcpu_id综合计算得到. 在注入#PF时, 会当作 CR2 传入GUEST, 方便guest管理每一次的async PF. 上面说提到的kvm_async_pf-&gt;link,kvm_async_pf-&gt;queue所链接的队列, 如下:CHANGE of struct kvm_vcpu@@ -104,6 +125,15 @@ struct kvm_vcpu { gpa_t mmio_phys_addr; #endif+#ifdef CONFIG_KVM_ASYNC_PF+ struct {+ u32 queued;+ struct list_head queue;+ struct list_head done;+ spinlock_t lock;+ } async_pf;+#endif queue: 链接所有kvm_async_pf(work) done: 链接以完成的kvm_async_pf(work) lock: 队列锁change of struct kvm_vcpu_archstruct kvm_vcpu_arch { ...+ struct {+ bool halted;+ gfn_t gfns[roundup_pow_of_two(ASYNC_PF_PER_VCPU)];+ struct gfn_to_hva_cache data;+ u64 msr_val;+ u32 id;+ bool send_user_only;+ } apf; ...} 该数据结构变动涉及多个patch, 这里把最终的数据结构变动列出. halted: 表示是否因为async PF halt 了vcpu gfns : 这里做了一个数组, 用于记录所有现存的async pf work 的 gfn data: 相当于HVA-&gt;HPA的cache, 这个映射关系一直存在且不变(大多数情况下, 除非执行__kvm_set_memory_region更改映射关系), 该HPA 指向上面提到的 share memory 该部分被作者做成了一个通用功能, 相当于是 memslot-cached kvm_put_guest()and kvm_get_guest(). 我们放到另一篇文章中介绍. 主要介绍这个功能引入和其实现. msr_val: 记录guest设置的msr值 id: 记录下一个async pf work的id, 和kvm_vcpu-&gt;vcpu_id一起,唯一标识一次async PF send_user_only: 表示只有trigger EPT violation in guest user space, host才能做async PFSTRUCT - GUESTguest 数据结构主要是用于管理, 因为async PF 调度出去的task.数据结构图 数据结构图 graphviz-e889c62c04290bdb7d5685fb187da7d7digraph G { sleep_head [ shape=&quot;record&quot; label=&quot;{ kvm_task_sleep_head|| [0]| &lt;key0&gt;link(key0)|| [1]| &lt;key1&gt;link(key1)|| [2]| &lt;key2&gt;link(key2) }&quot; ] subgraph cluster_cpu0 { sleep_node0 [ shape=&quot;record&quot; label=&quot;{ kvm_task_sleep_node|| &lt;link&gt;link| token=[id=0, vcpu=0]| cpu=0| mm=mm_struct of task0| halted=false }&quot; ] sleep_node1 [ shape=&quot;record&quot; label=&quot;{ kvm_task_sleep_node|| &lt;link&gt;link| token=[id=1, vcpu=0]| cpu=0| mm=mm_struct of task1| halted=false }&quot; ] run_task_vcpu0 [ label=&quot;current task: task2&quot; shape=&quot;record&quot; color=&quot;red&quot; ] label=&quot;cpu0 RUNNING&quot; } subgraph cluster_cpu1 { run_task_vcpu1 [ label=&quot;current task: task4&quot; shape=&quot;record&quot; color=&quot;red&quot; ] sleep_node3 [ shape=&quot;record&quot; label=&quot;{ kvm_task_sleep_node|| &lt;link&gt;link| token=[id=0, vcpu=1]| cpu=1| mm=mm_struct of task3| halted=false }&quot; ] sleep_node4 [ shape=&quot;record&quot; label=&quot;{ kvm_task_sleep_node|| &lt;link&gt;link| token=[id=1, vcpu=1]| cpu=1| mm=mm_struct of task4| halted=true }&quot; color=&quot;red&quot; ] label=&quot;cpu1 HALT&quot; } sleep_head:key0-&gt;sleep_node0:link [ color=&quot;blue&quot; ] sleep_head:key1-&gt;sleep_node1:link [ color=&quot;gold&quot; ] sleep_head:key2-&gt; sleep_node3:link-&gt; sleep_node4:link [ color=&quot;green&quot; ] sleep_node4-&gt;run_task_vcpu1 [ arrowhead=none style=dashed ]}Gcluster_cpu0cpu0 RUNNINGcluster_cpu1cpu1 HALTsleep_headkvm_task_sleep_head [0]link(key0) [1]link(key1) [2]link(key2)sleep_node0kvm_task_sleep_node linktoken=[id=0, vcpu=0]cpu=0mm=mm_struct of task0halted=falsesleep_head:key0&#45;&gt;sleep_node0:linksleep_node1kvm_task_sleep_node linktoken=[id=1, vcpu=0]cpu=0mm=mm_struct of task1halted=falsesleep_head:key1&#45;&gt;sleep_node1:linksleep_node3kvm_task_sleep_node linktoken=[id=0, vcpu=1]cpu=1mm=mm_struct of task3halted=falsesleep_head:key2&#45;&gt;sleep_node3:linkrun_task_vcpu0current task: task2run_task_vcpu1current task: task4sleep_node4kvm_task_sleep_node linktoken=[id=1, vcpu=1]cpu=1mm=mm_struct of task4halted=truesleep_node3:link&#45;&gt;sleep_node4:linksleep_node4&#45;&gt;run_task_vcpu1 图中一共有4个涉及async PF的task, 同时每个task关联一个kvm_task_sleep_node kvm_task_sleep_head[]-&gt;link负责将所有key相同的 sleep_node串联起来, 方便查找 每个kvm_task_sleep_node有一个唯一的 identify kvm_task_sleep_node-&gt;token cpu0 上之前触发过两次async PF, 并且涉及到的task调度走了,目前正在运行task2 cpu1 上触发过两次async PF, 当task3 触发时, 成功将task3 sched out, 当task4触发时, 由于此时guest vcpu 不能调度, 所以将该cpu halt. 目前该cpu正在task4的上下文中halt. kvm_task_sleep_headstatic struct kvm_task_sleep_head { spinlock_t lock; struct hlist_head list;} async_pf_sleepers[KVM_TASK_SLEEP_HASHSIZE];该数据结构是一个hash map, 使用token作为hash key. lock: 可以看到是每个hash key, 有一个lock. 减少race情况kvm_task_sleep_nodestruct kvm_task_sleep_node { struct hlist_node link; wait_queue_head_t wq; u32 token; int cpu; bool halted; struct mm_struct *mm;};该数据结构作为hash node, 描述每一个因为async pf 调度出去的task 这里并不一定指被调度出去的task, 可能链接着即将发生调度的task信息,我们下面会介绍到. wq: 等待队列 token: 和上面描述一样, 唯一标识一次async PF halted: 有时候kvm注入async PF时, guest在这个时间点不能做schedule, 又 为了再次避免执行该代码流, 只能halt 该cpu. 这里用于标识是否该task halt了cpuinitiate async pf-&gt;inject async pf上面提到了为了使用GUP noio接口, 将tdp_page_fault中的gfn_to_pfn改动为try_async_pf. 我们来看下该接口try_async_pfstatic bool try_async_pf(struct kvm_vcpu *vcpu, gfn_t gfn, gva_t gva, pfn_t *pfn){ bool async; //==(1)== *pfn = gfn_to_pfn_async(vcpu-&gt;kvm, gfn, &amp;async); //==(2)== if (!async) return false; /* *pfn has correct page already */ //==(3)== put_page(pfn_to_page(*pfn)); //==(4)== if (can_do_async_pf(vcpu)) { trace_kvm_try_async_get_page(async, *pfn); //==(5)== if (kvm_find_async_pf_gfn(vcpu, gfn)) { trace_kvm_async_pf_doublefault(gva, gfn); kvm_make_request(KVM_REQ_APF_HALT, vcpu); return true; //==(6)== } else if (kvm_arch_setup_async_pf(vcpu, gva, gfn)) return true; } //==(7)== *pfn = gfn_to_pfn(vcpu-&gt;kvm, gfn); return false;} 前面提到过, 在try_async_pf 中会执行到gfn_to_pfn_async(), async作为oparam 表示是否需要做async pf, 另外还有一个返回值, 该返回值表示在该过程中得到的 pfn of gfn 当然, 如果得到的async为false, 说明不需要async pf, 那肯定得到了pfn所以直接返回 false put_page 这里会判断当前vcpu的状态是否可以做async pf can_do_async_pf细节 +static bool can_do_async_pf(struct kvm_vcpu *vcpu)+{+\tif (unlikely(!irqchip_in_kernel(vcpu-&gt;kvm) ||+\t\t kvm_event_needs_reinjection(vcpu)))+\t\treturn false;++\treturn kvm_x86_ops-&gt;interrupt_allowed(vcpu);+} 我们这里详细讲解下, 这三个判断条件, irqchip_in_kernel() kvm_event_need_reinjection(): static inline bool kvm_event_needs_reinjection(struct kvm_vcpu *vcpu){ return vcpu-&gt;arch.exception.pending || vcpu-&gt;arch.interrupt.pending || vcpu-&gt;arch.nmi_injected;} 可以看到这里, 在检测到有其他pending 事件的情况下, 不允许做async pf. 自己的理解 关于pending的event, 我们需要参考__vmx_complete_interrupts, 但是这里我们不过度展开, 大概就是在 VM entry inject event 期间, 由于某些原因, 触发了VM exit, 此时, VM entry, 还没有完成, 所以这些事件并没有被inject, 需要再次VM entry时注入. 再这种情况下, 就会有这样的顺序 inject_event1-&gt; VM entry-&gt; VM exit(get uncomplete event)-&gt; get vm exit reason: EPT violation PAGE not present-&gt; (do some handler)-&gt; VM entry 那现在问题来了, 本次是该注入async PF, 还是注入 uncomplete event呢? 我个人认为是注入uncomplete event. 首先按照顺序 uncomplete event先发生.如果不注入 uncomplete event的情况下, 直接注入async pf, 给guest感觉是某些event延后了. 另外, uncomplete event是由于 EPT violation 而触发的. 所以在本次处理完EPT violation之后,正好可以注入 uncomplete event, 并且大概率不会再次触发VM exit during EVENTinject. 以上是自己的理解, 而且不确定处理 tdp_page_fault()时, 所有的event是否都来自于上一次注入失败的uncomplete event. 遗留问题 interrupt_allowed: 我们来看下intel vmx 代码 static int vmx_interrupt_allowed(struct kvm_vcpu *vcpu){ return (vmcs_readl(GUEST_RFLAGS) &amp; X86_EFLAGS_IF) &amp;&amp; !(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) &amp; (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS));} 该部分代码, 主要是检测当前interrupt windows 是否open, 这里对 这些判断条件不做过多解释, 详细见virtual interrupt 文章 还未写该文章 遗留部分 但是我们需要理解下, 为什么要关注guest 是否能接收中断呢? 毕竟async pf 注入的是#PF首先我们需要明确的是: 自己的理解 Q: async pf的目的是什么? A: 调度 Q: 该调度能发生在guest 运行的任何时机么 A: 需要满足guest意愿 所以, 综上所述, 得需要在guest认为自己可以调度的情况下, 才能做async pf这个事情. 否则,即使去启动了一个dedicated thread, 让guest调度, guest也不会去调度, 这样就没有意义了. 那好在这样的背景下, 我们分情况考虑: non-para virt: halt 在halt vcpu之后, 能够wakeup vcpu的方式有两种event interrupt async pf work complete 那在guest 不能注入中断的情况下, 只能由第二种event wakeup, 那就变成了sync的方式. 没有意义. para virt, 因为是半虚拟化方式, 相当于通知guest去主动做一次调度, 但是也得满足guest意愿.这实际上就像是和guest 协商的过程, 需要去关心guest这一刻是否能做调度. 作者在介绍MSR_KVM_ASYNC_PF_EN明确了, guest在关中断时, 不能去再次注入async PF, guest可能还处在APF handler中. 如果在此期间再次注入APF, 可能会导致 APF information 被覆盖, 例如: host guest cr2write token(a) to cr2 value: ainject APF1 trigger #PF (disable interrupt in VM-entry) do some thing...write token(b) to cr2 value: binject APF2 intend to read cr2 to get APF1 token, loss it !!! 在avi 的自问自答 中, 我们也能看到关于interrupt allow的解释. 这里说明之前, 该vcpu触发过该地址的 EPT violation , 并且已经做了async pf, 相当于再次遇到了.说明频率比较高, 那么直接halt该vcpu ????????? 下个小节中介绍 如果上述条件不满足, 则直接同步去做.kvm_setup_async_pfint kvm_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, gfn_t gfn, struct kvm_arch_async_pf *arch){ struct kvm_async_pf *work; //==(1)== if (vcpu-&gt;async_pf.queued &gt;= ASYNC_PF_PER_VCPU) return 0; /* setup delayed work */ /* * do alloc nowait since if we are going to sleep anyway we * may as well sleep faulting in page */ //==(2)== work = kmem_cache_zalloc(async_pf_cache, GFP_NOWAIT); if (!work) return 0; work-&gt;page = NULL; work-&gt;done = false; work-&gt;vcpu = vcpu; work-&gt;gva = gva; work-&gt;addr = gfn_to_hva(vcpu-&gt;kvm, gfn); work-&gt;arch = *arch; work-&gt;mm = current-&gt;mm; atomic_inc(&amp;work-&gt;mm-&gt;mm_count); kvm_get_kvm(work-&gt;vcpu-&gt;kvm); /* this can't really happen otherwise gfn_to_pfn_async would succeed */ if (unlikely(kvm_is_error_hva(work-&gt;addr))) goto retry_sync; //==(2.1)== INIT_WORK(&amp;work-&gt;work, async_pf_execute); //==(3)== if (!schedule_work(&amp;work-&gt;work)) goto retry_sync; //==(4)== list_add_tail(&amp;work-&gt;queue, &amp;vcpu-&gt;async_pf.queue); vcpu-&gt;async_pf.queued++; //==(5)== kvm_arch_async_page_not_present(vcpu, work); return 1;retry_sync: kvm_put_kvm(work-&gt;vcpu-&gt;kvm); mmdrop(work-&gt;mm); kmem_cache_free(async_pf_cache, work); return 0;} 说明per cpu async_pf(work)超过了最大限制 – ASYNC_PF_PER_VCPU 申请,work并做相关初始化, 在(2.1)中将work hook设置为async_pf_execute schedule work 将work加到 vcpu-&gt;async_pf.queue队列中 代码如下: void kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu, struct kvm_async_pf *work){ trace_kvm_async_pf_not_present(work-&gt;arch.token, work-&gt;gva); kvm_add_async_pf_gfn(vcpu, work-&gt;arch.gfn); //==(1)== if (!(vcpu-&gt;arch.apf.msr_val &amp; KVM_ASYNC_PF_ENABLED) || (vcpu-&gt;arch.apf.send_user_only &amp;&amp; kvm_x86_ops-&gt;get_cpl(vcpu) == 0)) kvm_make_request(KVM_REQ_APF_HALT, vcpu); //==(2)== else if (!apf_put_user(vcpu, KVM_PV_REASON_PAGE_NOT_PRESENT)) { vcpu-&gt;arch.fault.error_code = 0; vcpu-&gt;arch.fault.address = work-&gt;arch.token; kvm_inject_page_fault(vcpu); }} 和can_do_async_pf, 这里也有一些判断当前状态是否合适向guest注入async pf的条件, 我们放到下面介绍 如果可以注入, 则将KVM_PV_REASON_PAGE_NOT_PRESENT其写入 guest host 共享的内存中, 表示本次注入的是page not present类型的 async pf. 另外, 设置好本次注入异常的 address和 error code async pf workstatic void async_pf_execute(struct work_struct *work){ struct page *page = NULL; struct kvm_async_pf *apf = container_of(work, struct kvm_async_pf, work); struct mm_struct *mm = apf-&gt;mm; struct kvm_vcpu *vcpu = apf-&gt;vcpu; unsigned long addr = apf-&gt;addr; gva_t gva = apf-&gt;gva; might_sleep(); use_mm(mm); down_read(&amp;mm-&gt;mmap_sem); //==(1)== get_user_pages(current, mm, addr, 1, 1, 0, &amp;page, NULL); up_read(&amp;mm-&gt;mmap_sem); unuse_mm(mm); spin_lock(&amp;vcpu-&gt;async_pf.lock); //==(2)== list_add_tail(&amp;apf-&gt;link, &amp;vcpu-&gt;async_pf.done); apf-&gt;page = page; apf-&gt;done = true; spin_unlock(&amp;vcpu-&gt;async_pf.lock); /* * apf may be freed by kvm_check_async_pf_completion() after * this point */ trace_kvm_async_pf_completed(addr, page, gva); //==(3)== if (waitqueue_active(&amp;vcpu-&gt;wq)) wake_up_interruptible(&amp;vcpu-&gt;wq); mmdrop(mm); kvm_put_kvm(vcpu-&gt;kvm);} 调用get_user_pages, 该接口可以处理MAJOR fault get_user_pages() 第四个参数, 如果不为空,则会设置FOLL_GET int get_user_pages(struct task_struct *tsk, struct mm_struct *mm, unsigned long start, int nr_pages, int write, int force, struct page **pages, struct vm_area_struct **vmas){ int flags = FOLL_TOUCH; if (pages) flags |= FOLL_GET; ...} 如果设置了FOLL_GET, 则会在get_user_pages()的过程中, pin this page.也就是get_page(), 但是需要注意的是, 该接口可能会返回错误, 但是看起来此流程并没有判断该接口是否执行成功. IOW, 无论该接口是否执行成功, 都认为该work已经complete, 都需要再次wakeup GUEST blocking thread. 将该work, 链接到vcpu-&gt;async_pf.done链表中 如果vcpu在等待队列中(halt), 唤醒该vcpu接下来, 我们来看下, host是如何检测 page present事件, 并注入page present async pf的host inject PAGE PRESENT aync pf@@ -5272,6 +5288,9 @@ static int __vcpu_run(struct kvm_vcpu *vcpu) \t\t\tvcpu-&gt;run-&gt;exit_reason = KVM_EXIT_INTR; \t\t\t++vcpu-&gt;stat.request_irq_exits; \t\t}+\t\t+\t\tkvm_check_async_pf_completion(vcpu);+ \t\tif (signal_pending(current)) { \t\t\tr = -EINTR;在vm exit后, 检测是否有需要 async pf completevoid kvm_check_async_pf_completion(struct kvm_vcpu *vcpu){ struct kvm_async_pf *work; //==(1)== if (list_empty_careful(&amp;vcpu-&gt;async_pf.done) || !kvm_arch_can_inject_async_page_present(vcpu)) return; spin_lock(&amp;vcpu-&gt;async_pf.lock); work = list_first_entry(&amp;vcpu-&gt;async_pf.done, typeof(*work), link); list_del(&amp;work-&gt;link); spin_unlock(&amp;vcpu-&gt;async_pf.lock); //==(2)== if (work-&gt;page) kvm_arch_async_page_ready(vcpu, work); //==(3)== kvm_arch_async_page_present(vcpu, work); list_del(&amp;work-&gt;queue); vcpu-&gt;async_pf.queued--; if (work-&gt;page) put_page(work-&gt;page); kmem_cache_free(async_pf_cache, work);} 有两个判断条件: 判断是否有完成的work guest此时是否适合注入 page present async PF (下面章节介绍) 如果work-&gt;page为 NULL, 说明async work中, 执行get_user_pages()失败了, 那么本次就不需要在执行kvm_arch_async_page_ready(), 该函数作用是, 再次执行tdp_page_fault, 如果page is ready, 那只需要执行get_user_page fast path和__direct_map建立GPA-&gt;HPA的映射. 但是如果page is not ready(work-&gt;page)为NULL, 作者的想法是, 让其在次vm entry,wakeup guest blocking thread, 让其再次触发EPT violation, 然后再发起async pf.所以在这里没有必要在做一次kvm_arch_async_page_ready-&gt;tdp_page_fault, 那可能有同学会说, 那为什么不在HOST中, 等待get_user_pages()一定返回成功之后, 再注入 page present #PF, 实话说,我也不知道, 但这里总感觉作者不想增加复杂的代码逻辑, 需要关注下后续的patch,看看是否对这部分有优化 遗留问题 kvm_arch_async_page_present void kvm_arch_async_page_present(struct kvm_vcpu *vcpu, struct kvm_async_pf *work){ trace_kvm_async_pf_ready(work-&gt;arch.token, work-&gt;gva); //==(1)== if (is_error_page(work-&gt;page)) work-&gt;arch.token = ~0; /* broadcast wakeup */ else kvm_del_async_pf_gfn(vcpu, work-&gt;arch.gfn); //==(2)== if ((vcpu-&gt;arch.apf.msr_val &amp; KVM_ASYNC_PF_ENABLED) &amp;&amp; !apf_put_user(vcpu, KVM_PV_REASON_PAGE_READY)) { vcpu-&gt;arch.fault.error_code = 0; vcpu-&gt;arch.fault.address = work-&gt;arch.token; kvm_inject_page_fault(vcpu); }} 关于error page, 我们放在另一篇文章中讲述. 遗留问题 置入KVM_ASYNC_PF_PF_ENABLED, 准备注入 page present async #PF guest handle async PFdotraplinkage void __kprobesdo_async_page_fault(struct pt_regs *regs, unsigned long error_code){ //==(1)== switch (kvm_read_and_reset_pf_reason()) { default: //==(2)== do_page_fault(regs, error_code); break; case KVM_PV_REASON_PAGE_NOT_PRESENT: //==(3)== /* page is swapped out by the host. */ kvm_async_pf_task_wait((u32)read_cr2()); break; //==(4)== case KVM_PV_REASON_PAGE_READY: kvm_async_pf_task_wake((u32)read_cr2()); break; }}该部分代码逻辑很清晰, async PF event 是使用了原有的#PF exception vector,guest 需要在exception handler 中判断这个#PF的类型, 然后执行相应的handler 从share memory 中获取 async pf reason indicate NORMAL #PF indicate PAGE NOT PRESENT async pf indicate PAGE PRESENT async pfpage not present async pfvoid kvm_async_pf_task_wait(u32 token){ u32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS); struct kvm_task_sleep_head *b = &amp;async_pf_sleepers[key]; struct kvm_task_sleep_node n, *e; DEFINE_WAIT(wait); int cpu, idle; cpu = get_cpu(); idle = idle_cpu(cpu); put_cpu(); spin_lock(&amp;b-&gt;lock); //===(1)== e = _find_apf_task(b, token); if (e) { /* dummy entry exist -&gt; wake up was delivered ahead of PF */ hlist_del(&amp;e-&gt;link); kfree(e); spin_unlock(&amp;b-&gt;lock); return; } //===(2)== n.token = token; n.cpu = smp_processor_id(); n.mm = current-&gt;active_mm; //===(2.1)== n.halted = idle || preempt_count() &gt; 1; atomic_inc(&amp;n.mm-&gt;mm_count); init_waitqueue_head(&amp;n.wq); //===(3)== hlist_add_head(&amp;n.link, &amp;b-&gt;list); spin_unlock(&amp;b-&gt;lock); for (;;) { //===(4)== if (!n.halted) prepare_to_wait(&amp;n.wq, &amp;wait, TASK_UNINTERRUPTIBLE); if (hlist_unhashed(&amp;n.link)) break; //===(4)== if (!n.halted) { local_irq_enable(); schedule(); local_irq_disable(); } else { /* * We cannot reschedule. So halt. */ native_safe_halt(); local_irq_disable(); } } if (!n.halted) finish_wait(&amp;n.wq, &amp;wait); return;} 在kernel doc介绍MSR_KVM_ASYNC_PF_EN, 作者有提到过. 一对[type2 APF, type1 APF] 不一定会在同一个vcpu上触发, 那也就意味着两者可能并行执行(虽然现在的host kvm 没有这样做,但是guest不能依赖它), 如下: kvm vcpu1 vcpu21.inject type1 APF to VCPU1 2. inject type2 APF to VCPU2 3. handle type2 APF 4. handle type1 APF 可以看到kvm虽然是按照顺序注入的type1 APF, 和type2 APF, 但是注入到了不同的vcpu. vcpu在处理时,handle type2 APF先执行, 此时page 已经present了, 不需要再sched out, 这里会在type2 APFhandler中预先将带有该token的sleep_node放到head中, 以便type 1 APF handler 可以跳过这次的sched out(需要结合type2 APF handle – kvm_async_pf_task_wake().) 将task(current-&gt;active_mm)和token绑定, 这样当type2 APF触发时, 可以根据token找到当前block的task 需要注意的时, guest在某些情况下不能sched out, 这时, 只能halt当前cpu 我们放到另一篇文章中去介绍 遗留问题 将sleep_node链到sleep_head上 如果guest此时可以调度, 则将进程D住, sched outpage present async pfvoid kvm_async_pf_task_wake(u32 token){ u32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS); struct kvm_task_sleep_head *b = &amp;async_pf_sleepers[key]; struct kvm_task_sleep_node *n; if (token == ~0) { apf_task_wake_all(); return; }again: spin_lock(&amp;b-&gt;lock); //===(1)== n = _find_apf_task(b, token); //===(2)== if (!n) { /* * async PF was not yet handled. * Add dummy entry for the token. */ n = kmalloc(sizeof(*n), GFP_ATOMIC); if (!n) { /* * Allocation failed! Busy wait while other cpu * handles async PF. */ spin_unlock(&amp;b-&gt;lock); cpu_relax(); goto again; } n-&gt;token = token; n-&gt;cpu = smp_processor_id(); n-&gt;mm = NULL; init_waitqueue_head(&amp;n-&gt;wq); hlist_add_head(&amp;n-&gt;link, &amp;b-&gt;list); } else //===(3)== apf_task_wake_one(n); spin_unlock(&amp;b-&gt;lock); return;} 根据token, 在sleep_head中查找sleep_node 同type1 APF handler, type2 APF可能在于type1 APF不同的cpu上先执行, 此时在sleep_head中找不到和该token相关的sleep_node, 这时, 需要新创建一个sleep_node将其添加到sleep_head中, 以便type1 APF handler可以查找到,避免block该task 如果查找到了, 说明type1 APF handler已经触发, task已经block, 需要wakeup该task参考链接 MAIL list:v1 v2 v3 v4 v5 v6 v7 " } ]
