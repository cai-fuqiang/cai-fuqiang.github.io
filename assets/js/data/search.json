[ { "title": "async pf -- GUP change", "url": "/posts/async-pf-gup-change/", "categories": "kvm, async_pf", "tags": "para_virt", "date": "2024-04-16 15:00:00 +0800", "snippet": "参考代码 该部分代码, mail list中和commit中内容不同, 而且mail list中也没有提到为什么当时没有全部合入, 我们先以mail list 为准: [KVM: Add host swap event notifications for PV guest][v7] 后续, 我们在另一篇文章中详细介绍: [link][2] 遗留问题 get_user_pag...", "content": "参考代码 该部分代码, mail list中和commit中内容不同, 而且mail list中也没有提到为什么当时没有全部合入, 我们先以mail list 为准: [KVM: Add host swap event notifications for PV guest][v7] 后续, 我们在另一篇文章中详细介绍: [link][2] 遗留问题 get_user_pages_noio 该部分代码来自于: [Add get_user_pages() variant that fails if major fault is required.][3]社区并没有合入该patch该patch 引入了 get_user_page()的noio 版的变体, 他只在不需要 major fault的情况下才会成功的 get page reference 以上来自该patch的commit message This patch add get_user_pages() variant that only succeeds if gettinga reference to a page doesn't require major fault. 具体改动是: 增加了新的flow flag和fault flag flow/fault flag细节 flow flag: 该flag会作为gup_flags入参传入__get_user_pages(), 可以作为一些约束e.g., FOLL_WRITE 表明要要get的pages必须是可写入的 会对比vma-&gt;vm_flags 是否是可写的. 另外, 会在 follow page期间, 影响handle_mm_fault的fault flag e.g., FOLL_WRITE-&gt;FAULT_FLAG_WRITE 新增的flag为: +#define FOLL_MINOR\t0x20\t/* do only minor page faults */ 表明要在 follow page期间, 只允许处理 minor page fault.(不能处理swapin) fault flag: 用于handle_mm_fault()入参flags, 表明本次fault的类型.该参数可以用于一些优化和约束. e.g. 如果检测到没有FAULT_FLAG_WRITE, 说明是read access, 而又是第一次建立映射, 那么可以建立和zero page的映射 do_anonymous_page(){ ... if (!(flags &amp; FAULT_FLAG_WRITE)) { entry = pte_mkspecial(pfn_pte(my_zero_pfn(address), vma-&gt;vm_page_prot)); page_table = pte_offset_map_lock(mm, pmd, address, &amp;ptl); if (!pte_none(*page_table)) goto unlock; goto setpte; } ...} 新增的flag为: +#define FAULT_FLAG_MINOR\t0x08\t/* Do only minor fault */ 是由 FOLL_MINOR转化而来, 和其作用一样. vm fault reason: 其实, 还有一些flag, 是用于表示handle_mm_fault()的原因, 例如这里我们遇到的:VM_FAULT_MAJOR, 实际上是表明, 该函数返回失败是由于该fault是 major fault. 我们下面会结合代码改动详细看下, 这些flag的应用 关于处理这些\"flags\"具体代码流程改动: \"flags\"具体代码改动 __get_user_pages-&gt;handle_mm_fault @@ -1441,10 +1441,13 @@ int __get_user_pages(struct task_struct *tsk, struct mm_struct *mm, \t\t\tcond_resched(); \t\t\twhile (!(page = follow_page(vma, start, foll_flags))) { \t\t\t\tint ret;+\t\t\t\tunsigned int fault_fl =+\t\t\t\t\t((foll_flags &amp; FOLL_WRITE) ?+\t\t\t\t\tFAULT_FLAG_WRITE : 0) |+\t\t\t\t\t((foll_flags &amp; FOLL_MINOR) ?+\t\t\t\t\tFAULT_FLAG_MINOR : 0); -\t\t\t\tret = handle_mm_fault(mm, vma, start,-\t\t\t\t\t(foll_flags &amp; FOLL_WRITE) ?-\t\t\t\t\tFAULT_FLAG_WRITE : 0);+\t\t\t\tret = handle_mm_fault(mm, vma, start, fault_fl); 可以看到, 这里会将 FOLL_WRITE-&gt;FAULT_FLAG_WRITE, FOLL_MINOR-&gt;FAULT_FLAG_MINOR do_swap_page @@ -2648,6 +2670,9 @@ static int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma, \tdelayacct_set_flag(DELAYACCT_PF_SWAPIN); \tpage = lookup_swap_cache(entry); \tif (!page) {+\t\tif (flags &amp; FAULT_FLAG_MINOR)+\t\t\treturn VM_FAULT_MAJOR | VM_FAULT_ERROR;+ \t\tgrab_swap_token(mm); /* Contend for token _before_ read-in */ \t\tpage = swapin_readahead(entry, \t\t\t\t\tGFP_HIGHUSER_MOVABLE, vma, address); 如果没有在swap cache中找到, 说明该page 被swap出去, 并且被free了,需要swapin, 这时, 如果有FAULT_FLAG_MINOR,表明只允许处理 minor fault, 而swapin, 属于 major fault, 不允许处理, 需要返回错误, 同时把错误原因:VM_FAULT_MAJOR也返回 filemap_fault 和swapcache 相对应的还有pagecache diff --git a/mm/filemap.c b/mm/filemap.cindex 3d4df44..ef28b6d 100644--- a/mm/filemap.c+++ b/mm/filemap.c@@ -1548,6 +1548,9 @@ int filemap_fault(struct vm_area_struct *vma, struct vm_fault *vmf) \t\t\tgoto no_cached_page; \t\t} \t} else {+\t\tif (vmf-&gt;flags &amp; FAULT_FLAG_MINOR)+\t\t\treturn VM_FAULT_MAJOR | VM_FAULT_ERROR; 也是同样的处理逻辑 handle_mm_fault() --return-&gt;__get_user_pages() --handle retval @@ -1452,6 +1455,8 @@ int __get_user_pages(struct task_struct *tsk, struct mm_struct *mm, \t\t\t\t\tif (ret &amp; \t\t\t\t\t (VM_FAULT_HWPOISON|VM_FAULT_SIGBUS)) \t\t\t\t\t\treturn i ? i : -EFAULT;+\t\t\t\t\telse if (ret &amp; VM_FAULT_MAJOR)+\t\t\t\t\t\treturn i ? i : -EFAULT; \t\t\t\t\tBUG(); 如果是 FAULT_MAJOR并且没有get 到 page, 直接返回错误 新增get_user_pages_noio接口 +int get_user_pages_noio(struct task_struct *tsk, struct mm_struct *mm,+\t\tunsigned long start, int nr_pages, int write, int force,+\t\tstruct page **pages, struct vm_area_struct **vmas)+{+\tint flags = FOLL_TOUCH | FOLL_MINOR;++\tif (pages)+\t\tflags |= FOLL_GET;+\tif (write)+\t\tflags |= FOLL_WRITE;+\tif (force)+\t\tflags |= FOLL_FORCE;++\treturn __get_user_pages(tsk, mm, start, nr_pages, flags, pages, vmas);+}+EXPORT_SYMBOL(get_user_pages_noio);+ 不多解释, 在该接口中将FOLL_MINOR置位. NOTE 所以该部分patch的主要作用就是增加了get_user_pages_noio(), 使其, 只处理minor fault(假如只是alloc page, 那属于minor fault), 但是不能处理major fault.(例如swapin, pagecachein) 目前个人理解是这样, 之后还需要看下page fault的相关细节 我们接下来看下, async pf 框架是如何利用上面GUP noio接口的usage of GUP noio in async pf我们先看下, 触发 async pf 的入口, 我们上面介绍到, 在 EPT violation hook 中会去start 该work, 过程如下:tdp_page_fault@@ -2609,7 +2655,11 @@ static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, mmu_seq = vcpu-&gt;kvm-&gt;mmu_notifier_seq; smp_rmb(); //==(1)==- pfn = gfn_to_pfn(vcpu-&gt;kvm, gfn);+ //==(2)==+ if (try_async_pf(vcpu, gfn, gpa, &amp;pfn))+ return 0;++ /* mmio */ if (is_error_pfn(pfn)) 将现有的gfn_to_pfn(), 替换为try_async_pf(), 之前的gfn2pfn接口, 是必须async, 也就是上面提到的使用GUP时, 可以处理 MAJOR FAULT. 而现在替换为了 try_async_pf(), 打算尝试执行 async pf(也可能不需要, 例如遇到了 MINOR FAULT. 我们接下来会详细看下该接口 tdp_page_fault()中会执行try_async_pf()该函数返回值为true, 表示已经做了async pf,所以现在还不能去 map GPA-&gt;HPA. 需要该接口直接返回. 对于HALT的处理方式, 则是让vcpublock. 我们下面会看到.old version of gfn_to_pfn在看try_async_pf之前, 我们先看下合入patch之前的 gfn_to_pfn接口.gfn_to_pfn { __gfn_to_pfn(atomic=false) { gfn_to_hva { gfn_to_hva_many gfn_to_hva_memslot } //gfn_to_hva ----- 上面 gfn_to_hva 下面 hva_to_pfn ----- hva_to_pfn(atomic=false) { if (atomic) __get_user_pages_fast() else //走这个路径 get_user_pages_fast() } //hva_to_pfn } //__gfn_to_pfn} //gfn_to_pfn关于__get_user_pages_fast和get_user_pages_fast的不同, 主要是: __get_user_pages_fast()是atomic版本(IRQ-safe), 主要是因为get_user_pages_fast需要走slow path, 这个时候需要开中断, 而__get_user_pages_fast则不需要, 所以其过程是关中断的, 也可以在关中断的情况下执行 由于上面提到的原因, get_user_pages_fast 并不保存中断状态, 所以该函数必须在开中断的情况下执行 两个接口前的代码注释, 以及大致流程 get_user_pages_fast /** * get_user_pages_fast() - pin user pages in memory * @start: starting user address * @nr_pages: number of pages from start to pin * @write: whether pages will be written to * @pages: array that receives pointers to the pages pinned. * Should be at least nr_pages long. * * Attempt to pin user pages in memory without taking mm-&gt;mmap_sem. * If not successful, it will fall back to taking the lock and * calling get_user_pages(). * * &gt; 在不拿mm-&gt;mmap_sem 锁的情况下, 尝试将user page pin 到memory中. * &gt; 如果没有成功, 它将fall back 来拿锁, 并且调用get_user_pages() * * Returns number of pages pinned. This may be fewer than the number * requested. If nr_pages is 0 or negative, returns 0. If no pages * were pinned, returns -errno. * * &gt; 返回 page 被 pinned数量. 他可能比所需的数量要少. 如果nr_pages是0, * 或者是负数, 返回0. 如果没有page被pinned, 返回 -errno */get_user_pages_fast { local_irq_disable fast_path { //仅去看有多少page present } local_irq_enable get_user_page} __get_user_pages_fast /* * Like get_user_pages_fast() except its IRQ-safe in that it won't fall * back to the regular GUP. * * 除了他的 IRQ-safe(因为他不会fall bak to regular GUP), 其他的和 * get_user_pages_fast()一样 */__get_user_pages_fast { local_irq_save() fast_path local_irq_restore()} 这里, 我们不再过多展开GUP的代码, 总之, 早期的__get_user_pages_fast不会fall back到 regular GPU(slow pathget_user_pages)我们再来看下其改动,gfn_to_pfn-&gt;get_user_pages新增gfn_to_pfn_async(), 替代现有流程中的gfn_to_pfn(), 该接口新增了async:bool*参数, 该参数是一个iparam &amp;&amp; oparam iparam: 表示只走get_user_page fast path也就是__get_user_pages_fast oparam: 表示是否需要做 async pf具体改动如下+pfn_t gfn_to_pfn_async(struct kvm *kvm, gfn_t gfn, bool *async)+{+ return __gfn_to_pfn(kvm, gfn, false, async);+}+EXPORT_SYMBOL_GPL(gfn_to_pfn_async);+ pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn) {- return __gfn_to_pfn(kvm, gfn, false); //可以走slow path+ return __gfn_to_pfn(kvm, gfn, false, NULL); } EXPORT_SYMBOL_GPL(gfn_to_pfn);/* * !!MY NOTE!! * __gfn_to_pfn { * ... * //先初始化为false * if (async) * *async = false; * ... * return hva_to_pfn(kvm, addr, atomic, async); * } */我们再来看下hva_to_pfn改动:+static pfn_t hva_to_pfn(struct kvm *kvm, unsigned long addr, bool atomic,+ bool *async) { struct page *page[1];- int npages;+ int npages = 0; pfn_t pfn;- if (atomic)+ /* we can do it either atomically or asynchronously, not both */+ BUG_ON(atomic &amp;&amp; async); //==(1)==+ if (atomic || async) npages = __get_user_pages_fast(addr, 1, 1, page);- else {+ //==(2)==+ if (unlikely(npages != 1) &amp;&amp; !atomic) { might_sleep();- npages = get_user_pages_fast(addr, 1, 1, page);+ + if (async) {+ down_read(&amp;current-&gt;mm-&gt;mmap_sem);+ npages = get_user_pages_noio(current, current-&gt;mm,+ \t\t\t addr, 1, 1, 0, page, NULL);+ up_read(&amp;current-&gt;mm-&gt;mmap_sem);+ } else+ npages = get_user_pages_fast(addr, 1, 1, page); } if (unlikely(npages != 1)) { struct vm_area_struct *vma; if (atomic) goto return_fault_page; down_read(&amp;current-&gt;mm-&gt;mmap_sem); if (is_hwpoison_address(addr)) { up_read(&amp;current-&gt;mm-&gt;mmap_sem); get_page(hwpoison_page); return page_to_pfn(hwpoison_page); } vma = find_vma(current-&gt;mm, addr); if (vma == NULL || addr &lt; vma-&gt;vm_start || !(vma-&gt;vm_flags &amp; VM_PFNMAP)) { //==(3)==+ if (async &amp;&amp; !(vma-&gt;vm_flags &amp; VM_PFNMAP) &amp;&amp;+ (vma-&gt;vm_flags &amp; VM_WRITE))+ *async = true; up_read(&amp;current-&gt;mm-&gt;mmap_sem);return_fault_page: get_page(fault_page); return page_to_pfn(fault_page); } pfn = ((addr - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT) + vma-&gt;vm_pgoff; up_read(&amp;current-&gt;mm-&gt;mmap_sem); BUG_ON(!kvm_is_mmio_pfn(pfn)); } else pfn = page_to_pfn(page[0]); return pfn;} 如果是async, 会先尝试走一次fast path, 如果成功了, 则 npages = 1 如果上面fast path 失败了, 并且还是async, 则会执行get_user_pages_noio()该函数上面也提到过, 该过程不处理 MAJOR fault. 这里说明失败了, 也就是因为遇到了MAJOR fault, 所以该fault 并没有handle,需要异步处理, 那么就将 oparam async 置为true. 这里我们先不关心这里的几个判断条件, 之后放到GUP/内存管理的章节中介绍 遗留问题 " }, { "title": "async pf -- gfn2hva cache", "url": "/posts/async-pf-gfn2hva-cache/", "categories": "kvm, async_pf", "tags": "para_virt", "date": "2024-04-16 15:00:00 +0800", "snippet": "introduce该功能仅通过name就可以得知, 是为了缓存gfn(gpa)到hva的映射. 但是这个映射关系不是一直存在么, 为什么设计看似比较复杂的机制, 我们一步步来看user memory region support我们知道,在比较早期的版本, kvm 创建memslot APIkvm_vm_ioctl KVM_SET_USER_MEMORY_REGION就已经支持了对 use...", "content": "introduce该功能仅通过name就可以得知, 是为了缓存gfn(gpa)到hva的映射. 但是这个映射关系不是一直存在么, 为什么设计看似比较复杂的机制, 我们一步步来看user memory region support我们知道,在比较早期的版本, kvm 创建memslot APIkvm_vm_ioctl KVM_SET_USER_MEMORY_REGION就已经支持了对 user memory region申请的支持. 涉及patch Patch: KVM: Support assigning userspace memory to the guest mail list: mail 这里只展示下, 用户态入参的数据结构: +/* for KVM_SET_USER_MEMORY_REGION */+struct kvm_userspace_memory_region {+ __u32 slot;+ __u32 flags;+ __u64 guest_phys_addr;+ __u64 memory_size; /* bytes */+ __u64 userspace_addr; /* start of the userspace allocated memory */+}; 可以看到最后一个参数为, userspace_addr在该接口的支持下, qemu为guest申请memory region同时, 该内存也作为qemu进程的 anon memoryspace 存在, qemu 可以通过管理匿名页的方式, 对该地址空间进行管理, kernel其他组建, 也可以通过操作这部分匿名页来操作guest memory, 例如: memory reclaim, memory migrate…所以基本流程是: guest先通过mmap申请匿名内存空间, 调用完成后, kernel已经申请好了这段内存空间的virtual base address(userspace_addr) 调用kvm_vm_ioctl -- KVM_SET_USER_MEMORY_REGION 执行完成时, kvm 已经建立起了 hva-&gt;gpa映射关系 guest访问gpa, 触发EPT violation trap kvm, kvm 调用 get_user_page() 申请page, 并建立hva-&gt;hpa 同时, 创建gpa-&gt;hpa的mmu pgtables(if guest enable EPT feature, is ept pgtable)re-set memory region所以, 既然两者在set memory region 接口中就已经确立了映射关系, 那是不是只是保存下[hva, gpa]就相当于cache了.大部分情况下是这样, 但是在下面情况下, hva-&gt;gpa的映射关系会改变 guest call kvm_vm_ioctl -- KVM_SET_USER_MEMORY_REGION, map [hva-&gt;gpa]-&gt;[hpa_1, gpa] guest call kvm_vm_ioctl -- KVM_SET_USER_MEMORY_REGION again, remap [hva-&gt;gpa]-&gt;[hpa_2, gpa]在这种情况下, 映射关系就改变了.所以, 我们需要一个机制, 在re-set memory region 发生之后, 我们再次 “access this cache”时, 需要“invalidate this cache”, 这就是该patch要做的事情.patch 细节change of struct struct kvm_memslots { \tint nmemslots;+\tu32 generation; \tstruct kvm_memory_slot memslots[KVM_MEMORY_SLOTS + \t\t\t\t\tKVM_PRIVATE_MEM_SLOTS]; }; generation: 表示当前memslots 的generation, 也就是latest.+struct gfn_to_hva_cache {+\tu32 generation;+\tgpa_t gpa;+\tunsigned long hva;+\tstruct kvm_memory_slot *memslot;+}; generation: 获取cache时, memslots的generation, 可能是old的. memslot: 当前gpa所属的memslot, 主要用于 mark_page_dirty 作者在这里有个小心思, 因为这个地方没有该成员也没有关系, 也可以执行, mark_page_dirty, 但是在执行时, 需要每次获取memslot, 所以作者想了,既然缓存, 那为什么不缓存多一些, 将memslot也缓存 interfacecache init+int kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,+\t\t\t gpa_t gpa)+{+\tstruct kvm_memslots *slots = kvm_memslots(kvm);+\tint offset = offset_in_page(gpa);+\tgfn_t gfn = gpa &gt;&gt; PAGE_SHIFT;++\tghc-&gt;gpa = gpa;+\t//==(1)==+\tghc-&gt;generation = slots-&gt;generation;+\tghc-&gt;memslot = __gfn_to_memslot(kvm, gfn);+\tghc-&gt;hva = gfn_to_hva_many(ghc-&gt;memslot, gfn, NULL);+\t//==(2)==+\tif (!kvm_is_error_hva(ghc-&gt;hva))+\t\tghc-&gt;hva += offset;+\telse+\t\treturn -EFAULT;++\treturn 0;+} 将此时slots-&gt;generation赋值给ghc-&gt;generation 错误情况暂时不看.write cache+int kvm_write_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,+\t\t\t void *data, unsigned long len)+{+\tstruct kvm_memslots *slots = kvm_memslots(kvm);+\tint r;++\t//==(1)==+\tif (slots-&gt;generation != ghc-&gt;generation)+\t\tkvm_gfn_to_hva_cache_init(kvm, ghc, ghc-&gt;gpa);+\t+\tif (kvm_is_error_hva(ghc-&gt;hva))+\t\treturn -EFAULT;++\t//==(2)==+\tr = copy_to_user((void __user *)ghc-&gt;hva, data, len);+\tif (r)+\t\treturn -EFAULT;+\t//==(3)==+\tmark_page_dirty_in_slot(kvm, ghc-&gt;memslot, ghc-&gt;gpa &gt;&gt; PAGE_SHIFT);++\treturn 0;+} 如果slots-&gt;generation 和当前cache generation(ghc-&gt;generation)不一致, 说明 该cache已经是stale的了, 需要update, 那就直接重新init cache(调用 kvm_gfn_to_hva_cache_init()) 将数据写入hva 该接口是新增的, 为mark_page_dirty()的变体. -void mark_page_dirty(struct kvm *kvm, gfn_t gfn)+void mark_page_dirty_in_slot(struct kvm *kvm, struct kvm_memory_slot *memslot,+\t\t\t gfn_t gfn) {-\tstruct kvm_memory_slot *memslot;--\tmemslot = gfn_to_memslot(kvm, gfn); \tif (memslot &amp;&amp; memslot-&gt;dirty_bitmap) { \t\tunsigned long rel_gfn = gfn - memslot-&gt;base_gfn; @@ -1284,6 +1325,14 @@ void mark_page_dirty(struct kvm *kvm, gfn_t gfn) \t} } +void mark_page_dirty(struct kvm *kvm, gfn_t gfn)+{+\tstruct kvm_memory_slot *memslot;++\tmemslot = gfn_to_memslot(kvm, gfn);+\tmark_page_dirty_in_slot(kvm, memslot, gfn);+} 该变体较mark_page_dirty()来说, 主要是增加memslot参数. 原因在介绍数据结构的时候已经说明 该功能和dirty log功能相关, guest可以通过bitmap知道那些page是dirty的,在热迁移的时候会用到, 这里不过多介绍 那slots-&gt;generation 什么时候改变的呢bump slots-&gt;generationdiff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.cindex db58a1b..45ef50c 100644--- a/virt/kvm/kvm_main.c+++ b/virt/kvm/kvm_main.cint __kvm_set_memory_region(struct kvm *kvm, struct kvm_userspace_memory_region *mem, int user_alloc){skip_lpage: //==(1)== if (!npages) { r = -ENOMEM; slots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL); if (!slots) goto out_free; memcpy(slots, kvm-&gt;memslots, sizeof(struct kvm_memslots)); if (mem-&gt;slot &gt;= slots-&gt;nmemslots) slots-&gt;nmemslots = mem-&gt;slot + 1;+\t\tslots-&gt;generation++; slots-&gt;memslots[mem-&gt;slot].flags |= KVM_MEMSLOT_INVALID; old_memslots = kvm-&gt;memslots; rcu_assign_pointer(kvm-&gt;memslots, slots); synchronize_srcu_expedited(&amp;kvm-&gt;srcu); /* From this point no new shadow pages pointing to a deleted * memslot will be created. * * validation of sp-&gt;gfn happens in: * - gfn_to_hva (kvm_read_guest, gfn_to_pfn) * - kvm_is_visible_gfn (mmu_check_roots) */ kvm_arch_flush_shadow(kvm); kfree(old_memslots); } //==(2)== r = kvm_arch_prepare_memory_region(kvm, &amp;new, old, mem, user_alloc); if (r) goto out_free; /* map the pages in iommu page table */ if (npages) { r = kvm_iommu_map_pages(kvm, &amp;new); if (r) goto out_free; } r = -ENOMEM; slots = kzalloc(sizeof(struct kvm_memslots), GFP_KERNEL); if (!slots) goto out_free; memcpy(slots, kvm-&gt;memslots, sizeof(struct kvm_memslots)); if (mem-&gt;slot &gt;= slots-&gt;nmemslots) slots-&gt;nmemslots = mem-&gt;slot + 1;+ slots-&gt;generation++; 说明不是内存, 有可能是mmio, 这里变动memslots, 需要使用rcu机制,这样可以保证在无锁的情况下把这个动作完成 normal 内存因为更新到了memslots, 说明hva-&gt;gpa的关系有改变, 所以需要更新generationhistory of change avi在Re: [PATCH v2 02/12] Add PV MSR to enable asynchronous page faults delivery. 中提到, 目前这一版本patch可能在遇到 memslots 情况下, 会有问题 原文 &gt; +static int kvm_pv_enable_async_pf(struct kvm_vcpu *vcpu, u64 data)&gt; +{&gt; +\tu64 gpa = data&amp; ~0x3f;&gt; +\tint offset = offset_in_page(gpa);&gt; +\tunsigned long addr;&gt; +&gt; +\taddr = gfn_to_hva(vcpu-&gt;kvm, gpa&gt;&gt; PAGE_SHIFT);&gt; +\tif (kvm_is_error_hva(addr))&gt; +\t\treturn 1;&gt; + //只初始化一次&gt; +\tvcpu-&gt;arch.apf_data = (u32 __user*)(addr + offset);&gt; +&gt; +\t/* check if address is mapped */&gt; +\tif (get_user(offset, vcpu-&gt;arch.apf_data)) {&gt; +\t\tvcpu-&gt;arch.apf_data = NULL;&gt; +\t\treturn 1;&gt; +\t}&gt; What if the memory slot arrangement changes? This needs to be revalidated (and gfn_to_hva() called again).&gt; validate &lt;==&gt; invalidate&gt; revalidate : 重新生效, 重新验证 在[PATCH v3 07/12] Maintain memslot version number和[PATCH v3 08/12] Inject asynchronous page fault into a guest if page is swapped out.中, 作者引入了该功能, 不过该功能是嵌入到async pf 功能中, 并非独立接口 代码 diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.hindex 600baf0..3f5ebc2 100644--- a/include/linux/kvm_host.h+++ b/include/linux/kvm_host.h@@ -163,6 +163,7 @@ struct kvm { \tspinlock_t requests_lock; \tstruct mutex slots_lock; \tstruct mm_struct *mm; /* userspace tied to this vm */+\tu32 memslot_version; \tstruct kvm_memslots *memslots; \tstruct srcu_struct srcu;@@ -364,7 +364,9 @@ struct kvm_vcpu_arch { \tunsigned long singlestep_rip; \tu32 __user *apf_data;+\tu32 apf_memslot_ver; \tu64 apf_msr_val;+\tu32 async_pf_id; };+static int apf_put_user(struct kvm_vcpu *vcpu, u32 val)+{+\tif (unlikely(vcpu-&gt;arch.apf_memslot_ver !=+\t\t vcpu-&gt;kvm-&gt;memslot_version)) {+\t\tu64 gpa = vcpu-&gt;arch.apf_msr_val &amp; ~0x3f;+\t\tunsigned long addr;+\t\tint offset = offset_in_page(gpa);++\t\taddr = gfn_to_hva(vcpu-&gt;kvm, gpa &gt;&gt; PAGE_SHIFT);+\t\tvcpu-&gt;arch.apf_data = (u32 __user*)(addr + offset);+\t\tif (kvm_is_error_hva(addr)) {+\t\t\tvcpu-&gt;arch.apf_data = NULL;+\t\t\treturn -EFAULT;+\t\t}+\t}++\treturn put_user(val, vcpu-&gt;arch.apf_data);+} 可以看到, 相当于引入了两个version, 并在apf_put_user()时,比对两个version. 作者在Re: [PATCH v4 08/12] Inject asynchronous page fault into a guest if page is swapped out.回答了为什么不使用kvm_write_guest Q: why A: want to cache gfn_to_hva_translation avi 在Re: [PATCH v5 08/12] Inject asynchronous page fault into a guest if page is swapped out.建议将该功能剥离, 因为这个功能很好,其他代码也可以用. 原文 This nice cache needs to be outside apf to reduce complexity for reviewers and since it is useful for others. Would be good to have memslot-cached kvm_put_guest() and kvm_get_guest(). 作者在Re: [PATCH v5 08/12] Inject asynchronous page fault into a guest if page is swapped out.首次提供该接口. Marcelo Tosatti 在Re: [PATCH v6 04/12] Add memory slot versioning and use it to provide fast guest write interface提到两个问题: 在kvm_gfn_to_hva_cache_init中使用gfn_to_memslot 获取memslot, 可能会造成如下问题 自己的理解 thread1 thread2 guestkvm_write_guest_cached kvm_gfn_to_hva_cache_init { __kvm_set_memory_region slots = kvm_memslots(kvm) { rcu_dereference_check } ghc-&gt;generation = slots-&gt;generation; slots-&gt;generation++; ghc-&gt;memslot = gfn_to_memslot( slots, gfn) { rcu_dereference_check { //may have a gp rcu_assign_pointer( kvm-&gt;memslots, slots); } ghc-&gt;hva = gfn_to_hva_many( ghc-&gt;memslot, gfn, NULL); }copy_to_user(); kvm_arch_commit_memory_region do_munmap access apf reason, LOSS 这样可能会导致在这个函数中, 前面和后面获取的信息来自于不同的memslots, 个人认为, 不仅仅是这样, 还可能导致, thread2 因为中间释放了rcu, 导致其流程和thread1有race, 最终导致 本次copy_to_user()数据丢失 作者在下一版patch中将gfn_to_memslot修改为了__gfn_to_memslot, 该接口不会在使用rcu_dereference_check " }, { "title": "embedding markdown in HTML tag", "url": "/posts/embedding_markdown_in_html_tag/", "categories": "markdown", "tags": "markdown, html-details", "date": "2024-04-12 10:53:00 +0800", "snippet": "ISSUEWhen I try to use the markdown syntax in &lt;details&gt; HTML tags, for example,code blocks, encounter the problem of code blocks that cannot be rendered.The source code is as follows:&lt;deta...", "content": "ISSUEWhen I try to use the markdown syntax in &lt;details&gt; HTML tags, for example,code blocks, encounter the problem of code blocks that cannot be rendered.The source code is as follows:&lt;details&gt;&lt;summary&gt; aaa &lt;/summary&gt;` ` `cppint a = 1;` ` `&lt;/details&gt; ` char seems unable to be translated in code block, so I added spacecharacters between themIt will display in browser as follows:aaa```cppint a = 1;```SOLUTIONThis issue seems to occur in the kramdowm markup process, rather than in GFM: GFM allows embedding HTML inside Markdown Embedding Markdown in Jekyll HTMLAnd in the link Embedding Markdown in Jekyll HTML, a solution is provided:Use &lt;details markdown=\"1\"&gt; instead &lt;details&gt;.It will run as expected aaa int a = 1; " }, { "title": "async pf", "url": "/posts/async-pf/", "categories": "kvm, async_pf", "tags": "para_virt", "date": "2024-04-10 12:20:00 +0800", "snippet": "introduce在支持EPT的架构中, 对于GVA-&gt;HPA一般有两段映射: GVA-&gt;GPA GPA-&gt;HPA而host kernel (kvm) 需要关心的是 GPA-&gt;HPA的映射, 需要host做的事情主要有以下几个: 捕捉相关 VM-exit event (EPT violation), 得到 GPA 分配page 建立映射关系(当然这个映射关系...", "content": "introduce在支持EPT的架构中, 对于GVA-&gt;HPA一般有两段映射: GVA-&gt;GPA GPA-&gt;HPA而host kernel (kvm) 需要关心的是 GPA-&gt;HPA的映射, 需要host做的事情主要有以下几个: 捕捉相关 VM-exit event (EPT violation), 得到 GPA 分配page 建立映射关系(当然这个映射关系, 不止是GPA-&gt;HPA的mmu pgtable, 还有 HVA – GPA,在这里不展开, 总之分配好具体的page(分配HPA), 以及为其建立好 mmu pgtable, 就可以完成该事件的处理)如下图:图示graphviz-ae4ca25f7bf30b9e61f0f3b83bc12338digraph G { subgraph cluster_guest { EPT_violation [ label=&quot;EPT mapping(GPA-&gt;HPA) \\nloss, trigger EPT violation&quot; ] &quot;access a VA&quot;-&gt; &quot;trigger PF in VMX\\n non-root operation&quot;-&gt; &quot;mapping GVA-&gt;GPA\\n in GUEST #PF hook&quot;-&gt; &quot;fixup #PF, continue \\naccess this VA&quot;-&gt; EPT_violation label=&quot;guest&quot; } subgraph cluster_host { &quot;find HVA though GPA&quot;-&gt; &quot;GUP(HVA)&quot;-&gt; &quot;mapping GPA-&gt;HPA&quot; label=&quot;host&quot; } &quot;mapping GPA-&gt;HPA&quot;-&gt;&quot;access a VA&quot; [ label=&quot;fixup EPT violation,\\n VM entry&quot; ] EPT_violation-&gt;&quot;find HVA though GPA&quot; [ label=&quot;VM exit&quot; ]}Gcluster_guestguestcluster_hosthostEPT_violationEPT mapping(GPA&#45;&gt;HPA) loss, trigger EPT violationfind HVA though GPAfind HVA though GPAEPT_violation&#45;&gt;find HVA though GPAVM exitaccess a VAaccess a VAtrigger PF in VMX\\n non&#45;root operationtrigger PF in VMX non&#45;root operationaccess a VA&#45;&gt;trigger PF in VMX\\n non&#45;root operationmapping GVA&#45;&gt;GPA\\n in GUEST #PF hookmapping GVA&#45;&gt;GPA in GUEST #PF hooktrigger PF in VMX\\n non&#45;root operation&#45;&gt;mapping GVA&#45;&gt;GPA\\n in GUEST #PF hookfixup #PF, continue \\naccess this VAfixup #PF, continue access this VAmapping GVA&#45;&gt;GPA\\n in GUEST #PF hook&#45;&gt;fixup #PF, continue \\naccess this VAfixup #PF, continue \\naccess this VA&#45;&gt;EPT_violationGUP(HVA)GUP(HVA)find HVA though GPA&#45;&gt;GUP(HVA)mapping GPA&#45;&gt;HPAmapping GPA&#45;&gt;HPAGUP(HVA)&#45;&gt;mapping GPA&#45;&gt;HPAmapping GPA&#45;&gt;HPA&#45;&gt;access a VAfixup EPT violation, VM entry但是, 已经建立好映射的页面, 也是qemu进程的虚拟地址空间(匿名页), 是可以被swap out,当被swap out后, GUEST 访问该HPA对应的 GVA/GPA时, 仍然会触发 EPT violation. 这时还会再走一次 VM-exit, 而且也需要完成上面所述的三件事, 其中第二件:分配page, 需要swap in之前被swap out的page, 路径比较长, 如下:VM-exit handle_ept_violation kvm_mmu_page_fault tdp_page_fault gfn_to_pfn hva_to_pfn get_user_pages --slow pathget_user_pages会走到slow path, 由于会走swap in流程, 所以该过程执行较慢. 所以大佬们就想着能不能让其异步执行, 然后让vcpu先不complete 造成 EPT violation 的 instruction, 去干别的事情, 等page present后, 再去执行该指令. 另外将 get_user_pages 让一个 dedicated thread 去完成,这样, 对于虚拟机来说, 就相当于搞了一个额外的 硬件, 专门去处理 swap in, 解放了vcpu的算力. NOTE 大家思考下, 如果要达到该目的, 一定是让GUEST有意无意的 sche out 造成 EPT violation的进程,该上面流程总结如下:流程图graphviz-f8c81926c9687c237f8513f3a6fb3624digraph G { subgraph cluster_host { style=&quot;filled&quot; color=&quot;#693886699&quot; subgraph cluster_host_dedicated_thread { do_slow_path [ shape=&quot;note&quot; label=&quot;I&#39;m a delicated \\nthread, Like a \\nspecial hardware, \\nsharing the \\npressure of VCPU&quot; ] label=&quot;dedicated thread&quot; have_got_page_success [ label=&quot;work in done!\\n tell the guest&quot; ] do_slow_path-&gt;have_got_page_success [ label=&quot;a. get page, swap in...&quot; fontcolor=&quot;blue&quot; color=&quot;blue&quot; ] } subgraph cluster_host_kvm_vcpu_thread { ept_violation_handler [ label=&quot;ept violation handler&quot; ] dont_do_slow_path [ shape=&quot;note&quot; label=&quot;I don&#39;t want \\nhandle slow path, \\nit will speed\\nto much time&quot; ] tell_guest_sched_out [ shape=&quot;note&quot; label=&quot;work is doing,\\nneed wait\\n a a bit time,\\n let guest do\\n other things&quot; ] dont_do_slow_path -&gt;tell_guest_sched_out [ label=&quot;4.let guest \\ndo other thing&quot; fontcolor=&quot;green&quot; color=&quot;green&quot; ] ept_violation_handler-&gt; dont_do_slow_path [ label=&quot;2.find page swap out&quot; fontcolor=&quot;green&quot; color=&quot;green&quot; ] label=&quot;host kvm vcpu thread&quot; } label = &quot;host&quot; } subgraph cluster_guest { style=&quot;filled&quot; color=&quot;#77323456&quot; subgraph cluster_trigger_ept_violation_task { task1_access_a_memory [ label=&quot;acesss a memory\\n address [BEG]&quot; color=&quot;white&quot; style=&quot;filled&quot; ] label=&quot;TASK1 trigger ept vioaltion&quot; } subgraph cluster_sched_in_task2 { task2_run_a_time [ label=&quot;task2_run_a_time&quot; ] label=&quot;task2&quot; } label=&quot;guest&quot; } dont_do_slow_path-&gt;do_slow_path [ label=&quot;3. start a work \\nto do it&quot; fontcolor=&quot;green&quot; color=&quot;green&quot; ] task1_access_a_memory -&gt; ept_violation_handler [ label=&quot;1.page NOT present,\\ntrigger EPT violation&quot; fontcolor=&quot;green&quot; color=&quot;green&quot; ] have_got_page_success -&gt; task2_run_a_time [ label=&quot;b. page NOT present\\n SCHED IN&quot; fontcolor=&quot;blue&quot; color=&quot;blue&quot; ] tell_guest_sched_out -&gt; task1_access_a_memory [ label=&quot;5. page NOT present\\n SCHED OUT&quot; fontcolor=&quot;green&quot; color=&quot;green&quot; ] task2_run_a_time-&gt;task1_access_a_memory [ label=&quot;c. sched in\\n task1&quot; fontcolor=&quot;blue&quot; color=&quot;blue&quot; ] task1_access_a_memory-&gt;task2_run_a_time [ label=&quot;6.sched out\\n task1&quot; fontcolor=&quot;green&quot; color=&quot;green&quot; ]}Gcluster_hosthostcluster_host_dedicated_threaddedicated threadcluster_host_kvm_vcpu_threadhost kvm vcpu threadcluster_guestguestcluster_trigger_ept_violation_taskTASK1 trigger ept vioaltioncluster_sched_in_task2task2do_slow_pathI&#39;m a delicated thread, Like a special hardware, sharing the pressure of VCPUhave_got_page_successwork in done! tell the guestdo_slow_path&#45;&gt;have_got_page_successa. get page, swap in...task2_run_a_timetask2_run_a_timehave_got_page_success&#45;&gt;task2_run_a_timeb. page NOT present SCHED INept_violation_handlerept violation handlerdont_do_slow_pathI don&#39;t want handle slow path, it will speedto much timeept_violation_handler&#45;&gt;dont_do_slow_path2.find page swap outdont_do_slow_path&#45;&gt;do_slow_path3. start a work to do ittell_guest_sched_outwork is doing,need wait a a bit time, let guest do other thingsdont_do_slow_path&#45;&gt;tell_guest_sched_out4.let guest do other thingtask1_access_a_memoryacesss a memory address [BEG]tell_guest_sched_out&#45;&gt;task1_access_a_memory5. page NOT present SCHED OUTtask1_access_a_memory&#45;&gt;ept_violation_handler1.page NOT present,trigger EPT violationtask1_access_a_memory&#45;&gt;task2_run_a_time6.sched out task1task2_run_a_time&#45;&gt;task1_access_a_memoryc. sched in task1由上图可见, 引入async pf 的逻辑是让其能够在触发 EPT violation后, 能够让VCPU 调度到另外一个task, 从而阻塞触发 EPT violation 的进程执行. 为了达到这一目的, 做了以下改动: VCPU 线程在执行get_user_page()时, 仅执行fast path, 如果page 不是present的, 该接口直接返回, 而剩下的工作, 则交给另外一个dedicated thread 去做 KVM 会通过一些方式, 让 GUEST 执行调度, 从而避免再次执行触发EPT violation的指令. 而dedicatedthread 完成了swap in 的动作后, 会通知guest再次唤醒该之前调度出去的进程代码细节para virt interface一般的半虚拟化实现往往都有一下几个特征: use CPUID report this feature use MSR transparent less information, e.g. : a share memory address enable/disable use a share memory transparent more information而 para virt async PF 也是这样实现的. 在v1 Add shared memory hypercall to PV Linux guest版本中, 作者以hypercall的方式实现了半虚拟化, 但是avi在随后建议(link)使用MSR来替代 hypercall, 因为该方式在INIT和热迁移流程中有现成的 save/restore 接口 原文如下: Better to set this up as an MSR (with bit zero enabling, bits 1-5 features, and 64-byte alignment). This allows auto-reset on INIT and live migration using the existing MSR save/restore infrastructure.最好将其设置为MSR - bit 0: enabling - bit 1-5: features - 64-byte alignment他允许在INIT时 auto-reset, 并且可以使用现有的 MSR save/restore infrastructure 完成热迁移 接口流程图图示graphviz-5816c8c78422729ad7411897407bd311digraph G { subgraph cluster_host { host_page_not_present [ label=&quot;initiate page \\nnot present\\n APF&quot; color=&quot;red&quot; ] host_page_present [ label=&quot;initiate page \\nhave been\\n present APF&quot; color=&quot;green&quot; ] label=&quot;host&quot; } subgraph cluster_guest { pf_handler [ label=&quot;page fault handler&quot; ] guest_invoke_task [ label=&quot;invoke task&quot; ] label=&quot;guest&quot; } cpuid [ shape=&quot;record&quot; label=&quot;cpuid:\\n KVM_FEATURE_ASYNC_PF:\\n 1&quot; ] subgraph cluster_msr { msr_bit_map [ shape=&quot;record&quot; label=&quot;{ bit0\\n enable bit\\n value 1(enable)| bit 1-5\\n reserved\\n value 0| &lt;shm_gpa&gt;bit 63-6\\n 64-byte aligned GPA\\n value 0xabc }&quot; ] label=&quot;MSR_KVM_ASYNC_PF_EN&quot; } subgraph cluster_cr2 { token [ shape=&quot;record&quot; label=&quot;token: \\n unique id&quot; ] label=&quot;cr2&quot; } subgraph cluster_shm { shm [ shape=&quot;record&quot; label=&quot;APF reason&quot; ] label=&quot;share memory&quot; } cpuid-&gt;msr_bit_map [ arrowhead=&quot;none&quot; style=&quot;dashed&quot; label=&quot;indicate apf \\nfeature \\navailable,\\n so access\\n MSR_KVM_ASYNC_PF_EN \\nis valid&quot; ] msr_bit_map:shm_gpa-&gt;shm [ arrowhead=&quot;none&quot; style=&quot;dashed&quot; label=&quot;point base GPA \\nof this share\\n memory&quot; ] host_page_not_present-&gt;token [ label=&quot;1. initiate page \\nnot present \\nAPF, generate \\ntoken write \\nto CR2&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] host_page_not_present-&gt;shm [ label=&quot;2. update apf \\nreason to 1&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] host_page_not_present-&gt;pf_handler [ label=&quot;3. inject page \\nnot present \\n#APF&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] pf_handler-&gt;shm [ label=&quot;4. get reason \\nfrom shm:\\n PAGE \\nnot present\\n&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] pf_handler-&gt;token [ label=&quot;5. get token \\nfrom cr2,\\nbind sched\\n out thread\\n and token\\n&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] pf_handler-&gt;guest_invoke_task [ label=&quot;6. sched\\n out it&quot; color=&quot;red&quot; fontcolor=&quot;red&quot; ] host_page_present-&gt;token [ label=&quot;a. initiate page \\n present APF, \\nwrite prev \\ntoken write \\nto CR2&quot; color=&quot;green&quot; fontcolor=&quot;green&quot; ] host_page_present-&gt;shm [ label=&quot;b. update apf \\nreason to 2&quot; color=&quot;green&quot; fontcolor=&quot;green&quot; ] host_page_present-&gt;pf_handler [ label=&quot;c. inject page \\n present \\n#APF&quot; color=&quot;green&quot; fontcolor=&quot;green&quot; ] pf_handler-&gt;shm [ label=&quot;d. get reason \\nfrom shm:\\nPAGE present &quot; color=&quot;green&quot; fontcolor=&quot;green&quot; ] pf_handler-&gt;token [ label=&quot;e. get token \\nfrom cr2,\\n find sched \\nout thread \\nby token&quot; color=&quot;green&quot; fontcolor=&quot;green&quot; ] pf_handler-&gt;guest_invoke_task [ label=&quot;f.wakeup it&quot; color=&quot;green&quot; fontcolor=&quot;green&quot; ]}Gcluster_hosthostcluster_guestguestcluster_msrMSR_KVM_ASYNC_PF_ENcluster_cr2cr2cluster_shmshare memoryhost_page_not_presentinitiate page not present APFpf_handlerpage fault handlerhost_page_not_present&#45;&gt;pf_handler3. inject page not present #APFtokentoken: unique idhost_page_not_present&#45;&gt;token1. initiate page not present APF, generate token write to CR2shmAPF reasonhost_page_not_present&#45;&gt;shm2. update apf reason to 1host_page_presentinitiate page have been present APFhost_page_present&#45;&gt;pf_handlerc. inject page present #APFhost_page_present&#45;&gt;tokena. initiate page present APF, write prev token write to CR2host_page_present&#45;&gt;shmb. update apf reason to 2guest_invoke_taskinvoke taskpf_handler&#45;&gt;guest_invoke_task6. sched out itpf_handler&#45;&gt;guest_invoke_taskf.wakeup itpf_handler&#45;&gt;token5. get token from cr2,bind sched out thread and tokenpf_handler&#45;&gt;tokene. get token from cr2, find sched out thread by tokenpf_handler&#45;&gt;shm4. get reason from shm: PAGE not presentpf_handler&#45;&gt;shmd. get reason from shm:PAGE present cpuidcpuid: KVM_FEATURE_ASYNC_PF: 1msr_bit_mapbit0 enable bit value 1(enable)bit 1&#45;5 reserved value 0bit 63&#45;6 64&#45;byte aligned GPA value 0xabccpuid&#45;&gt;msr_bit_mapindicate apf feature available, so access MSR_KVM_ASYNC_PF_EN is validmsr_bit_map:shm_gpa&#45;&gt;shmpoint base GPA of this share memory图中描述了host, guest在处理async pf时, 对寄存器/share memory 的操作从图中可以看出, 会涉及到cpuid, MSR_KVM_ASYNC_PF_EN, share memory, 由于async pf 的实现,需要注入#PF, 所以还会涉及 CR2cpuid新增半虚拟化cpuid bit: KVM_FEATURE_ASYNC_PFdiff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h+#define KVM_FEATURE_ASYNC_PF\t\t4关于该bit的文档说明diff --git a/Documentation/kvm/cpuid.txt b/Documentation/kvm/cpuid.txt+KVM_FEATURE_ASYNC_PF || 4 || async pf can be enabled by+ || || writing to msr 0x4b564d02大致意思是, 该cpuid如果时能, 表示可以通过write to MSR (0x4b564d02) 来enable async pfMSR – share memaddr &amp;&amp; enable bitdiff --git a/arch/x86/include/asm/kvm_para.h b/arch/x86/include/asm/kvm_para.h+#define MSR_KVM_ASYNC_PF_EN 0x4b564d02文档说明:diff --git a/Documentation/kvm/msr.txt b/Documentation/kvm/msr.txt+ MSR_KVM_ASYNC_PF_EN: 0x4b564d02+ data: Bits 63-6 hold 64-byte aligned physical address of a+ 64 byte memory area which must be in guest RAM and must be+ zeroed. Bits 5-1 are reserved and should be zero. Bit 0 is 1+ when asynchronous page faults are enabled on the vcpu 0 when+ disabled. &gt; Bits 63-6 保存着 64-byte 对其的 一个64 byte memory area 的物理地址, &gt; 该memory area 必须是 guest RAM, 并且必须是被赋值为0. &gt; &gt; Bit 5-1 被reserved并且应该为0. &gt; &gt; 当 在 vcpu 0 启用 async pf enable async pf(当是disable时), &gt; Bit 0 是1该段主要介绍了MSR的 bit 组成: MSR bit Bit [63, 6]: a 64-byte aligned physical address Bit [5, 1]: reserved Bit 0 : enable bit 其实文档中还介绍了. share memory format 和 CR2, 但是为了方便阅读, 我们将拆分开到各个小节shared memory structure – APF reasondiff --git a/Documentation/kvm/msr.txt b/Documentation/kvm/msr.txt ...+ First 4 byte of 64 byte memory location will be written to by+ the hypervisor at the time of asynchronous page fault (APF)+ injection to indicate type of asynchronous page fault. Value+ of 1 means that the page referred to by the page fault is not+ present. Value 2 means that the page is now available. Disabling+ interrupt inhibits APFs. Guest must not enable interrupt+ before the reason is read, or it may be overwritten by another+ APF. Since APF uses the same exception vector as regular page+ fault guest must reset the reason to 0 before it does+ something that can generate normal page fault. If during page+ fault APF reason is 0 it means that this is regular page+ fault. &gt; 在 hypervisor 触发 APF 注入时, 4 byte memory location的前4个byte将被 &gt; 写入 来指示 APF 的类型. &gt; 1: page fault 涉及到的page 是 not present的. &gt; 2: page 现在已经 available &gt; 另外Disabling interrupt 将会 inhibits APF. &gt; &gt; Guest必须不能enable interrupt 在reason 被read之前, 否则可能会被另一个 &gt; APF覆盖. 因为 APF 使用 相同的 exception vector 作为 regular page &gt; fault, 所以在做可能生成normal page fault 的事情之前, guest 必须 reset &gt; reason to 0. 如果 在 page fault 期间, APF reason 为0, 他意味着这是一个 &gt; regular page fault.shared memory 一共有64 byte, 其中前4个byte(32 bit) 用来indicate apf type. hostkvm 在注入 apf之前会将type写入该地址.APF 有两种type(APF reason): 1: page is not present 2: not present page becomes available另外, 在处理APF时, guest和host有下面约束: 如果guest处于 disable interrupt, host不能注入apf guest必须在enable interrupt 之前, 处理完当前的apf guest必须在触发 normal #PF时, 处理完当前的apf, 并且reset reason to 0CR2diff --git a/Documentation/kvm/msr.txt b/Documentation/kvm/msr.txt ...+ During delivery of type 1 APF cr2 contains a token that will+ be used to notify a guest when missing page becomes+ available. When page becomes available type 2 APF is sent with+ cr2 set to the token associated with the page. There is special+ kind of token 0xffffffff which tells vcpu that it should wake+ up all processes waiting for APFs and no individual type 2 APFs+ will be sent. &gt; 在 type1 APF delivery 期间, cr2 包含了一个token, 当missing page &gt; becomes available, 该token将会用于通知guest. &gt; &gt; 当page becomes available, type2 APF 将会把 cr2 设置为和该page相关的 &gt; token. &gt; &gt; 这里有一个特殊的类型 token 0xffffffff, 他将告诉vcpu, 需要wakeup 所有 &gt; 等待APF的process 并且不会有单独的 type 2 APF 将会再发送 + If APF is disabled while there are outstanding APFs, they will+ not be delivered. &gt; 当 outstanding APFs时, 如果APF 被disabled, 他们将不会被delivered. + Currently type 2 APF will be always delivered on the same vcpu as+ type 1 was, but guest should not rely on that. &gt; 当前 type 2 APF 将始终在与type 1 相同的vcpu上deliver, 但是guest不应该依赖它.cr2 包含了一个token, 该token 用来唯一标识, 当前正在发生的APF 的 id. 但是其有一个特殊value 0xffffffff, 该值用来告诉vcpu, 需要wakeup所有的正在等待 APF (type 2) 的 进程. 并且不会有单独的type2再发送.另外还有几点约束和限制 如果还有 outstanding APFs 时, 如果 APF 被disable了, 他们将不会被deliver guest 不应该依赖 type2 APF 和 type1 APF在相同vcpu上deliver, 虽然目前是这样实现的. 大家可以思考下, 为什么要支持wake up all这样的API 可以想象一下热迁移场景. 当进行热迁移时, 我们先suspend vcpu, 然后迁移memory, 这时, 会等所有page swapin,然后在进行迁移, 但是这时, guest已经不能再去注入异常了, 只能等dest端在注入. 此时来到dest端, 这时所有的memory都是present的. 所以直接注入wakeup all就可以唤醒所有wait task.(当然, 也可能再此期间有swapout, 无非是再触发一次async pf)GUP change关于GUP 改动的细节我们放到link中介绍.STRUCT – host总体数据结构图比较简单, 如下: struct 结构图 graphviz-7842836ccacf1278b495c08c9dc5b30cdigraph G {\tsubgraph cluster_vcpu0 {\t\tkvm_vcpu0 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;{struct kvm_vcpu||&lt;queue&gt;queue|&lt;done&gt;done}&quot;\t\t]\t\tsubgraph cluster_uncomplete_work {\t\t\twork_uncomplete_1 [\t\t\t\tshape=&quot;record&quot;\t\t\t\tlabel=&quot;{kvm_vcpu_pf||&lt;queue&gt;queue|&lt;link&gt;link}&quot;\t\t\t]\t\t\twork_uncomplete_2 [\t\t\t\tshape=&quot;record&quot;\t\t\t\tlabel=&quot;{kvm_vcpu_pf||&lt;queue&gt;queue|&lt;link&gt;link}&quot;\t\t\t]\t\t\tlabel=&quot;uncomplete work&quot;\t\t}\t\tsubgraph cluster_done_work {\t\t\twork_done_1 [\t\t\t\tshape=&quot;record&quot;\t\t\t\tlabel=&quot;{kvm_vcpu_pf||&lt;queue&gt;queue|&lt;link&gt;link}&quot;\t\t\t]\t\t\twork_done_2 [\t\t\t\tshape=&quot;record&quot;\t\t\t\tlabel=&quot;{kvm_vcpu_pf||&lt;queue&gt;queue|&lt;link&gt;link}&quot;\t\t\t]\t\t\tlabel=&quot;done work&quot;\t\t}\t\tlabel = &quot;vcpu 0&quot;\t}\tkvm_vcpu0:queue-&gt;\t\twork_done_1:queue-&gt;\t\twork_done_2:queue-&gt;\t\twork_uncomplete_1:queue-&gt;\t\twork_uncomplete_2:queue [\t\tcolor=&quot;red&quot;\t]\tkvm_vcpu0:done-&gt;\t\twork_done_1:link-&gt;\t\twork_done_2:link [\t\tcolor=&quot;blue&quot;\t]\tsubgraph cluster_vcpu1 {\t\tkvm_vcpu1 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;struct kvm_vcpu&quot;\t\t]\t\twork_5 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;kvm_vcpu_pf&quot;\t\t]\t\twork_6 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;kvm_vcpu_pf&quot;\t\t]\t\tlabel = &quot;vcpu 1&quot;\t\tkvm_vcpu1-&gt;work_5-&gt;work_6\t}\tsubgraph cluster_vcpu2 {\t\tkvm_vcpu2 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;struct kvm_vcpu&quot;\t\t]\t\twork_7 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;kvm_vcpu_pf&quot;\t\t]\t\twork_8 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;kvm_vcpu_pf&quot;\t\t]\t\tlabel = &quot;vcpu 2&quot;\t\tkvm_vcpu2-&gt;work_7-&gt;work_8\t}\tsubgraph cluster_vcpu3 {\t\tkvm_vcpu3 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;struct kvm_vcpu&quot;\t\t]\t\twork_9 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;kvm_vcpu_pf&quot;\t\t]\t\twork_10 [\t\t\tshape=&quot;record&quot;\t\t\tlabel=&quot;kvm_vcpu_pf&quot;\t\t]\t\tlabel = &quot;vcpu 3&quot;\t\tkvm_vcpu3-&gt;work_9-&gt;work_10\t}}Gcluster_vcpu0vcpu 0cluster_uncomplete_workuncomplete workcluster_done_workdone workcluster_vcpu1vcpu 1cluster_vcpu2vcpu 2cluster_vcpu3vcpu 3kvm_vcpu0struct kvm_vcpu queuedonework_done_1kvm_vcpu_pf queuelinkkvm_vcpu0:queue&#45;&gt;work_done_1:queuekvm_vcpu0:done&#45;&gt;work_done_1:linkwork_uncomplete_1kvm_vcpu_pf queuelinkwork_uncomplete_2kvm_vcpu_pf queuelinkwork_uncomplete_1:queue&#45;&gt;work_uncomplete_2:queuework_done_2kvm_vcpu_pf queuelinkwork_done_1:queue&#45;&gt;work_done_2:queuework_done_1:link&#45;&gt;work_done_2:linkwork_done_2:queue&#45;&gt;work_uncomplete_1:queuekvm_vcpu1struct kvm_vcpuwork_5kvm_vcpu_pfkvm_vcpu1&#45;&gt;work_5work_6kvm_vcpu_pfwork_5&#45;&gt;work_6kvm_vcpu2struct kvm_vcpuwork_7kvm_vcpu_pfkvm_vcpu2&#45;&gt;work_7work_8kvm_vcpu_pfwork_7&#45;&gt;work_8kvm_vcpu3struct kvm_vcpuwork_9kvm_vcpu_pfkvm_vcpu3&#45;&gt;work_9work_10kvm_vcpu_pfwork_9&#45;&gt;work_10 每个cpu有自己链表, 串起属于该cpu的async pf work, 其中有两条链. queue: 串起所有work done: 串起所有完成的work struct kvm_async_pf该数据结构主要用来描述上面提到的dedicated threadstruct kvm_async_pf { struct work_struct work; struct list_head link; struct list_head queue; struct kvm_vcpu *vcpu; struct mm_struct *mm; gva_t gva; unsigned long addr; struct kvm_arch_async_pf arch; struct page *page; bool done;}; work: dedicated thread实例, 使用 workqueue机制 link: 在patch中, 链接点主要有一个: vcpu 的work完成队列 queue: 用于链接该vcpu上的所有 kvm_async_pf gva: 触发EPT violation, 需要get_user_page_slow的 GVA addr: hva done: indicate该work完没完成 kvm_arch_async_pf: struct kvm_arch_async_pf { u32 token; gfn_t gfn;}; token: 该成员用于唯一标识一次async PF, 由kvm_vcpu.arch.apf.id和vcpu-&gt;vcpu_id综合计算得到. 在注入#PF时, 会当作 CR2 传入GUEST, 方便guest管理每一次的async PF. 上面说提到的kvm_async_pf-&gt;link,kvm_async_pf-&gt;queue所链接的队列, 如下:CHANGE of struct kvm_vcpu@@ -104,6 +125,15 @@ struct kvm_vcpu { gpa_t mmio_phys_addr; #endif+#ifdef CONFIG_KVM_ASYNC_PF+ struct {+ u32 queued;+ struct list_head queue;+ struct list_head done;+ spinlock_t lock;+ } async_pf;+#endif queue: 链接所有kvm_async_pf(work) done: 链接以完成的kvm_async_pf(work) lock: 队列锁change of struct kvm_vcpu_archstruct kvm_vcpu_arch { ...+ struct {+ bool halted;+ gfn_t gfns[roundup_pow_of_two(ASYNC_PF_PER_VCPU)];+ struct gfn_to_hva_cache data;+ u64 msr_val;+ u32 id;+ bool send_user_only;+ } apf; ...} 该数据结构变动涉及多个patch, 这里把最终的数据结构变动列出. halted: 表示是否因为async PF halt 了vcpu gfns : 这里做了一个数组, 用于记录所有现存的async pf work 的 gfn data: 相当于HVA-&gt;HPA的cache, 这个映射关系一直存在且不变(大多数情况下, 除非执行__kvm_set_memory_region更改映射关系), 该HPA 指向上面提到的 share memory 该部分被作者做成了一个通用功能, 相当于是 memslot-cached kvm_put_guest()and kvm_get_guest(). 我们放到另一篇文章中介绍. 主要介绍这个功能引入和其实现. msr_val: 记录guest设置的msr值 id: 记录下一个async pf work的id, 和kvm_vcpu-&gt;vcpu_id一起,唯一标识一次async PF send_user_only: 表示只有trigger EPT violation in guest user space, host才能做async PFSTRUCT - GUESTguest 数据结构主要是用于管理, 因为async PF 调度出去的task.数据结构图 数据结构图 graphviz-e889c62c04290bdb7d5685fb187da7d7digraph G { sleep_head [ shape=&quot;record&quot; label=&quot;{ kvm_task_sleep_head|| [0]| &lt;key0&gt;link(key0)|| [1]| &lt;key1&gt;link(key1)|| [2]| &lt;key2&gt;link(key2) }&quot; ] subgraph cluster_cpu0 { sleep_node0 [ shape=&quot;record&quot; label=&quot;{ kvm_task_sleep_node|| &lt;link&gt;link| token=[id=0, vcpu=0]| cpu=0| mm=mm_struct of task0| halted=false }&quot; ] sleep_node1 [ shape=&quot;record&quot; label=&quot;{ kvm_task_sleep_node|| &lt;link&gt;link| token=[id=1, vcpu=0]| cpu=0| mm=mm_struct of task1| halted=false }&quot; ] run_task_vcpu0 [ label=&quot;current task: task2&quot; shape=&quot;record&quot; color=&quot;red&quot; ] label=&quot;cpu0 RUNNING&quot; } subgraph cluster_cpu1 { run_task_vcpu1 [ label=&quot;current task: task4&quot; shape=&quot;record&quot; color=&quot;red&quot; ] sleep_node3 [ shape=&quot;record&quot; label=&quot;{ kvm_task_sleep_node|| &lt;link&gt;link| token=[id=0, vcpu=1]| cpu=1| mm=mm_struct of task3| halted=false }&quot; ] sleep_node4 [ shape=&quot;record&quot; label=&quot;{ kvm_task_sleep_node|| &lt;link&gt;link| token=[id=1, vcpu=1]| cpu=1| mm=mm_struct of task4| halted=true }&quot; color=&quot;red&quot; ] label=&quot;cpu1 HALT&quot; } sleep_head:key0-&gt;sleep_node0:link [ color=&quot;blue&quot; ] sleep_head:key1-&gt;sleep_node1:link [ color=&quot;gold&quot; ] sleep_head:key2-&gt; sleep_node3:link-&gt; sleep_node4:link [ color=&quot;green&quot; ] sleep_node4-&gt;run_task_vcpu1 [ arrowhead=none style=dashed ]}Gcluster_cpu0cpu0 RUNNINGcluster_cpu1cpu1 HALTsleep_headkvm_task_sleep_head [0]link(key0) [1]link(key1) [2]link(key2)sleep_node0kvm_task_sleep_node linktoken=[id=0, vcpu=0]cpu=0mm=mm_struct of task0halted=falsesleep_head:key0&#45;&gt;sleep_node0:linksleep_node1kvm_task_sleep_node linktoken=[id=1, vcpu=0]cpu=0mm=mm_struct of task1halted=falsesleep_head:key1&#45;&gt;sleep_node1:linksleep_node3kvm_task_sleep_node linktoken=[id=0, vcpu=1]cpu=1mm=mm_struct of task3halted=falsesleep_head:key2&#45;&gt;sleep_node3:linkrun_task_vcpu0current task: task2run_task_vcpu1current task: task4sleep_node4kvm_task_sleep_node linktoken=[id=1, vcpu=1]cpu=1mm=mm_struct of task4halted=truesleep_node3:link&#45;&gt;sleep_node4:linksleep_node4&#45;&gt;run_task_vcpu1 图中一共有4个涉及async PF的task, 同时每个task关联一个kvm_task_sleep_node kvm_task_sleep_head[]-&gt;link负责将所有key相同的 sleep_node串联起来, 方便查找 每个kvm_task_sleep_node有一个唯一的 identify kvm_task_sleep_node-&gt;token cpu0 上之前触发过两次async PF, 并且涉及到的task调度走了,目前正在运行task2 cpu1 上触发过两次async PF, 当task3 触发时, 成功将task3 sched out, 当task4触发时, 由于此时guest vcpu 不能调度, 所以将该cpu halt. 目前该cpu正在task4的上下文中halt. kvm_task_sleep_headstatic struct kvm_task_sleep_head { spinlock_t lock; struct hlist_head list;} async_pf_sleepers[KVM_TASK_SLEEP_HASHSIZE];该数据结构是一个hash map, 使用token作为hash key. lock: 可以看到是每个hash key, 有一个lock. 减少race情况kvm_task_sleep_nodestruct kvm_task_sleep_node { struct hlist_node link; wait_queue_head_t wq; u32 token; int cpu; bool halted; struct mm_struct *mm;};该数据结构作为hash node, 描述每一个因为async pf 调度出去的task 这里并不一定指被调度出去的task, 可能链接着即将发生调度的task信息,我们下面会介绍到. wq: 等待队列 token: 和上面描述一样, 唯一标识一次async PF halted: 有时候kvm注入async PF时, guest在这个时间点不能做schedule, 又 为了再次避免执行该代码流, 只能halt 该cpu. 这里用于标识是否该task halt了cpuinitiate async pf-&gt;inject async pf上面提到了为了使用GUP noio接口, 将tdp_page_fault中的gfn_to_pfn改动为try_async_pf. 我们来看下该接口try_async_pfstatic bool try_async_pf(struct kvm_vcpu *vcpu, gfn_t gfn, gva_t gva, pfn_t *pfn){ bool async; //==(1)== *pfn = gfn_to_pfn_async(vcpu-&gt;kvm, gfn, &amp;async); //==(2)== if (!async) return false; /* *pfn has correct page already */ //==(3)== put_page(pfn_to_page(*pfn)); //==(4)== if (can_do_async_pf(vcpu)) { trace_kvm_try_async_get_page(async, *pfn); //==(5)== if (kvm_find_async_pf_gfn(vcpu, gfn)) { trace_kvm_async_pf_doublefault(gva, gfn); kvm_make_request(KVM_REQ_APF_HALT, vcpu); return true; //==(6)== } else if (kvm_arch_setup_async_pf(vcpu, gva, gfn)) return true; } //==(7)== *pfn = gfn_to_pfn(vcpu-&gt;kvm, gfn); return false;} 前面提到过, 在try_async_pf 中会执行到gfn_to_pfn_async(), async作为oparam 表示是否需要做async pf, 另外还有一个返回值, 该返回值表示在该过程中得到的 pfn of gfn 当然, 如果得到的async为false, 说明不需要async pf, 那肯定得到了pfn所以直接返回 false put_page 这里会判断当前vcpu的状态是否可以做async pf can_do_async_pf细节 +static bool can_do_async_pf(struct kvm_vcpu *vcpu)+{+\tif (unlikely(!irqchip_in_kernel(vcpu-&gt;kvm) ||+\t\t kvm_event_needs_reinjection(vcpu)))+\t\treturn false;++\treturn kvm_x86_ops-&gt;interrupt_allowed(vcpu);+} 我们这里详细讲解下, 这三个判断条件, irqchip_in_kernel() kvm_event_need_reinjection(): static inline bool kvm_event_needs_reinjection(struct kvm_vcpu *vcpu){ return vcpu-&gt;arch.exception.pending || vcpu-&gt;arch.interrupt.pending || vcpu-&gt;arch.nmi_injected;} 可以看到这里, 在检测到有其他pending 事件的情况下, 不允许做async pf. 自己的理解 关于pending的event, 我们需要参考__vmx_complete_interrupts, 但是这里我们不过度展开, 大概就是在 VM entry inject event 期间, 由于某些原因, 触发了VM exit, 此时, VM entry, 还没有完成, 所以这些事件并没有被inject, 需要再次VM entry时注入. 再这种情况下, 就会有这样的顺序 inject_event1-&gt; VM entry-&gt; VM exit(get uncomplete event)-&gt; get vm exit reason: EPT violation PAGE not present-&gt; (do some handler)-&gt; VM entry 那现在问题来了, 本次是该注入async PF, 还是注入 uncomplete event呢? 我个人认为是注入uncomplete event. 首先按照顺序 uncomplete event先发生.如果不注入 uncomplete event的情况下, 直接注入async pf, 给guest感觉是某些event延后了. 另外, uncomplete event是由于 EPT violation 而触发的. 所以在本次处理完EPT violation之后,正好可以注入 uncomplete event, 并且大概率不会再次触发VM exit during EVENTinject. 以上是自己的理解, 而且不确定处理 tdp_page_fault()时, 所有的event是否都来自于上一次注入失败的uncomplete event. 遗留问题 interrupt_allowed: 我们来看下intel vmx 代码 static int vmx_interrupt_allowed(struct kvm_vcpu *vcpu){ return (vmcs_readl(GUEST_RFLAGS) &amp; X86_EFLAGS_IF) &amp;&amp; !(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) &amp; (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS));} 该部分代码, 主要是检测当前interrupt windows 是否open, 这里对 这些判断条件不做过多解释, 详细见virtual interrupt 文章 还未写该文章 遗留部分 但是我们需要理解下, 为什么要关注guest 是否能接收中断呢? 毕竟async pf 注入的是#PF首先我们需要明确的是: 自己的理解 Q: async pf的目的是什么? A: 调度 Q: 该调度能发生在guest 运行的任何时机么 A: 需要满足guest意愿 所以, 综上所述, 得需要在guest认为自己可以调度的情况下, 才能做async pf这个事情. 否则,即使去启动了一个dedicated thread, 让guest调度, guest也不会去调度, 这样就没有意义了. 那好在这样的背景下, 我们分情况考虑: non-para virt: halt 在halt vcpu之后, 能够wakeup vcpu的方式有两种event interrupt async pf work complete 那在guest 不能注入中断的情况下, 只能由第二种event wakeup, 那就变成了sync的方式. 没有意义. para virt, 因为是半虚拟化方式, 相当于通知guest去主动做一次调度, 但是也得满足guest意愿.这实际上就像是和guest 协商的过程, 需要去关心guest这一刻是否能做调度. 作者在介绍MSR_KVM_ASYNC_PF_EN明确了, guest在关中断时, 不能去再次注入async PF, guest可能还处在APF handler中. 如果在此期间再次注入APF, 可能会导致 APF information 被覆盖, 例如: host guest cr2write token(a) to cr2 value: ainject APF1 trigger #PF (disable interrupt in VM-entry) do some thing...write token(b) to cr2 value: binject APF2 intend to read cr2 to get APF1 token, loss it !!! 在avi 的自问自答 中, 我们也能看到关于interrupt allow的解释. 这里说明之前, 该vcpu触发过该地址的 EPT violation , 并且已经做了async pf, 相当于再次遇到了.说明频率比较高, 那么直接halt该vcpu ????????? 下个小节中介绍 如果上述条件不满足, 则直接同步去做.kvm_setup_async_pfint kvm_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, gfn_t gfn, struct kvm_arch_async_pf *arch){ struct kvm_async_pf *work; //==(1)== if (vcpu-&gt;async_pf.queued &gt;= ASYNC_PF_PER_VCPU) return 0; /* setup delayed work */ /* * do alloc nowait since if we are going to sleep anyway we * may as well sleep faulting in page */ //==(2)== work = kmem_cache_zalloc(async_pf_cache, GFP_NOWAIT); if (!work) return 0; work-&gt;page = NULL; work-&gt;done = false; work-&gt;vcpu = vcpu; work-&gt;gva = gva; work-&gt;addr = gfn_to_hva(vcpu-&gt;kvm, gfn); work-&gt;arch = *arch; work-&gt;mm = current-&gt;mm; atomic_inc(&amp;work-&gt;mm-&gt;mm_count); kvm_get_kvm(work-&gt;vcpu-&gt;kvm); /* this can't really happen otherwise gfn_to_pfn_async would succeed */ if (unlikely(kvm_is_error_hva(work-&gt;addr))) goto retry_sync; //==(2.1)== INIT_WORK(&amp;work-&gt;work, async_pf_execute); //==(3)== if (!schedule_work(&amp;work-&gt;work)) goto retry_sync; //==(4)== list_add_tail(&amp;work-&gt;queue, &amp;vcpu-&gt;async_pf.queue); vcpu-&gt;async_pf.queued++; //==(5)== kvm_arch_async_page_not_present(vcpu, work); return 1;retry_sync: kvm_put_kvm(work-&gt;vcpu-&gt;kvm); mmdrop(work-&gt;mm); kmem_cache_free(async_pf_cache, work); return 0;} 说明per cpu async_pf(work)超过了最大限制 – ASYNC_PF_PER_VCPU 申请,work并做相关初始化, 在(2.1)中将work hook设置为async_pf_execute schedule work 将work加到 vcpu-&gt;async_pf.queue队列中 代码如下: void kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu, struct kvm_async_pf *work){ trace_kvm_async_pf_not_present(work-&gt;arch.token, work-&gt;gva); kvm_add_async_pf_gfn(vcpu, work-&gt;arch.gfn); //==(1)== if (!(vcpu-&gt;arch.apf.msr_val &amp; KVM_ASYNC_PF_ENABLED) || (vcpu-&gt;arch.apf.send_user_only &amp;&amp; kvm_x86_ops-&gt;get_cpl(vcpu) == 0)) kvm_make_request(KVM_REQ_APF_HALT, vcpu); //==(2)== else if (!apf_put_user(vcpu, KVM_PV_REASON_PAGE_NOT_PRESENT)) { vcpu-&gt;arch.fault.error_code = 0; vcpu-&gt;arch.fault.address = work-&gt;arch.token; kvm_inject_page_fault(vcpu); }} 和can_do_async_pf, 这里也有一些判断当前状态是否合适向guest注入async pf的条件, 我们放到下面介绍 如果可以注入, 则将KVM_PV_REASON_PAGE_NOT_PRESENT其写入 guest host 共享的内存中, 表示本次注入的是page not present类型的 async pf. 另外, 设置好本次注入异常的 address和 error code async pf workstatic void async_pf_execute(struct work_struct *work){ struct page *page = NULL; struct kvm_async_pf *apf = container_of(work, struct kvm_async_pf, work); struct mm_struct *mm = apf-&gt;mm; struct kvm_vcpu *vcpu = apf-&gt;vcpu; unsigned long addr = apf-&gt;addr; gva_t gva = apf-&gt;gva; might_sleep(); use_mm(mm); down_read(&amp;mm-&gt;mmap_sem); //==(1)== get_user_pages(current, mm, addr, 1, 1, 0, &amp;page, NULL); up_read(&amp;mm-&gt;mmap_sem); unuse_mm(mm); spin_lock(&amp;vcpu-&gt;async_pf.lock); //==(2)== list_add_tail(&amp;apf-&gt;link, &amp;vcpu-&gt;async_pf.done); apf-&gt;page = page; apf-&gt;done = true; spin_unlock(&amp;vcpu-&gt;async_pf.lock); /* * apf may be freed by kvm_check_async_pf_completion() after * this point */ trace_kvm_async_pf_completed(addr, page, gva); //==(3)== if (waitqueue_active(&amp;vcpu-&gt;wq)) wake_up_interruptible(&amp;vcpu-&gt;wq); mmdrop(mm); kvm_put_kvm(vcpu-&gt;kvm);} 调用get_user_pages, 该接口可以处理MAJOR fault get_user_pages() 第四个参数, 如果不为空,则会设置FOLL_GET int get_user_pages(struct task_struct *tsk, struct mm_struct *mm, unsigned long start, int nr_pages, int write, int force, struct page **pages, struct vm_area_struct **vmas){ int flags = FOLL_TOUCH; if (pages) flags |= FOLL_GET; ...} 如果设置了FOLL_GET, 则会在get_user_pages()的过程中, pin this page.也就是get_page(), 但是需要注意的是, 该接口可能会返回错误, 但是看起来此流程并没有判断该接口是否执行成功. IOW, 无论该接口是否执行成功, 都认为该work已经complete, 都需要再次wakeup GUEST blocking thread. 将该work, 链接到vcpu-&gt;async_pf.done链表中 如果vcpu在等待队列中(halt), 唤醒该vcpu接下来, 我们来看下, host是如何检测 page present事件, 并注入page present async pf的host inject PAGE PRESENT aync pf@@ -5272,6 +5288,9 @@ static int __vcpu_run(struct kvm_vcpu *vcpu) \t\t\tvcpu-&gt;run-&gt;exit_reason = KVM_EXIT_INTR; \t\t\t++vcpu-&gt;stat.request_irq_exits; \t\t}+\t\t+\t\tkvm_check_async_pf_completion(vcpu);+ \t\tif (signal_pending(current)) { \t\t\tr = -EINTR;在vm exit后, 检测是否有需要 async pf completevoid kvm_check_async_pf_completion(struct kvm_vcpu *vcpu){ struct kvm_async_pf *work; //==(1)== if (list_empty_careful(&amp;vcpu-&gt;async_pf.done) || !kvm_arch_can_inject_async_page_present(vcpu)) return; spin_lock(&amp;vcpu-&gt;async_pf.lock); work = list_first_entry(&amp;vcpu-&gt;async_pf.done, typeof(*work), link); list_del(&amp;work-&gt;link); spin_unlock(&amp;vcpu-&gt;async_pf.lock); //==(2)== if (work-&gt;page) kvm_arch_async_page_ready(vcpu, work); //==(3)== kvm_arch_async_page_present(vcpu, work); list_del(&amp;work-&gt;queue); vcpu-&gt;async_pf.queued--; if (work-&gt;page) put_page(work-&gt;page); kmem_cache_free(async_pf_cache, work);} 有两个判断条件: 判断是否有完成的work guest此时是否适合注入 page present async PF (下面章节介绍) 如果work-&gt;page为 NULL, 说明async work中, 执行get_user_pages()失败了, 那么本次就不需要在执行kvm_arch_async_page_ready(), 该函数作用是, 再次执行tdp_page_fault, 如果page is ready, 那只需要执行get_user_page fast path和__direct_map建立GPA-&gt;HPA的映射. 但是如果page is not ready(work-&gt;page)为NULL, 作者的想法是, 让其在次vm entry,wakeup guest blocking thread, 让其再次触发EPT violation, 然后再发起async pf.所以在这里没有必要在做一次kvm_arch_async_page_ready-&gt;tdp_page_fault, 那可能有同学会说, 那为什么不在HOST中, 等待get_user_pages()一定返回成功之后, 再注入 page present #PF, 实话说,我也不知道, 但这里总感觉作者不想增加复杂的代码逻辑, 需要关注下后续的patch,看看是否对这部分有优化 遗留问题 kvm_arch_async_page_present void kvm_arch_async_page_present(struct kvm_vcpu *vcpu, struct kvm_async_pf *work){ trace_kvm_async_pf_ready(work-&gt;arch.token, work-&gt;gva); //==(1)== if (is_error_page(work-&gt;page)) work-&gt;arch.token = ~0; /* broadcast wakeup */ else kvm_del_async_pf_gfn(vcpu, work-&gt;arch.gfn); //==(2)== if ((vcpu-&gt;arch.apf.msr_val &amp; KVM_ASYNC_PF_ENABLED) &amp;&amp; !apf_put_user(vcpu, KVM_PV_REASON_PAGE_READY)) { vcpu-&gt;arch.fault.error_code = 0; vcpu-&gt;arch.fault.address = work-&gt;arch.token; kvm_inject_page_fault(vcpu); }} 关于error page, 我们放在另一篇文章中讲述. 遗留问题 置入KVM_ASYNC_PF_PF_ENABLED, 准备注入 page present async #PF guest handle async PFdotraplinkage void __kprobesdo_async_page_fault(struct pt_regs *regs, unsigned long error_code){ //==(1)== switch (kvm_read_and_reset_pf_reason()) { default: //==(2)== do_page_fault(regs, error_code); break; case KVM_PV_REASON_PAGE_NOT_PRESENT: //==(3)== /* page is swapped out by the host. */ kvm_async_pf_task_wait((u32)read_cr2()); break; //==(4)== case KVM_PV_REASON_PAGE_READY: kvm_async_pf_task_wake((u32)read_cr2()); break; }}该部分代码逻辑很清晰, async PF event 是使用了原有的#PF exception vector,guest 需要在exception handler 中判断这个#PF的类型, 然后执行相应的handler 从share memory 中获取 async pf reason indicate NORMAL #PF indicate PAGE NOT PRESENT async pf indicate PAGE PRESENT async pfpage not present async pfvoid kvm_async_pf_task_wait(u32 token){ u32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS); struct kvm_task_sleep_head *b = &amp;async_pf_sleepers[key]; struct kvm_task_sleep_node n, *e; DEFINE_WAIT(wait); int cpu, idle; cpu = get_cpu(); idle = idle_cpu(cpu); put_cpu(); spin_lock(&amp;b-&gt;lock); //===(1)== e = _find_apf_task(b, token); if (e) { /* dummy entry exist -&gt; wake up was delivered ahead of PF */ hlist_del(&amp;e-&gt;link); kfree(e); spin_unlock(&amp;b-&gt;lock); return; } //===(2)== n.token = token; n.cpu = smp_processor_id(); n.mm = current-&gt;active_mm; //===(2.1)== n.halted = idle || preempt_count() &gt; 1; atomic_inc(&amp;n.mm-&gt;mm_count); init_waitqueue_head(&amp;n.wq); //===(3)== hlist_add_head(&amp;n.link, &amp;b-&gt;list); spin_unlock(&amp;b-&gt;lock); for (;;) { //===(4)== if (!n.halted) prepare_to_wait(&amp;n.wq, &amp;wait, TASK_UNINTERRUPTIBLE); if (hlist_unhashed(&amp;n.link)) break; //===(4)== if (!n.halted) { local_irq_enable(); schedule(); local_irq_disable(); } else { /* * We cannot reschedule. So halt. */ native_safe_halt(); local_irq_disable(); } } if (!n.halted) finish_wait(&amp;n.wq, &amp;wait); return;} 在kernel doc介绍MSR_KVM_ASYNC_PF_EN, 作者有提到过. 一对[type2 APF, type1 APF] 不一定会在同一个vcpu上触发, 那也就意味着两者可能并行执行(虽然现在的host kvm 没有这样做,但是guest不能依赖它), 如下: kvm vcpu1 vcpu21.inject type1 APF to VCPU1 2. inject type2 APF to VCPU2 3. handle type2 APF 4. handle type1 APF 可以看到kvm虽然是按照顺序注入的type1 APF, 和type2 APF, 但是注入到了不同的vcpu. vcpu在处理时,handle type2 APF先执行, 此时page 已经present了, 不需要再sched out, 这里会在type2 APFhandler中预先将带有该token的sleep_node放到head中, 以便type 1 APF handler 可以跳过这次的sched out(需要结合type2 APF handle – kvm_async_pf_task_wake().) 将task(current-&gt;active_mm)和token绑定, 这样当type2 APF触发时, 可以根据token找到当前block的task 需要注意的时, guest在某些情况下不能sched out, 这时, 只能halt当前cpu 我们放到另一篇文章中去介绍 遗留问题 将sleep_node链到sleep_head上 如果guest此时可以调度, 则将进程D住, sched outpage present async pfvoid kvm_async_pf_task_wake(u32 token){ u32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS); struct kvm_task_sleep_head *b = &amp;async_pf_sleepers[key]; struct kvm_task_sleep_node *n; if (token == ~0) { apf_task_wake_all(); return; }again: spin_lock(&amp;b-&gt;lock); //===(1)== n = _find_apf_task(b, token); //===(2)== if (!n) { /* * async PF was not yet handled. * Add dummy entry for the token. */ n = kmalloc(sizeof(*n), GFP_ATOMIC); if (!n) { /* * Allocation failed! Busy wait while other cpu * handles async PF. */ spin_unlock(&amp;b-&gt;lock); cpu_relax(); goto again; } n-&gt;token = token; n-&gt;cpu = smp_processor_id(); n-&gt;mm = NULL; init_waitqueue_head(&amp;n-&gt;wq); hlist_add_head(&amp;n-&gt;link, &amp;b-&gt;list); } else //===(3)== apf_task_wake_one(n); spin_unlock(&amp;b-&gt;lock); return;} 根据token, 在sleep_head中查找sleep_node 同type1 APF handler, type2 APF可能在于type1 APF不同的cpu上先执行, 此时在sleep_head中找不到和该token相关的sleep_node, 这时, 需要新创建一个sleep_node将其添加到sleep_head中, 以便type1 APF handler可以查找到,避免block该task 如果查找到了, 说明type1 APF handler已经触发, task已经block, 需要wakeup该task参考链接 MAIL list:v1 v2 v3 v4 v5 v6 v7 " } ]
