[ { "title": null, "url": "/posts/2025-12-26-non-scalable-vs-scalable-spinlock/", "categories": "", "tags": "", "date": "2025-12-29 14:52:50 +0800", "content": "自旋锁是一种会让尝试获取它的线程陷入循环 （“自旋”）并不断检查锁是否可用的锁 1。 和mutex 不同，mutex 可以睡眠，将cpu让渡给其他的程序，而自旋锁则是占据着cpu资源忙 等。忙等最主要的有点时，避免了调度所带来的上下文开销, 可以提升等锁进程获得锁的延 迟。另外，如果加锁的临界区很小，自旋锁忙等所带来的开销，可能会小于上下文切换的开 销，自旋锁的收益就会非常大。所以，自旋锁适用于临界区小的场景。 overflow 本文主要是来讲述, spinlock 的可伸缩性(scalable). 可扩展性是指系统处理不断增长的 工作量的能力。软件系统的可扩展性定义之一是，可以通过向系统添加资源来实现 2. 而对于spinlock而言, 如何增加其工作量呢？ 增加并行调用spinlock的cpu 在介绍之前我们先思考下，自旋锁和 “smp”, “up” 关系3。首先按照自旋锁 的逻辑，自旋锁的临界区是不能睡眠的，这个动作是非常危险的，容易造成死锁。所以 UP下(单CPU) 自旋锁是没有意义的。 所以自旋锁是服务于smp的。而随着cpu的发展，cpu的核心越来越多, 并行调用spinlock的 数量也大大增加。而传统的spinlock则在 smp 场景中体现出了 non-scalable。 接下来的章节，我们首先介绍一些硬件背景，包括 cache snoop method cost of cache coherence 而之后，便介绍因cache coherence 给spinlock带来的性能影响。最后，介绍scalable spinlock 是如何规避这一问题的。 文本主要参考老黄和狗哥的文章4,5 以及Non-scalable locks are dangerous 论文6. cache coherence 关于cache coherence 很值得单独写一篇文章去总结( TODO ). 这里我们简单看下 snoop-based coherence non-scalable 问题。以及其改进版本 directory-based coherence 给spinlock 所带来的non-scalable问题。 本节内容(包括大量的图片) 均参考 CMU 15-418/618 lecture7, 8 为了降低cpu访问内存的延迟, 在cpu 和 内存之间加了一层cache。但是在smp架构下，每个 cpu都有自己的cache，所以相当于main memory 所存储的数据在各个cpu上都有了副本。 这回带来一致性问题。而cpu硬件实现了一套无需软件参与的缓存一致性协议，用来维护各 个 cpu cache之间以及和主存之间的一致性。 在早期的实现中，cpu数量不多。cache一致性使用snoop-based的方式，该方式的特点是 广播，而随着cpu数量越来越多，尤其是NUMA架构的兴起，广播的代价越来越大， snoop-based方式则变得non-scalable. 于是大佬们搞出了非广播的点对点的 directory-based coherence一致性协议。 snoop based snoop-based coherence 是在缓存状态改变时，通过广播MESI 消息来维护各个cpu. 图中状态左侧部分表示，当前cpu 发起访存动作是要做的一些动作(包括发一些广播信号, 以 及置当前cacheline的状态)。而图中右侧则表示 cpu 被动的收到广播信号时, 所需要执行 的动作（包括置cacheline状态以及一些额外的动作, E.g. flush). 我们举一个例子，不展开各个状态。 如果当前cpu要write一个内存，该内存所对应的cache是 invalid(对应右侧I-&gt;M) 该cpu需要 发起BusRdx 消息广播. (BuxRdx 表示一次写广播请求, 表示当前有人要写该 cacheline, 这里有一些serialization的问题这里不展开) 将该内存地址对应的cacheline的状态由 I -&gt; M 执行PrWr(PrWr表示cpu发起写操作，可以理解为改cacheline的内容了) 而其他cpu收到BusRdx后需要 将cacheline状态修改X-&gt;I(X为M, E, S 任意状态) 如果是M状态, 可能还要执行flush动作.(这里有一些序列化的问题, 例如这个flush 的动作要不要发生发起写操作的cpu的PrWr 之前) snoop-based non-scalable 原因主要来自广播。每次cache miss(invalid)发生时，需要通知 其他所有的cache。而在numa架构下这个问题更为突出。 在numa架构下，软件尽可能的设计成numa亲和性方案，即让当前numa上cpu运行的程序，访 问该numa的内存，但是由于 snoop-based conherence 的存在, 当我们访问near memory时 还是需要向所有的cpu进行广播, 这导致软件层面的优化几乎不起作用。 于是大佬们在思考能不能不再广播，而是通过点对点的方式，向特定的cpu发送cache y conherence 消息。而这就需要一个数据库，记录cache在各个cpu中的状态，从而辅助 发起消息的cpu选择向哪些cpu发送。 directory-based 既然需要一个数据库，我们则维护一个名为directory 的database: 每个内存 在directory中维护了一个条目，每一个条目包括: Dirty bit: 表示在某个cpu-cache中。其缓存是dirty的。 $p$ presence bit: 这是一个数组，每个字节表示该内存在其代表的cpu cache中 是否存在 directory存在于L3 缓存组中(可能哈, 个人认为将l3当成了一个directory，该缓存条目中 除了包括常规的缓存信息外，还包括了和 directory 相关的信息。个人瞎猜. 我们来看下两个场景, read miss to dirty line 和write miss(这对应于竞争较激烈的 spinlock场景) read miss to dirty line 初始状态 cpu 0 要访问 near cpu 1 memory cpu 0 中没有cache, directory entry dirtybit 为1 cpu2 中有该地址的cache, 并且没有writeback, directory entry $p$ 中有 cpu 2 执行步骤: (步骤1)cpu 0 发起一个read miss msg的request. (步骤2)对应的memory 的 directory 查询其该条目中的 $p$ array, 查到cpu 2。并返 回消息给 cpu0 (步骤3) cpu0 向cpu2 请求缓存内容 (步骤4) cpu2 返回给cpu0 (步骤5) cpu2 返回给cpu1, dir的变更信息包括:cpu 0 请求了该cacheline，所以 $p$中 要置位 cpu0 bit 另外, 将dirty data 也传递到cpu1, cpu1 收到后，将dirty data flush 到memory, 并 clear dirty bit non-scalable spinlock 的问题主要就在这一过程中，我们在之后的后面的章节中展开 我们接下来看下write miss write-miss 初始状态 cpu0 要写 near cpu 1 memory cpu0 没有cache，directory entry dirtybit为0 cpu0 没有cache, cpu1, cpu2 上有clean cache, $p$ 中有 cpu1, cpu2 执行步骤: (步骤1) cpu0 向directory 发 write miss msg (步骤2) 由于cache 是clear的, directory 直接将clear data + ids 返回给cpu0 directory 修改 $p$ clean {0, 1, 1} –&gt; dirty {1, 0, 0} (步骤3) cpu0 向cpu1, 2发送 invalid msg 消息 (步骤4) cpu1, cpu2 发送ack 给cpu0 其实这个过程性能并不低，因为这是一个一对多，并且允许每个点对点的{request, response} 可以异步执行。但是这是一个触发器，其在spinlock过程中会将其他cpu的cache clean line掉。 其他cpu 会再执行read miss流程… 文章4, 5介绍缓存一致性时，从write-update, write-invalid角度展开 两个缓存一致性协议，个人查找资料发现write-update似乎不主流的缓存一致性实现中 9 大家可以设想下，write-update一个好处是，在write操作发起时，会将cache 更新到 其他的缓存中，而不是让其失效。这样其他cpu 后续访问该缓存时，可以直接从当前 cpu local cache 中获取到干净的（而不是从主存)。 但是只update 不invalid 会有明显的问题，因为被update 的cpu可能很长一段时间不会访问 该cache，而这段时间内如果有cpu write 这个地址，都会update 该cpu的cache。这会 造成很大的浪费。而基于 invalid 协议，会在写操作时invalid，在read时 触发cache-miss, 同时，也会从其他cpu的缓存中获取数据而不是直接从缓存获取, 总之，这种在 read-miss时按需获取再搭配 directory-based conherence 协议看起来比简单的 write-update 协议优秀很多。(当然可能cpu有一些更高级的预测功能，可以知道其他cpu可能在短时间 内需要访问这个cache(例如预取等等), 此时带宽又很空闲，会去做write-update? 但这远远 超出了我的知识范畴，也远远超出了本文要讲述的知识的范畴) 总之综上所述，本文接下来关于所有的cache conherence, 都不考虑write update, 均以write invalid 为例. ok, 关于cache conherence 这个基础而又宏大的话题就再此戛然而止吧。我们来简单总结 下: 总结 snoop-based conherence在核非常多(尤其在numa架构下)面临着non-scalable的问题， 其主要的问题在于 MESI的各个消息都需要采用广播的机制。为了解决这一问题, 大佬们搞出了directory-based conherence，其通过点对点的方式，按需和某些cpu 消息 通信。 但是, 在某些场景下，这仍然造成了non-scalable的问题。 non-scalable spinlock How to cause non-scalable 来设想下: non-scalable 一般是怎么出现的。往往是在资源扩张后(例如cpu数量), 执行 一些操作时，这些操作的复杂度往往会随着资源扩张线性增长。 而 spinlock 尤为突出，因为spinlock这种阻塞忙等性质的同步机制不会会影响发起慢操 作的cpu运行，而且由于其阻塞性质，会影响全局的进度。 在介绍具体的触发机制之前，我们先来看下 non-scalable spinlock的实现方式 non-scalable spinlock implementation wild spinlock wild 的意思是粗暴, 未经训化的意思, 俗称原始人。我们也用比较原始的汇编 指令展现其简介性: lock 代码 lock: .long 0 # 锁变量，初始为0 spin_lock: movl $1, %eax # eax = 1，表示“想要锁” spin_loop: xchgl %eax, [lock] # 原子交换 eax 和 lock testl %eax, %eax # 判断之前的 lock 是否为0 jne spin_loop # 如果不为0，继续自旋 # 获取锁成功，继续执行临界区代码 unlock代码: spin_unlock: movl $0, lock # 直接写0，释放锁 但是 wild spinlock有个很大的问题。就是公平性。不同的cpu”看到”的[lock] 变为0的时间不同, 所以哪个核获取到锁充满了随机性。这种不公平性体现在 获取自旋锁的延迟极不稳定，而软件又往往期待平稳的延迟。 这个地方其实涉及同时发起atomic操作的串性执行的仲裁问题。这个仲裁是 有谁来做的，是如何排序pending的atomic操作。这些知识在本人的知识储备 外，代后续补充. 于是 ticket spinlock 闪亮登场。 ticket spinlock ticket spinlock 会让每个自旋锁的申请者记录自己的”号牌”, 然后等待叫号. 这个很 像显示中的 排号系统。大家从排号机顺序取号，叫号时也会按顺序。 简单的小程序如下5: 数据结构 struct spinlock_t { // 上锁者自己的排队号 int my_ticket; // 当前叫号 int curr_ticket; } 加锁: void spin_lock(spinlock_t *lock) { int my_ticket; // 顺位拿到自己的ticket号码； my_ticket = atomic_inc(lock-&gt;my_ticket) - 1; while (my_ticket != lock-&gt;curr_ticket) ; // 自旋等待！ } 解锁: void spin_unlock(spinlock_t *lock) { // 呼叫下一位！ lock-&gt;curr_ticket++; } non-scalable ticket spinlock 我们以ticket spinlock为例来看下其在directory-based conherence 中 的non-scala. 按照ticket spinlock的设计, 当持有锁的cpu释放锁后，将会有一个确定的锁的等待着 获取锁。但是按照前文提到的 directory-based conherence协议并不能第一时间让 这个确定的cpu看到，这就是矛盾所在。 我们来画图说明: 初始状态 初始状态为 CPU0 持有自旋锁，CPU1, CPU2 等待自旋锁。CPU 0 已经持有了一段时间， 此时CPU1, CPU2 中的cache已经更新为最近更新的值。 CPU0 解锁 CPU0 准备释放自旋锁，首先请求 directory 查询哪些cpu中还有该地址的cache, 并要求directory做一些invalid操作。 directory clean掉这些cpu bit，并且标记该cache 为dirty. 另外directory 将信息返回 cpu0. CPU0 发起invalid cpu1, cpu2 该cache的请求。 CPU1, CPU2 返回该req处理完成。该cacheline已经被invalid CPU2 read-miss 此时cpu2 仍然在自旋 while (my_ticket != lock-&gt;curr_ticket) 由于此时CPU0 在解锁过程中，将cpu 2的lock-&gt;curr_ticket更新了，cpu2的cacheline 被invalid，其读取会遇到 read-miss。 cpu2 read-miss 需要向directory 请求 directory 告诉cpu2这个cacheline 是directory的，最新的数据在cpu0 CPU2 向CPU0请求最新数据 CPU0 将最新的数据返回 CPU0 请求directory 将请求flush cache(writeback)并请求更新ids(将CPU2 bit置位) if cpu2 not own the update curr_ticket 如果CPU2 没有持有更新后的curr_ticket, 此时cpu2 仍然会自旋，而其他的cpu也会重复 cpu2 的流程，但是如果cpu特别多的话，directory 处理消息则会是瓶颈（假设directory 处理消息为串行). 见上图, CPU (0-&gt;9) 都自旋 curr_ticket, CPU5 是当前ticket的owner但是CPU5 “看到” 该cacheline的更新比较晚（接收到CPU 0 invalid REQ, 或者因为其他的一些原因) 导致其 在directory 中排队比较靠后，所以在CPU5 即便是发出了 read-miss 相关的消息后， 也会延迟一段时间，等待directory 处理完前面排队的消息。参与wait spinlock的cpu越多， 该延迟越明显。 总结 本段用前面讲述的directory-based cache conherence协议 描述了ticket spinlock 在多核架构下的non-scalable 的底层原因。其原因主要是在消息通信过程中，会不可 避免的发生多对一的消息通信，导致”一”这一侧的处理成为了瓶颈。 接下来的章节我们将看下大佬们在 Linux kernel 侧的一些实践，将看到 non-scalable spinlock 是多么可怕么，并用数学方法解释相关现象。 non-scalable lock are dangerous NOTE 本文主要是参照6 在实践过程中，大佬们发现system throughput(系统吞吐)会因non-scalable lock突然崩溃。例如，在系统在25 个核心的cpu 上运行的良好，但是在30个核心 CPU上运行却完全崩溃. 并且令人惊讶的是，造成崩溃的原因居然可以 临界区非常小 的锁。 上图中的曲线有一些共同点，均是当Cpu和增长到一定的数量后，再增长几个核就会出现 极具的性能下降。 下图是各个负载的具体情况（其中EXIM, 是临界区特别小的场景)。 文中提出了几个问题: 为什么崩溃会如此早的开始 (例如FOPS 居然在3，4个核心就出发崩溃 )。 为什么性能会最终会大幅下降。 为什么性能会迅速下降。 为了解释上面这些问题，文中建立了马可夫链(Markov chain)模型。 一共有 $0, 1, 2, …. n$ 即$n + 1$个状态, $k$ 表示系统中共有 $k$ 个cpu在等待自旋 锁解锁。$A[k]$ 表示 状态从 $k$ 转换到 $k+1$ 的频率。(加锁的频率, 但是注意并不是 一个cpu申请锁的频率，而是 系统中共有$k$ 个等锁cpu到 $k+1$ 个等锁cpu的频率)。 而 $S[k]$ 我们定义 $a$ 为单个核心上连续两次获取锁之间的平均时间。在没有竞争的情况下，单个 核心尝试获取锁的速率为 $\\frac{1}{a}$, 因此，如果已有 k 个核心在等待锁，则新竞争 者的到达率为 $(n − k)/a$.( $n-k$ 增在从不等待自旋锁到等待自旋锁的状态，每个cpu的 频率为 $1/a$, 所以两者相乘). 因此图中的 $a[k]$为: \\[A[k] = \\frac{n-k}{a} \\tag{1}\\] 我们再关注下解锁阶段: $S[k]$ 。我们将 $S[k]$ 阶段所消耗的时间分为两部分，一部分 是临界区代码所消耗的时间, 记做$E$, 另一部分是$unlock$操作消耗的时间，记做 $R$。 根据我们上面讲的 directory-based cache conherence的简单模型，在解锁阶段有一些 流程需要是串行的。所以我们假设处理一个更新核心的cache所消耗时间为 $c$, 那么 $k$ 个核在等待相同自旋锁的情况下，将所有核的cache更新需要的时间为 $k \\times c$, 但是 先处理哪个cpu，这个是随机的，所以curr_ticket owner cache 更新时间取个这些cpu更新 时间的平均值 $k \\times c /2$, 那么可得: \\[S[k]=\\dfrac{1}{E+\\dfrac{k×c}{2}} \\tag{2}\\] 可以看到 $S[k]$ 和$k$ 是成反比的关系。核心越多，更新cacheline的cost越高。 有了上面基本的结论，搭配下面一个稳态 Markov chain 状态转换率守恒的基本原则，即可 导出模型本身： 假設 $P_0, P_1, P_2, … P_n$ 系统处在这 $n$ 个状态的机率，显然 \\(\\sum P_k=1 \\tag{3}\\) 当系统处在稳态时，下方式子成立： \\[P_k \\times A[k] = P_{k+1} \\times S[k]\\] 这时一个递推, 结合(1), (2), (3)可以求出 $P_k$ 关于 $k$ 的表达式: \\[P_k = \\dfrac{\\frac{1}{T_{arrive}^k(n-k)!}\\prod_{i=1}^k (E+ic)}{\\sum_{i=0}^{n}\\frac{1}{T_{arrive}^i(n-i)!}\\prod_{j=1}^i (E+jc)}\\] 有了上面的公式，我们可以求出任意时刻整个系统等待自旋锁状态的CPU总量: （有1个cpu 等锁概率 + 有两个cpu等锁概率+ ... + 有n个cpu等锁) \\[C = \\sum_{i=0}^{n}iP_i\\] 我们定义一个加速比的该你那，即在 $x$ 个cpu争抢 spinlock 时，有多少cpu未处于自旋锁 等锁的状态. \\[S=x - C\\] 下图是作者使用上面公式得到的推测值以及实际测试过程中得到的数据的对比图: 可以发现两者较为重合。 NOTE 上面公式本人未推导。均参考论文6以及狗哥文章4,5 scalable spinlock NOTE 本文不讨论scalable spinlock的具体实现。关于这部分我们在另外一篇文章 介绍内核qspinlock的实现. 论文中提到了几种scalable spinlock并对这些锁做了性能测试. 其中MCS和 CLH lock性能较好。 另外，作者还对比了 MCS vs ticket spinlock在上面提到的四种场景下的性能对比 : 均取得了良好的效果。 NOTE 有小伙伴可能不理解为什么采用scalable spinlock后仍然会导致性能吞吐量下降的问 题。但这并不是锁的消耗导致的。文章有一段介绍FOPS性能降低原因: Figure 12(a) shows the performance of FOPS with MCS locks. Going from one to two cores, performance with both ticket locks and MCS locks increases. For more than two cores, performance with the ticket spin lock decreases continuously. Performance with MCS locks initially also decreases from two to four cores, then re- mains relatively stable. The reason for this decrease in performance is that the time spent executing the critical section increases from 450 cycles on two cores to 852 cycles on four cores. The critical section is executed multiple times per-operation and modifies shared data, which incurs costly cache misses. As more cores are added, it is less likely that a core will have been the last core to execute the critical section, and therefore it will incur more cache misses, which will increase the length of the critical section 大概的意思是, 临界区中的代码会遇到较严重的cache miss, 核数越多，这种现象就越明 显(因为核数越多，下次运行到该核上的概率就越低) 另外大家可以想一下，即便是没有cache-miss 核数一直增长性能就会一直增长么? 答案是肯定不是，因为这些程序并不全是完全并行的，要不就不会用到spinlock来保护 临界区。因为临界区的存在导致一部分流程必须串行。临界区越大的这种现象越明显。 而在结合临界区中因cache-miss而变长的情况，所以上面的测试得到的结果往往会 性能平稳的略微下降。(个人瞎猜) 结论 non-scalable spinlock 会因 cache 一致性而造成非常严重的性能问题。通过替换scalable spinlock会大大缓解该问题。 参考链接 Wikipedia: Spinlock Scalability 深入理解Linux内核之自旋锁 sysprog: 从CPU cache一致性的角度看Linux spinlock的不可伸缩性(non-scalable) dog250: 从CPU cache一致性的角度看Linux spinlock的不可伸缩性(non-scalable) Non-scalable locks are dangerous Snooping-Based Cache Coherence Directory-Based Cache Coherence Cache Coherence - when do modern CPUs update invalidated cache lines " }, { "title": "non-blocking algorithm", "url": "/posts/overflow/", "categories": "synchronization", "tags": "synchronization, non-blocking algorithm, rcu preliminaries", "date": "2025-12-25 10:00:00 +0800", "content": "在计算机科学中，如果任何线程的故障或挂起不会导致其他线程的故障或挂起，则称该算法 为非阻塞算法1。根据非阻塞算法的达到效果，可以分为两类: wait-free: if there is also guaranteed per-thread progress lock-free: if there is guaranteed system-wide progress Obstruction-free: 只要某个线程能独占执行（其他线程不干扰），该线程的操作必在有限步骤内完成。 pre-thread progress vs system-wide progress progress的意思是进展。system-wide表示整个程序或者整个系统，而pre-thread要求每 个thread. 拿CAS来说, 可能某个thread在非常多次交换中都失败，但是从整个程序在说，在每一 次交换，都有thread 交换成功。所以其在system-wide取得进展，而不是pre-thread Motivation 在并行编程中，保护共享数据是一个很重要的议题。通常的方法有两种: blocking synchronization non-blocking synchronization 阻塞同步实现起来简单: 我们将访问共享资源的代码区成为临界区。步骤如下: 当程序进入临界区之前，首先检测该临界区是否上锁，如果上锁则等待其解锁。 如果发现并未上锁，或者已经解锁，则对该临界区加锁，离开临界区后，解锁。 总之, 阻塞同步的方法就是让某个程序在临界区中独享该资源。阻塞其他程序进在该时刻进 入临界区。通常的方法有: mutex, sem 等。 但是，阻塞式同步有很多缺点1, 导致其可能不适合很多场景。一个比较明显的 原因是: 线程在被阻塞时, 无法做任何操作: 当被阻塞的线程正在执行高优先级任务，或者 实时任务时，这是非常不明智的。 还有一些其他的原因，例如deadlock, livelock, priority inversion.(这个是wiki1 中写的，难道上面的例子不是priority inversion 么) 而非阻塞式算法没有这些问题。因为非阻塞式算法最低的要求(obstruction-free), 如果某个 线程因某种原因不再执行, 而另一个线程可以在有限步骤之内完成. 并不会无限期阻塞。 这也就 意味着，其他线程可以安全的抢占该当前访问共享资源的线程。另外使用无锁结构可以提高并行性 能, 因为对共享数据结构的访问不需要串行化以保持一致性。 wait-free vs lock-free vs obstruction-free blocking 我们先来看下, blocking synchronization的一个例子: mov eax, 1 spin: xchg eax, [lock] test eax, eax jnz spin //临界区 do something mov 0, [lock] 这是一个简单的自旋锁, 当一个线程持有自旋锁被中断时，其他线程也无法继续执行 如下图所示: obstruction-free 我们来设想一个场景。某个程序是以事务机制写入数据(例如xfs文件系统), 大概流程如下: BEG record do what do this completed 而另外一个程序用来检测是否有病毒入侵，如果有，立即中断程序。并杀死病毒（有内鬼， 终止交易).如果在事务未完成时终止程序, 下次程序再次启动时，会认为该事务未完成， 回滚掉这个事务。 可以看出，整个的事务写入是不需要加任何锁的，如果没有病毒侵入，该事务写入是可以在 一定的步骤内完成. 但是在疯狂的病毒入侵下, 该程序会不断的写入事务，回滚事务. 可能 在很长一段时间内, no system-wide progress.(这个例子并不太好，但是想不出其他恰当 的例子) 在该图中, Slip3时刻，T1, T2, T3均未取得进展。 NOTE 图片来自于4 *** lock-free 举例: 班级有一个班费账户，班级的人向这个账户存钱。由于大家在不同的银行柜台同时存 钱，这里会涉及并行. 设计程序如下: spin: mov [total_¥], eax mov eax, ebx mov eax, ecx ## 存钱 + 1 inc ebx CMPXCHG [total_¥], ebx test ecx, eax ## 发现有别人在存钱 jnz spin 运行该程序的线程时，可能会在某个时间段都不会有进展，但是在system-wide, 总有人可以存钱进去，所以属于lock-free. 虽然在Slip3, T3并未取得进展，但是此时T1, T2取得进展. wait-free wait-free 则是在per-thread层次的，要求在一段时间内，某个thread一定会有进展。 我们如下设计上面程序，在上面spin循环中，增加如果尝试次数超过100次，则启动一个 额外的线程, 继续尝试，不再阻塞当前线程。 这也类似于rcu callbak的作用，rcu callbak不会因读者临界区，而阻塞写者更新流程。 而写者关于old data的delete流程放到其他流程异步执行。所以对于写者update而言，其 是wait-free的。(单写者, 多读者) T1, T2, T3在所有时刻均取得进展。 参考链接 Non-blocking algorithm 并行程式设计: Lock-Free Programming 深入理解RCU|核心原理 对wait-free和lock-free的理解 RCU初学参考资料 " }, { "title": "qemu coroutine", "url": "/posts/coroutine/", "categories": "qemu, coroutine", "tags": "qemu, qemu_coroutine, completed", "date": "2025-02-25 11:00:00 +0800", "content": " Introduction Linux User Context Switch qemu coroutine 协程状态机 CREATE and INIT enter switch yield Use Case for QEMU Introduction 多线程和协程都可以用于并行编程，但是他们实现方式和使用场景 有很大的区别，我们来对比下: 对比项 协程 多线程 实现方式 在用户态单线程中，完成上下文切换 内核态完成上下文切换 开销 开销较低 线程创建销毁，以及切换都需要进入内核态，开销较高 并发 只能在单个线程中来回切换完成并发 可以实现真正的并行处理（在多核cpu) 调度(切换) 协程类似于非抢占式调度，只能在主动切换 线程可以在任何时刻被中断和切换 程序复杂度 协程处理同步和资源共享较简单 多线程编程需要处理线程间的同步和资 源共享问题, 复杂度更高, 往往需要借助系统api(锁，信号量) 使用场景上: 协程 当应用程序主要是 I/O 密集型任务，如网络请求、文件操作等 当需要高并发但不需要并行计算 使用多线程的场景 当应用程序是 CPU 密集型任务，需要利用多核 CPU 的并行计算能力 当需要处理大量需要同时执行的计算任务时 协程比较适合那种需要wait的任务, 例如上面提到的I/O 密集型任务(qemu中的 aio, 可以在协程中下发多个aio，然后等待io complete event) 我们举个例子: 在该图中, 有两个cpu core, A进程有3个thread, 其中thread1和thread2在 cpu0上运行，thread 3 有四个协程，在cpu1上运行, task A 的计算负载可 以分别落在cpu 0 和cpu 1上, 这也是多线程的很大的优点: 可以最大化的利 用多核cpu的并行处理能力。 thread1和thread2其靠kernel的任务抢占机制，来共享cpu 0, 在任何时间都有 可能被对方抢占. 而thread3 中的各个协程则是 根据自己任务的完成情况， 或者当前任务是否需要等待而主动选择调度。 Linux User Context Switch 我们需要思考下，context switch 完成哪些任务: init new task context like pthread_create() need init IP, SP(a new stack), Params context switch save… load… destroy 如果用户态要完成context switch，需要处理好上面所列的三件事。而这些事情涉及的东西 太底层了，如设置ip，如传参等等，所以libc中提供了ucontext系列接口来完成这些事情: ucontext ucontext API API name 作用 getcontext(ucontext_t *ucp) 获取当前上下文, 保存到ucp中 setcontext(ucp) 切换到目标(ucp)上下文 makecontext(ucp, (*func)(), int argc, …) 用来modify ucp, 下面详述 swapcontext(oucp, ucp) saves current thread context in oucp and makes *ucp the currently active context. 在执行makecontext()之前，需要做一些准备工作: 调用 getcontext() 来init ucp, 需要为其分配stack, init ucontext_t.uc_stack 相关成员 ss_sp: 指向具体的堆栈地址 ss_size: 堆栈大小 ss_flags: 设置ucp-&gt;uc_link参数，根据是否设置ucp-&gt;uc_link 来确定func() 返回时， 所执行的动作: NULL: 进程退出 隐式调用 setcontext(ucp-&gt;uc_link) 我们编写一个例子来演示下，该接口的使用方法和效果 ucontext example 测试程序展开 #include &lt;stdio.h&gt; #include &lt;ucontext.h&gt; #include &lt;stdlib.h&gt; #define STACK_SIZE (4096 * 2) void print_current_stack() { unsigned long stack_pointer; __asm__(\"movq %%rsp, %0\" : \"=r\"(stack_pointer)); printf(\"stack pointer(%lx)\\n\", stack_pointer); } void func(int a, int b) { printf(\"the co exec, sum(%d)\\n\", a+b); printf(\"print co stack \\n\"); print_current_stack(); return; } int main() { int ret; char *stack = (char *)malloc(STACK_SIZE); ucontext_t uc, old_uc; int a, b = 0; printf(\"the new stack is %p\\n\", stack); printf(\"print main stack:\\n\"); print_current_stack(); getcontext(&amp;uc); uc.uc_stack.ss_sp = stack; uc.uc_stack.ss_size = STACK_SIZE; uc.uc_link = &amp;old_uc; while(1) { printf(\"main co a(%d) b(%d)\\n\", a, b); makecontext(&amp;uc, (void (*)(void))func, 2, a, b); printf(\"swap context\\n\"); swapcontext(&amp;old_uc, &amp;uc); printf(\"swap context end\\n\"); if (a++ == 3) break; b=b+2; } return 0; } 在main中jum构建一个循环，来在另一个上下文中调用func(), 并设置 返回的context为调用者(main())的context，这样func()返回后， 直接返回到main()的while的上下文, 继续执行循环。 输出示例 输出如下: the new stack is 0x9d22a0 print main stack: stack pointer(7fff0939ee50) main co a(0) b(0) swap context the co exec, sum(0) print co stack stack pointer(9d4250) swap context end main co a(1) b(2) swap context the co exec, sum(3) print co stack stack pointer(9d4250) swap context end main co a(2) b(4) swap context the co exec, sum(6) print co stack stack pointer(9d4250) swap context end main co a(3) b(6) swap context the co exec, sum(9) print co stack stack pointer(9d4250) swap context end 由上图可见，func()和main()运行在两个上下文，并且两个上下文切换示意图 如下: 另外，linux中还支持另外一组上下文切换的API – sigsetjmp, siglongjmp sigsetjmp, siglongjmp 该系列函数一般用于实现C语言中的异常处理，如在信号处理流程中，跳转到 其他的执行流程. 避免再次执行到异常代码. 我们先来看下其API sigsetjmp(sigjmp_buf env, int savemask) 功能: 保存当前的上下文和信号掩码，以便以后可以通过 siglongjmp 恢复 参数: env: 保存上下文信息 savemask: 如果非0， 当前的信号掩码也会被保存 返回值: 调用者返回0 如果通过siglongjmp恢复，而返回siglongjmp 传递的值 siglongjmp(sigjmp_buf env, int val) 功能: 恢复由 sigsetjmp 保存的上下文信息和信号掩码，并从 sigsetjmp 返回。 参数: env: 由sigsetjmp保存的环境信息 val: sigsetjmp 0: return 1 x(x != 0) : return x 没有返回值(因为已经跳走了) 看起来sigxxxjmp也可以实现上下文切换，但是该系列接口有个很大的问题，比较适合 recover, 但不适合new。其不像ucontext接口, 可以通过makecontext()接口先new一个context, sigsetjmp 只能保存当前的现场, 所以相当于只能先走到要切换的流程中埋好点，然后才能切换，很不方便. 但是sigxxxjmp()对比makecontext()也有好处. 其更加轻量化. 它不会涉及完整的上下文切换, 例如其可以设置不切换信号掩码，减少因系统调用而产生的切换损耗. 而qemu中的协程实现主要有三种 ucontext + sigjmp: util/coroutine-ucontext.c sigaltstack: util/coroutine-sigaltstack.c coroutine-win32 本文主要介绍第一种，由ucontext和sigjmp结合实现。其中，ucontext系列接口负责 new context, 为sigjmp接口埋点, 而sigjmp 系列接口负责协程切换. 接下来，我们来看下qemu实现: qemu coroutine 协程状态机 这是一个典型的由 leader 创建协程的状态机，进入协程上下文会做两种事: 埋sigxxxjmp跳转点, 为之后再次切换进协程做准备 work… 协程运行期间，可能因为wait io等事件选择先切出协程(COROUTINE_YIELD), 此时协程是suspend状态。 等待协程处理完完整的事物后，会切出协程上下文，并置为terminal 状态. 另外除了首次进入协程是使用ucontext接口, 剩余的协程/leader之间的切换， 均使用sigxxxjmp系列接口，这样可以尽量减少因切换上下文带来性能损耗。 整体流程 整个流程如下图: CREATE and INIT create流程主要是为协程准备好上下文环境, init 流程主要是在协程中 打好跳转点, 流程包括: 为协程分配堆栈空间 使用makecontext(), swapcontext() 执行到一个新的上下文 在协程上下文中，埋 sig jmp的点 跳转回leader 上下文 INIT流程只是为协程搭建了一个上下文，但是该上下文接下来要执行什么任务， 需要leader指明，所以在切回leader上下文后，leader还需要为协程准备协程要 执行的函数，以及函数参数(红底蓝字部分) enter 在create &amp;&amp; INIT 章节中，我们介绍到首次进入协程是通过swapcontext()接口, 而之后再次进入协程，就需要使用sigxxxjmp系列接口，本章节主要介绍第二种。 而enter这个动作既有可能发生在leader上下文，也有可能发生在协程上下文, 所以我们以下面的场景为例子，看下qemu是怎么处理的。 leader enter 协程A 协程A enter 协程B 假设协程A, 协程B 在处理过程中不会yield, 直接terminal. 整个流程如下图: 这样处理，会导致协程只能串行，不能嵌套执行。 我们来想下为什么要这样做, 首先我们来看下，两者上下文切换次数: 串行执行 leader-&gt;A-&gt;leader-&gt;B-&gt;leader 切换4次 嵌套执行 leader-&gt;A-&gt;B-&gt;A-&gt;leader 切换4次 两者切换次数相同。 所以这里的原因(猜测)很可能是，防止协程可能带来的 同步问题（避免A上下文中嵌入B的上下文从而带来死锁) switch 接下来，我们再来看下switch过程。switch过程比较简单。主要的函数是, qemu_coroutine_switch(), 函数原型: CoroutineAction qemu_coroutine_switch(Coroutine *from_, Coroutine *to_, CoroutineAction action); 参数有三个: from: 切出的协程 to: 切入的协程 action: 本次操作的类型 COROUTINE_YIELD: 暂停from协程 COROUTINE_TERMINATE: 终止from协程 COROUTINE_ENTER: 进入to协程 我们以一个没有执行过yield协程生命周期来看下switch的细节: 可以看到在执行qemu_coroutine_switch()时，action参数会作为 siglongjmp(, action)传入，这样在另一个上下文中，会通过 sigsetjmp()的返回值，获取到action, 而qemu_aio_coroutine_enter() 会根据协程返回状态，来选择一些action: COROUTINE_TERMINATE: 销毁协程 COROUTINE_YIELD: 忽略，继续执行leader流程 这里我们来总结下，不同的switch过程: leader-&gt;co ENTER: co-&gt;leader TERMINATE YIELD yield yield是一个比较特殊的存在，因为yield动作时，还需要保存协程的现场， 以便之后，再次切回协程。并且在协程yield切回leader后，leader会继续 运行执行其他流程。等待该协程的等待的事件到来后，需要再次执行enter 切换回该协程，如下图所示: Use Case for QEMU 附录 virtio-blk触发堆栈 virtio_blk_handle_vq ## 从avail ring中获取req =&gt; blk_io_plug() =&gt; while (virtio_blk_get_request()) =&gt; virtio_blk_submit_multireq() =&gt; foreach request: ## 可能会merge submit =&gt; submit_requests() =&gt; init qemu iovc =&gt; blk_aio_pwritev/blk_aio_preadv =&gt; blk_io_unplug() blk_aio_pwritev =&gt; blk_aio_prwv(,,,,co_entry::blk_aio_write_entry, flags, cb:: virtio_blk_rw_complete,opaque) =&gt; init acb::BlkAioEmAIOCB =&gt; qemu_coroutine_create(co_entry, acb) =&gt; bdrv_coroutine_enter(blk_bs(blk), co) 参考资料 huangyong – 深入理解qemu协程 _银叶先生 – 协程的原理与实现：qemu 之 Coroutine " }, { "title": "qspinlock", "url": "/posts/qspinlock/", "categories": "synchronization", "tags": "synchronization, spinlock", "date": "2023-09-11 10:00:00 +0800", "content": "简介 内核中的自旋锁是互斥锁。而内核中的自旋锁经过多个版本的演进， 最终是在 mcs 自旋锁 算法之上，根据kernel 本身的需求，作了改进。 我们这里不去回顾 Linux 自旋锁的历史，简单介绍下 mcs 自旋锁算法， 并详细讲解 kernel 中的 mcs自旋锁的变体。 NOTE 如果想要了解 kernel 自旋锁的演进，可以看下 深入理解Linux内核之自旋锁 1 该文章详细讲解了kernel自旋锁的演进，并通过举例子 的方式，讲解了各个算法（包括最新的算法），十分值 得看, 本文主要分析kernel 最新的 自旋锁算法。 MCS 自旋锁 MCS 自旋锁是在为了解决票号自旋锁带来的 cache line bouncing2, 实现了一个队列，使其各自自旋各自的地址, 而不是自旋一个地址，这样就 解决了这个问题。 NOTE 1 这个我们放到附录1中讲述 什么是cache line bouncing，为什么cache line bouncing在 spinlock场景会尤为突出。 所以 MCS自旋锁 即保持票号自旋锁的保证线程获取锁的顺序的优点， 同时解决票号自旋锁带来的cache 抖动问题。但是也有缺点，我们下面会介绍。 我们先来看下 MCS自旋锁的实现 而MCS自旋锁有什么缺点呢 ? 更改了spinlock的相关接口 锁占用的内存变大 kernel 对此做了一些优化，我们来看下 KERNEL MCS 变体 spinlock的相关数据结构(struct qspinlock)，仍然保持之前的样子 大小为sizeof(u32)。但是该空间分割成主要的三个成员 (locked, pending,tail)。我们来看下kernel代码定义: typedef struct qspinlock { union { atomic_t val; /* ¦* By using the whole 2nd least significant byte for the ¦* pending bit, we can allow better optimization of the lock ¦* acquisition for the pending bit holder. ¦*/ #ifdef __LITTLE_ENDIAN struct { u8 locked; u8 pending; }; struct { u16 locked_pending; u16 tail; }; #else struct { u16 tail; u16 locked_pending; }; struct { u8 reserved[2]; u8 pending; u8 locked; }; #endif }; } arch_spinlock_t; 可以看到访问该数据结构，应该使用原子操作(qspinlock.val), 根据的线程(cpu)的情况可能需要关注的东西不同, 例如: mcs.locked = 1的cpu需要关注 (qspinlock.locked_pending)成员。 而pending的cpu仅需要关注(qspinlock.locked), 我们下面会介绍具体的流程 简单来说，是占据pending 的cpu抢占locked, 而 占据 mcs head 的cpu 抢占 locked_pending, 而非mcs head的cpu, 先抢占 self.mcs.locked == 1, 等待该条件满足时，表示其为 mcs head, 然后再抢占 locked_pending 每个 cpu有四个 mcs0, 代表4中状态（代表4层执行流，线程、软中断、 硬中断、屏蔽中断), 举个例子，线程拿到了锁，这时候来了一个硬 中断，硬中断处理完后，进入软中断处理及流程，这时拿了一把锁， 此时又来了一个硬中断，该中断处理中又拿了一个自旋锁，还未解锁时， 来了一个NMI又拿了一个自旋锁。 这样每个CPU最多持有四把自旋锁。而 每个自旋锁，如果都需要使用 mcs结构 enqueue， 最多需要4个mcs结构。 tail的计算也是基于上面。每个cpu最多拿四个mcs, 所以tail[bit:1,bit:0] 用来表示当前用了几个自旋锁。cpu index记录在剩余的bits中。另外 tail == 0有特殊的含义 – 表示没有 mcs 在抢占自旋锁。所以不存在 cpu0.mcs0的这种情况，需要将 cpu_index++, 也就是下面的公式: tail = ((cpu_index + 1) &lt;&lt; 2) + mcs_index 代码分析 NOTE 下面 频繁使用三元组 (tail, pending, locked) , 例如(0, 0, 1) 表示 locked = 1, pending = 0, tail = 0 另外 tail = n , 表示tail位被占用 tail = z, 则表示tail 为任意值 locked, pending == x, y 表示任意值 我们直接看 queue_spin_lock()的相关代码: queued_spin_lock static __always_inline void queued_spin_lock(struct qspinlock *lock) { u32 val; //=============(1)================= val = atomic_cmpxchg_acquire(&amp;lock-&gt;val, 0, _Q_LOCKED_VAL); if (likely(val == 0)) return; //============(2)================== queued_spin_lock_slowpath(lock, val); } 查看lock-&gt;val 是否为0, 如果为0, 则说明没有人在使用该锁, 将 (0, 0, 0) 修改为 (0, 0, 1) 如果有人占用锁，则走slowpath流程 slow path lock pending – part 1 void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val) { struct mcs_spinlock *prev, *next, *node; u32 old, tail; int idx; BUILD_BUG_ON(CONFIG_NR_CPUS &gt;= (1U &lt;&lt; _Q_TAIL_CPU_BITS)); //===============(1)================== if (pv_enabled()) goto pv_queue; if (virt_spin_lock(lock)) return; /* * Wait for in-progress pending-&gt;locked hand-overs with a bounded * number of spins so that we guarantee forward progress. * * 0,1,0 -&gt; 0,0,1 */ //===============(2)================== if (val == _Q_PENDING_VAL) { int cnt = _Q_PENDING_LOOPS; val = atomic_cond_read_relaxed(&amp;lock-&gt;val, (VAL != _Q_PENDING_VAL) || !cnt--); } /* ¦* If we observe any contention; queue. ¦*/ //===============(3)================== if (val &amp; ~_Q_LOCKED_MASK) goto queue; /* * trylock || pending * * 0,0,* -&gt; 0,1,* -&gt; 0,0,1 pending, trylock */ //===============(4)================== val = queued_fetch_set_pending_acquire(lock); ... 我们这里先不关注半虚拟化 如果是(0, 1, 0)， 则等待其进入(0, 0, 1), 这样做的好处是，如果其进入了 (0,0,1) 则直接抢占pending位，状态为(0,1,1), 就不用走queue的流程 如果除 locked位，其他位不为0, 则说明有被别的线程抢占了，则走queue的流程 该代码为: static __always_inline u32 queued_fetch_set_pending_acquire(struct qspinlock *lock) { return atomic_fetch_or_acquire(_Q_PENDING_VAL, &amp;lock-&gt;val); } 这里进行按位或操作，并返回之前的值，我们来想下在原子操作前有那 几种可能的情况。 (z, 1, y) : 进行逻辑或操作无影响。但是本次抢占锁失败 (n, 0, y) : !! 这种情况就打乱了顺序，不允许, 并且需要将pending位还原 (0, 0, 1) : 抢占pending位，spin lock位 (0, 0, 0) : 抢占了pending位，然后这时，只有自己能抢占lock位，再抢占lock位 我们继续分析(4) 之后的代码: lock pending – part2 /* * If we observe contention, there is a concurrent locker. * * Undo and queue; our setting of PENDING might have made the * n,0,0 -&gt; 0,0,0 transition fail and it will now be waiting * on @next to become !NULL. */ //============(1)==================== if (unlikely(val &amp; ~_Q_LOCKED_MASK)) { //============(1.1)==================== /* Undo PENDING if we set it. */ if (!(val &amp; _Q_PENDING_MASK)) clear_pending(lock); goto queue; } /* * We're pending, wait for the owner to go away. * * 0,1,1 -&gt; 0,1,0 * * this wait loop must be a load-acquire such that we match the * store-release that clears the locked bit and create lock * sequentiality; this is because not all * clear_pending_set_locked() implementations imply full * barriers. */ //===============(2)================ if (val &amp; _Q_LOCKED_MASK) atomic_cond_read_acquire(&amp;lock-&gt;val, !(VAL &amp; _Q_LOCKED_MASK)); /* * take ownership and clear the pending bit. * * 0,1,0 -&gt; 0,0,1 */ clear_pending_set_locked(lock); lockevent_inc(lock_pending); return; 除了locked字段以外还有值，则抢锁失败 pending字段有值，则为(z, 1, y), 抢锁失败不需要做什么事情, pending 字段未有值，那tail字段肯定有值，则为(n, 0, y), 抢锁 失败，还需要将pending位还原为0 以上两种情况都需要入队 这种情况为 (0, 0, y) 如果为 (0, 0, 1), 则spin lock位，等待其变为0 如果位 (0, 0, 0), clear pending 并且 抢占 lock位 queue – part1 void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val) { ... queue: lockevent_inc(lock_slowpath); pv_queue: //=================(1)======================= node = this_cpu_ptr(&amp;qnodes[0].mcs); idx = node-&gt;count++; tail = encode_tail(smp_processor_id(), idx); /* * 4 nodes are allocated based on the assumption that there will * not be nested NMIs taking spinlocks. That may not be true in * some architectures even though the chance of needing more than * 4 nodes will still be extremely unlikely. When that happens, * we fall back to spinning on the lock directly without using * any MCS node. This is not the most elegant solution, but is * simple enough. */ //=================(2)======================= if (unlikely(idx &gt;= MAX_NODES)) { lockevent_inc(lock_no_node); while (!queued_spin_trylock(lock)) cpu_relax(); goto release; } //=================(3)======================= node = grab_mcs_node(node, idx); ... } 获取mcs结构，关于该node的count,在 qnodes[0].mcs中记录。 encode_tail() 会根据 当前cpu idx和原来的count值计算出一个 值作为tail的值。 idx &gt;= MAX_NODES其实是不正常的（说明已经获取了 MAX_NODES(4) 次锁), 注释中写了可能的原因，我们这里先不细看 (!!后续补充!!) 获取 qnodes[idx].mcs node queue – part2 void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val) { ... /* * 上节代码: * idx = node-&gt;count++; */ \t\t... /* * Keep counts of non-zero index values: */ lockevent_cond_inc(lock_use_node2 + idx - 1, idx); //=============(1)=============== /* ¦* Ensure that we increment the head node-&gt;count before initialising ¦* the actual node. If the compiler is kind enough to reorder these ¦* stores, then an IRQ could overwrite our assignments. ¦*/ barrier(); //=============(2)=============== node-&gt;locked = 0; node-&gt;next = NULL; pv_init_node(node); /* ¦* We touched a (possibly) cold cacheline in the per-cpu queue node; ¦* attempt the trylock once more in the hope someone let go while we ¦* weren't watching. ¦*/ //=============(3)=============== if (queued_spin_trylock(lock)) goto release; \t\t... } locking/qspinlock: Ensure node-&gt;count is updated before initialising node commit 11dc13224c975efcec96647a4768a6f1bb7a19a8 该 barrier是为了避免编译器将 init node 和 idx = node-&gt;count++顺序搞乱。 在单执行流跑的时候不会有问题。当遇到下面的情况 ``` (1) 未加barrier()之前, 编译器乱序 normal thread 触发中断 //乱序 init node[0] init node[0] idx = node-&gt;count++ (2) 加了barrier normal thread idx = node-&gt;count++ 触发中断 init node[1] init node[0] ``` 未加 barrier()之前，会遇到中断执行流和normal thread执行流，使用 一个node的情况。 初始化该node 这时候，我们只修改了node, 如果能抢到锁，恢复原来的node也很好恢复。 所以尝试抢锁，抢到就血赚。 queue – part3 void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val) { /* * 上节代码:(1) * node-&gt;locked = 0; * node-&gt;next = NULL; */ ... /* ¦* Ensure that the initialisation of @node is complete before we ¦* publish the updated tail via xchg_tail() and potentially link ¦* @node into the waitqueue via WRITE_ONCE(prev-&gt;next, node) below. ¦*/ //=============(1)=============== smp_wmb(); /* ¦* Publish the updated tail. ¦* We have already touched the queueing cacheline; don't bother with ¦* pending stuff. ¦* ¦* p,*,* -&gt; n,*,* ¦*/ //==============(2)=============== old = xchg_tail(lock, tail); next = NULL; /* ¦* if there was a previous node; link it and wait until reaching the ¦* head of the waitqueue. ¦*/ //==============(3)=============== if (old &amp; _Q_TAIL_MASK) { prev = decode_tail(old); /* Link @node into the waitqueue. */ //==============(4)=============== WRITE_ONCE(prev-&gt;next, node); pv_wait_node(node, prev); //==============(5)=============== arch_mcs_spin_lock_contended(&amp;node-&gt;locked); /* ¦* While waiting for the MCS lock, the next pointer may have ¦* been set by another lock waiter. We optimistically load ¦* the next pointer &amp; prefetch the cacheline for writing ¦* to reduce latency in the upcoming MCS unlock operation. ¦*/ //==============(6)=============== next = READ_ONCE(node-&gt;next); if (next) prefetchw(next); } ... } 关于此处的分析，请看 locking/qspinlock: Ensure node is initialised before updating prev-&gt;next locking/qspinlock: Elide back-to-back RELEASE operations with smp_wmb publish the updated tail (替换tail) 判断 old 是否有 tail值，如果有，则说明mcs链表中有成员, 需要获取 prev, 并且更新prev-&gt;next 更新 prev-&gt;next 这时需要自旋等待 node-&gt;locked == 1 #define arch_mcs_spin_lock_contended(l) \\ do { \\ smp_cond_load_acquire(l, VAL); \\ } while (0) 因为在(5)处等待了一段时间，很可能node-&gt;next就有值了，因为这时候抢到了 锁，如果有next需要再将next-&gt;locked = 1, 但是距离写操作还有一些指令， 这里执行prefetchw 操作，暗示cpu会有对next地址的写操作，使其在写操作 发生之前，将该地址的内容load到 cache中。 queue – part4 void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val) { ... /* ¦* we're at the head of the waitqueue, wait for the owner &amp; pending to ¦* go away. ¦* ¦* *,x,y -&gt; *,0,0 ¦* ¦* this wait loop must use a load-acquire such that we match the ¦* store-release that clears the locked bit and create lock ¦* sequentiality; this is because the set_locked() function below ¦* does not imply a full barrier. ¦* ¦* The PV pv_wait_head_or_lock function, if active, will acquire ¦* the lock and return a non-zero value. So we have to skip the ¦* atomic_cond_read_acquire() call. As the next PV queue head hasn't ¦* been designated yet, there is no way for the locked value to become ¦* _Q_SLOW_VAL. So both the set_locked() and the ¦* atomic_cmpxchg_relaxed() calls will be safe. ¦* ¦* If PV isn't active, 0 will be returned instead. ¦* ¦*/ //================(1)====================== if ((val = pv_wait_head_or_lock(lock, node))) goto locked; //================(2)====================== val = atomic_cond_read_acquire(&amp;lock-&gt;val, !(VAL &amp; _Q_LOCKED_PENDING_MASK)); ... } pv先不看 !!!!! 自旋 lock-&gt;lock_pending == 0 (n, 1, 0) 或者 (n,0,1) –&gt;(n, 0, 0), 这里只有该进程自旋这个地址，所以如果变为(n, 0, 0) , 只有这个进程可以 抢到。 locked void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val) { locked: /* ¦* claim the lock: ¦* ¦* n,0,0 -&gt; 0,0,1 : lock, uncontended ¦* *,*,0 -&gt; *,*,1 : lock, contended ¦* ¦* If the queue head is the only one in the queue (lock value == tail) ¦* and nobody is pending, clear the tail code and grab the lock. ¦* Otherwise, we only need to grab the lock. ¦*/ /* ¦* In the PV case we might already have _Q_LOCKED_VAL set, because ¦* of lock stealing; therefore we must also allow: ¦* ¦* n,0,1 -&gt; 0,0,1 ¦* ¦* Note: at this point: (val &amp; _Q_PENDING_MASK) == 0, because of the ¦* above wait condition, therefore any concurrent setting of ¦* PENDING will make the uncontended transition fail. ¦*/ //==============(1)==================== if ((val &amp; _Q_TAIL_MASK) == tail) { //==============(2)==================== if (atomic_try_cmpxchg_relaxed(&amp;lock-&gt;val, &amp;val, _Q_LOCKED_VAL)) goto release; /* No contention */ } /* ¦* contended path; wait for next if not observed yet, release. ¦*/ //==============(3)==================== if (!next) next = smp_cond_load_relaxed(&amp;node-&gt;next, (VAL)); /* ¦* Either somebody is queued behind us or _Q_PENDING_VAL got set ¦* which will then detect the remaining tail and queue behind us ¦* ensuring we'll see a @next. ¦*/ //==============(4)==================== set_locked(lock); //==============(5)==================== arch_mcs_spin_unlock_contended(&amp;next-&gt;locked); pv_kick_node(lock, next); release: /* ¦* release the node ¦*/ __this_cpu_dec(qnodes[0].mcs.count); } 如果tail 就是 当前的mcs, 说明，没有其他人抢了，这时将 状态变为 (0, 0, 1) atomic_try_cmpxchg_relaxed() 如果lock-&gt;val == val , val保持不变，返回值为true 如果Lock-&gt;val != val, val=lock-&gt;val, 返回值为false 所以, 如果返回为真，说明为(this_node, 0, 0)则 直接替换成(0, 0, 1) 即可, 并且释放该mcs 如果next没有值，并且这时lock-&gt;tail != this_node, 说明 有人抢锁，并执行完old = xchg_tail(lock, tail); 这行代码， 但是还没有执行WRITE_ONCE(prev-&gt;next, node);, 这行代码， 这时，需要等待 node-&gt;next 被赋值。 这时，可以抢占锁了。 commit c61da58d8a9ba9238250a548f00826eaf44af0f7 将next-&gt;locked 赋值为1 #define arch_mcs_spin_unlock_contended(l) \\ smp_store_release((l), 1) 附录 附录1 : cacheline bouncing 参考链接 深入理解Linux内核之自旋锁 What is cache line bouncing? How may a spinlock trigger this frequently? 從 CPU cache coherence 談 Linux spinlock 可擴展能力議題 " } ]
