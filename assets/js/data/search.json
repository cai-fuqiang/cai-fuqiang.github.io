[ { "title": "Ebpf", "url": "/posts/ebpf/", "categories": "", "tags": "", "date": "2025-05-21 00:00:00 +0800", "content": " " }, { "title": "Bpf Verify", "url": "/posts/bpf-verify/", "categories": "", "tags": "", "date": "2025-05-21 00:00:00 +0800", "content": " " }, { "title": "Bpf Jit", "url": "/posts/bpf-jit/", "categories": "", "tags": "", "date": "2025-05-21 00:00:00 +0800", "content": " " }, { "title": "Bpf Isa", "url": "/posts/bpf-ISA/", "categories": "", "tags": "", "date": "2025-05-21 00:00:00 +0800", "content": " " }, { "title": "Bpf Overflow", "url": "/posts/bpf-overflow/", "categories": "", "tags": "", "date": "2025-05-20 00:00:00 +0800", "content": "BPF 起源 BPF 起源于1992, Steven McCanne 和 Van Jacobso 发布了论文: &lt;&lt;The BSD Packet Filter: A New Architecture for User-level Packet Capture&gt;&gt;1, 该论文 主要提供了一种包过滤的技术，比其当时的包过滤技术快20倍.1 abstract. 该包过滤技术和原有的包过滤技术最大的不同是: 原有的包过滤技术, 过滤步骤发生在用户态, 也就是说，如果要对包进行过滤，需要 将 所有 包传给用户态，然后，在用户态使 用规则进行过滤。 而 bpf 则将过滤这个步骤放到了内核态, 在内核态过滤后，将过滤之后的数据，传递给用 户态。避免了不必要的数据copy. 由上图所示, 假如我们在用户态需要获取port=3000的所有tcp报文, 在传统模式下， 即便是port != 3000的报文也需要copy到用户态。但是在bpf模式下，bpf会在kernel侧 进行过滤动作，让port = 3000的报文筛选出来，然后copy到用户态, 这样就避免了 一些不必要的copy。 bpf模式下，包过滤的整体过程如下: From 1 网卡收到包后，先将数据包额外copy一份, copy的副本主要用于包过滤+传递到用户态. 而原来的包则走协议栈。该数据包会先交给BPF的程序进行处理, 而该BPF程序的作用就是 根据用户的规则过滤报文，然后如果匹配成功，再将该报文copy到用户空间。 那这里就暴露了一个问题, BPF 是如何根据用户的规则来过滤呢 ? 我们可以想到的是， 通过ioctl()等系统调用，传递过滤规则，但是这样太不灵活了。而BPF则是通过插庄 代码的方式来实现。这样就相当于在内核态中编程过滤。大大提升了灵活性。但是 问题又来了, 让用户态可以在内核态编程，这不就赋予了用户态无限的权力？可以 access 系统中的任意资源 ? BPF 虚拟机 相关链接 The BSD Packet Filter: A New Architecture for User-level Packet Capture eBPF (作者分析了bpf的起源) ebpf.io eBPF 的发展历程及工作原理 eBPF 运行原理和流程 – 极客时间 eBPF 核心技术与实战 的学习笔记 其他 包含论文 " }, { "title": "Cpuidle", "url": "/posts/cpuidle/", "categories": "", "tags": "", "date": "2025-04-28 00:00:00 +0800", "content": " introduce struct cpuidle_device cpuidle_driver cpuidle_state cpuidle_governor struct cpuidle_device struct cpuidle_device { unsigned int registered:1; unsigned int enabled:1; //表示上次poll idle时，达到了poll的时间上限 unsigned int poll_time_limit:1; unsigned int cpu; ktime_t next_hrtimer; //上次idle的最后一个idle state 的index int last_state_idx; //上次idle的最后一个idle state 的 residency_ns u64 last_residency_ns; // poll idle 的时间上限，如果达到了该上限，则退出poll idle // 状态 u64 poll_limit_ns; u64 forced_idle_latency_limit_ns; //主要用来统计 struct cpuidle_state_usage states_usage[CPUIDLE_STATE_MAX]; // /sys/devices/system/cpu/cpuxxx/cpuidle struct cpuidle_state_kobj *kobjs[CPUIDLE_STATE_MAX]; struct cpuidle_driver_kobj *kobj_driver; struct cpuidle_device_kobj *kobj_dev; struct list_head device_list; #ifdef CONFIG_ARCH_NEEDS_CPU_IDLE_COUPLED cpumask_t coupled_cpus; struct cpuidle_coupled *coupled; #endif }; cpuidle_driver bctimer: cpuidle_state: state_count:: cpuidle_state 数量 safe_state_index: cpumask: 表示该driver在哪些cpu上正在运行 governor: 注册是首选的 governor cpuidle_state exit_latency : 从该idle状态退出所需的延迟(us单位) exit_latency_ns : 同上, ns 单位 target_residency : 处理器进入到该idle状态后, 应停留的最短时间 target_residency_ns : 同上, ns 单位 power_usage : 在这个状态下的功耗 enter : 进入该状态的方法 enter_dead : enter_s2idle : cpuidle_governor struct cpuidle_governor { char name[CPUIDLE_NAME_LEN]; struct list_head governor_list; /* * 该值用来表示 governor 的使用优先级，rating 大的优先选择; * 但是如果用户通过module param: governor指定的类型优先级最高 */ unsigned int rating; int (*enable) (struct cpuidle_driver *drv, struct cpuidle_device *dev); void (*disable) (struct cpuidle_driver *drv, struct cpuidle_device *dev); int (*select) (struct cpuidle_driver *drv, struct cpuidle_device *dev, bool *stop_tick); void (*reflect) (struct cpuidle_device *dev, int index); }; select: 用来选择进入哪个 idle 状态 reflect: 反馈调节。在该idle state结束时调用, 相当于根据idle的结果来反馈 调节部分参数. 大体流程 register governor device driver state poll_idle_init void cpuidle_poll_state_init(struct cpuidle_driver *drv) { struct cpuidle_state *state = &amp;drv-&gt;states[0]; snprintf(state-&gt;name, CPUIDLE_NAME_LEN, \"POLL\"); snprintf(state-&gt;desc, CPUIDLE_DESC_LEN, \"CPUIDLE CORE POLL IDLE\"); state-&gt;exit_latency = 0; state-&gt;target_residency = 0; state-&gt;exit_latency_ns = 0; state-&gt;target_residency_ns = 0; state-&gt;power_usage = -1; state-&gt;enter = poll_idle; state-&gt;flags = CPUIDLE_FLAG_POLLING; } 参考链接 Linux 服务器功耗与性能管理（三）：cpuidle 子系统的实现（2024） Linux Cpuidle介绍 Advances in CPU Idle Time Management Advances in CPU Idle Time Management - YOUTUBE cpuidle cluster ‘Green’ Code Development " }, { "title": "qemu coroutine", "url": "/posts/coroutine/", "categories": "qemu, coroutine", "tags": "qemu, qemu_coroutine, completed", "date": "2025-02-25 11:00:00 +0800", "content": " Introduction Linux User Context Switch qemu coroutine 协程状态机 CREATE and INIT enter switch yield Use Case for QEMU Introduction 多线程和协程都可以用于并行编程，但是他们实现方式和使用场景 有很大的区别，我们来对比下: 对比项 协程 多线程 实现方式 在用户态单线程中，完成上下文切换 内核态完成上下文切换 开销 开销较低 线程创建销毁，以及切换都需要进入内核态，开销较高 并发 只能在单个线程中来回切换完成并发 可以实现真正的并行处理（在多核cpu) 调度(切换) 协程类似于非抢占式调度，只能在主动切换 线程可以在任何时刻被中断和切换 程序复杂度 协程处理同步和资源共享较简单 多线程编程需要处理线程间的同步和资 源共享问题, 复杂度更高, 往往需要借助系统api(锁，信号量) 使用场景上: 协程 当应用程序主要是 I/O 密集型任务，如网络请求、文件操作等 当需要高并发但不需要并行计算 使用多线程的场景 当应用程序是 CPU 密集型任务，需要利用多核 CPU 的并行计算能力 当需要处理大量需要同时执行的计算任务时 协程比较适合那种需要wait的任务, 例如上面提到的I/O 密集型任务(qemu中的 aio, 可以在协程中下发多个aio，然后等待io complete event) 我们举个例子: 在该图中, 有两个cpu core, A进程有3个thread, 其中thread1和thread2在 cpu0上运行，thread 3 有四个协程，在cpu1上运行, task A 的计算负载可 以分别落在cpu 0 和cpu 1上, 这也是多线程的很大的优点: 可以最大化的利 用多核cpu的并行处理能力。 thread1和thread2其靠kernel的任务抢占机制，来共享cpu 0, 在任何时间都有 可能被对方抢占. 而thread3 中的各个协程则是 根据自己任务的完成情况， 或者当前任务是否需要等待而主动选择调度。 Linux User Context Switch 我们需要思考下，context switch 完成哪些任务: init new task context like pthread_create() need init IP, SP(a new stack), Params context switch save… load… destroy 如果用户态要完成context switch，需要处理好上面所列的三件事。而这些事情涉及的东西 太底层了，如设置ip，如传参等等，所以libc中提供了ucontext系列接口来完成这些事情: ucontext ucontext API API name 作用 getcontext(ucontext_t *ucp) 获取当前上下文, 保存到ucp中 setcontext(ucp) 切换到目标(ucp)上下文 makecontext(ucp, (*func)(), int argc, …) 用来modify ucp, 下面详述 swapcontext(oucp, ucp) saves current thread context in oucp and makes *ucp the currently active context. 在执行makecontext()之前，需要做一些准备工作: 调用 getcontext() 来init ucp, 需要为其分配stack, init ucontext_t.uc_stack 相关成员 ss_sp: 指向具体的堆栈地址 ss_size: 堆栈大小 ss_flags: 设置ucp-&gt;uc_link参数，根据是否设置ucp-&gt;uc_link 来确定func() 返回时， 所执行的动作: NULL: 进程退出 隐式调用 setcontext(ucp-&gt;uc_link) 我们编写一个例子来演示下，该接口的使用方法和效果 ucontext example 测试程序展开 #include &lt;stdio.h&gt; #include &lt;ucontext.h&gt; #include &lt;stdlib.h&gt; #define STACK_SIZE (4096 * 2) void print_current_stack() { unsigned long stack_pointer; __asm__(\"movq %%rsp, %0\" : \"=r\"(stack_pointer)); printf(\"stack pointer(%lx)\\n\", stack_pointer); } void func(int a, int b) { printf(\"the co exec, sum(%d)\\n\", a+b); printf(\"print co stack \\n\"); print_current_stack(); return; } int main() { int ret; char *stack = (char *)malloc(STACK_SIZE); ucontext_t uc, old_uc; int a, b = 0; printf(\"the new stack is %p\\n\", stack); printf(\"print main stack:\\n\"); print_current_stack(); getcontext(&amp;uc); uc.uc_stack.ss_sp = stack; uc.uc_stack.ss_size = STACK_SIZE; uc.uc_link = &amp;old_uc; while(1) { printf(\"main co a(%d) b(%d)\\n\", a, b); makecontext(&amp;uc, (void (*)(void))func, 2, a, b); printf(\"swap context\\n\"); swapcontext(&amp;old_uc, &amp;uc); printf(\"swap context end\\n\"); if (a++ == 3) break; b=b+2; } return 0; } 在main中jum构建一个循环，来在另一个上下文中调用func(), 并设置 返回的context为调用者(main())的context，这样func()返回后， 直接返回到main()的while的上下文, 继续执行循环。 输出示例 输出如下: the new stack is 0x9d22a0 print main stack: stack pointer(7fff0939ee50) main co a(0) b(0) swap context the co exec, sum(0) print co stack stack pointer(9d4250) swap context end main co a(1) b(2) swap context the co exec, sum(3) print co stack stack pointer(9d4250) swap context end main co a(2) b(4) swap context the co exec, sum(6) print co stack stack pointer(9d4250) swap context end main co a(3) b(6) swap context the co exec, sum(9) print co stack stack pointer(9d4250) swap context end 由上图可见，func()和main()运行在两个上下文，并且两个上下文切换示意图 如下: 另外，linux中还支持另外一组上下文切换的API – sigsetjmp, siglongjmp sigsetjmp, siglongjmp 该系列函数一般用于实现C语言中的异常处理，如在信号处理流程中，跳转到 其他的执行流程. 避免再次执行到异常代码. 我们先来看下其API sigsetjmp(sigjmp_buf env, int savemask) 功能: 保存当前的上下文和信号掩码，以便以后可以通过 siglongjmp 恢复 参数: env: 保存上下文信息 savemask: 如果非0， 当前的信号掩码也会被保存 返回值: 调用者返回0 如果通过siglongjmp恢复，而返回siglongjmp 传递的值 siglongjmp(sigjmp_buf env, int val) 功能: 恢复由 sigsetjmp 保存的上下文信息和信号掩码，并从 sigsetjmp 返回。 参数: env: 由sigsetjmp保存的环境信息 val: sigsetjmp 0: return 1 x(x != 0) : return x 没有返回值(因为已经跳走了) 看起来sigxxxjmp也可以实现上下文切换，但是该系列接口有个很大的问题，比较适合 recover, 但不适合new。其不像ucontext接口, 可以通过makecontext()接口先new一个context, sigsetjmp 只能保存当前的现场, 所以相当于只能先走到要切换的流程中埋好点，然后才能切换，很不方便. 但是sigxxxjmp()对比makecontext()也有好处. 其更加轻量化. 它不会涉及完整的上下文切换, 例如其可以设置不切换信号掩码，减少因系统调用而产生的切换损耗. 而qemu中的协程实现主要有三种 ucontext + sigjmp: util/coroutine-ucontext.c sigaltstack: util/coroutine-sigaltstack.c coroutine-win32 本文主要介绍第一种，由ucontext和sigjmp结合实现。其中，ucontext系列接口负责 new context, 为sigjmp接口埋点, 而sigjmp 系列接口负责协程切换. 接下来，我们来看下qemu实现: qemu coroutine 协程状态机 这是一个典型的由 leader 创建协程的状态机，进入协程上下文会做两种事: 埋sigxxxjmp跳转点, 为之后再次切换进协程做准备 work… 协程运行期间，可能因为wait io等事件选择先切出协程(COROUTINE_YIELD), 此时协程是suspend状态。 等待协程处理完完整的事物后，会切出协程上下文，并置为terminal 状态. 另外除了首次进入协程是使用ucontext接口, 剩余的协程/leader之间的切换， 均使用sigxxxjmp系列接口，这样可以尽量减少因切换上下文带来性能损耗。 整体流程 整个流程如下图: CREATE and INIT create流程主要是为协程准备好上下文环境, init 流程主要是在协程中 打好跳转点, 流程包括: 为协程分配堆栈空间 使用makecontext(), swapcontext() 执行到一个新的上下文 在协程上下文中，埋 sig jmp的点 跳转回leader 上下文 INIT流程只是为协程搭建了一个上下文，但是该上下文接下来要执行什么任务， 需要leader指明，所以在切回leader上下文后，leader还需要为协程准备协程要 执行的函数，以及函数参数(红底蓝字部分) enter 在create &amp;&amp; INIT 章节中，我们介绍到首次进入协程是通过swapcontext()接口, 而之后再次进入协程，就需要使用sigxxxjmp系列接口，本章节主要介绍第二种。 而enter这个动作既有可能发生在leader上下文，也有可能发生在协程上下文, 所以我们以下面的场景为例子，看下qemu是怎么处理的。 leader enter 协程A 协程A enter 协程B 假设协程A, 协程B 在处理过程中不会yield, 直接terminal. 整个流程如下图: 这样处理，会导致协程只能串行，不能嵌套执行。 我们来想下为什么要这样做, 首先我们来看下，两者上下文切换次数: 串行执行 leader-&gt;A-&gt;leader-&gt;B-&gt;leader 切换4次 嵌套执行 leader-&gt;A-&gt;B-&gt;A-&gt;leader 切换4次 两者切换次数相同。 所以这里的原因(猜测)很可能是，防止协程可能带来的 同步问题（避免A上下文中嵌入B的上下文从而带来死锁) switch 接下来，我们再来看下switch过程。switch过程比较简单。主要的函数是, qemu_coroutine_switch(), 函数原型: CoroutineAction qemu_coroutine_switch(Coroutine *from_, Coroutine *to_, CoroutineAction action); 参数有三个: from: 切出的协程 to: 切入的协程 action: 本次操作的类型 COROUTINE_YIELD: 暂停from协程 COROUTINE_TERMINATE: 终止from协程 COROUTINE_ENTER: 进入to协程 我们以一个没有执行过yield协程生命周期来看下switch的细节: 可以看到在执行qemu_coroutine_switch()时，action参数会作为 siglongjmp(, action)传入，这样在另一个上下文中，会通过 sigsetjmp()的返回值，获取到action, 而qemu_aio_coroutine_enter() 会根据协程返回状态，来选择一些action: COROUTINE_TERMINATE: 销毁协程 COROUTINE_YIELD: 忽略，继续执行leader流程 这里我们来总结下，不同的switch过程: leader-&gt;co ENTER: co-&gt;leader TERMINATE YIELD yield yield是一个比较特殊的存在，因为yield动作时，还需要保存协程的现场， 以便之后，再次切回协程。并且在协程yield切回leader后，leader会继续 运行执行其他流程。等待该协程的等待的事件到来后，需要再次执行enter 切换回该协程，如下图所示: Use Case for QEMU 附录 virtio-blk触发堆栈 virtio_blk_handle_vq ## 从avail ring中获取req =&gt; blk_io_plug() =&gt; while (virtio_blk_get_request()) =&gt; virtio_blk_submit_multireq() =&gt; foreach request: ## 可能会merge submit =&gt; submit_requests() =&gt; init qemu iovc =&gt; blk_aio_pwritev/blk_aio_preadv =&gt; blk_io_unplug() blk_aio_pwritev =&gt; blk_aio_prwv(,,,,co_entry::blk_aio_write_entry, flags, cb:: virtio_blk_rw_complete,opaque) =&gt; init acb::BlkAioEmAIOCB =&gt; qemu_coroutine_create(co_entry, acb) =&gt; bdrv_coroutine_enter(blk_bs(blk), co) 参考资料 huangyong – 深入理解qemu协程 _银叶先生 – 协程的原理与实现：qemu 之 Coroutine " } ]
